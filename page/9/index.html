<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Yuan - 记录学习中的点点滴滴</title><meta name="author" content="Qiyuan-Z"><meta name="copyright" content="Qiyuan-Z"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="偉大な魂は目的を持ち、そうでないものは願望を持つ"><meta property="og:type" content="website"><meta property="og:title" content="Yuan"><meta property="og:url" content="https://qiyuan-z.github.io/page/9/index.html"><meta property="og:site_name" content="Yuan"><meta property="og:description" content="偉大な魂は目的を持ち、そうでないものは願望を持つ"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://qiyuan-z.github.io/img/avatar.jpg"><meta property="article:author" content="Qiyuan-Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://qiyuan-z.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qiyuan-z.github.io/page/9/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:200,languages:{author:"作者: Qiyuan-Z",link:"链接: ",source:"来源: Yuan",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"bottom-right"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!1,isHome:!0,isHighlightShrink:!1,isToc:!1,postUpdate:"2022-01-12 15:05:00"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));const n=saveToLocal.get("global-font-size");void 0!==n&&document.documentElement.style.setProperty("--global-font-size",n+"px")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://unpkg.com/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">121</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yuan</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2020/02/19/%E6%9C%80%E7%AE%80%E5%8D%95%E6%9C%80%E6%98%93%E5%AE%9E%E7%8E%B0%E7%9A%84SE%E6%A8%A1%E5%9D%97/" title="最简单最易实现的SE模块"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="最简单最易实现的SE模块"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/02/19/%E6%9C%80%E7%AE%80%E5%8D%95%E6%9C%80%E6%98%93%E5%AE%9E%E7%8E%B0%E7%9A%84SE%E6%A8%A1%E5%9D%97/" title="最简单最易实现的SE模块">最简单最易实现的SE模块</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-02-19T02:51:28.774Z" title="发表于 2020-02-19 10:51:28">2020-02-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">Squeeze-and-Excitation NetworksSENet是Squeeze-and-Excitation Networks的简称，拿到了ImageNet2017分类比赛冠军，其效果得到了认可，其提出的SE模块思想简单，易于实现，并且很容易可以加载到现有的网络模型框架中。SENet主要是学习了channel之间的相关性，筛选出了针对通道的注意力，稍微增加了一点计算量，但是效果比较好。 通过上图可以理解他的实现过程，通过对卷积的到的feature map进行处理，得到一个和通道数一样的一维向量作为每个通道的评价分数，然后将修改的分数分别施加到对应的通道上，得到其结果，就在原有的基础上只添加了一个模块，下边我们用pytorch实现这个很简单的模块。 12345678910111213141516class SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.Ada ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2020/02/19/ECCV-2018-Convolutional-Block-Attention-Module/" title="ECCV 2018 Convolutional Block Attention Module"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="ECCV 2018 Convolutional Block Attention Module"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/02/19/ECCV-2018-Convolutional-Block-Attention-Module/" title="ECCV 2018 Convolutional Block Attention Module">ECCV 2018 Convolutional Block Attention Module</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-02-19T03:10:17.114Z" title="发表于 2020-02-19 11:10:17">2020-02-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言目前cv领域借鉴了nlp领域的attention机制以后生产出了很多有用的基于attention机制的论文，attention机制也是在2019年论文中非常火。这篇cbam虽然是在2018年提出的，但是其影响力比较深远，在很多领域都用到了该模块，所以一起来看一下这个模块有什么独到之处，并学着实现它。 什么是注意力机制？注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。 通俗来讲：注意力机制就是希望网络能够自动学出来图片或者文字序列中的需要注意的地方。比如人眼在看一幅画的时候，不会将注意力平等地分配给画中的所有像素，而是将更多注意力分配给人们关注的地方。 从实现的角度来讲：注意力机制通过神经网络的操作生成一个掩码mask,，mask上的值一个打分，评价当前需要关注的点的评分。 注意力机制可以分为： 通道注意力机制：对通道生成掩码mask，进行打分，代表是SENet, Channel Attention Module 空间注意力机制：对空间进行掩码的生成，进行打分，代表是Spa ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2020/03/07/ECCV-2018-RFBNet%EF%BC%8C%E5%9C%A8%E6%A3%80%E6%B5%8B%E4%B8%AD%E8%B0%83%E6%84%9F%E5%8F%97%E9%87%8E/" title="ECCV 2018 RFBNet，在检测中调感受野"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="ECCV 2018 RFBNet，在检测中调感受野"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/03/07/ECCV-2018-RFBNet%EF%BC%8C%E5%9C%A8%E6%A3%80%E6%B5%8B%E4%B8%AD%E8%B0%83%E6%84%9F%E5%8F%97%E9%87%8E/" title="ECCV 2018 RFBNet，在检测中调感受野">ECCV 2018 RFBNet，在检测中调感受野</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-03-07T03:13:19.856Z" title="发表于 2020-03-07 11:13:19">2020-03-07</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言看了不少的目标检测论文了，个人认为多数论文的出发点就两个，一是感受野，二是特征融合。此外，解决数据不平衡和轻量化也是另外两个重要的方向。今天为大家科普一篇ECCV 2018的一篇目标检测网络RFBNet就是从感受野角度来改善了SSD检测器，论文全名为：Receptive Field Block Net for Accurate and Fast Object Detection 。这篇论文主要的贡献点主要是在SSD网络中提出了一个Receptive Field Block (RFB) 模块，RFB模块主要是在Inception的基础上加入了空洞卷积层从而有效的增大了感受野。另外，RFB模块是嵌在SSD上的，所以检测的速度比较快，精度比SSD更高。 RFB模块RFB模块的效果示意图如图所示，其中虚线部分就是指RFB模块。 RFB模块主要有两个特点： RFB模块有多个分支，每个分支的第一层都由特定大小卷积核的卷积核构成，例如图上的1 x 1， 3 x 3，5 x 5。 RFB模块引入了空洞卷积，主要作用是为了增加感受野，空洞卷积之前是应用在分割网络DeepLab中，这里将其应用 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2020/03/06/RetinaNet%EF%BC%88%E5%BC%95%E5%85%A5Focal-Loss%EF%BC%89/" title="RetinaNet（引入Focal Loss）"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="RetinaNet（引入Focal Loss）"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/03/06/RetinaNet%EF%BC%88%E5%BC%95%E5%85%A5Focal-Loss%EF%BC%89/" title="RetinaNet（引入Focal Loss）">RetinaNet（引入Focal Loss）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-03-06T02:46:14.942Z" title="发表于 2020-03-06 10:46:14">2020-03-06</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言今天来介绍一下目标检测算法中RetinaNet，这篇论文是CVPR2018的作品，Kaiming He大神也是作者之一，同时这篇论文提出的Focal Loss也对工程上训练更好的目标检测模型做出了很大贡献，论文地址为：https://arxiv.org/pdf/1708.02002.pdf 研究背景前面介绍了一些One-Stage目标检测算法和Two-Stage目标检测算法，这些算法在精度和速度上都各有特点，现在画个图总结一下之前介绍的各种算法的速度和精度： 可以看到One-Stage算法的精度相对于Two_Stage偏低，然后作者把这种问题的原因归结于正负类别不平衡（简单难分类别不平衡）。因此论文通过重新设计标准的交叉熵损失来解决这种难易样本不平衡的问题，即文章的核心Focal Loss。结合了Focal Loss的One-Stage的目标检测器被称为RetinaNet，该检测器在COCO数据集上MAP值可以和FPN（特征金字塔目标检测器）和MaskRCNN接近。 一些问题什么是hard/esay postive/negtive example网上找到一张图解释在目标检测任 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2020/04/07/CVPR-2018-Cascade-R-CNN/" title="CVPR 2018 Cascade R-CNN"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="CVPR 2018 Cascade R-CNN"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/04/07/CVPR-2018-Cascade-R-CNN/" title="CVPR 2018 Cascade R-CNN">CVPR 2018 Cascade R-CNN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-04-07T03:35:25.694Z" title="发表于 2020-04-07 11:35:25">2020-04-07</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言Cascade R-CNN这个算法是CVPR 2018提出的，通过级联多个检测网络达到不断优化预测结果的目的。但是和普通的级联检测器不同，Cascade R-CNN的多个检测网络是基于不同的IOU阈值进而确定不同的正负样本训练出来的，在COCO数据集上Cascade R-CNN取得了非常出色的结果，并且也成为了当前目标检测比赛中的有力Trick。 简单回顾R-CNN结构 首先，以经典的Faster R-CNN为例。整个网络可以分为两个阶段，training阶段和inference阶段，如上图所示。 training阶段，RPN网络提出了2000左右的proposals，这些proposals被送入到Fast R-CNN结构中，在Fast R-CNN结构中，首先计算每个proposal和gt之间的iou，通过人为的设定一个IoU阈值（通常为0.5），把这些Proposals分为正样本（前景）和负样本（背景），并对这些正负样本采样，使得他们之间的比例尽量满足（1:3，二者总数量通常为128），之后这些proposals（128个）被送入到Roi Pooling，最后进行类别分类和 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2020/04/10/CVPR-2017-ResNeXt%EF%BC%88ResNet%E8%BF%9B%E5%8C%96%E7%89%88%EF%BC%89/" title="CVPR 2017 ResNeXt（ResNet进化版）"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="CVPR 2017 ResNeXt（ResNet进化版）"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/04/10/CVPR-2017-ResNeXt%EF%BC%88ResNet%E8%BF%9B%E5%8C%96%E7%89%88%EF%BC%89/" title="CVPR 2017 ResNeXt（ResNet进化版）">CVPR 2017 ResNeXt（ResNet进化版）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-04-10T02:55:56.187Z" title="发表于 2020-04-10 10:55:56">2020-04-10</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言传统的卷积神经网络在提高性能时都是加深和加宽网络，但随着超参数数量的增加(如通道数，卷积核大小等)网络变得非常难调，且网络的计算开销和网络结构设计也变得越来越难。此外这些大模型针对性比较强，即在特定数据集上表现好的网络放到新数据集上就需要修改很多的参数才能工作良好，因此可扩展性比较一般。针对上述问题，Saining Xie, Ross Girshick, Kaiming He在CVPR2017上提出了ResNeXt。 贡献 网络结构更加简单和模块化。 大量减少了需要手动调节的超参数，扩展性更强。 和ResNet相比，相同的参数个数，结果更好。具体来说，一个101层的ResNeXt网络和200层的ResNet准确度差不多，但是计算量只有后者的一半。 方法网络结构ResNeXt的网络结构如Table1所示： Table 1的左边网络为ResNet-50，Table 1的右边网络为ResNeXt-50，括号代表残差块，括号外面的数字代表残差块的堆叠次数，而C代表的ResNeXt引入的卷积分组数，同时可以看到这两个网络的FLOPs基本一致，也即是说模型复杂度一致。那ResNeXt有什么 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2020/04/10/%E4%B8%8D%E9%9C%80%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95DSOD/" title="不需要预训练模型的目标检测算法DSOD"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="不需要预训练模型的目标检测算法DSOD"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/04/10/%E4%B8%8D%E9%9C%80%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95DSOD/" title="不需要预训练模型的目标检测算法DSOD">不需要预训练模型的目标检测算法DSOD</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-04-10T02:00:53.366Z" title="发表于 2020-04-10 10:00:53">2020-04-10</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言DSOD(Deeply Supervised Object Detectors)是ICCV 2017的一篇文章，它表达了一个非常有意思的东西。这篇论文不是从目标检测的高mAP值或者速度更快出发，而是从另外一个角度切入来说明fine-tune后的检测模型和直接训练的检测模型的差距其实是可以减少的，也即是说一些检测模型可以摆脱fine-tune这一过程，并且相比于fine-tune训练出来的模型效果并不会变差。 介绍DSOD这一算法是在SSD的基础上进行的改进，可以简单的看成： DSOD=SSD+DenseNet 作者在论文中提到他也实验了从0开始训练Region Proposal Based的检测算法比如Faster RCNN，R-FCN等，但这些模型很难收敛。而One-Stage的目标检测算法比如SSD却可以收敛，虽然效果很一般，因此最后作者使用SSD作为了这篇论文接下来讨论的BaseLine。 然后本文基于SSD改进的DSOD在VOC2007 trainval和2012 trainval数据集上训练模型，然后在VOC2007 testset上测试的表现(77.7%mAP)超过 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2020/03/02/FPN/" title="FPN"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="FPN"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/03/02/FPN/" title="FPN">FPN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-03-02T03:16:50.715Z" title="发表于 2020-03-02 11:16:50">2020-03-02</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">背景Faster-RCNN选取一个特征提取网络如VGG16做backbone，然后在高层特征（如VGG16后面的conv4）接RPN和检测头进行网络。正是由于Faster-RCNN基于图像的高级特征，这就导致对小目标的检测效果很差。而CV领域常用的处理尺度问题的办法就是特征金字塔，将原图以不同的比例采样，然后得到不同分辨率的图像进行训练和测试，在多数情况下确实是有效的。但是特征金字塔的时间开销非常大，导致在工程中应用是及其困难。FPN从新的角度出发提出了一个独特的特征金字塔网络来避免图像金字塔产生的超高计算量，同时可以较好的处理目标检测中的尺度变化问题，对小目标检测更鲁棒，同时在VOC和COCO数据集上MAP值均超过了Faster-RCNN。 简介使用下图来阐释是如何处理尺度变化大的物体检测的。 上图(a)是处理这类问题最常用的方法，即特征金字塔，这种方法在传统的手动设计特征的方法中非常常用，例如DPM方法使用了接近10种不同的尺度获得了不错的效果。 上图(b)是在CNN提出之后出现的，因为神经网络模型对物体尺度本身有一定的鲁棒性，所以也取得了不错的性能，但最近的研究表明将特征金字 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2020/04/20/Feature-Pyramid-Network/" title="Feature Pyramid Network"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Feature Pyramid Network"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/04/20/Feature-Pyramid-Network/" title="Feature Pyramid Network">Feature Pyramid Network</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-04-20T04:04:24.896Z" title="发表于 2020-04-20 12:04:24">2020-04-20</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言FPN全称是Feature Pyramid Network, 也就是特征金字塔网络，主要是针对图像中目标的多尺度的这个特点提出的，多尺度在目标检测中非常常见，而且对应不同的问题应该设计不同的FPN。FPN是Facebook于2017年提出的用于目标检测的模块化结构，但FPN在很多计算机视觉任务中都有使用，比如姿态估计、语义分割等领域。 FPN 在深度学习兴起以前，很多传统方法都会使用到图像金字塔。图像金字塔如上图所示，就是将图片resize到不同的大小，然后分别得到对应大小的特征，然后进行预测。这种方法虽然可以一定程度上解决多尺度的问题，但是很明显，带来的计算量也非常大。 上图是使用单个feature map进行检测，这种结构在17年的时候是很多人在使用的结构，比如YOLOv1、YOLOv2、Faster R-CNN中使用的就是这种架构。直接使用这种架构导致预测层的特征尺度比较单一，对小目标检测效果比较差。ps: YOLOv2中使用了multi-scale training的方式一定程度上缓解了尺度单一的问题，能够让模型适应更多输入尺度。 上图进行了在不同大小的featu ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2020/04/19/SSD%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E8%BF%9B%E7%89%88Rainbow-SSD/" title="SSD算法的改进版Rainbow SSD"><img class="post_bg" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="SSD算法的改进版Rainbow SSD"></a></div><div class="recent-post-info"><a class="article-title" href="/2020/04/19/SSD%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E8%BF%9B%E7%89%88Rainbow-SSD/" title="SSD算法的改进版Rainbow SSD">SSD算法的改进版Rainbow SSD</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-04-19T06:00:38.635Z" title="发表于 2020-04-19 14:00:38">2020-04-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="content">前言这是BMVC 2017的一个SSD的改进算法R-SSD。这里先看一下SSD的网络结构图吧。 带有特征图维度信息的更清晰的骨干网络和VGG16的对比图如下： 出发点一般来说，深度神经网络的特征图数量越多，我们获得的性能一般会更好。但是这并不一定代表着简单的增加特征图的数量就能使得效果变好，这一点在实验部分有说明。这篇论文在SSD的基础上并没有改变BackBone网络，即还是应用稍加修改的VGG16为BackBone。这篇论文的贡献是提出了新的特征融合方式来提升了SSD的效果，这一改进使得SSD可以充分利用特征，虽然速度稍慢于原始的SSD算法，但mAP却获得了较大的提升。 介绍传统的SSD算法通过不同层的特征来做检测，使得其对尺度变化有较好的鲁棒性，在速度和精度的Trade-Off上也做得比较好，但是SSD有2个明显的问题： 在SSD中，不同层的特征图都是独立作为分类网络的输入，因此容易出现相同物体被不同大小的框同时检测出来的情况。 对小目标的检测效果比较差，当然这也是大多数目标检测算法的通病了。 因此，这篇算法也主要从这两点出发来改进传统的SSD算法。首先，本文利用分类网络增 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/8/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/10/"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/avatar.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"><div class="author-info__name">Qiyuan-Z</div><div class="author-info__description">偉大な魂は目的を持ち、そうでないものは願望を持つ</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">121</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Qiyuan-Z" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zhengqiyuan@stu.jiangnan.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-pixiv"><div class="card-content"><div class="item-headline"><i class="fa fa-image" aria-hidden="true"></i><span>Pixiv日榜Top50</span><iframe src="https://fun.hujingnb.com/pixiv/i" frameborder="0" style="width:99%;height:380px;margin:0"></iframe></div></div></div><div class="sticky_layout"><div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height:100px;overflow:hidden"><div class="history_swiper-container" id="history-container" style="width:100%;height:100%"><div class="swiper-wrapper" id="history_container_wrapper" style="height:20px"></div></div></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/12/BitComet%E9%85%8D%E7%BD%AE/" title="BitComet配置">BitComet配置</a><time datetime="2022-01-12T06:56:56.861Z" title="发表于 2022-01-12 14:56:56">2022-01-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" title="如何使用PyTorch分布式训练">如何使用PyTorch分布式训练</a><time datetime="2022-01-11T02:01:07.163Z" title="发表于 2022-01-11 10:01:07">2022-01-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/10/Grad-CAM%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/" title="Grad-CAM原理和实现">Grad-CAM原理和实现</a><time datetime="2022-01-10T01:58:38.768Z" title="发表于 2022-01-10 09:58:38">2022-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/07/TensorRT%E5%AE%89%E8%A3%85/" title="TensorRT安装">TensorRT安装</a><time datetime="2022-01-07T05:08:13.073Z" title="发表于 2022-01-07 13:08:13">2022-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/06/%E8%A7%A3%E5%86%B3nvidia-smi%E5%92%8Cnvcc%E6%98%BE%E7%A4%BA%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%89%80%E5%AE%89%E8%A3%85CUDA%E7%89%88%E6%9C%AC%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/" title="解决nvidia-smi和nvcc显示信息与所安装CUDA版本不一致问题">解决nvidia-smi和nvcc显示信息与所安装CUDA版本不一致问题</a><time datetime="2022-01-06T02:09:53.081Z" title="发表于 2022-01-06 10:09:53">2022-01-06</time></div></div></div></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size:1.5em;color:#2c5bc8">目标检测</a><a href="/tags/YOLOv3/" style="font-size:1.4em;color:#36bf0f">YOLOv3</a><a href="/tags/BitComet/" style="font-size:1.1em;color:#2c9848">BitComet</a><a href="/tags/Ubuntu/" style="font-size:1.45em;color:#0b754a">Ubuntu</a><a href="/tags/Attention%E6%9C%BA%E5%88%B6/" style="font-size:1.15em;color:#7f0725">Attention机制</a><a href="/tags/Fast-RCNN/" style="font-size:1.1em;color:#be9fb4">Fast RCNN</a><a href="/tags/Pytorch/" style="font-size:1.3em;color:#c803aa">Pytorch</a><a href="/tags/Latex/" style="font-size:1.15em;color:#c76475">Latex</a><a href="/tags/potplayer/" style="font-size:1.1em;color:#147172">potplayer</a><a href="/tags/Python/" style="font-size:1.35em;color:#7e173d">Python</a><a href="/tags/Opencv/" style="font-size:1.15em;color:#c026bc">Opencv</a><a href="/tags/PyQt5/" style="font-size:1.1em;color:#95263c">PyQt5</a><a href="/tags/Pycharm/" style="font-size:1.15em;color:#483943">Pycharm</a><a href="/tags/SSH/" style="font-size:1.15em;color:#2bc011">SSH</a><a href="/tags/RCNN/" style="font-size:1.1em;color:#4f158b">RCNN</a><a href="/tags/TensorRT/" style="font-size:1.1em;color:#314548">TensorRT</a><a href="/tags/CUDA/" style="font-size:1.2em;color:#603479">CUDA</a><a href="/tags/cuDNN/" style="font-size:1.2em;color:#63c2a5">cuDNN</a><a href="/tags/Proxy/" style="font-size:1.25em;color:#584587">Proxy</a><a href="/tags/Tensorflow/" style="font-size:1.15em;color:#9db70e">Tensorflow</a><a href="/tags/virtualenv/" style="font-size:1.1em;color:#0837ab">virtualenv</a><a href="/tags/C/" style="font-size:1.15em;color:#34064b">C++</a><a href="/tags/VSCode/" style="font-size:1.2em;color:#2a537b">VSCode</a><a href="/tags/Xshell/" style="font-size:1.1em;color:#8e5c48">Xshell</a><a href="/tags/Windows10/" style="font-size:1.1em;color:#685293">Windows10</a><a href="/tags/VNC/" style="font-size:1.1em;color:#86bab9">VNC</a><a href="/tags/arxiv/" style="font-size:1.1em;color:#07a1a6">arxiv</a><a href="/tags/Keras/" style="font-size:1.1em;color:#2c1096">Keras</a><a href="/tags/onnx/" style="font-size:1.1em;color:#1c852e">onnx</a><a href="/tags/Teamviewer/" style="font-size:1.1em;color:#025cc5">Teamviewer</a><a href="/tags/%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98/" style="font-size:1.1em;color:#854e3c">百度网盘</a><a href="/tags/Onedrive/" style="font-size:1.1em;color:#593134">Onedrive</a><a href="/tags/frp/" style="font-size:1.1em;color:#3850a9">frp</a><a href="/tags/%E8%B7%AF%E7%94%B1%E5%99%A8/" style="font-size:1.1em;color:#912405">路由器</a><a href="/tags/Docker/" style="font-size:1.1em;color:#7e1b88">Docker</a><a href="/tags/leetcode/" style="font-size:1.1em;color:#770bb9">leetcode</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多"><i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/01/"><span class="card-archive-list-date">一月 2022</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/12/"><span class="card-archive-list-date">十二月 2021</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/09/"><span class="card-archive-list-date">九月 2021</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/03/"><span class="card-archive-list-date">三月 2021</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/12/"><span class="card-archive-list-date">十二月 2020</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/09/"><span class="card-archive-list-date">九月 2020</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/08/"><span class="card-archive-list-date">八月 2020</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/07/"><span class="card-archive-list-date">七月 2020</span><span class="card-archive-list-count">2</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">121</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">286.7k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastpushdate="2022-01-12T07:04:57.730Z"></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022<i id="heartbeat" class="fa fas fa-heartbeat"></i> Qiyuan-Z</div><div class="footer_custom_text"><p><a style="margin-inline:5px" target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"></a><a style="margin-inline:5px" target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"></a><a style="margin-inline:5px" target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用 Jsdelivr 为静态资源提供CDN加速" alt="Jsdelivr"></a><a style="margin-inline:5px" target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"></a><a style="margin-inline:5px" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a><br>昨日までの私は、もうどこにもいない<br></p></div></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"></div><script defer src="https://cdn.jsdelivr.net/gh/Qiyuan-Z/live2d-widget/autoload.js"></script><script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script><script src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>