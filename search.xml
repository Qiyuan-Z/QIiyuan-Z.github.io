<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ubuntu美化</title>
    <url>/2020/01/19/Ubuntu%E7%BE%8E%E5%8C%96/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><ul>
<li>安装Unity-tweak-tool<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install unity-tweak-tool</span><br></pre></td></tr></table></figure>
<img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909004455285.png" alt></li>
<li>更新源<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:noobslab&#x2F;themes</span><br><span class="line">sudo apt-add-repository ppa:numix&#x2F;ppa </span><br><span class="line">sudo apt update</span><br></pre></td></tr></table></figure></li>
<li>安装主题<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:noobslab&#x2F;themes  </span><br><span class="line">sudo apt-get update  </span><br><span class="line">sudo apt-get install flatabulous-theme </span><br><span class="line">sudo apt-get install arc-theme</span><br></pre></td></tr></table></figure></li>
<li>安装图标<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:noobslab&#x2F;icons  </span><br><span class="line">sudo apt-get update  </span><br><span class="line">sudo apt-get install ultra-flat-icons </span><br><span class="line">sudo apt-get install numix-icon-theme-circle</span><br></pre></td></tr></table></figure></li>
<li>选择主题<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909005108750.png" alt></li>
<li>选择图标<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909005211146.png" alt></li>
<li>最终效果<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909005447430.png" alt></li>
</ul>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows10安装ubuntu16.04双系统教程</title>
    <url>/2020/01/19/Windows10%E5%AE%89%E8%A3%85ubuntu16.04%E5%8F%8C%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>BIOS模式有传统的MBR模式和新式UEFI模式，这将对安装双系统的方法产生直接影响。目前来看，大部分电脑都属于新式UEFI模式，不过也存在一些老机子仍然属于传统MBR模式。本教程只介绍新式UEFI模式下的双系统安装方法，如果你的电脑属于传统MBR模式，强烈建议你重装windows系统来更新BIOS模式到UEFI。</p>
<h2 id="制作系统盘"><a href="#制作系统盘" class="headerlink" title="制作系统盘"></a>制作系统盘</h2><p>需要准备以下工具：<br>①ubuntu系统镜像<br>②刻录软件，推荐”软碟通”，会提示注册，选择继续试用就好<br>③一个大于 2G 的 U 盘<br>1.安装并打开软碟通，插上 U 盘，并且最好备份你的 U 盘，因为之后需要格式化；<br>2.进入软碟通，选择文件，浏览到你的ubuntu镜像所在的目录，选择ubuntu镜像文件，双击打开，如图：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422030.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422031.jpg" alt><br>3.在软碟通界面菜单栏选择”启动”，选择”写入硬盘映像”，如图所示：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422032.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422033.jpg" alt><br>接下来很重要，注意次序：<br>1）看你的硬盘驱动器是否对应的是你的 U 盘（必须是） ，一般默认是；<br>2）看映像文件是否对应你的 ubuntu 镜像；<br>3）如果上述均没有错误，选择格式化，之后就会格式化你的 U 盘；<br>4）在 U 盘格式化完毕之后，选择写入，之后就是慢慢等待了，等待写入完毕；</p>
<h2 id="在windows下创建空白分区"><a href="#在windows下创建空白分区" class="headerlink" title="在windows下创建空白分区"></a>在windows下创建空白分区</h2><p>说明：这一步是为ubuntu系统分配空间，单硬盘和双硬盘存在一点区别。<br>1.”此电脑”点击右键，点击”管理”，点击”磁盘管理”：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422034.jpg" alt><br>2.为ubuntu分配空间<br>（1）如果是单硬盘，任选一个盘，在该盘点击右键，选择压缩卷，如下，输入压缩空间量，单位为M，如果空间充足，建议分出80G或100G，空间不足也可以分60G（1G=1024M）：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/20190520082545559.png" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422136.jpg" alt><br>（2）如果是双硬盘，需要先在系统盘分出200M的空白分区用来安装ubuntu的启动项，然后再在另一块硬盘分出空间，在该盘点击右键，选择压缩卷，如下，输入压缩空间量，单位为M,如果空间充足，建议分出80G或100G，空间不足也可以分60G（1G=1024M）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422138.jpg" alt></p>
<h2 id="用做好的系统盘安装系统"><a href="#用做好的系统盘安装系统" class="headerlink" title="用做好的系统盘安装系统"></a>用做好的系统盘安装系统</h2><p>先点击桌面右下角：电源图标→电源设置→电源和睡眠→其他电源设置→选择电源按钮的功能→更改当前不可用的设置，将“关机设置”中的快速启动项关闭，保存修改。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/20190520084629381.png" alt>若呈灰色，无法修改，win+r进入gpedit.msc将系统配置中的关机选项，快速启动改为禁用，重启完即可更改。<br>1.插好系统盘，重启电脑，开机进bios，在Security页面，关掉secure boot（不同电脑secure boot可能在不同位置），然后到Boot页面，如果有Fast Boot这一项（部分联想电脑有），也把它关掉，没有忽略；然后保存更改，在Boot页面下方启动项选择 USB启动，回车，如果顺利进入安装页面，继续往下做；如果点击USB启动项无法进入，保存并退出，电脑会重启，根据自己电脑按相应的键进boot manager，找到USB启动项，回车即可进入。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422139.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422140.jpg" alt><br>2.然后会进入这个界面，选择Install Ubuntu，回车确认<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422141.jpg" alt><br>3.安装过程记住断网，否则很慢<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422143.jpg" alt><br>全不选，边安装边下载更新很慢，点击”继续”<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/20171229140050194.png" alt><br>出现以下或类似界面，一定要选择”其他选项”，因为需要手动分区<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422145.jpg" alt><br>分区界面如下<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422246.jpg" alt><br>在这里，我们进行手动分区，选择留出的空闲分区，点击”+”进行分区，如下：<br>1）efi：如果是单硬盘，在唯一的一个空闲分区上添加，大小200M，逻辑分区，空间起始位置，用于efi；如果是双硬盘，找到事先分好的200M空闲分区添加，逻辑分区，空间起始位置，用于efi。这个分区必不可少，用于安装ubuntu启动项。<br>2）swap:中文是”交换空间”，充当ubuntu的虚拟内存，一般的大小与电脑物理内存一样，可以将其分为 8G，逻辑分区，空间起始位置，用于”swap”或”交换空间”<br>3) /:这是ubuntu 的根目录,用于安装系统和软件，相当于windows的C盘，我们将其分为 20G，主分区，空间起始位置，用于”ext4日志文件系统”，挂载点为”/“（根据你的磁盘空间调整，可以大一点，毕竟ubuntu装软件都是默认装在根目录的）<br>4）/home:相当于windows的其他盘，剩下的全分给它，逻辑分区，空间起始位置，用于”ext4日志文件系统”，挂载点为”/home”<br>分区完毕，在分区界面的下方，选择安装启动项的位置，我们刚刚不是创建了200M的efi分区吗，现在你看看这个区前面的编号是多少，比如是/dev/sda1,不同的机子会有不同的编号，下拉列表选择这个efi分区编号（这里一定要注意，windows的启动项也是efi文件，大小大概是100M，而我们创建的ubuntu的efi大小是200M，一定要选对），之后点击”Install Now”<br>设置地区不重要，按你需要设置，也可以直接继续，不影响<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422248.jpg" alt><br>键盘布局默认是英语的，建议不改（默认中文也行）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422249.jpg" alt><br>这里设置用户，自己输入就可以了，例如英文字母，尽量简单点，密码也简单点<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422250.jpg" alt><br>系统开始安装，耐心等待安装完毕就可以了<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422251.jpg" alt><br>全部完成之后，会提醒你重启，把U盘拔了，点”现在重启”，如果卡死就强制关机再重启就好<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422252.jpg" alt><br>重启后你会看到以下界面，第一项是ubuntu启动项，第二项是ubuntu高级设置，第三项是windows启动项，第四项不用管，默认选择的是第一个，回车进ubuntu系统<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422253.jpg" alt><br>若没有出现，则进入bios将启动顺序改为Ubuntu先。</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Windows10</tag>
      </tags>
  </entry>
  <entry>
    <title>CMake Error at /usr/share/cmake-3.5/Modules/FindQt4.cmake:634 (message)</title>
    <url>/2020/01/19/CMake%20Error/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>编译工程时出现cmake找不到Qt4的问题,如下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMake Warning at &#x2F;usr&#x2F;share&#x2F;cmake-3.5&#x2F;Modules&#x2F;FindQt4.cmake:626 (message):</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;qmake-qt4 reported QT_INSTALL_LIBS as &quot;&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&quot;</span><br><span class="line"> but QtCore could not be found there.  </span><br><span class="line"> Qt is NOT installed correctly for the target build environment.</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMake Error at &#x2F;usr&#x2F;share&#x2F;cmake-3.5&#x2F;Modules&#x2F;FindQt4.cmake:634 (message):</span><br><span class="line">  Could NOT find QtCore.</span><br></pre></td></tr></table></figure><br>解决办法<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install cmake gcc g++ qt&#123;4,5&#125;-qmake libqt4-dev</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>对目录 /var/lib/apt/lists/ 加锁 问题解决方法</title>
    <url>/2020/01/19/%E5%AF%B9%E7%9B%AE%E5%BD%95-varlibaptlists-%E5%8A%A0%E9%94%81-%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo rm &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;lock</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu apt 更换为国内的源</title>
    <url>/2020/01/19/ubuntu-apt-%E6%9B%B4%E6%8D%A2%E4%B8%BA%E5%9B%BD%E5%86%85%E7%9A%84%E6%BA%90/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>备份原来的源：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.bak</span><br></pre></td></tr></table></figure><br>更换源:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;apt&#x2F;sources.list</span><br></pre></td></tr></table></figure><br>将里面文件内容全部替换成下面：<br><a href="https://wiki.ubuntu.org.cn/%E6%A8%A1%E6%9D%BF:16.04source" target="_blank" rel="noopener">点击此处获得最新源</a><br>保存退出<br>执行更新：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Win10和Ubuntu双系统下时间不对问题</title>
    <url>/2020/01/19/Win10%E5%92%8CUbuntu%E5%8F%8C%E7%B3%BB%E7%BB%9F%E4%B8%8B%E6%97%B6%E9%97%B4%E4%B8%8D%E5%AF%B9%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>电脑安装完win10和Ubuntu双系统后，Ubuntu时间总会和Windows时间相差8小时，这是因为windows认为bios时间是本地时间，Ubuntu认为bios时间是UTC时间，所以我们需要将Ubuntu时间改成本地时间。<br>执行这条语句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timedatectl set-local-rtc 1 --adjust-system-clock</span><br></pre></td></tr></table></figure>
<p>重启进入windows，时间设置中点击立即同步即可解决。</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>如何获取OneDrive 5T网盘空间(可升级25T)</title>
    <url>/2020/01/16/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96OneDrive-5T%E7%BD%91%E7%9B%98%E7%A9%BA%E9%97%B4(%E5%8F%AF%E5%8D%87%E7%BA%A725T)/</url>
    <content><![CDATA[<p><a href="https://blog.wulel.com/post/how-to-get-5t-onedrive/" target="_blank" rel="noopener">点此跳转</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Onedrive</tag>
      </tags>
  </entry>
  <entry>
    <title>使 Onedrive 同步任意文件夹</title>
    <url>/2020/01/16/%E4%BD%BF-Onedrive-%E5%90%8C%E6%AD%A5%E4%BB%BB%E6%84%8F%E6%96%87%E4%BB%B6%E5%A4%B9/</url>
    <content><![CDATA[<h1 id><a href="#" class="headerlink" title></a><a id="more"></a></h1><p>onedrive默认只同步指定的onedrive文件夹，为了让它同步其他的文件夹，可以在命令行（以管理员身份运行的）使用以下代码创建一个软链接。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mklink &#x2F;j &quot;onedrive文件夹地址\需要同步的文件夹名&quot; &quot;需要同步的文件夹地址&quot;</span><br></pre></td></tr></table></figure><br>比如说我的onedrive在D:\Onedrive，我的目标文件夹test在D:\。那么<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mklink &#x2F;j &quot;D:\Onedrive\test&quot; &quot;D:\test&quot;</span><br></pre></td></tr></table></figure><br>如果成功的的话，会提示<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/onedrive/1392594-20181005094204092-1998469109.png" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Onedrive</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测调研</title>
    <url>/2020/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/</url>
    <content><![CDATA[<h3 id="目标检测的任务"><a href="#目标检测的任务" class="headerlink" title=" 目标检测的任务"></a><a id="more"></a> 目标检测的任务</h3><ul>
<li><strong>分类-Classification</strong>：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。</li>
<li><strong>定位-Location</strong>：解决“在哪里？”的问题，即定位出这个目标的的位置。</li>
<li><strong>检测-Detection</strong>：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。</li>
<li><strong>分割-Segmentation</strong>：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。</li>
</ul>
<h3 id="所面临的挑战"><a href="#所面临的挑战" class="headerlink" title="所面临的挑战"></a>所面临的挑战</h3><ul>
<li>目标可能出现在图像的任何位置。</li>
<li>目标有各种不同的大小。</li>
<li>目标可能有各种不同的形状。</li>
</ul>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/deep_learning_object_detection_dataset.PNG" alt></p>
<ul>
<li><p>Pascal Visual Object Classes(05-12) 是计算机视觉领域最重要的赛事之一。包含多任务，图像分类、目标检测、语义分割和行为检测。主要有两个版本的 Pascal-VOC 用于检测：VOC07 和 VOC12，前者包含 5K 张训练图像，共 12K 个标注目标。后者包含 11K 张训练图像，共 27K 个标注目标。两个数据集中包含了 20 个生活中常见的目标类（Person：person; Animal：bird, cat, cow, dog, horse, sheep; Vehicle：aeroplane, bicycle, boat, bus, car, motor-bike, train; Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor）</p>
</li>
<li><p>ImageNet Large Scale Visual Recognition Challenge(10-17) 包含检测挑战赛。Imagenet数据集有1400多万幅图片，涵盖2万多个类别，ILSVRC比赛会每年从ImageNet数据集中抽出部分样本, 检测数据集包括 200 类视觉目标，图像/目标比 VOC 大两个数量级。例如，ILSVRC-14 包含517K 图像以及 534K 注释目标。</p>
</li>
<li><p>MS-COCO 是当前最有挑战性的目标检测数据集。从 15 年开始举办比赛。它的目标类别比 ILSVRC 少，但是目标实例更多。例如，MS-COCO-17 包含 164K 张图像，来自 80 类的 897K 个标注目标。与 VOC 和 ILSVRC 相比，MS-COCO 除了边界框注释以外，每个目标通过实例分割进一步标注来帮助精确定位。此外，它包含更多小目标（面积小于图像的 1%），以及密集定位目标。这些特点使 MS−COCO中的目标分布更接近于真实世界。</p>
</li>
<li><p>Open Images</p>
<p>2018 年推出了 Open ImagesOpen~ImagesOpen Images 检测挑战赛。包含两个任务：</p>
<p>标准目标检测<br>视觉关系检测，用于检测特定关系中的匹配目标</p>
<p>目标检测，数据集包含 1910K 张图像，600 个目标类别的 15440K 个标注目标。</p>
</li>
</ul>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><div class="table-container">
<table>
<thead>
<tr>
<th>预测\实际</th>
<th>正</th>
<th>负  </th>
</tr>
</thead>
<tbody>
<tr>
<td>正</td>
<td>TP</td>
<td>FP</td>
<td></td>
</tr>
<tr>
<td>负</td>
<td>FN</td>
<td>TN</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Recall=TP/(TP+FN)，召回率，<strong>可理解为正确的被判断为正确的</strong></p>
<p>Precision=TP/(TP+FP)，准确度，<strong>预测为正类中，实际为正类的比例</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2018042521300715.png" alt></p>
<p><strong>准确率-召回率曲线（P-R曲线）</strong>：以召回率为横坐标，精确率为纵坐标</p>
<script type="math/tex; mode=display">
\mathrm{AP}=\int_{0}^{1} P(R) d(R)</script><script type="math/tex; mode=display">
\mathrm{mAP}=\frac{1}{\text { classes }} \sum_{i=1}^{\text {classes }} \int_{0}^{1} P(R) d(R)</script><p><strong>IoU</strong>：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180423003716617.png" alt></p>
<p>VOC07 以后使用 AP 评价检测性能。AP 定义为不同召回率下的平均检测精度，通常在某一特定类别下进行评估。为比较所有目标类的检测性能，通常使用 mAP 作为最终性能指标。</p>
<p>为度量目标定位精度，使用交并比（IoU）来检查预测框和 GT 框之间的 IoU是否大于预定义的阈值，如，0.5。如果大于阈值则表示成功检测，否则表示漏检。基于 0.5IoU 的 mAP 已经成为多年来用于目标检测问题的事实上的度量标准。</p>
<p>MS−COCO 的 AP 是在 0.5−0.95多个 IoU 阈值上的平均值。</p>
<h3 id="深度学习目标检测方法"><a href="#深度学习目标检测方法" class="headerlink" title="深度学习目标检测方法"></a>深度学习目标检测方法</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/deep_learning_object_detection_history.PNG" alt></p>
<p><strong>Two-stage Detectors（两阶段目标检测器）</strong></p>
<p>首先由算法（algorithm）生成一系列作为样本的候选框，再通过卷积神经网络进行样本（Sample）分类。</p>
<p>诸如R-CNN，Fast R-CNN，Faster R-CNN到最新的Mask Scoring R-CNN等网络结构，都属于Two-stage检测方法。</p>
<p><strong>One-stage Detectors（单阶段目标检测器）</strong></p>
<p>不需要产生候选框，直接将目标框定位的问题转化为回归（Regression）问题处理(Process)。</p>
<p>从最早的OverFeat到现在的YOLO，SSD，RetinaNet，YOLOv2，CornerNet等都属于one stage目标检测方法。</p>
<p>即：</p>
<p><strong>基于候选区域（Region Proposal）</strong>的，如R-CNN、SPP-net、Fast R-CNN、Faster R-CNN、R-FCN；</p>
<p><strong>基于端到端（End-to-End）</strong>，无需候选区域（Region Proposal）的，如YOLO、SSD。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180509095302426.png" alt></p>
<p>对于上述两种方式，基于候选区域（Region Proposal）的方法在检测准确率和定位精度上占优，基于端到端（End-to-End）的算法速度占优。相对于R-CNN系列的“看两眼”（候选框提取和分类），YOLO只需要“看一眼”。总之，目前来说，基于候选区域（Region Proposal）的方法依然占据上风，但端到端的方法速度上优势明显。</p>
<h3 id="各方法在各数据集上的精度"><a href="#各方法在各数据集上的精度" class="headerlink" title="各方法在各数据集上的精度"></a>各方法在各数据集上的精度</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Detector</th>
<th style="text-align:center">VOC07 (mAP@IoU=0.5)</th>
<th style="text-align:center">VOC12 (mAP@IoU=0.5)</th>
<th style="text-align:center">COCO (mAP@IoU=0.5:0.95)</th>
<th style="text-align:center">Published In</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">R-CNN</td>
<td style="text-align:center">58.5</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’14</td>
</tr>
<tr>
<td style="text-align:center">SPP-Net</td>
<td style="text-align:center">59.2</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ECCV’14</td>
</tr>
<tr>
<td style="text-align:center">MR-CNN</td>
<td style="text-align:center">78.2 (07+12)</td>
<td style="text-align:center">73.9 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ICCV’15</td>
</tr>
<tr>
<td style="text-align:center">Fast R-CNN</td>
<td style="text-align:center">70.0 (07+12)</td>
<td style="text-align:center">68.4 (07++12)</td>
<td style="text-align:center">19.7</td>
<td style="text-align:center">ICCV’15</td>
</tr>
<tr>
<td style="text-align:center">Faster R-CNN</td>
<td style="text-align:center">73.2 (07+12)</td>
<td style="text-align:center">70.4 (07++12)</td>
<td style="text-align:center">21.9</td>
<td style="text-align:center">NIPS’15</td>
</tr>
<tr>
<td style="text-align:center">YOLO v1</td>
<td style="text-align:center">66.4 (07+12)</td>
<td style="text-align:center">57.9 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">G-CNN</td>
<td style="text-align:center">66.8</td>
<td style="text-align:center">66.4 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">AZNet</td>
<td style="text-align:center">70.4</td>
<td style="text-align:center">-</td>
<td style="text-align:center">22.3</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">ION</td>
<td style="text-align:center">80.1</td>
<td style="text-align:center">77.9</td>
<td style="text-align:center">33.1</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">HyperNet</td>
<td style="text-align:center">76.3 (07+12)</td>
<td style="text-align:center">71.4 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">OHEM</td>
<td style="text-align:center">78.9 (07+12)</td>
<td style="text-align:center">76.3 (07++12)</td>
<td style="text-align:center">22.4</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">MPN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">33.2</td>
<td style="text-align:center">BMVC’16</td>
</tr>
<tr>
<td style="text-align:center">SSD</td>
<td style="text-align:center">76.8 (07+12)</td>
<td style="text-align:center">74.9 (07++12)</td>
<td style="text-align:center">31.2</td>
<td style="text-align:center">ECCV’16</td>
</tr>
<tr>
<td style="text-align:center">GBDNet</td>
<td style="text-align:center">77.2 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">27.0</td>
<td style="text-align:center">ECCV’16</td>
</tr>
<tr>
<td style="text-align:center">CPF</td>
<td style="text-align:center">76.4 (07+12)</td>
<td style="text-align:center">72.6 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ECCV’16</td>
</tr>
<tr>
<td style="text-align:center">R-FCN</td>
<td style="text-align:center">79.5 (07+12)</td>
<td style="text-align:center">77.6 (07++12)</td>
<td style="text-align:center">29.9</td>
<td style="text-align:center">NIPS’16</td>
</tr>
<tr>
<td style="text-align:center">DeepID-Net</td>
<td style="text-align:center">69.0</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">PAMI’16</td>
</tr>
<tr>
<td style="text-align:center">NoC</td>
<td style="text-align:center">71.6 (07+12)</td>
<td style="text-align:center">68.8 (07+12)</td>
<td style="text-align:center">27.2</td>
<td style="text-align:center">TPAMI’16</td>
</tr>
<tr>
<td style="text-align:center">DSSD</td>
<td style="text-align:center">81.5 (07+12)</td>
<td style="text-align:center">80.0 (07++12)</td>
<td style="text-align:center">33.2</td>
<td style="text-align:center">arXiv’17</td>
</tr>
<tr>
<td style="text-align:center">TDM</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">37.3</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">FPN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">36.2</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">YOLO v2</td>
<td style="text-align:center">78.6 (07+12)</td>
<td style="text-align:center">73.4 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">RON</td>
<td style="text-align:center">77.6 (07+12)</td>
<td style="text-align:center">75.4 (07++12)</td>
<td style="text-align:center">27.4</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">DeNet</td>
<td style="text-align:center">77.1 (07+12)</td>
<td style="text-align:center">73.9 (07++12)</td>
<td style="text-align:center">33.8</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">CoupleNet</td>
<td style="text-align:center">82.7 (07+12)</td>
<td style="text-align:center">80.4 (07++12)</td>
<td style="text-align:center">34.4</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">RetinaNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">39.1</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">DSOD</td>
<td style="text-align:center">77.7 (07+12)</td>
<td style="text-align:center">76.3 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">SMN</td>
<td style="text-align:center">70.0</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">Light-Head R-CNN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">41.5</td>
<td style="text-align:center">arXiv’17</td>
</tr>
<tr>
<td style="text-align:center">YOLO v3</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">33.0</td>
<td style="text-align:center">arXiv’18</td>
</tr>
<tr>
<td style="text-align:center">SIN</td>
<td style="text-align:center">76.0 (07+12)</td>
<td style="text-align:center">73.1 (07++12)</td>
<td style="text-align:center">23.2</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">STDN</td>
<td style="text-align:center">80.9 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">RefineDet</td>
<td style="text-align:center">83.8 (07+12)</td>
<td style="text-align:center">83.5 (07++12)</td>
<td style="text-align:center">41.8</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">SNIP</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">45.7</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">Relation-Network</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">32.5</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">Cascade R-CNN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.8</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">MLKP</td>
<td style="text-align:center">80.6 (07+12)</td>
<td style="text-align:center">77.2 (07++12)</td>
<td style="text-align:center">28.6</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">Fitness-NMS</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">41.8</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">RFBNet</td>
<td style="text-align:center">82.2 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ECCV’18</td>
</tr>
<tr>
<td style="text-align:center">CornerNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.1</td>
<td style="text-align:center">ECCV’18</td>
</tr>
<tr>
<td style="text-align:center">PFPNet</td>
<td style="text-align:center">84.1 (07+12)</td>
<td style="text-align:center">83.7 (07++12)</td>
<td style="text-align:center">39.4</td>
<td style="text-align:center">ECCV’18</td>
</tr>
<tr>
<td style="text-align:center">Pelee</td>
<td style="text-align:center">70.9 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">NIPS’18</td>
</tr>
<tr>
<td style="text-align:center">HKRM</td>
<td style="text-align:center">78.8 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">37.8</td>
<td style="text-align:center">NIPS’18</td>
</tr>
<tr>
<td style="text-align:center">M2Det</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.2</td>
<td style="text-align:center">AAAI’19</td>
</tr>
<tr>
<td style="text-align:center">R-DAD</td>
<td style="text-align:center">81.2 (07++12)</td>
<td style="text-align:center">82.0 (07++12)</td>
<td style="text-align:center">43.1</td>
<td style="text-align:center">AAAI’19</td>
</tr>
<tr>
<td style="text-align:center">ScratchDet</td>
<td style="text-align:center">84.1 (07++12)</td>
<td style="text-align:center">83.6 (07++12)</td>
<td style="text-align:center">39.1</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">Libra R-CNN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">43.0</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">Reasoning-RCNN</td>
<td style="text-align:center">82.5 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">43.2</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">FSAF</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.6</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">AmoebaNet + NAS-FPN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">47.0</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">Cascade-RetinaNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">41.1</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">TridentNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">48.4</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">DAFS</td>
<td style="text-align:center"><strong>85.3 (07+12)</strong></td>
<td style="text-align:center">83.1 (07++12)</td>
<td style="text-align:center">40.5</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">Auto-FPN</td>
<td style="text-align:center">81.8 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">40.5</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">FCOS</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.7</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">FreeAnchor</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.8</td>
<td style="text-align:center">NeurIPS’19</td>
</tr>
<tr>
<td style="text-align:center">DetNAS</td>
<td style="text-align:center">81.5 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.0</td>
<td style="text-align:center">NeurIPS’19</td>
</tr>
<tr>
<td style="text-align:center">NATS</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.0</td>
<td style="text-align:center">NeurIPS’19</td>
</tr>
<tr>
<td style="text-align:center">AmoebaNet + NAS-FPN + AA</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">50.7</td>
<td style="text-align:center">arXiv’19</td>
</tr>
<tr>
<td style="text-align:center">EfficientDet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><strong>51.0</strong></td>
<td style="text-align:center">arXiv’19</td>
</tr>
</tbody>
</table>
</div>
<h3 id="目标检测的候选框-Proposal"><a href="#目标检测的候选框-Proposal" class="headerlink" title="目标检测的候选框(Proposal)"></a>目标检测的候选框(Proposal)</h3><p>物体候选框获取，当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)。</p>
<p>根据目标候选区域的提取方式不同，传统目标检测算法可以分为基于滑动窗口的目标检测算法和基于选择性搜索的目标检测算法。</p>
<p><strong>滑动窗口（Sliding Window）</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/1423648-20190316210029111-840970217.png" alt></p>
<p>基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类</p>
<p>缺点：不知道要检测的目标大小是什么规模，所以要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量。</p>
<p>具体步骤：首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选。最终，经过NMS筛选后获得检测到的物体。</p>
<p><strong>非极大值抑制(Non-Maximum Suppression,NMS)</strong></p>
<p>根据分类器类别分类概率做排序</p>
<p>　　(1)从最大概率矩形框开始，分别得分后面的矩形框与其的重叠度IOU是否大于某个设定的阈值;</p>
<p>　　(2)假设重叠度超过阈值，那么就扔掉；并标记第一个矩形框，是我们保留下来的。</p>
<p>　　(3)从剩下的矩形框中，选择概率最大的，然后判断得分后面的矩形框与其的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记是我们保留下来的第二个矩形框。</p>
<p>　　如此循环往复知道没有剩余的矩形框，然后找到所有被保留下来的矩形框，就是我们认为最可能包含目标的矩形框。</p>
<p><strong>R-CNN算法中NMS的具体做法</strong>：</p>
<p>　　假设有20类，2000个建议框，最后输出向量维数2000*20，则每列对应一类，一行是各个建议框的得分，NMS算法步骤如下：<br>　　① 对2000×20维矩阵中每列按从大到小进行排序；<br>　　② 从每列最大的得分建议框开始，分别与该列后面的得分建议框进行IoU计算，若IoU&gt;阈值，则剔除得分较小的建议框，否则认为图像中存在多个同一类物体；<br>　　③ 从每列次大的得分建议框开始，重复步骤②；<br>　　④ 重复步骤③直到遍历完该列所有建议框；<br>　　⑤ 遍历完2000×20维矩阵所有列，即所有物体种类都做一遍非极大值抑制；<br>　　⑥ 最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框。</p>
<p><strong>选择性搜索(Selective Search)</strong></p>
<p>滑窗法类似穷举进行图像子区域搜索，但是一般情况下图像中大部分子区域是没有物体的。学者们自然而然想到只对图像中最有可能包含物体的区域进行搜索以此来提高计算效率。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/1423648-20190317100459585-1587811888.png" alt></p>
<p>选择搜索算法的主要观点：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。因此，选择搜索基于上面这一想法采用子区域合并的方法进行提取候选边界框（bounding boxes）。首先，对输入图像进行分割算法产生许多小的子区域(大约2000个子区域)。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做外切矩形（bounding boxes），这些子区域外切矩形就是通常所说的候选框。</p>
<p>优点：<br>　　　　（a）计算效率优于滑窗法。<br>　　　　（b）由于采用子区域合并策略，所以可以包含各种大小的疑似物体框。<br>　　　　（c）合并区域相似的指标多样性，提高了检测物体的概率。</p>
<h3 id="边界框回归-Bounding-Box-regression"><a href="#边界框回归-Bounding-Box-regression" class="headerlink" title="边界框回归(Bounding-Box regression)"></a>边界框回归(Bounding-Box regression)</h3><p>窗口一般使用四维向量$(x,y,w,h)$来表示， 分别表示窗口的中心点坐标和宽高。 对于下图, 红色的框 $P$ 代表原始的Proposal, 绿色的框 $G$ 代表目标的 Ground Truth， 目标是寻找一种关系使得输入原始的窗口 $P$ 经过映射得到一个跟真实窗口 $G$ 更接近的回归窗口$\hat{G}$。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/TIM%E5%9B%BE%E7%89%8720200106182123.png" alt></p>
<p>边框回归的目的既是：给定$\left(P_{x}, P_{y}, P_{w}, P_{h}\right)$寻找一种映射$f$， 使得$f\left(P_{x}, P_{y}, P_{w}, P_{h}\right)=\left(\hat{G}_{x}, \hat{G}_{y}, \hat{G}_{w}, \hat{G}_{h}\right)$并且$\left(\hat{G}_{x}, \hat{G}_{y}, \hat{G}_{w}, \hat{G}_{h}\right) \approx\left(G_{x}, G_{y}, G_{w}, G_{h}\right)$</p>
<p>思路: 平移 + 尺度放缩</p>
<ol>
<li><p>先做平移$(\Delta x, \Delta y), \quad \Delta x=P_{w} d_{x}(P), \Delta y=P_{h} d_{y}(P)$</p>
<p>$\hat{G}_{x}=P_{w} d_{x}(P)+P_{x}$<br>$\hat{G}_{y}=P_{h} d_{y}(P)+P_{y}$</p>
</li>
<li><p>然后再做尺度缩放$\left(S_{w}, S_{h}\right), S_{w}=\exp \left(d_{w}(P)\right), S_{h}=\exp \left(d_{h}(P)\right)$</p>
<p>$\hat{G}_{w}=P_{w} \exp \left(d_{w}(P)\right)$<br>$\hat{G}_{h}=P_{h} \exp \left(d_{h}(P)\right)$</p>
</li>
</ol>
<p>边框回归就是学习$d_{x}(P), d_{y}(P), d_{w}(P), d_{h}(P)$这四个变换。</p>
<p>线性回归就是给定输入的特征向量$X$, 学习一组参数 $W$, 使得经过线性回归后的值跟真实值 $Y$(Ground Truth)非常接近. 即$Y ≈ WX$。</p>
<p>输入为：$P=\left(P_{x}, P_{y}, P_{w}, P_{h}\right)$这个窗口对应的 CNN 特征(注：训练阶段输入还包括 Ground Truth， 也就是下边提到的$t_{*}=\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$ )</p>
<p>输出为：四个变换，因为有了这四个变换我们就可以直接得到 Ground Truth，真实值所对应的真正的变化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
&t_{x}=\left(G_{x}-P_{x}\right) / P_{w}\\
&t_{y}=\left(G_{y}-P_{y}\right) / P_{h}\\
&t_{w}=\log \left(G_{w} / P_{w}\right)\\
&t_{h}=\log \left(G_{h} / P_{h}\right)
\end{aligned}</script><p>目标函数可以表示为$d_{<em>}(P)=w_{</em>}^{T} \Phi_{5}(P)$， $\Phi_{5}(P)$是输入 Proposal 的特征向量， $w_{<em>}$是要学习的参数（</em> 表示 $x,y,w,h$， 也就是每一个变换对应一个目标函数）, $d_{*}(P)$ 是得到的预测值。</p>
<p>损失函数：要让预测值跟真实值$t_{*}=\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$差距最小</p>
<p>$Loss=\sum_{i}^{N}\left(t_{<em>}^{i}-\hat{w}_{</em>}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}$</p>
<p>函数优化目标为：</p>
<script type="math/tex; mode=display">
W_{*}=\operatorname{argmin}_{w_{*}} \sum_{i}^{N}\left(t_{*}^{i}-\hat{w}_{*}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{w}_{*}\right\|^{2}</script><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>Rich feature hierarchies for accurate object detection and semantic segmentation(CVPR 2014)</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_09-57-14.jpg" alt></p>
<p><strong>算法流程</strong>：</p>
<ul>
<li>使用Selective Search提取大约2000个候选区域（proposal）;</li>
<li>对每个候选区域的图像进行拉伸形变，使之成为固定大小的正方形图像，并将该图像输入到CNN中提取特征;</li>
<li>使用线性的SVM对提取的特征进行分类。</li>
</ul>
<p><strong>创新点</strong>：</p>
<ul>
<li>将大型卷积神经网络(CNNs)应用于自下而上的候选区域以定位和分割物体。</li>
<li>当带标签的训练数据不足时，先针对辅助任务进行有监督预训练，再进行特定任务的调优，就可以产生明显的性能提升。</li>
</ul>
<p><strong>R-CNN图片缩放</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180425214236957.png" alt></p>
<p><strong>缺点</strong>：</p>
<ul>
<li><p><strong>重复计算</strong> R-CNN虽然不再是穷举，但通过Proposal（Selective Search）的方案依然有两千个左右的候选框，这些候选框都需要单独经过backbone网络提取特征，计算量依然很大，候选框之间会有重叠，因此有不少其实是重复计算。</p>
</li>
<li><p><strong>训练测试不简洁</strong> 候选区域提取、特征提取、分类、回归都是分开操作，中间数据还需要单独保存。</p>
</li>
<li><p><strong>速度慢</strong> 前面的缺点最终导致R-CNN出奇的慢，GPU上处理一张图片需要十几秒，CPU上则需要更长时间。</p>
</li>
<li><p><strong>输入的图片Patch必须强制缩放成固定大小</strong> （原文采用227×227），会造成物体形变，导致检测性能下降。</p>
</li>
</ul>
<h3 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h3><p>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(ECCV 2014)</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-28-09.jpg" alt></p>
<p>卷积层的参数和输入图像的尺寸无关，它仅仅是一个卷积核在图像上滑动，不管输入图像是多少都没关系，只是对不同大小的图片卷积出不同大小的特征图，池化层对输入图像的尺寸也没有任何限制，只是获得不同的特征图而已，但是全连接层的参数就和输入图像大小有关，因为它要把输入的所有像素点连接起来,需要指定输入层神经元个数和输出层神经元个数，所以需要规定输入的feature的大小。</p>
<p>通过在卷积层和全连接层之间加入空间金字塔池化结构（Spatial Pyramid Pooling）代替R-CNN算法在输入卷积神经网络前对各个候选区域进行剪裁、缩放操作使其图像子块尺寸一致的做法。</p>
<p><strong>算法流程</strong>：</p>
<ul>
<li>首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。</li>
<li>特征提取阶段。这一步就是和R-CNN最大的区别了，同样是用卷积神经网络进行特征提取，但是SPP-Net用的是金字塔池化。这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度更快啊。</li>
<li>最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。</li>
</ul>
<p><strong>一次特征提取</strong><br>RCNN是多个regions+多次CNN+单个pooling，而SPP则是单个图像+单次CNN+多个region+多个pooling<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180425215129861.png" alt></p>
<p><strong>金字塔池化结构</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-41-12.jpg" alt><br>利用不同大小的刻度，对一张图片进行了划分。图中，利用了三种不同大小的刻度，对一张输入的图片进行了划分，最后总共可以得到16+4+1=21个块，从这21个图片块中，分别计算每个块的最大值，从而得到一个输出特征向量。最后把一张任意大小的图片转换成了一个固定大小的21维特征（当然可以设计其它维数的输出，增加金字塔的层数，或者改变划分网格的大小）。上面的三种不同刻度的划分，每一种刻度称之为金字塔的一层，使用多个不同刻度的层，可以提高所提取特征的鲁棒性。每一个图片块大小称之为：Sliding Windows Size。如果希望金字塔的某一层输出nxn个特征，那么就要用Windows Size大小为：(w/n,h/n)进行池化。（这里有一个问题，就是如果(5x5）也要得到3x3的话，计算得到的size=2，stride=1，利用公式算出来得到的池化为（4x4）与预期的3x3不符，这里暂时还有问题，不清楚具体原因是什么）。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-46-02.jpg" alt></p>
<p><strong>创新点：</strong></p>
<ul>
<li>利用空间金字塔池化结构；</li>
<li>对整张图片只进行了一次特征提取，加快运算速度。</li>
</ul>
<p><strong>缺点</strong><br>　    和R-CNN一样，它的训练要经过多个阶段，特征也要存在磁盘中，另外，SPP中的微调只更新SPP层后面的全连接层，对很深的网络这样肯定是不行的。</p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>Girshick, R., Fast R-CNN, in 2015 IEEE International Conference on Computer Vision (ICCV). 2015. p. 1440-1448.</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-7569280b566d0e58.png" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-5519b489e581557a.png" alt></p>
<p>Fast R-CNN在特征提取上可以说很大程度借鉴了SPPnet，首先将图片用选择搜索算法（selective search）得到2000个候选区域（region proposals）的坐标信息。另一方面，直接将图片归一化到CNN需要的格式，整张图片送入CNN（本文选择的网络是VGG），将第五层的普通池化层替换为RoI池化层，图片然后经过5层卷积操作后，得到一张特征图（feature maps），开始得到的坐标信息通过一定的映射关系转换为对应特征图的坐标，截取对应的候选区域，经过RoI层后提取到固定长度的特征向量，送入全连接层。</p>
<p><strong>算法流程</strong>:</p>
<ul>
<li>输入的是一张完整图片图像归一化为224×224和一组（约2000个）物体建议框（也叫RoIs）送入网络。</li>
<li>对Conv feature map进行特征提取。每一个区域经过RoI pooling layer和FC layers得到一个固定长度的feature vector，这里需要注意的是，输入到后面RoI pooling layer的feature map是在Conv feature map上提取的。虽然在最开始也提取出了大量的RoI，但他们还是作为整体输入进卷积网络的，最开始提取出的RoI区域只是为了最后的Bounding box 回归时使用，用来输出原图中的位置。</li>
<li>这些特征向量在经过全接连层之后进入两个并列的输出层。第一个是分类，使用softmax，第二个是每一类的bounding box回归。利用SoftMax Loss和Smooth L1 Loss对分类概率和边框回归（Bounding Box Regression）联合训练。<br>整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。</li>
</ul>
<p><strong>什么是ROI呢？</strong></p>
<p>RoI是Region of Interest的简写，指的是在“<strong>特征图上的框</strong>”；在Fast RCNN中， RoI是指Selective Search完成后得到的“候选框”在特征图上的映射;</p>
<p><strong>ROI Pooling的输入</strong><br>输入有两部分组成：</p>
<ul>
<li>特征图：指的是特征图，在Fast RCNN中，它位于RoI Pooling之前，通常我们常常称之为“share_conv”；</li>
<li>ROIS ：在Fast RCNN中，指的是Selective Search的输出，一堆矩形候选框框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）。</li>
</ul>
<p><strong>ROI Pooling的输出</strong><br>　　输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel x w x h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w x h）的矩形框；</p>
<p><strong>ROI Pooling的过程</strong><br>　　先把RoI中的坐标映射到feature map上，映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标后，使用Pooling得到输出；由于输入的图片大小不一，所以这里使用的类似Spp Pooling，在Pooling的过程中需要计算Pooling后的结果对应到feature map上所占的范围，然后在那个范围中进行取max或者取average。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-a6127730f6d21abd.png" alt></p>
<ul>
<li>cls_score层用于分类，输出K+1维数组p，表示属于K类和背景的概率。</li>
<li>bbox_prdict层用于调整候选区域位置，输出4*K维数组t，表示分别属于K类时，应该平移缩放的参数。</li>
</ul>
<p><strong>最后一个阶段的特征输入到两个并行的全连层中。</strong><br>一个是对区域的分类Softmax（包括背景），另一个是对bounding box回归的微调。在SVM和Softmax的对比实验中说明，SVM的优势并不明显，故直接用Softmax将整个网络整合训练更好。总代价为两者加权和，Fast-RCNN把两个回归的loss进行联合训练。</p>
<p><strong>创新点</strong></p>
<ul>
<li>提出RoI Pooling 层，它将不同大小候选框的卷积特征图统一采样成固定大小的特征。</li>
<li>独立的SVM分类器和回归器需要大量特征作为训练样本，需要大量的硬盘空间，Fast-RCNN把类别判断和位置回归统一用深度神经网络实现，不再需要额外存储。</li>
</ul>
<p><strong>缺点</strong>：<br>检测速度仍然受限于 Selective Search</p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>Ren, S., et al., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Trans Pattern Anal Mach Intell, 2017. 39(6): p. 1137-1149.</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-e64a99b38f411c337f538eb5f093bdf3_r.jpg" alt></p>
<p><strong>算法流程</strong>:</p>
<ul>
<li>把整张图片送入CNN，进行特征提取；</li>
<li>在最后一层卷积feature map上生成region proposal（通过RPN），每张图片大约300个建议窗口；</li>
<li>通过RoI pooling层（其实是单层的SPP layer）使得每个建议窗口生成固定大小的feature map；</li>
<li>继续经过两个全连接层（FC）得到特征向量。特征向量经由各自的FC层，得到两个输出向量。第一个是分类，使用softmax，第二个是每一类的bounding box回归。利用SoftMax Loss和Smooth L1 Loss对分类概率和边框回归（Bounding Box Regression）联合训练。</li>
</ul>
<p><strong>Region Proposal Networks</strong>。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-1908feeaba591d28bee3c4a754cca282_r.jpg" alt></p>
<p>可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。</p>
<p>所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure><br>其中每行的4个值$\left(x_{1}, y_{1}, x_{2}, y_{2}\right)$表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为width: height $\in\{1: 1,1: 2,2: 1\}$三种，如图。实际上通过anchors就引入了检测中常用到的多尺度方法。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-7abead97efcc46a3ee5b030a2151643f_hd.jpg" alt></p>
<p>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p>
<p><strong>那么这9个anchors是做什么的呢？</strong>遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。</p>
<p><strong>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</strong></p>
<p>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-1ab4b6c3dd607a5035b5203c76b078f3_r.jpg" alt></p>
<p>可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9<em>2)大小的矩阵。<em>*为何这样做？</em></em>后面接softmax分类获得positive anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在positive anchors中）。</p>
<p><strong>那么为何要在softmax前后都接一个reshape layer？</strong>其实只是为了便于softmax分类,存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行positive/negative二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。</p>
<p>RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，原理类似）。</p>
<p><strong>对proposals进行bounding box regression</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-8241c8076d60156248916fe2f1a5674a_r.jpg" alt></p>
<p>可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$\left[d_{x}(A), d_{y}(A), d_{w}(A), d_{h}(A)\right]$变化量</p>
<p><strong>创新点</strong></p>
<ul>
<li>采用RPN(Region Proposal Network)代替选择性搜索(Selective Search)，利用GPU进行计算大幅度缩减提取region proposal的速度。</li>
<li>产生建议窗口的CNN和目标检测的CNN共享。</li>
</ul>
<p>在主干网络中增加了RPN （Region Proposal Network）网络，通过一定规则设置不同尺度的锚点（Anchor）在RPN的卷积特征层提取候选框来代替Selective Search等传统的候选框生成方法，实现了网络的端到端训练。候选区域生成、候选区域特征提取、框回归和分类全过程一气呵成，在训练过程中模型各部分不仅学习如何完成自己的任务，还自主学习如何相互配合。这也是第一个真正意义上的深度学习目标检测算法。</p>
<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>Redmon, J., et al., You Only Look Once: Unified, Real-Time Object Detection, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. p. 779-788.</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-d54136f1eb0cd733.png" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-0d1e8d42480e1c1a.png" alt></p>
<p><strong>算法流程</strong></p>
<ul>
<li>将图像resize到448 * 448作为神经网络的输入 ；</li>
<li>运行神经网络，得到一些bounding box坐标、box中包含物体的置信度和class probabilities ；</li>
<li>进行非极大值抑制，筛选Boxes。</li>
</ul>
<p><strong>创新点</strong></p>
<ul>
<li>YOLO将目标检测问题转化为回归问题，不需要复杂的流程。在实时检测系统中， YOLO的效果是最好的。</li>
<li>YOLO 在做出预测时是推理整个图像的。与滑动窗口和候选区域算法不同， YOLO 在训练和测试时，从整个图像综合考虑，不仅分析物体的 appearance 还分析其 contextual 信息。Fast R-CNN 比较容易将背景误检测为物体，因为它不考虑 contextual 信息。YOLO 把背景误检测为物体的概率不到 Fast R-CNN 的一半。</li>
<li>YOLO 对物体的泛化能力比较好。当在自然图像上训练，在艺术图像上检测时，YOLO的效果要比 DPM 和 R-CNN 好很多。</li>
</ul>
<p>YOLO没有选择滑动窗口（silding window）或提取proposal的方式训练网络，而是直接选用整图训练模型，将 Object Detection 的问题转化成一个 Regression 问题。</p>
<p>将图片分为$S\times S$个单元格(原文中S=7)，之后的输出是以单元格为单位进行的：</p>
<ul>
<li>如果一个object的中心落在某个单元格上，那么这个单元格负责预测这个物体。</li>
<li>每个单元格需要预测B个bbox值(bbox值包括坐标和宽高，原文中B=2)，同时为每个bbox值预测一个置信度(confidence scores)。也就是每个单元格需要预测B×(4+1)个值。</li>
<li>每个单元格需要预测C(物体种类个数，原文C=20，这个与使用的数据库有关)个条件概率值.</li>
</ul>
<p>所以，最后网络的输出维度为$S \times S \times (B\times 5 + C)$，这里输出为$7\times7\times(2\times5+20)$, 虽然每个单元格负责预测一种物体(这也是这篇文章的问题，当有小物体时可能会有问题)，但是每个单元格可以预测多个bbox值(这里可以认为有多个不同形状的bbox，为了更准确的定位出物体，如下图所示)。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-29a33d2e79ed722b.png" alt></p>
<p><strong>(x,y)是bbox的中心相对于单元格的offset</strong><br>单元格坐标为$(x_{col},y_{row})$，假设它预测的输出的bbox中心坐标为$(x_c,y_c)$,那么最终预测出来的(x,y)是经过归一化处理的，表示的是中心相对于单元格的offset</p>
<p><strong>(w,h)是bbox相对于整个图片的比例</strong><br>预测的bbox的宽高为$w_b,h_b$，(w,h)表示的是bbox的是相对于整张图片的占比</p>
<p><strong>confidence</strong><br>这个置信度是由两部分组成，一是格子内是否有目标，二是bbox的准确度。定义置信度为$P_r(Object)∗IOU^{truth}_{pred}$<br>这里，如果格子内有物体，则$P_r(Object)=1$，此时置信度等于IoU。如果格子内没有物体，则$P_r(Object)=0$，此时置信度为0</p>
<p><strong>C类的条件概率</strong><br>条件概率定义为$P_r(Class_i |Object)$，表示该单元格存在物体且属于第i类的概率。</p>
<p>在测试的时候每个单元格预测最终输出的概率定义为，如下两图所示（两幅图不一样，代表一个框会输出B列概率值）<br>$P_r(Class_i|Object) ∗ P_r(Object) ∗ IOU^{truth}_{pred} = P_r(Class_i) ∗ IOU^{truth}_{pred}$</p>
<p>最后将$(S\times S)\times B\times20$ 列的结果送入NMS，最后即可得到最终的输出框结果</p>
<p><strong>损失函数</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180511204751564.png" alt></p>
<p>这里强调两点：<br> 1.每个图片的每个单元格不一定都包含object，如果没有object，那么confidence就会变成0，这样在优化模型的时候可能会让梯度跨越太大，模型不稳定跑飞了。为了平衡这一点，在损失函数中，设置两个参数$λ_{corrd}$和$λ_{noobj}$，其中$λ_{corrd}$控制bbox预测位置的损失，$λ_{noobj}$控制单个格内没有目标的损失。<br> 2.对于大的物体，小的偏差对于小的物体影响较大，为了减少这个影响，所以对bbox的宽高都开根号。</p>
<p><strong>缺点</strong></p>
<ul>
<li>YOLO的物体检测精度低于其他state-of-the-art的物体检测系统。</li>
<li>YOLO容易产生物体的定位错误。</li>
<li>YOLO对小物体的检测效果不好（尤其是密集的小物体，因为一个栅格只能预测2个物体）。</li>
</ul>
<h3 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h3><p>Redmon, J. and A. Farhadi, YOLO9000: Better, Faster, Stronger. (CVPR 2017)</p>
<p><strong>改进</strong>：</p>
<ul>
<li><p><strong>Batch Normalization</strong></p>
<p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p>
</li>
<li><p><strong>High Resolution Classifier</strong></p>
<p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为$224 \times 224$的图片作为输入，分辨率相对较低，不利于检测模型。所以YOLOv1在采用$224 \times 224$分类模型预训练后，将分辨率增加至$448 \times 448$，并使用这个高分辨率在检测数据集上fine tune。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。所以YOLOv2增加了在ImageNet数据集上使用$448 \times 448$来fine tune分类网络这一中间过程（10 epochs），这可以使得模型在检测数据集上fine tune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p>
</li>
<li><p><strong>Convolutionlal With Anchor Boxes</strong></p>
<p>anchor是RNP网络中的一个关键步骤，说的是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的建议框。</p>
<p><strong>YOLO v1： S*S* (B*5 + C) =&gt; 7*7（2*5+20）</strong><br>　　 其中B对应Box数量，5对应 Rect 定位+置信度。每个Grid只能预测对应两个Box，这两个Box共用一个分类结果（20 classes），这是很不合理的临时方案。</p>
<p><strong>YOLO v2： S*S*K* (5 + C) =&gt; 13*13*9（5+20）</strong><br>　　分辨率改成了13*13，更细的格子划分对小目标适应更好，再加上与Faster一样的K=9，计算量增加了不少。通过Anchor Box改进，mAP由69.5下降到69.2，Recall由81%提升到了88%。<br>　　<br>　　为了引入anchor boxes来预测bounding boxes，作者在网络中果断去掉了全连接层。首先，作者去掉了后面的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416x416，这一步的目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。作者观察到，大物体通常占据了图像的中间位置， 就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（factor为32），使得输入卷积网络的416x416图片最终得到13x13的卷积特征图（416/32=13）。<br>　　加入了anchor boxes后，可以预料到的结果是召回率上升，准确率下降。我们来计算一下，假设每个cell预测9个建议框，那么总共会预测13x13x9 = 1521个boxes，而之前的网络仅仅预测7x7x2 = 98个boxes。具体数据为：没有anchor boxes，模型recall为81%，mAP为69.5%；加入anchor boxes，模型recall为88%，mAP为69.2%。这样看来，准确率只有小幅度的下降，而召回率则提升了7%，说明可以通过进一步的工作来加强准确率，的确有改进空间。</p>
</li>
<li><p><strong>New Network——Darknet-19</strong></p>
<p>YOLOv2使用了一个新的分类网络作为特征提取部分，参考了前人的先进经验，比如类似于VGG，作者使用了较多的3x3卷积核，在每一次池化操作后把通道数翻倍。借鉴了network in network的思想，网络使用了全局平均池化（global average pooling），把1x1的卷积核置于3x3的卷积核之间，用来压缩特征。也用了batch normalization稳定模型训练。模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_18-40-15.jpg" alt></p>
</li>
</ul>
<ul>
<li><p><strong>Dimension Clusters</strong></p>
<p>在Faster R-CNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： $d(\text { box }, \text { centroid })=1-\mathrm{IOU}(\text { box }, \text { centroid })$</p>
</li>
<li><p><strong>Direct location prediction</strong></p>
<script type="math/tex; mode=display">
x=\left(t_{x} * w_{a}\right)-x_{a}</script><script type="math/tex; mode=display">
y=\left(t_{y} * h_{a}\right)-y_{a}</script><p>这个公式的理解为：当预测tx=1，就会把box向右边移动一定距离（具体为anchor box的宽度），预测tx=-1，就会把box向左边移动相同的距离，该公式没有任何约束，中心点可能会出现在图像任何位置，这就有可能导致回归过程震荡，甚至无法收敛。</p>
<p><strong>强约束方法</strong>：</p>
<ul>
<li>对应 Cell 距离左上角的边距为$\left(C_{x}, C_{y}\right)$，σ定义为sigmoid激活函数，将函数值约束到［0，1］，用来预测相对于该Cell中心的偏移（不会偏离cell）；</li>
<li>预定Anchor（文中描述为bounding box prior）对应的宽高为$\left(P_{w}, P_{h}\right)$，预测 Location 是相对于Anchor的宽高乘以系数得到；</li>
</ul>
</li>
</ul>
<p>  如果这个cell距离图像左上角的边距为$\left(C_{x}, C_{y}\right)$以及该cell对应的box维度（bounding box prior）的长和宽分别为$\left(P_{w}, P_{h}\right)$，那么预测值可以表示为： </p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_18-53-58.jpg" alt></p>
<ul>
<li><p><strong>Fine-Grained Features</strong><br>YOLO最终在13x13的特征图上进行预测，虽然这足以胜任大尺度物体的检测，但对于小物体还需要更精细的特征图（Fine-Grained Features），前面更精细的特征图可以用来预测小物体。<br>YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26x26大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为26x26x512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2x2的局部区域，然后将其转化为channel维度，对于26x26x512的特征图，经passthrough层处理之后就变成了13x13x2048的新特征图（特征图大小降低4倍，而channles增加4倍），这样就可以与后面的13x13x1024特征图连接在一起形成13x13x3072的特征图，然后在此特征图基础上卷积做预测。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_19-14-51.jpg" alt></p>
</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20161229150249883.jpg" alt></p>
<ul>
<li><p><strong>Multi-Scale Training</strong><br>　　为了让 YOLOv2 适应不同Scale下的检测任务，作者尝试通过不同分辨率图片的训练来提高网络的适应性。(网络只用到了卷积层和池化层，可以进行动态调整（检测任意大小图片）)<br>具体做法是：<br>　　 每经过10次训练（10 epoch），就会随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度池化{320,352，…，608}。最终最小的尺寸为320 <em> 320，最大的尺寸为608 </em> 608。接着按照输入尺寸调整网络进行训练。</p>
<p>​         这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。</p>
</li>
<li><p><strong>交叉数据训练</strong></p>
<p>YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p>
<p>作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_09-22-20.jpg" alt><br>分类时的概率计算借用了决策树思想，某个节点的概率值等于 该节点到根节点的所有条件概率之积。<br>如果想求得特定节点的绝对概率，只需要沿着路径做连续乘积。例如 如果想知道一张图片是不是“Norfolk terrier ”需要计算：</p>
<p>$\operatorname{Pr}(\text { Norfolk terrier })=\operatorname{Pr}(\text { Norfolk terrier } | \text { terrier })$<br>*$\operatorname{Pr}(\text { terrier } | \text { hunting dog })$</p>
<p><em>. . . </em></p>
<p><em>$\operatorname{Pr}(\text { mammal } |\text { animal })$
  </em>$\operatorname{Pr}(\text { animal } | \text { physical object })$<br>softmax操作也同时应该采用分组操作，基于所有“同义词集”计算softmax，其中“同义词集”是同一概念的下位词。</p>
</li>
</ul>
<h3 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h3><p>Redmon, J. and A. Farhadi, YOLOv3: An Incremental Improvement. 2018.</p>
<ul>
<li><p><strong>新的网络结构Darknet-53</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-e65c08c61bfaa7c7.png" alt></p>
<p>借鉴了残差网络residual network的做法，在一些层之间设置了快捷链路（shortcut connections）。每个残差组 件有两个卷积层和一个快捷链路。</p>
</li>
<li><p><strong>利用多尺度特征进行对象检测</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-bc5def91d05e4d3a.png" alt><br>YOLO2曾采用passthrough结构来检测细粒度特征，在YOLO3更进一步采用了3个不同尺度的特征图来进行对象检测。</p>
<p>结合上图看，卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416x416的话，这里的特征图就是13x13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。</p>
<p>为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。</p>
<p>最后，第91层特征图再次上采样，并与第36层特征图融合（Concatenation），最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-9b28d0f1c682b80a.png" alt></p>
<p>对于一个416x416的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13x13x3 + 26x26x3 + 52x52x3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。</p>
<p>对比一下，YOLO2采用13x13x5 = 845个预测，YOLO3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。</p>
</li>
<li><p><strong>9种尺度的先验框</strong><br>随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。</p>
<p>分配上，在最小的13x13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26x26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52x52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-5cc00a60004e0473.png" alt></p>
</li>
<li><p><strong>对象分类softmax改成logistic</strong><br>YOLOv3 不使用 Softmax 对每个框进行分类，主要考虑因素有两个：</p>
<ul>
<li>Softmax 使得每个框分配一个类别（得分最高的一个），而对于 Open Images这种数据集，目标可能有重叠的类别标签，因此 Softmax不适用于多标签分类。</li>
<li>Softmax 可被独立的多个 logistic 分类器替代，且准确率不会下降。<br>所以预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象。</li>
</ul>
</li>
</ul>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p>Liu, W., et al., SSD: Single Shot MultiBox Detector. (ECCV 2016)</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_14-51-11.jpg" alt></p>
<p><strong>创新点</strong></p>
<ul>
<li><p><strong>在多层多尺度特征图上进行检测</strong></p>
<p>低层特征图具有细节信息，看得比较细，而高层特征图中具有高级语义信息，看得比较广，SSD提出同时利用低层特征图和高层特征图进行检测。SSD为了避免利用太低层的特征，从VGG后面开始，又往后添加了4层卷积层，如此就得到了多层次的特征图。这6层特征图的大小分别为：38x38, 19x19, 10x10, 5x5, 3x3, 1x1</p>
</li>
<li><p><strong>default boxes</strong></p>
<p>two-stage 方法太慢，计算代价大。SSD中避免使用region proposals,而采用default boxes。SSD借鉴RPN网络中的anchor box概念。首先将feature map划分为小格子叫做feature map cell，再在每个cell中设置一系列不同长宽比的default box。将其应用于不同分辨率的特征图中。在多个特征图中使用不同的默认框形状，可以有效地离散可能的输出框形状空间。</p>
</li>
<li><p><strong>用卷积层来预</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2148039-cec47c5d71ce8f22.png" alt>3x3的卷积在特征图上进行操作，在特征图的每个点提取特征，然后对每个default box生成四个偏移量来表示生成的bounding box，这四个偏移量分别表示生成框中心x坐标、y坐标、宽度、高度对应于当前default box的偏移量，这里的计算方法采用了RPN中的方法。同时，针对每个bounding box, 卷积会进行分类操作，输出C个分类数值，注意这里的C包括背景类。因此对于每个default box有C+4个输出值，而K个default box就有K(C+4)个输出值，因此，总共需要K*(C+4)个3x3卷积。</p>
<p>总的来说就是采用default box的方式，在多层多尺度的特征图上使用卷积进行检测（分类+回归）。</p>
</li>
<li><p><strong>数据增广</strong><br>为了使得模型对于不同大小、不同形状的物体具有鲁棒性，因此采用了data augmentation。这篇论文在训练时，首先对一张图片进行random crop，然后将其resize的原图尺度，然后以0.5的概率进行水平翻转、同时添加一些色彩变换。</p>
</li>
<li><p><strong>加入atrous</strong><br>将 VGG 中的 FC6 layer、FC7 layer 转成为 卷积层，并从模型的 FC6、FC7 上的参数，进行采样得到这两个卷积层的 parameters。还将 Pool5 layer 的参数，从2×2−s2转变成 3×3−s4，外加一个 pad（1）（猜想是不想reduce特征图大小），为了配合这种变化，采用了一种Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_15-32-00.jpg" alt><br>带孔卷积并不是卷积核里带孔，而是在卷积的时候，跳着的去卷积map（比如dilated＝2的孔卷积，就是隔一个像素点，“卷”一下，这就相当于把卷积核给放大了（3x3的核变成7x7的核，多出位置的weights给0就是。）这样就使得3x3的卷积核也能达到7x7卷积核的感受野</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2018051716490729.png" alt></p>
</li>
</ul>
<p><strong>缺点</strong><br>SSD存在的缺点：</p>
<ul>
<li>需要手动设置参数prior box，无法通过训练得到，依赖经验。</li>
<li>存在着对小目标检测效果不好的现象。</li>
</ul>
<h3 id="DSSD"><a href="#DSSD" class="headerlink" title="DSSD"></a>DSSD</h3><p>Fu, C., et al., DSSD : Deconvolutional Single Shot Detector. (CVPR 2017)</p>
<p>DSSD针对小目标鲁棒性太差，提出了以下两个贡献：</p>
<ul>
<li>把SSD的基准网络从VGG换成了Resnet-101，增强了特征提取能力；</li>
<li>使用反卷积层（deconvolution layer ）增加了大量上下文信息。</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-00-32.jpg" alt></p>
<p>换完base network之后，作者通过实验发现，准确率从77.5%降至了76.4%，说明只换网络并不能够提高准确率。如果能够提高每一个子网络的准确率，那整个网络的准确率也会得到提升。所以作者在每次预测之前加入了一个“预测模块”（prediction module）：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-11-49.jpg" alt><br>如上图所示：<br>(a)为原SSD对于特征信息进行分类与定位的模型，其实并没有预测模块，直接进行预测；<br>(b)为作者在预测之前加入了residual block模块；<br>(c)将residual block模块中直接映射identity mapping换成了1x1卷积<br>(d)堆积residual block模块</p>
<p>经过实验证明，（c）能够获得更好的准确率。</p>
<p>第二个贡献就是添加了反卷积模块，引入了空间上下文信息，从而大大提高了检测准确率。添加了反卷积之后，整个网络形成不对称的沙漏结构。</p>
<p>蓝色块为卷积层，红色块为反卷积层。红色块做反卷积操作，然后与同大小的卷积层融合，之后再进行物体检测。其中的“反卷积模块”（Deconvolution module）</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-17-17.jpg" alt></p>
<p>其中Eltw Product是元素点积操作。作者也尝试了元素求和，但是点积操作的准确率稍高一些。</p>
<h3 id="目标检测进展"><a href="#目标检测进展" class="headerlink" title="目标检测进展"></a>目标检测进展</h3><ul>
<li><p>用更好的引擎检测</p>
<p>AlexNet<br>VGG： 16—19层，使用 3×3 卷积核取代 5×5 和 7×7。<br>GoogLeNet： 即，Inception 网络家族，增加卷积神经网络深度和宽度。<br>ResNet<br>DenseNet： 受 short cut 连接的影响，作者将每一层以前馈的方式和其他所有层相连。<br>SENet： 主要贡献是将全局池化与 shuffling 结合，学习特征图通道的重要性。</p>
</li>
<li><p>用更好的特征检测<br>研究者在最新检测引擎的基础上，努力提高图像特征质量，最重要的两组方法是：</p>
<ul>
<li>特征融合<br>-自底向上融合：通过跳跃连接将浅层特征传递到深层<br>-自顶向下融合：将深层特征反馈给浅层<br>特征融合可以被看做是不同特征图间的元素运算。主要有几组方法：<br>元素求和<br>元素乘积<br>串联<br>元素乘积的优点是可以抑制或者强调某一区域内特征，可能有利于小目标检测。特征串联的优点是可以整合不同区域的上下文特征，缺点是增加内存消耗。</li>
<li>学习具有大感受野的高分辨率特征<br>小感受野更关注局部细节。特征图分辨率越小，越难检测小目标。增加特征分辨率最直接的方法是去除池化层，或者减小卷积下采样率。但是这样同时会减小感受野。<br>同时增加感受野和分辨率的方法是引入空洞卷积。</li>
</ul>
</li>
<li>定位提升<br>主要有两种方法：<br>边界框精炼<br>设计新的损失函数</li>
<li>对旋转和尺度变化鲁棒的检测<br>数据增强<br>为不同方位训练独立检测器</li>
</ul>
<h3 id="未来的研究"><a href="#未来的研究" class="headerlink" title="未来的研究"></a>未来的研究</h3><ul>
<li>轻量级目标检测：加速检测算法，使其能够在移动设备上平稳运行。</li>
<li>检测与AutoML：未来的一个方向是使用神经结构搜索，减少设计检测模型时的人为干预(例如，如何设计引擎，如何设置Anchor)</li>
<li>Proposal的Anchor生成方式</li>
<li>弱监督检测</li>
<li>小目标检测</li>
<li>视频中的检测（实时）</li>
<li>信息融合检测</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>网站说明</title>
    <url>/2019/12/12/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>这是我学习制作的第一个博客，主要用于记录研究生学习过程中所遇到的问题以及一些解决方法，顺便会分享一些实用教程，希望能对有需要的人给予帮助</p>
]]></content>
  </entry>
  <entry>
    <title>PyQt5与Opencv的小小融合</title>
    <url>/2019/03/20/PyQt5%E4%B8%8EOpencv%E7%9A%84%E5%B0%8F%E5%B0%8F%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<h2 id="一些核心代码"><a href="#一些核心代码" class="headerlink" title="一些核心代码"></a><a id="more"></a>一些核心代码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class myLabel(QLabel):</span><br><span class="line">    x0 &#x3D; 0</span><br><span class="line">    y0 &#x3D; 0</span><br><span class="line">    x1 &#x3D; 0</span><br><span class="line">    y1 &#x3D; 0</span><br><span class="line">    flag &#x3D; False</span><br><span class="line"></span><br><span class="line">    def mousePressEvent(self,event):</span><br><span class="line">        self.flag &#x3D; True</span><br><span class="line">        self.x0 &#x3D; event.x()</span><br><span class="line">        self.y0 &#x3D; event.y()</span><br><span class="line">    def mouseReleaseEvent(self,event):</span><br><span class="line">        self.flag &#x3D; False</span><br><span class="line"></span><br><span class="line">    def mouseMoveEvent(self,event):</span><br><span class="line">        if self.flag:</span><br><span class="line">            self.x1 &#x3D; event.x()</span><br><span class="line">            self.y1 &#x3D; event.y()</span><br><span class="line">            self.update()</span><br><span class="line">    def paintEvent(self, event):</span><br><span class="line">        super().paintEvent(event)</span><br><span class="line">        rect &#x3D;QRect(self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">        painter &#x3D; QPainter(self)</span><br><span class="line">        painter.setPen(QPen(Qt.red,4,Qt.SolidLine))</span><br><span class="line">        painter.drawRect(rect)</span><br><span class="line"></span><br><span class="line">        pqscreen  &#x3D; QGuiApplication.primaryScreen()</span><br><span class="line">        pixmap2 &#x3D; pqscreen.grabWindow(self.winId(), self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">        pixmap2.save(&#39;555.png&#39;)</span><br><span class="line">    </span><br><span class="line">class Example(QWidget):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.initUI()</span><br><span class="line">    def initUI(self):</span><br><span class="line">        self.resize(675, 300)</span><br><span class="line">        self.setWindowTitle(&#39;关注微信公众号：学点编程吧--opencv、PyQt5的小小融合&#39;)</span><br><span class="line"></span><br><span class="line">        self.lb &#x3D; myLabel(self)</span><br><span class="line">        self.lb.setGeometry(QRect(140, 30, 511, 241))</span><br><span class="line"></span><br><span class="line">        img &#x3D; cv2.imread(&#39;xxx.jpg&#39;)</span><br><span class="line">        height, width, bytesPerComponent &#x3D; img.shape</span><br><span class="line">        bytesPerLine &#x3D; 3 * width</span><br><span class="line">        cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)</span><br><span class="line">        QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br><span class="line">        pixmap &#x3D; QPixmap.fromImage(QImg)</span><br><span class="line"></span><br><span class="line">        self.lb.setPixmap(pixmap)</span><br><span class="line">        self.lb.setCursor(Qt.CrossCursor)</span><br><span class="line"></span><br><span class="line">        self.show()</span><br></pre></td></tr></table></figure>
<h2 id="实现大体思路"><a href="#实现大体思路" class="headerlink" title="实现大体思路"></a>实现大体思路</h2><ul>
<li>重新实现QLabel类，在类中重新实现了鼠标的点击、拖动、释放、以及绘画事件</li>
<li>在窗体上新建了一个label标签，然后载入图片</li>
<li>label标签载入的图像是由Opencv实现的</li>
</ul>
<h2 id="鼠标画矩形的思路"><a href="#鼠标画矩形的思路" class="headerlink" title="鼠标画矩形的思路"></a>鼠标画矩形的思路</h2><ul>
<li>新建一个矩形是否完成标志flag，默认是Flase，表示未完成</li>
<li>鼠标点击的时候，记录当前鼠标所在位置的坐标，flag标志置为True，表示开始画矩形了</li>
<li>鼠标拖动的时候，因为flag为True，所以记录当前鼠标所在位置的坐标</li>
<li>鼠标释放的时候，flag置为False，表示矩形画完了，准备画下一个了</li>
</ul>
<h2 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h2><h3 id="Opencv图像的转换"><a href="#Opencv图像的转换" class="headerlink" title="Opencv图像的转换"></a>Opencv图像的转换</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv2.imread(&#39;xxx.jpg&#39;)</span><br><span class="line">height, width, bytesPerComponent &#x3D; img.shape</span><br><span class="line">bytesPerLine &#x3D; 3 * width</span><br><span class="line">cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)</span><br><span class="line">QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br><span class="line">pixmap &#x3D; QPixmap.fromImage(QImg)</span><br></pre></td></tr></table></figure>
<p>这个就是Opencv和PyQt对象的转化了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv2.imread(&#39;xxx.jpg&#39;)</span><br></pre></td></tr></table></figure>
<p>使用Opencv读取图像。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">height, width, bytesPerComponent &#x3D; img.shape</span><br></pre></td></tr></table></figure>
<p>在OpenCV-Python绑定中，图像使用NumPy数组的属性(这就解释了为什么要更新numpy)来表示图像的尺寸和通道信息。此时如果我们输出img.shape，将得到(200, 360, 3)。最后的3表示这是一个RGB图像。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)</span><br></pre></td></tr></table></figure>
<p>将图像从一个颜色空间转换为另一个颜色空间。<br>Python中的函数要求是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Python：cv2.cvtColor（src，code [，dst [，dstCn]]）→dst</span><br></pre></td></tr></table></figure>
<p>参数：</p>
<ol>
<li>src - 输入图像：8位无符号，16位无符号（CV_16UC …）或单精度浮点数。</li>
<li>dst - 输出与src相同大小和深度的图像。</li>
<li>code - 颜色空间转换代码（请参阅下面的说明）。</li>
<li>dstCn - 目标图像中的通道数量；如果参数是0，则通道的数量是从src和代码自动导出的。</li>
</ol>
<p>该函数将输入图像从一个颜色空间转换为另一个颜色空间。在从RGB颜色空间转换到RGB颜色空间的情况下，通道的顺序应明确指定（RGB或BGR）。请注意，OpenCV中的默认颜色格式通常被称为RGB，但实际上是BGR（字节相反）。因此，标准（24位）彩色图像中的第一个字节将是一个8位蓝色分量，第二个字节将是绿色，而第三个字节将是红色。第四，五，六字节将是第二个像素（蓝色，然后是绿色，然后是红色），依此类推。</p>
<p>这里我们就是要求从Opencv的BGR图像转换成RGB图像了。为什么？因为要转换成PyQt5可以识别的啊！</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bytesPerLine &#x3D; 3 * width</span><br><span class="line">QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br></pre></td></tr></table></figure>
<p>QImage类提供了独立于硬件的图像表示形式，允许直接访问像素数据，并可用作绘画设备。Qt提供了四个类来处理图像数据：QImage，QPixmap，QBitmap和QPicture。QImage是为I/O设计和优化的，并且可以直接进行像素访问和操作，而QPixmap则是针对在屏幕上显示图像而设计和优化的。 </p>
<p>QBitmap只是一个继承QPixmap的便利类，深度为1。最后，QPicture类是一个记录和重放QPainter命令的绘图设备。</p>
<p>因为QImage是一个QPaintDevice子类，QPainter可以用来直接绘制图像。在QImage上使用QPainter时，可以在当前GUI线程之外的另一个线程中执行绘制。QImage提供了一系列功能，可用于获取有关图像的各种信息。也有几个功能，使图像转换。<br>详见官网介绍：<a href="https://doc.qt.io/qt-5/qimage.html" target="_blank" rel="noopener">QImage Class | Qt GUI 5.10</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br></pre></td></tr></table></figure>
<p>函数原型是：<strong>QImage(str, int, int, int, QImage.Format)</strong>，用给定的宽度，高度和格式构造一个使用现有内存缓冲区数据的图像。宽度和高度必须以像素指定。bytesPerLine指定每行的字节数。</p>
<p><strong>这里有个疑问：为什么bytesPerLine = 3 * width？</strong><br>我的理解是：当1个像素占3个字节，此时图像为真彩色图像。</p>
<p><strong>QImage.Format_RGB888</strong>表示的是图像存储使用8-8-8 24位RGB格式。当然还有更多的格式，详见QImage的官方介绍，限于篇幅这里不展开。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pixmap &#x3D; QPixmap.fromImage(QImg)</span><br></pre></td></tr></table></figure>
<p>这个很好理解，就是想QImage对象转换成QPixmap对象，便于下步我们将Label标签中设置图像。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.lb.setPixmap(pixmap)</span><br></pre></td></tr></table></figure>
<p>设置标签的图像信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.lb.setCursor(Qt.CrossCursor)</span><br></pre></td></tr></table></figure>
<p>设置鼠标在QLabel对象中的样式，只是为了画画好看些而已，没其它的意思。除了这个十字架的，还有其它很多样式，如下图：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2-cf7bf8978adbaa898f059207f52f3d5b_hd.jpg" alt></p>
<h3 id="鼠标事件"><a href="#鼠标事件" class="headerlink" title="鼠标事件"></a>鼠标事件</h3><p>按照上文中我们介绍的思路，我们自定义了一个QLabel类myLabel，当然是继承了QLabel。然后我们用几个类变量记录鼠标的坐标和矩形是否完成的标志。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">def mousePressEvent(self,event):</span><br><span class="line">    self.flag &#x3D; True</span><br><span class="line">    self.x0 &#x3D; event.x()</span><br><span class="line">    self.y0 &#x3D; event.y()</span><br><span class="line"></span><br><span class="line">def mouseReleaseEvent(self,event):</span><br><span class="line">    self.flag &#x3D; False</span><br><span class="line"></span><br><span class="line">def mouseMoveEvent(self,event):</span><br><span class="line">    if self.flag:</span><br><span class="line">        self.x1 &#x3D; event.x()</span><br><span class="line">        self.y1 &#x3D; event.y()</span><br><span class="line">        self.update()</span><br></pre></td></tr></table></figure>
<p>这里就是重载了鼠标产生的几个事件，是我们自定义的。分别记录了点击鼠标后初始的鼠标坐标，以及释放鼠标后的鼠标坐标。并在鼠标移动的时候更新UI。也就是我们上面所说的鼠标画矩形的思路。</p>
<h3 id="画矩形"><a href="#画矩形" class="headerlink" title="画矩形"></a>画矩形</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def paintEvent(self, event):</span><br><span class="line">    super().paintEvent(event)</span><br><span class="line">    rect &#x3D;QRect(self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">    painter &#x3D; QPainter(self)</span><br><span class="line">    painter.setPen(QPen(Qt.red,4,Qt.SolidLine))</span><br><span class="line">    painter.drawRect(rect)</span><br><span class="line"></span><br><span class="line">    pqscreen  &#x3D; QGuiApplication.primaryScreen()</span><br><span class="line">    pixmap2 &#x3D; pqscreen.grabWindow(self.winId(), self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">    pixmap2.save(&#39;555.png&#39;)</span><br></pre></td></tr></table></figure>
<p>这个是关键点啊！</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">super().paintEvent(event)</span><br></pre></td></tr></table></figure>
<p>调用父类的paintEvent()，这个是为了显示你设置的效果。否则会是一片空白。大家可以试试注释这句话，看看效果啊！</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rect &#x3D;QRect(self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br></pre></td></tr></table></figure>
<p>QRect类使用整数精度在平面中定义一个矩形。矩形通常表示为左上角和大小。QRect的大小（宽度和高度）始终等同于构成其渲染基础的数学矩形。QRect可以用一组左，上，宽和高整数，或者从QPoint和QSize构成。以下代码创建两个相同的矩形。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">QRect(100, 200, 11, 16)</span><br><span class="line">QRect(QPoint(100, 200), QSize(11, 16))</span><br><span class="line">painter &#x3D; QPainter(self)</span><br><span class="line">painter.setPen(QPen(Qt.red,4,Qt.SolidLine))</span><br><span class="line">painter.drawRect(rect)</span><br></pre></td></tr></table></figure>
<p>构建一个QPainter对象，设置它的画笔，然后画一个矩形。貌似感觉好简单！^_^”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pqscreen  &#x3D; QGuiApplication.primaryScreen()</span><br><span class="line">pixmap2 &#x3D; pqscreen.grabWindow(self.winId(), self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">pixmap2.save(&#39;555.png&#39;)</span><br></pre></td></tr></table></figure>
<p>截屏的原理呢，主要还是运用QScreen类中的grabWindow方法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">QScreen.grabWindow(WId window, int x &#x3D; 0, int y &#x3D; 0, int width &#x3D; -1, int height &#x3D; -1)</span><br></pre></td></tr></table></figure>
<p>大致意思是创建并返回通过抓取由QRect（x，y，width，height）限制的给定窗口构造的像素图。</p>
<p>参数（x，y）指定窗口中的偏移量，而（宽度，高度）指定要复制的区域。如果宽度为负数，则该函数将所有内容复制到窗口的右边界。如果高度为负数，则该函数将所有内容复制到窗口的底部。</p>
<p>窗口系统标识符（WId）可以使用QWidget.winId（）函数进行检索。grabWindow（）函数从屏幕抓取像素，而不是从窗口抓取像素，即，如果有另一个窗口部分或全部覆盖抓取的像素，则也会从上面的窗口获取像素。鼠标光标一般不会被抓取。详见官网介绍：QScreen Class | Qt GUI 5.10</p>
<p>由于QScreen类无构造函数，所以我们使用QGuiApplication.primaryScreen()创建了一个Qscreen类对象。最后使用pixmap2.save(‘555.png’)，保存具体的截图。</p>
<p>如果你想保存的图片没有红框，可以参考这里：<br><a href="http://www.xdbcb8.com/forum/topic/%e3%80%8apyqt5%e4%b8%8eopencv%e7%9a%84%e5%b0%8f%e5%b0%8f%e8%9e%8d%e5%90%88%e3%80%8b%e8%bf%99%e7%af%87%e6%96%87%e7%ab%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e4%bf%9d%e5%ad%98%e4%b8%8b%e6%9d%a5" target="_blank" rel="noopener">《pyqt5与opencv的小小融合》这篇文章中如何实现保存下来</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Opencv</tag>
        <tag>PyQt5</tag>
      </tags>
  </entry>
  <entry>
    <title>Python新环境下快速安装依赖包的小技巧</title>
    <url>/2019/03/19/Python%E6%96%B0%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<p>当你新创一个Python环境时，若还用pip一个个装你所需要的库，明显效率十分低下。这里有个小技巧，你可以从已配置好的旧环境中，导出一个requirements.txt 文件，用于记录所有依赖包及其精确的版本号。以便新环境部署。</p>
<a id="more"></a> 
<p>在旧环境中执行以下命令，生成requirements.txt文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure>
<p>requirements.txt中的内容类似如下，记录了你旧有环境的依赖包及其精确的版本号：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">appdirs&#x3D;&#x3D;1.4.3</span><br><span class="line">backports.functools-lru-cache&#x3D;&#x3D;1.5</span><br><span class="line">beautifulsoup4&#x3D;&#x3D;4.5.3</span><br><span class="line">bs4&#x3D;&#x3D;0.0.1</span><br><span class="line">cycler&#x3D;&#x3D;0.10.0</span><br><span class="line">kiwisolver&#x3D;&#x3D;1.0.1</span><br><span class="line">lxml&#x3D;&#x3D;3.7.3</span><br><span class="line">matplotlib&#x3D;&#x3D;2.2.0</span><br><span class="line">numpy&#x3D;&#x3D;1.14.1</span><br><span class="line">pandas&#x3D;&#x3D;0.22.0</span><br><span class="line">pyparsing&#x3D;&#x3D;2.2.0</span><br><span class="line">python-dateutil&#x3D;&#x3D;2.6.1</span><br><span class="line">pytz&#x3D;&#x3D;2018.3</span><br><span class="line">six&#x3D;&#x3D;1.11.0</span><br><span class="line">virtualenv&#x3D;&#x3D;15.1.0</span><br></pre></td></tr></table></figure>
<p>这时你可以把requirements.txt拷入新配置的Python目录下，执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>则会按照requirements.txt中所写的依赖包和版本依序进行安装。<br>注意：<br>若迁入的系统不同或Python版本不同，在安装过程中可能会因为找不到相应的依赖包版本而报错<br>这时你可以进入requirements.txt把报错的依赖包后的版本信息去掉，保存，重新执行命令即可，它会自动下载匹配的最新版本。<br>也可以把==改成&lt;=，代表它会搜索不大于此版本的最高版本进行安装。<br>由于pip下载源在国外，若无合适的VPN，此期间下载过程会十分漫长，这里提供几个常用的国内镜像源：</p>
<ul>
<li>清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple/</a></li>
<li>中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">http://pypi.mirrors.ustc.edu.cn/simple/</a></li>
<li>阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a></li>
<li>豆瓣 <a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a></li>
<li>华中科技大学 <a href="http://pypi.hustunique.com/" target="_blank" rel="noopener">http://pypi.hustunique.com/</a></li>
</ul>
<p>可以在使用pip的时候，加上参数-i和镜像地址，指定下载源，加速下载过程，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Permmision denied解决方法</title>
    <url>/2019/03/19/Permmision%20denied%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>这是因为要管理员权限的，而ubuntu又不想给普通用户赋予管理员权限。所以这里开启root账号</p>
<h2 id="首先设置-root-密码"><a href="#首先设置-root-密码" class="headerlink" title=" 首先设置 root 密码"></a><a id="more"></a> 首先设置 root 密码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo passwd</span><br></pre></td></tr></table></figure>
<p>会出现以下画面：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[sudo] password for luban:                        &#x2F;&#x2F;输入当前普通用户的密码</span><br><span class="line"></span><br><span class="line">Enter new UNIX password:                          &#x2F;&#x2F;给root设置密码</span><br><span class="line"></span><br><span class="line">Retype new UNIX password:                         &#x2F;&#x2F;确认输入密码</span><br><span class="line"></span><br><span class="line">passwd: password updated successfully</span><br></pre></td></tr></table></figure>
<h2 id="开启root"><a href="#开启root" class="headerlink" title="开启root"></a>开启root</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<h2 id="修改文件权限"><a href="#修改文件权限" class="headerlink" title="修改文件权限"></a>修改文件权限</h2><p>为了获得执行权限，借助chmod指令修改文件权限即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod 777 文件夹或文件路径</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下如何添加新用户并增加管理员权限</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8B%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%94%A8%E6%88%B7%E5%B9%B6%E5%A2%9E%E5%8A%A0%E7%AE%A1%E7%90%86%E5%91%98%E6%9D%83%E9%99%90/</url>
    <content><![CDATA[<h2 id="添加新用户"><a href="#添加新用户" class="headerlink" title=" 添加新用户"></a><a id="more"></a> 添加新用户</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo adduser xxx #xxx为用户名</span><br></pre></td></tr></table></figure>
<p>输入密码后，出现如下信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">正在添加用户&quot;xxx&quot;…</span><br><span class="line">正在添加新组&quot;xxx&quot; (1003)…</span><br><span class="line">正在添加新用户&quot;xxx&quot; (1003) 到组&quot;xxx&quot;…</span><br><span class="line">创建主目录&quot;&#x2F;home&#x2F;xxx&quot;…</span><br><span class="line">正在从&quot;&#x2F;etc&#x2F;skel&quot;复制文件…</span><br><span class="line">输入新的 UNIX 密码：</span><br><span class="line">重新输入新的 UNIX 密码：</span><br><span class="line">passwd：已成功更新密码</span><br><span class="line">正在改变 xxx 的用户信息</span><br><span class="line">请输入新值，或直接敲回车以使用默认值</span><br><span class="line">        全名 [ ]：</span><br><span class="line">        房间号码 [ ]：</span><br><span class="line">        工作电话 [ ]：</span><br><span class="line">        家庭电话 [ ]：</span><br><span class="line">        其它 [ ]：</span><br><span class="line">这些信息是否正确？ [Y&#x2F;n] y</span><br></pre></td></tr></table></figure>
<p>到这里，新用户添加成功。</p>
<h2 id="增加管理员权限"><a href="#增加管理员权限" class="headerlink" title="增加管理员权限"></a>增加管理员权限</h2><p>需要让此用户有root权限，执行命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo nano &#x2F;etc&#x2F;sudoers</span><br></pre></td></tr></table></figure>
<p>往下拉，修改文件并添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xxx ALL&#x3D;(ALL：ALL) ALL #xxx为你之前所创的用户名</span><br></pre></td></tr></table></figure>
<p>添加完成按下Ctrl + X保存并退出，再次按下Enter,完成<br>现在你可以点击右上角，看到你新创的用户，点击并切换了！</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下安装Pycharm</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85Pycharm/</url>
    <content><![CDATA[<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a><a id="more"></a>下载</h2><p>有社区版和专业版，专业版需要激活，社区版免费，我下载的是社区版。<br><a href="https://www.jetbrains.com/pycharm/download/#section=linux" target="_blank" rel="noopener">官方下载</a><br><a href="https://www.jianguoyun.com/p/DfZJG3QQlZ_3Bhj8mrEC" target="_blank" rel="noopener">网盘下载</a></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>进入download目录，对下载好的Pycharm压缩包进行解压操作，选中文件右键提取到此处即可。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D1664510513%2C942693123%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>进入解压后的Pycharm文件夹，进入bin目录，可以看到很多文件，其中有一个文件叫做pycharm.sh。也就是下图中所选中的文件：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D101055722%2C4258535600%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>此时在bin文件夹下右键打开终端，输入运行命令, 执行Pycharm程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;pycharm.sh</span><br></pre></td></tr></table></figure>
<p>如果提示没有权限，大家可以添加sudo命令进行操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo .&#x2F;pycharm.sh</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D805484803%2C1629350165%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>大家点击OK即可进入下一步操作，根据提示的内容进行勾选同意协议，然后点击continue进入下一步，大家可以点击don’t send进入下一步，选择喜欢的界面风格，然后再次点击next进入下一步，直到提示start启动程序。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D227808303%2C3066408894%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D22323540%2C1357101677%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D1914658415%2C2906990210%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>为了方便以后的使用，我们可以将Pycharm的图标（快捷键）锁定到启动器。之后不用再去在终端中启动Pycharm了，直接点击图标启动即可。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D1149108563%2C3867497858%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu 16.04下配置GPU版CUDA和cuDNN</title>
    <url>/2019/03/19/Ubuntu-16.04%E4%B8%8B%E9%85%8D%E7%BD%AEGPU%E7%89%88CUDA%E5%92%8CcuDNN/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>先介绍下我自己配置的环境</p>
<ul>
<li>Ubuntu 16.04</li>
<li>GTX2080ti显卡</li>
<li>NVIDIA 418.3</li>
<li>CUDA 10.0</li>
<li>cuDNN 7.6.2</li>
</ul>
<p>以下教程针对从零开始的用户，若系统中已装有CUDA和cuDNN，却在当前用户下无法使用，请<a href="#添加环境变量">添加环境变量</a></p>
<h2 id="安装NVIDIA显卡驱动"><a href="#安装NVIDIA显卡驱动" class="headerlink" title="安装NVIDIA显卡驱动"></a>安装NVIDIA显卡驱动</h2><p>1.先在<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">NVIDIA官网</a>上下载对应的驱动程序，可根据自己的GPU的型号下载相应的.run文件<br>例如NVIDIA-Linux-x86_64-3xx.xx.run形式的文件名</p>
<p>2.禁用开源nouveau驱动<strong>（非常重要）</strong><br>Ubuntu系统集成的显卡驱动程序是nouveau，它是第三方为NVIDIA开发的开源驱动，我们需要先将其屏蔽才能安装NVIDIA官方驱动。<br>将驱动添加到黑名单blacklist.conf中，但是由于该文件的属性不允许修改。所以需要先修改文件属性。<br>查看属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ls -lh &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br></pre></td></tr></table></figure>
<p>修改属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod 666 &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br></pre></td></tr></table></figure>
<p>用gedit编辑器打开</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br></pre></td></tr></table></figure>
<p>在该文件后添加一下几行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blacklist vga16fb</span><br><span class="line">blacklist nouveau</span><br><span class="line">blacklist rivafb</span><br><span class="line">blacklist rivatv</span><br><span class="line">blacklist nvidiafb</span><br></pre></td></tr></table></figure>
<p>3.开始安装<br>先按Ctrl + Alt + F1到控制台，关闭当前图形环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure>
<p>再安装驱动程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod a+x NVIDIA-Linux-x86_64-xxx.run</span><br><span class="line">sudo .&#x2F;NVIDIA-Linux-x86_64-xxx.run -no-x-check -no-nouveau-check -no-opengl-files</span><br></pre></td></tr></table></figure>
<p>最后重新启动图形环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo service lightdm start</span><br></pre></td></tr></table></figure>
<p>在终端里输入：nvidia-smi ，输出以下图片的代码则安装成功<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CUDA%26cuDNN/2019051413224071.png" alt></p>
<h2 id="cuda-10-0安装"><a href="#cuda-10-0安装" class="headerlink" title="cuda 10.0安装"></a>cuda 10.0安装</h2><p><a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">官方下载</a><br><a href="https://pan.baidu.com/s/1piTbzIIL3wTx1dCeUDDiOw" target="_blank" rel="noopener">网盘下载</a> 提取码: aarn</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod a+x cuda_10.0.130_410.48_linux.run &#x2F;&#x2F; 获取权限</span><br><span class="line">sudo sh cuda_10.0.130_410.48_linux.run --tmpdir&#x3D;&#x2F;home&#x2F;max&#x2F;temp</span><br></pre></td></tr></table></figure>
<p>这里加 –tmpdir 主要是直接运行后，会提示空间不足的问题<br>接下来进入英文选择界面按住空格键可以快速浏览<br>在安装过程中选项选择：<br>accept #同意安装<br>n #不安装Driver，因为已安装驱动<strong>（这里需要强调一下）</strong><br>y #安装CUDA Toolkit<br>#安装到默认目录<br>y #创建安装目录的软链接<br>n #不复制Samples，因为在安装目录下有/samples</p>
<h2 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h2><p>home文件下 ctrl+H显示隐藏文件 打开 .bashrc文件在最后添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.0&#x2F;bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.0&#x2F;lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>终端运行如下命令，保存操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure>
<p>检查cuda是否安装成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CUDA%26cuDNN/20190514132130343.png" alt></p>
<h2 id="cuDnn-10-0安装"><a href="#cuDnn-10-0安装" class="headerlink" title="cuDnn 10.0安装"></a>cuDnn 10.0安装</h2><p><a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">官方下载</a><br><a href="https://www.jianguoyun.com/p/DdS0_A4QlZ_3Bhjw8LEC" target="_blank" rel="noopener">网盘下载</a><br>注意：需跟CUDA版本对应<br>切换到下载目录进行解压：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo tar -zxvf .&#x2F;cudnn-10.0-linux-x64-xxx.tgz</span><br></pre></td></tr></table></figure>
<p>解压下载的文件，可以看到cuda文件夹，在当前目录打开终端，执行如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp cuda&#x2F;include&#x2F;cudnn.h &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;</span><br><span class="line"> </span><br><span class="line">sudo cp cuda&#x2F;lib64&#x2F;libcudnn* &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;</span><br><span class="line"> </span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h</span><br><span class="line"> </span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;libcudnn*</span><br></pre></td></tr></table></figure>
<p>在终端输入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure>
<p>如果出现下图所示版本信息，说明安装成功<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CUDA%26cuDNN/20180815114007852.png" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
        <tag>CUDA</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下安装更新pip</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85%E6%9B%B4%E6%96%B0pip/</url>
    <content><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a><a id="more"></a>安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install python-pip</span><br></pre></td></tr></table></figure>
<p>若同时装有Python2、Python3上条命令装的是pip2，若还需装pip3，则再执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install python3-pip</span><br></pre></td></tr></table></figure>
<h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>安装成功后进行更新：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip install --upgrade pip</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip3 install --upgrade pip</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下如何安装搜狗输入法</title>
    <url>/2019/03/19/sougou/</url>
    <content><![CDATA[<h2 id="Ubuntu系统配置"><a href="#Ubuntu系统配置" class="headerlink" title=" Ubuntu系统配置"></a><a id="more"></a> Ubuntu系统配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">system settings-&gt;language support-&gt;install&#x2F;remove languages</span><br></pre></td></tr></table></figure>
<p>在弹出的菜单中选择Chinese(simplified),点击apply<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927736.png" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927712.png" alt></p>
<h2 id="配置输入法框架"><a href="#配置输入法框架" class="headerlink" title="配置输入法框架"></a>配置输入法框架</h2><p>搜狗输入法是建立在fcitx框架之上的，所以要将输入法框架选择为fictx<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927666.png" alt><br>注意：如果没有fcitx选项，那么你就需要安装fcitx框架之后在进行配置，安装方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:fcitx-team&#x2F;nightly   &#x2F;&#x2F;添加FCITX仓库</span><br><span class="line"></span><br><span class="line">sudo apt-get update                              &#x2F;&#x2F;更新仓库</span><br><span class="line"></span><br><span class="line">sudo apt-get install fcitx                       &#x2F;&#x2F;安装fcitx输入法框架</span><br></pre></td></tr></table></figure>
<p>配置好输入法框架之后，重启ubuntu系统。重启之后如果配置成功，在任务栏的右上角会出现fcitx的设置选项（一个小键盘图标）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927651.png" alt></p>
<h2 id="去搜狗官网下载输入法for-Linux"><a href="#去搜狗官网下载输入法for-Linux" class="headerlink" title="去搜狗官网下载输入法for Linux"></a>去搜狗官网下载输入法for Linux</h2><p><a href="https://pinyin.sogou.com/linux/" target="_blank" rel="noopener">官方下载</a><br><a href="https://www.jianguoyun.com/p/DeShNo0QlZ_3Bhic5rAC" target="_blank" rel="noopener">网盘下载</a><br>下载完成之后，在download目录下找到下载的文件，双击安装即可，点击install即可<br>或者在终端的命令窗口中输入如下的指令安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i 安装包名称.deb</span><br></pre></td></tr></table></figure>
<p>如果在安装过程中出现相关依赖文件的错误。则需要先安装其依赖的软件包<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/Snipaste_2019-12-13_14-33-14.jpg" alt><br>在终端窗口来执行以下命令，安装缺少的依赖文件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get -f install</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/Snipaste_2019-12-13_14-33-49.jpg" alt><br>安装完成后，首先打开右上角系统设置，选择第一行的最后一个选项”Text Entry“，点击左下角的+号，在打开的窗口中找到搜狗输入法Sogou pinyin点击Add添加进去<br>仅显示当前语言一定要去掉那个勾，才可以找到搜狗输入法，然后添加就是<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/2019050512074094.png" alt><br>这时就可以在右上方小键盘选择搜狗输入法了！</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu16.04下Tensorflow-gpu安装</title>
    <url>/2019/03/19/Ubuntu16.04%E4%B8%8BTensorflow-gpu%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>在开始之前，请先确定安装好显卡驱动、CUDA、cuDNN，可以参考我的<a href="https://qiyuan-z.github.io/2019/03/19/Ubuntu-16.04下配置GPU版CUDA和cuDNN/">另一篇博客</a></p>
<a id="more"></a>
<p>1.安装Numpy<br>推荐安装1.16.0版本，若之前已安装，请先卸载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip uninstall numpy</span><br></pre></td></tr></table></figure>
<p>然后执行安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install numpy&#x3D;&#x3D;1.16.0</span><br></pre></td></tr></table></figure>
<p>2.安装Tensorflow-gpu<br>推荐安装1.14.0版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install tensorflow-gpu&#x3D;&#x3D;1.14.0</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下virtualenv的使用与pycharm的基本配置</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8Bvirtualenv%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8Epycharm%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="安装virtualenv"><a href="#安装virtualenv" class="headerlink" title="安装virtualenv"></a><a id="more"></a>安装virtualenv</h2><p>virtualenv是 Python 多版本管理的利器，不同版本的开发调试全靠它了（没有多版本尽量也装上吧）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip install virtualenv</span><br></pre></td></tr></table></figure>
<h2 id="创建一个virtualenv环境"><a href="#创建一个virtualenv环境" class="headerlink" title="创建一个virtualenv环境"></a>创建一个virtualenv环境</h2><p>使用如下语句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">virtualenv + 路径</span><br></pre></td></tr></table></figure>
<p>以这种方式创建环境将不包含系统的python包，新的环境里面只有pip、setuptools和wheel这些包，则许多包要用pip重新安装。</p>
<p>若需指定python版本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">virtualenv –p python3 + 路径</span><br></pre></td></tr></table></figure>
<h2 id="激活virtualenv环境"><a href="#激活virtualenv环境" class="headerlink" title="激活virtualenv环境"></a>激活virtualenv环境</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source 路径&#x2F;bin&#x2F;activate</span><br></pre></td></tr></table></figure>
<p>也可以直接进到所创环境的bin目录中右键终端，运行source activate<br>注意：激活只对当前终端有效，如果新打开了一个终端的话，重新运行上面的命令。 激活后终端前面会多一个(**)的东西，提示当前virtualenv的名称。</p>
<p>激活后可以在当前终端通过python 文件名.py的方式运行python脚本，如果脚本中使用了当前环境中没有的包，将会报错。</p>
<p>可以在激活环境后使用pip安装对应的包。<strong>注意不要使用sudo</strong>，否则包会安装到系统当中去，而不是当前的virtualenv目录中。</p>
<h2 id="退出virtualenv环境"><a href="#退出virtualenv环境" class="headerlink" title="退出virtualenv环境"></a>退出virtualenv环境</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure>
<p>也可直接关闭当前终端。</p>
<h2 id="删除virtualenv环境"><a href="#删除virtualenv环境" class="headerlink" title="删除virtualenv环境"></a>删除virtualenv环境</h2><p>直接删除对应目录即可删除virtualenv环境，不会对系统产生任何影响，所以在virtualenv中可以放心操作。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -rvf  + 路径</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafafdaa2aca.webp" alt></p>
<h2 id="Pycharm配置"><a href="#Pycharm配置" class="headerlink" title="Pycharm配置"></a>Pycharm配置</h2><p>新建项目<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafb0d3811f2.webp" alt></p>
<ul>
<li><p>New environment using Virtualenv: 将在项目的目录下创建一个virtualenv环境，然后使用它当作当前项目的python解释器，默认不包含系统的python包。<br>相当于： virtualenv + 路径</p>
</li>
<li><p>location:为新建的环境的位置，默认为当前工程下的venv。</p>
</li>
<li><p>Base interpreter:基于系统中的python版本，新建的环境中的python版本与此一致，可以选择python2或者python3, 取决于项目的需要，相当于virtualenv –p python版本 +路径。<br>勾选Inherit global site-packages，包含系统的python包，相当于： virtualenv –system-site-packages + 路径<br>勾选Make available to all projects，下次新建项目的时候会在Existing interpreter中找到这个环境， 可以重复使用这个环境。</p>
</li>
<li><p>Existing interpreter：使用已有的python环境，点击后会出现后面的设置会出现这个界面，分别是virtualenv, conda和系统的python环境。可以选择已有的virtualenv环境，或者直接使用系统的python解释器。 Conda是anaconda(一个科学计算的python发行版)的包管理器，也可以用来建立python环境。</p>
</li>
</ul>
<p>会发现生成的项目中有一个叫venv的文件夹，它实质上和直接用virtualenv创建的一样。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafb2bd99f29.webp" alt><br>可以用virtualenv的管理方法管理它，比如安装numpy，安装之后可以在pycharm正常使用。（注意在virtualenv中<strong>不要使用sudo</strong>）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafb2bdb011c.webp" alt><br>也可以在pycharm中使用 file-settings-project-project interpreter中管理环境中的python包，可以对该环境下的python包进行删除和安装。</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
        <tag>virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵分析（刘丁酉）课后答案及历年试卷</title>
    <url>/2019/03/19/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90%EF%BC%88%E5%88%98%E4%B8%81%E9%85%89%EF%BC%89%E8%AF%BE%E5%90%8E%E7%AD%94%E6%A1%88%E5%8F%8A%E5%8E%86%E5%B9%B4%E8%AF%95%E5%8D%B7/</url>
    <content><![CDATA[<p>考虑到刘丁酉版本只有部分习题的解答，还不够详细，于是我在学习过程中，将每道习题做了一遍，并记录了下来，希望能对学习矩阵分析的同学给予一定的参考。由于本人仍在学习阶段，难免答案会有做错的地方，希望大家不要尽信答案，只当作一个思路上的参考来看。顺便附赠历年的卷子与答案（看了一遍历年的卷子，答案有错!@-@）</p>
<a id="more"></a>  
<p>下载地址：<a href="https://www.jianguoyun.com/p/DQe8_9kQlZ_3Bhid4LYC" target="_blank" rel="noopener">点击下载</a></p>
]]></content>
      <categories>
        <category>资源分享</category>
      </categories>
      <tags>
        <tag>矩阵分析</tag>
      </tags>
  </entry>
  <entry>
    <title>如何解决出现 unable to resolve host 问题</title>
    <url>/2019/03/19/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%87%BA%E7%8E%B0-unable-to-resolve-host-%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>Ubuntu环境，有时候执行sudo 就出现这个警告讯息: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo: unable to resolve host abc</span><br></pre></td></tr></table></figure>
<p>虽然sudo 还是可以正常执行, 但是看到这样的通知还是会觉得烦，怎么去除这个警告呢？</p>
<p>这个警告是因为系统找不到一个叫做 abc的hostname </p>
<p>通过 修改 /etc/hosts 设定, 可以解决</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nano &#x2F;etc&#x2F;hostname</span><br></pre></td></tr></table></figure>

<p>在127.0.0.1 localhost 后面加上主机名称(hostname) 即可:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1 localhost abc</span><br></pre></td></tr></table></figure>
<p>问题解决！</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu下 teamviewer的安装方法</title>
    <url>/2019/03/19/ubuntu%E4%B8%8B-teamviewer%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="去官网下载安装包"><a href="#去官网下载安装包" class="headerlink" title=" 去官网下载安装包"></a><a id="more"></a> 去官网下载安装包</h2><p><a href="https://www.teamviewer.com/zhcn/download/linux/" target="_blank" rel="noopener">官网链接</a><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/teamviewer/20180128120840387.png" alt></p>
<h2 id="在命令行进行安装"><a href="#在命令行进行安装" class="headerlink" title="在命令行进行安装"></a>在命令行进行安装</h2><p>（在下载文件夹下打开命令行，输入：sudo dpkg -i  teamviewer_13.0.6634_amd64.deb）（teamviewer_13.0.6634_amd64.deb为安装包名，根据自己安装包）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i  teamviewer_13.0.6634_amd64.deb</span><br></pre></td></tr></table></figure>
<h2 id="安装出错，一行语句搞定依赖关系"><a href="#安装出错，一行语句搞定依赖关系" class="headerlink" title="安装出错，一行语句搞定依赖关系"></a>安装出错，一行语句搞定依赖关系</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/teamviewer/20180128120923823.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install -f</span><br></pre></td></tr></table></figure>
<h2 id="再次安装"><a href="#再次安装" class="headerlink" title="再次安装"></a>再次安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i  teamviewer_13.0.6634_amd64.deb</span><br></pre></td></tr></table></figure>
<h2 id="打开teamview"><a href="#打开teamview" class="headerlink" title="打开teamview"></a>打开teamview</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/teamviewer/20180128121156354.png" alt><br>或用命令行启动：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo teamviewer --daemon start</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Teamviewer</tag>
      </tags>
  </entry>
</search>
