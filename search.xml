<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>盘点不同类型的池化层、1x1卷积的作用和卷积核是否一定越大越好？</title>
    <url>/2020/03/03/%E7%9B%98%E7%82%B9%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%B1%A0%E5%8C%96%E5%B1%82%E3%80%811x1%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E6%98%AF%E5%90%A6%E4%B8%80%E5%AE%9A%E8%B6%8A%E5%A4%A7%E8%B6%8A%E5%A5%BD%EF%BC%9F/</url>
    <content><![CDATA[<h2 id="池化层的不同类型"><a href="#池化层的不同类型" class="headerlink" title=" 池化层的不同类型"></a><a id="more"></a> 池化层的不同类型</h2><p>池化通常也被称为下采样(Downsampling)，一般是用在卷积层之后，通过池化来降低卷积层输出特征图的维度，有效减少网络参数的同时还可以防止过拟合现象。池化层实际上真正起作用的地方在于他的非线性映射能力和可以保持一定量的平移不变性的能力。这个能力是因为在一个图像区域有用的特征很有可能在另一个区域同样有用。因此，为了描述一个大分辨率的图像特征，一个直观的方法就是对大分辨率图像中的不同位置的特征进行聚合统计。具体来说，常见池化的方法有以下几种：</p>
<ul>
<li>标准池化</li>
<li>重叠池化</li>
<li>空间金字塔池化</li>
</ul>
<p>接下来我们就仔细介绍一下每种池化。</p>
<h2 id="标准池化"><a href="#标准池化" class="headerlink" title="标准池化"></a>标准池化</h2><p>通常包括最大池化(MaxPooling)和平均池化(AveragePooling)。以最大池化为例，池化核尺寸为2 x 2，池化步长为2，可以看到特征图中的每一个像素点只会参与一次特征提取工作。这个过程可以用下图表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pooling/640.webp" alt></p>
<h2 id="重叠池化"><a href="#重叠池化" class="headerlink" title="重叠池化"></a>重叠池化</h2><p>操作和标准池化相同，但唯一不同地方在于滑动步长stride小于池化核的尺寸k，可以想象到这样的话特征图中的某些区域会参与到多次特征提取工作，最后得到的特征表达能力更强。这个过程可以表示为下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pooling/641.webp" alt></p>
<h2 id="空间金字塔池化"><a href="#空间金字塔池化" class="headerlink" title="空间金字塔池化"></a>空间金字塔池化</h2><p>这种池化是在进行多尺度训练的时候，第一次看到这种操作的时候是读目标检测算法之Fast RCNN算法的时候。对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同($w i n_{s i z e}=\left\lceil\frac{i n}{o u t}\right\rceil$;$stride =\left\lfloor\frac{i n}{o u t}\right\rfloor$)，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取。空间金字塔池化可以用下图表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pooling/642.gif" alt></p>
<h2 id="1-1卷积的作用"><a href="#1-1卷积的作用" class="headerlink" title="1*1卷积的作用"></a>1*1卷积的作用</h2><p>我最开始接触到1 x 1卷积应该是在阅读经典论文GoogleNet的时候，当然我说的是我第一次接触，并不代表GoogleNet(包含了InceptionV1-V4)是第一个使用卷积的。在InceptionV1网络中，Inception模块长下面这样：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pooling/643.webp" alt></p>
<p>可以看到这个Inception模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。这时候为了减少模型参数量，在每一个较大卷积核的卷积层前引入1 x 1卷积，将宽高和通道方向的卷积进行了分离。修改后的Inception模块表示为下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pooling/644.webp" alt></p>
<p>这样就实现了在不影响模型的特征表达能力条件下大大减少了模型的参数量。我们定量计算一下上下两个Inception模块的参数量，假设输入$C_1$输出$C_2$通道数都是16，则原始的Inception模块的参数量是：$(1 \times 1+3 \times 3+5 \times 5+0) \times C_{1} \times C_{2}=8960$，而使用了1 x 1卷积的Inception模块的参数量为：$\left(1 \times 1 \times\left(3 C_{1}+C_{2}\right)+3 \times 3 \times C_{2}+5 \times 5 \times C_{2}\right) \times C_{1}=5248$，可以看到计算量减少了接近一半。</p>
<p>因此，1 x 1卷积的作用可以总结为可以实现信息的通道整合和交互，以及具有升维/降维的能力。</p>
<h2 id="卷积核是否越大越好？"><a href="#卷积核是否越大越好？" class="headerlink" title="卷积核是否越大越好？"></a>卷积核是否越大越好？</h2><p>这是本文的最后一个问题，显然这个问题我们肯定会回答否。但你是否真的认真思考过其中的原因？在早期的一些经典网络中如Lenet-5和AlexNet，用到了一些大的卷积核例如11 x 11，5 x 5，受限于当时的计算资源，无法将网络堆叠得很深，因此需要将卷积核设得比较大以获得更大的感受野。但这种大卷积核导致计算量大幅增加，训练过程缓慢，更不利于训练深层模型。后来VGGNet，GoogleNet时代发现通过堆叠2个3 x 3卷积核可以获得和5 x 5卷积核相同的感受野，同时参数量也会减少，如$2 \times 3 \times 3&lt;5 \times 5$。因此，3 x 3卷积核被广泛应用在许多卷积神经网络中。所以基本可以认为在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核更加有效并且能获得计算资源节约。因此我们可以认为，CV领域小卷积核堆叠是好于大卷积核的。</p>
<p>那么是不是其他领域也是这样呢？并不是。在NLP领域，由于文本内容不像图像数据一样可以对特征进行很深层的抽象，因此该领域的特征提取网络都是比较浅的。这个时候为了获得较大的感受野，就需要使用大的卷积核。因此，我们可以认为在NLP领域大卷积是好于小卷积核的。</p>
<p>总结一下，卷积核是否越大越好呢？这个要具体问题具体分析，在不同的领域大卷积核和小卷积核分别能取得不错的效果。并且在设置卷积核的时候一个常识是不能设得过大也不能过小，1 x 1卷积只适合做分离卷积任务而不能对输入的原始特征做有效的特征抽取，而极大的卷积核通常会组合过多无用的特征浪费大量的计算资源。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>NMS后处理相关</title>
    <url>/2020/03/03/NMS%E5%90%8E%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title=" 介绍"></a><a id="more"></a> 介绍</h2><p>非极大值抑制(Non-Maximum  Suppression，NMS)，顾名思义就是抑制不是极大值的元素。在目标检测任务，例如行人检测中，滑动窗口经过特征提取和分类器识别后，每个窗口都会得到一个分数。但滑动窗口会导致很多窗口和其它窗口存在包含大部分交叉的情况。这个时候就需要用到NMS来选取那些邻域里分数最高，同时抑制那些分数低的窗口。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>在目标检测任务中，定义最后的候选框集合为$B$，每个候选框对应的置信度是$S$，IOU阈值设为$T$，然后NMS的算法过程可以表示如下：</p>
<ul>
<li><p>选择具有最大score的候选框$M$</p>
</li>
<li><p>将$M$从集合$B$中移除并加入到最终的检测结果$D$中</p>
</li>
<li><p>将$B$中剩余检测框中和$M$的交并比(IOU)大于阈值$T$的框从$B$中移除</p>
</li>
<li><p>重复上面的步骤，直到$B$为空</p>
</li>
</ul>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>rgb大神实现Faster-RCNN中的单类别物体nms代码解释如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> --------------------------------------------------------</span><br><span class="line"># Fast R-CNN</span><br><span class="line"># Copyright (c) 2015 Microsoft</span><br><span class="line"># Licensed under The MIT License [see LICENSE for details]</span><br><span class="line"># Written by Ross Girshick</span><br><span class="line"># --------------------------------------------------------</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def py_cpu_nms(dets, thresh):</span><br><span class="line">    &quot;&quot;&quot;Pure Python NMS baseline.&quot;&quot;&quot;</span><br><span class="line">    #x1、y1、x2、y2、以及score赋值</span><br><span class="line">    x1 &#x3D; dets[:, 0]</span><br><span class="line">    y1 &#x3D; dets[:, 1]</span><br><span class="line">    x2 &#x3D; dets[:, 2]</span><br><span class="line">    y2 &#x3D; dets[:, 3]</span><br><span class="line">    scores &#x3D; dets[:, 4]</span><br><span class="line">	#每一个检测框的面积</span><br><span class="line">    areas &#x3D; (x2 - x1 + 1) * (y2 - y1 + 1)</span><br><span class="line">    #按照score置信度降序排序</span><br><span class="line">    order &#x3D; scores.argsort()[::-1]</span><br><span class="line">    #保留的结果框集合</span><br><span class="line">    keep &#x3D; []</span><br><span class="line">    while order.size &gt; 0:</span><br><span class="line">        i &#x3D; order[0]</span><br><span class="line">        #保留该类剩余box中得分最高的一个</span><br><span class="line">        keep.append(i)</span><br><span class="line">        # 得到相交区域,左上及右下</span><br><span class="line">        xx1 &#x3D; np.maximum(x1[i], x1[order[1:]])</span><br><span class="line">        yy1 &#x3D; np.maximum(y1[i], y1[order[1:]])</span><br><span class="line">        xx2 &#x3D; np.minimum(x2[i], x2[order[1:]])</span><br><span class="line">        yy2 &#x3D; np.minimum(y2[i], y2[order[1:]])</span><br><span class="line">	    #计算相交的面积,不重叠时面积为0</span><br><span class="line">        w &#x3D; np.maximum(0.0, xx2 - xx1 + 1)</span><br><span class="line">        h &#x3D; np.maximum(0.0, yy2 - yy1 + 1)</span><br><span class="line">        inter &#x3D; w * h</span><br><span class="line">        #计算IoU：重叠面积 &#x2F;（面积1+面积2-重叠面积）</span><br><span class="line">        ovr &#x3D; inter &#x2F; (areas[i] + areas[order[1:]] - inter)</span><br><span class="line">		# 保留IoU小于阈值的box</span><br><span class="line">        inds &#x3D; np.where(ovr &lt;&#x3D; thresh)[0]</span><br><span class="line">        # 因为ovr数组的长度比order数组少一个,所以这里要将所有下标后移一位</span><br><span class="line">        order &#x3D; order[inds + 1]</span><br><span class="line"></span><br><span class="line">	return keep</span><br></pre></td></tr></table></figure>
<h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/nms/640.webp" alt></p>
<h2 id="Soft-NMS"><a href="#Soft-NMS" class="headerlink" title="Soft-NMS"></a>Soft-NMS</h2><p>上面说的NMS算法有一个缺点就是当两个候选框的重叠度很高时，NMS会将具有较低置信度的框去掉，也就是将其置信度变成0，如下图所示，红色框和绿色框是当前的检测结果，二者的得分分别是0.95和0.80。如果按照传统的NMS进行处理，首先选中得分最高的红色框，然后绿色框就会因为与之重叠面积过大而被删掉。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/nms/641.webp" alt></p>
<p>因此为了改善这个缺点，Soft-NMS被提出，核心思路就是不要粗鲁地删除所有IOU大于阈值的框，而是降低其置信度。这个方法的论文地址为：<a href="https://arxiv.org/pdf/1704.04503.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04503.pdf</a> 。算法伪代码如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/nms/642.jpg" alt></p>
<p>正如作者所说，改一行代码就OK了。这里的$f$函数可以是线性函数，也可以是高斯函数。我们来对比一下：</p>
<ul>
<li><p>线性函数：</p>
<script type="math/tex; mode=display">
s_{i}=\left\{\begin{array}{ll}s_{i}, & \text { iou }\left(\mathcal{M}, b_{i}\right)<N_{t} \\ s_{i}\left(1-\mathrm{i} o \mathrm{u}\left(\mathcal{M}, b_{i}\right)\right), & \text { iou }\left(\mathcal{M}, b_{i}\right) \geq N_{t}\end{array}\right.</script></li>
<li><p>高斯函数：</p>
<script type="math/tex; mode=display">
\boldsymbol{s}_{i}=\boldsymbol{s}_{i} e^{\frac{-\mathrm{iou}\left(\mathcal{M}, \boldsymbol{b}_{i}\right)^{2}}{\sigma}}, \forall b_{i} \notin \mathcal{D}</script></li>
</ul>
<h2 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h2><p>作者的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def cpu_soft_nms(np.ndarray[float, ndim&#x3D;2] boxes, float sigma&#x3D;0.5, float Nt&#x3D;0.3, float threshold&#x3D;0.001, unsigned int method&#x3D;0):</span><br><span class="line">    cdef unsigned int N &#x3D; boxes.shape[0]</span><br><span class="line">    cdef float iw, ih, box_area</span><br><span class="line">    cdef float ua</span><br><span class="line">    cdef int pos &#x3D; 0</span><br><span class="line">    cdef float maxscore &#x3D; 0</span><br><span class="line">    cdef int maxpos &#x3D; 0</span><br><span class="line">    cdef float x1,x2,y1,y2,tx1,tx2,ty1,ty2,ts,area,weight,ov</span><br><span class="line"></span><br><span class="line">    for i in range(N):</span><br><span class="line">        maxscore &#x3D; boxes[i, 4]</span><br><span class="line">        maxpos &#x3D; i</span><br><span class="line"></span><br><span class="line">        tx1 &#x3D; boxes[i,0]</span><br><span class="line">        ty1 &#x3D; boxes[i,1]</span><br><span class="line">        tx2 &#x3D; boxes[i,2]</span><br><span class="line">        ty2 &#x3D; boxes[i,3]</span><br><span class="line">        ts &#x3D; boxes[i,4]</span><br><span class="line"></span><br><span class="line">        pos &#x3D; i + 1</span><br><span class="line">    # get max box</span><br><span class="line">        while pos &lt; N:</span><br><span class="line">            if maxscore &lt; boxes[pos, 4]:</span><br><span class="line">                maxscore &#x3D; boxes[pos, 4]</span><br><span class="line">                maxpos &#x3D; pos</span><br><span class="line">            pos &#x3D; pos + 1</span><br><span class="line"></span><br><span class="line">    # add max box as a detection</span><br><span class="line">        boxes[i,0] &#x3D; boxes[maxpos,0]</span><br><span class="line">        boxes[i,1] &#x3D; boxes[maxpos,1]</span><br><span class="line">        boxes[i,2] &#x3D; boxes[maxpos,2]</span><br><span class="line">        boxes[i,3] &#x3D; boxes[maxpos,3]</span><br><span class="line">        boxes[i,4] &#x3D; boxes[maxpos,4]</span><br><span class="line"></span><br><span class="line">    # swap ith box with position of max box</span><br><span class="line">        boxes[maxpos,0] &#x3D; tx1</span><br><span class="line">        boxes[maxpos,1] &#x3D; ty1</span><br><span class="line">        boxes[maxpos,2] &#x3D; tx2</span><br><span class="line">        boxes[maxpos,3] &#x3D; ty2</span><br><span class="line">        boxes[maxpos,4] &#x3D; ts</span><br><span class="line"></span><br><span class="line">        tx1 &#x3D; boxes[i,0]</span><br><span class="line">        ty1 &#x3D; boxes[i,1]</span><br><span class="line">        tx2 &#x3D; boxes[i,2]</span><br><span class="line">        ty2 &#x3D; boxes[i,3]</span><br><span class="line">        ts &#x3D; boxes[i,4]</span><br><span class="line"></span><br><span class="line">        pos &#x3D; i + 1</span><br><span class="line">    # NMS iterations, note that N changes if detection boxes fall below threshold</span><br><span class="line">        while pos &lt; N:</span><br><span class="line">            x1 &#x3D; boxes[pos, 0]</span><br><span class="line">            y1 &#x3D; boxes[pos, 1]</span><br><span class="line">            x2 &#x3D; boxes[pos, 2]</span><br><span class="line">            y2 &#x3D; boxes[pos, 3]</span><br><span class="line">            s &#x3D; boxes[pos, 4]</span><br><span class="line"></span><br><span class="line">            area &#x3D; (x2 - x1 + 1) * (y2 - y1 + 1)</span><br><span class="line">            iw &#x3D; (min(tx2, x2) - max(tx1, x1) + 1)</span><br><span class="line">            if iw &gt; 0:</span><br><span class="line">                ih &#x3D; (min(ty2, y2) - max(ty1, y1) + 1)</span><br><span class="line">                if ih &gt; 0:</span><br><span class="line">                    ua &#x3D; float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)</span><br><span class="line">                    ov &#x3D; iw * ih &#x2F; ua #iou between max box and detection box</span><br><span class="line"></span><br><span class="line">                    if method &#x3D;&#x3D; 1: # linear</span><br><span class="line">                        if ov &gt; Nt:</span><br><span class="line">                            weight &#x3D; 1 - ov</span><br><span class="line">                        else:</span><br><span class="line">                            weight &#x3D; 1</span><br><span class="line">                    elif method &#x3D;&#x3D; 2: # gaussian</span><br><span class="line">                        weight &#x3D; np.exp(-(ov * ov)&#x2F;sigma)</span><br><span class="line">                    else: # original NMS</span><br><span class="line">                        if ov &gt; Nt:</span><br><span class="line">                            weight &#x3D; 0</span><br><span class="line">                        else:</span><br><span class="line">                            weight &#x3D; 1</span><br><span class="line"></span><br><span class="line">                    boxes[pos, 4] &#x3D; weight*boxes[pos, 4]</span><br><span class="line"></span><br><span class="line">            # if box score falls below threshold, discard the box by swapping with last box</span><br><span class="line">            # update N</span><br><span class="line">                    if boxes[pos, 4] &lt; threshold:</span><br><span class="line">                        boxes[pos,0] &#x3D; boxes[N-1, 0]</span><br><span class="line">                        boxes[pos,1] &#x3D; boxes[N-1, 1]</span><br><span class="line">                        boxes[pos,2] &#x3D; boxes[N-1, 2]</span><br><span class="line">                        boxes[pos,3] &#x3D; boxes[N-1, 3]</span><br><span class="line">                        boxes[pos,4] &#x3D; boxes[N-1, 4]</span><br><span class="line">                        N &#x3D; N - 1</span><br><span class="line">                        pos &#x3D; pos - 1</span><br><span class="line"></span><br><span class="line">            pos &#x3D; pos + 1</span><br><span class="line"></span><br><span class="line">    keep &#x3D; [i for i in range(N)]</span><br><span class="line">    return keep</span><br></pre></td></tr></table></figure>
<h2 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/nms/643.webp" alt></p>
<p>左边是使用了NMS的效果，右边是使用了Soft-NMS的效果</p>
<h2 id="论文的实验结果"><a href="#论文的实验结果" class="headerlink" title="论文的实验结果"></a>论文的实验结果</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/nms/644.jpg" alt></p>
<p>可以看到在MS-COCO数据集上mAP[0.5:0.95]可以获得大约1%的提升，如果应用到训练阶段的proposal选取过程理论上也能获得提升。顺便说一句，soft-NMS在不是基于Proposal的方法如SSD，YOLO中没什么提升。这里猜测原因可能是因为YOLO和SSD产生的框重叠率较低引起的。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>FPN</title>
    <url>/2020/03/02/FPN/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title=" 背景"></a><a id="more"></a> 背景</h2><p>Faster-RCNN选取一个特征提取网络如VGG16做backbone，然后在高层特征（如VGG16后面的conv4）接RPN和检测头进行网络。正是由于Faster-RCNN基于图像的高级特征，这就导致对小目标的检测效果很差。而CV领域常用的处理尺度问题的办法就是特征金字塔，将原图以不同的比例采样，然后得到不同分辨率的图像进行训练和测试，在多数情况下确实是有效的。但是特征金字塔的时间开销非常大，导致在工程中应用是及其困难。FPN从新的角度出发提出了一个独特的特征金字塔网络来避免图像金字塔产生的超高计算量，同时可以较好的处理目标检测中的尺度变化问题，对小目标检测更鲁棒，同时在VOC和COCO数据集上MAP值均超过了Faster-RCNN。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>使用下图来阐释是如何处理尺度变化大的物体检测的。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/FPN/640.webp" alt></p>
<ul>
<li>上图(a)是处理这类问题最常用的方法，即特征金字塔，这种方法在传统的手动设计特征的方法中非常常用，例如DPM方法使用了接近10种不同的尺度获得了不错的效果。</li>
<li>上图(b)是在CNN提出之后出现的，因为神经网络模型对物体尺度本身有一定的鲁棒性，所以也取得了不错的性能，但最近的研究表明将特征金字塔和CNN结合仍可以提升性能，这说明基于单层特征的检测系统仍存在对尺度变化敏感的缺点。</li>
<li>上图(c)表示除了使用图像金字塔，我们可以使用深度学习本身的多层次结构来提取多尺度特征。最常见的就是SSD算法中利用多个特征层来分别做预测。但这种方式也有一些缺点就是浅层的语义特征比较弱，在处理小物体时表现得不够好。</li>
<li>上图(d)表示本文提出的FPN（Feature Pyramid Network ），它能较好的让各个不同尺度的特征都具有较强的语义信息。FPN结合Faster  RCNN可以在COCO物体检测比赛中取得当前单模型的最佳性能（SOTA）。另外，通过对比实验发现，FPN能让Faster  RCNN中的RPN网络的召回率提高8个点；并且它也能使Fast RCNN的检测性能提升2.3个点（COCO）和3.8个点（VOC）。</li>
</ul>
<h2 id="FPN结构"><a href="#FPN结构" class="headerlink" title="FPN结构"></a>FPN结构</h2><p>下图表示FPN的整体结构：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/FPN/641.webp" alt></p>
<p>我们可以看到FPN的整体结构分为<strong>自底向上</strong>和<strong>自顶向下和侧向连接</strong>的过程。接下来我们分别解释一下这两个关键部分。</p>
<h3 id="自底向上"><a href="#自底向上" class="headerlink" title="自底向上"></a>自底向上</h3><p>这一部分就是普通的特征提取网络，特征分辨率不断缩小，容易想到这个特征提取网络可以换成任意Backbone，并且CNN网络一般都是按照特征图大小分为不同的stage，每个stage的特征图长宽差距为2倍。在这个自底向上的结构中，一个stage对应特征金字塔的一个level。以我们要用的ResNet为例，选取conv2、conv3、conv4、conv5层的最后一个残差block层特征作为FPN的特征，记为{C2、C3、C4、C5}，也即是FPN网络的4个级别。这几个特征层相对于原图的步长分别为4、8、16、32。</p>
<h3 id="自上向下和侧向连接"><a href="#自上向下和侧向连接" class="headerlink" title="自上向下和侧向连接"></a>自上向下和侧向连接</h3><p>自上向下是特征图放大的过程，我们一般采用上采样来实现。FPN的巧妙之处就在于从高层特征上采样既可以利用顶层的高级语义特征（有助于分类）又可以利用底层的高分辨率信息（有助于定位）。上采样可以使用插值的方式实现。为了将高层语义特征和底层的精确定位能力结合，论文提出了类似于残差结构的侧向连接。连接将上一层经过上采样后和当前层分辨率一致的特征，通过相加的方法进行融合。同时为了保持所有级别的特征层通道数都保持一致，这里使用1*1卷积来实现。在网上看到一张图，比较好的解释了这个过程：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/FPN/642.webp" alt></p>
<p>FPN只是一个特征金字塔结构，需要配合其他目标检测算法才能使用。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="1-FPN对RPN网络的影响"><a href="#1-FPN对RPN网络的影响" class="headerlink" title="1.FPN对RPN网络的影响"></a>1.FPN对RPN网络的影响</h3><p>如下表所示，论文做了6个实验。</p>
<ul>
<li>(a)基于conv4的RPN，原始的RPN。</li>
<li>(b)基于conv5的RPN。</li>
<li>(c) 完整FPN。</li>
<li>(d)只用了自底向上的多层特征，没有自顶向下的特征。</li>
<li>(e)用了自顶向下的特征，但不用侧向连接。</li>
<li>(f)用了自顶向下的特征，也用了横向特征融合，但只用最后的P2做预测。（完整的预测是使用每一个level的特征$P_{k}$做预测）</li>
</ul>
<p>分析表格可知，自顶向下的特征、横向连接、尺度分离、多个层次的预测是提升FPN性能的关键。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/FPN/643.webp" alt></p>
<p>为了更好的理解，放一张Faster-RCNN结合FPN的细致结构图如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/FPN/644.webp" alt></p>
<h2 id="2-FPN对Fast-RCNN的影响"><a href="#2-FPN对Fast-RCNN的影响" class="headerlink" title="2.FPN对Fast RCNN的影响"></a>2.FPN对Fast RCNN的影响</h2><p>使用和实验1相同的规则对Fast RCNN做了实验，结果如下表所示。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/FPN/645.png" alt></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了一种简单、有效的建立特征金字塔的方式。它的使用对RPN方法和Fast/Faster RCNN方法都有极大的性能提升。另外，它的训练和测试时间和普通的Faster RCNN方法相差很小。因此，它可以作为图像特征金字塔的一种较好的替代。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3及YOLOv3-Tiny</title>
    <url>/2020/03/02/YOLOv3%E5%8F%8AYOLOv3-Tiny/</url>
    <content><![CDATA[<h2 id="算法原理"><a href="#算法原理" class="headerlink" title=" 算法原理"></a><a id="more"></a> 算法原理</h2><p>YOLOv3应该是现在YOLO系列应用的最广泛的算法了，基本就很少有人做工程还用V2了。而YOLOv3的算法原理也很简单，就引入了2个东西，一个是残差模型，一个是FPN架构。</p>
<h2 id="残差模型Darknet-53"><a href="#残差模型Darknet-53" class="headerlink" title="残差模型Darknet-53"></a>残差模型Darknet-53</h2><p>YOLOv3在YOLOv2提出的Darknet-19的基础上引入了残差模块，并进一步加深了网络，改进后的网络有53个卷积层，命名为Darknet-53，网络结构如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-e65c08c61bfaa7c7.png" alt></p>
<p>同时为了说明Darknet-53的有效性，作者给出了在TitanX上，使用相同的条件将256 x 256的图片分别输入到以Darknet-19，Resnet-101，以及Resnet-152以及Darknet-53为基础网络的分类模型总，实验结果如下表：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv3/640.png" alt></p>
<p>从结果来看，Darknet-53比ResNet-101的性能更好，而且速度是其1.5倍，Darknet-53与ResNet-152性能相似但速度几乎是其2倍。同时，Darknet-53相比于其它网络结构实现了每秒最高的浮点数计算量，说明其网络结构可以更好的利用GPU。</p>
<h2 id="YOLOV3结构"><a href="#YOLOV3结构" class="headerlink" title="YOLOV3结构"></a>YOLOV3结构</h2><p>一张非常详细的结构图，其中YOLOv3有三个输出，维度分别是: (batchsize, 52,52,75 ) (batchsize, 26,26,75 ) (batchsize, 13,13,75 )这里的75代表的$3 \times(20+5)$，其中20代表的是COCO数据集目标类别数，5代表的是每个目标预测框的$t_{x}, t_{y}, t_{w}, t_{h}, t_{o}$，3代表的是某一个特征图的Anchor，也即先验框的数目。所以YOLOv3一共有9个Anchor，不过被平均分在了3个特征层中，这也实现了多尺度检测。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv3/641.webp" alt></p>
<h2 id="多尺度检测"><a href="#多尺度检测" class="headerlink" title="多尺度检测"></a>多尺度检测</h2><p>总结一下，YOLOv3借鉴了FPN的思想，从不同尺度提取特征。相比YOLOv2，YOLOv3提取最后3层特征图，不仅在每个特征图上分别独立做预测，同时通过将小特征图上采样到与大的特征图相同大小，然后与大的特征图拼接做进一步预测。用维度聚类的思想聚类出9种尺度的anchor box，将9种尺度的anchor box均匀的分配给3种尺度的特征图。</p>
<h2 id="补充：YOLOv3-Tiny"><a href="#补充：YOLOv3-Tiny" class="headerlink" title="补充：YOLOv3-Tiny"></a>补充：YOLOv3-Tiny</h2><p>或许对于速度要求比较高的项目，YOLOV3-tiny才是我们的首要选择，这个网络的原理不用多说了，就是在YOLOv3的基础上去掉了一些特征层，只保留了2个独立预测分支，具体的结构图如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv3/642.jpg" alt></p>
<p>这个是工程下更加常用的。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv2损失函数详解</title>
    <url>/2020/03/01/YOLOv2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>YOLOv2详细讲解了YOLOv2的算法原理，但官方论文没有像YOLOv1那样提供YOLOv2的损失函数，难怪Ng说YOLO是目标检测中最难懂的算法。今天我们尝试结合DarkNet的源码来分析YOLOv2的损失函数。</p>
<h2 id="关键点回顾"><a href="#关键点回顾" class="headerlink" title="关键点回顾"></a>关键点回顾</h2><h3 id="直接位置预测"><a href="#直接位置预测" class="headerlink" title="直接位置预测"></a>直接位置预测</h3><p>YOLOv2借鉴RPN网络使用Anchor boxes来预测边界框相对于先验框的offsets。边界框的实际中心位置$(x, y)$需要利用预测的坐标偏移值$\left(t_{x}, t_{y}\right)$，先验框的尺度$\left(w_{a}, h_{a}\right)$以及中心坐标$\left(x_{a}, y_{a}\right)$来计算，这里的${x}_{a}$和${y}_{a}$也即是特征图每个位置的中心点：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/644.png" alt></p>
<p>上面的公式也是Faster-RCNN中预测边界框的方式。但上面的预测方式是没有约束的，预测的边界框容易向任何方向偏移，例如当$t_{x}=1$时边界框将向右偏移Anchor的一个宽度大小，导致每个位置预测的边界框可以落在图片的任意位置，这就导致模型训练的不稳定性，在训练的时候要花很长时间才可以得到正确的offsets。所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。</p>
<p>综上，根据边界框预测的4个偏移值$t_{x}, t_{y}, t_{w}, t_{h}$，可以使用如下公式来计算边界框实际中心位置和长宽，公式在图中：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/645.webp" alt></p>
<p>其中，$\left(c_{x}, c_{y}\right)$为cell的左上角坐标。在Fig3中，当前的cell的左上角坐标为$(1,1)$。由于sigmoid函数的处理，边界框的中心位置会被约束在当前cell的内部，防止偏移过多，然后$p_{w}$和$p_{h}$是先验框的宽度与高度，它们的值也是相对于特征图（这里是13 x 13，我们把特征图的长宽记作H，W）大小的，在特征图中的cell长宽均为1。这样我们就可以算出边界框相对于整个特征图的位置和大小了，公式如下：</p>
<script type="math/tex; mode=display">
b_{x}=\left(\sigma\left(t_{x}\right)+c_{x}\right) / W \quad b_{y}=\left(\sigma\left(t_{y}\right)+c_{y}\right) / H \quad b_{w}=p_{w} e^{t_{w}} / W \quad b_{h}=p_{h} e^{t_{h}} / H</script><p>我们如果将上面边界框的4个值乘以输入图像长宽，就可以得到边界框在原图中的位置和大小了。</p>
<h3 id="细粒度特征"><a href="#细粒度特征" class="headerlink" title="细粒度特征"></a>细粒度特征</h3><p>YOLOv2提取Darknet-19最后一个max pool层的输入，得到26x26x512的特征图。经过1x1x64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass  through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%。这个过程可以在下面的YOLOv2的结构图中看得很清楚：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/646.webp" alt></p>
<p>这个地方今天还要补充一点，那就是passthrough层到底是怎么操作的，在DarkNet中passthough层叫作reorg_layer，可以用下图来表示这个操作：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/649.webp" alt></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>YOLOv2的训练分为三个阶段，具体就不再赘述了。这里主要重新关注一下训练后的维度变化，我们从上一小节可以看到最后YOLOv2的输出维度是$13 \times 13 \times 125$。这个125使用下面的公式来计算的：</p>
<script type="math/tex; mode=display">
numanchors \times(5+\text {num_classes})</script><p>和训练采用的数据集有关系。由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。这里以VOC数据集为例，最终的预测矩阵为$T$，shape为$\left[\text {batch}_{\text {size}}, 13,13,125\right]$，可以将其reshape成[batch_size, $13,13,5,25]$，这样$T[:, :, :, :, 0: 4]$是边界框的位置和大小$\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$，$T[:, :, :, :, 4]$表示边界框的置信度$t_{o}$，$T[:, :, :, :, 5:]$而表示类别预测值。</p>
<h2 id="YOLOv2的模型结构"><a href="#YOLOv2的模型结构" class="headerlink" title="YOLOv2的模型结构"></a>YOLOv2的模型结构</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/650.webp" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/651.webp" alt></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>接下来就说一说今天的主题，损失函数。损失函数我看网上的众多讲解，发现有两种解释。</p>
<h3 id="解释1"><a href="#解释1" class="headerlink" title="解释1"></a>解释1</h3><p>YOLOv2的损失函数和YOLOv1一样，对于训练集中的ground truth，中心落在哪个cell，那么该cell的5个Anchor  box对应的边界框就负责预测它，具体由哪一个预测同样也是根据IOU计算后的阈值来确定的，最后选IOU值最大的那个。这也是建立在每个Cell至多含有一个目标的情下，实际上也基本不会出现多余1个的情况。和ground  truth匹配上的先验框负责计算坐标误差，置信度误差以及分类误差，而其它4个边界框只计算置信度误差。这个解释参考的YOLOv2实现是darkflow。源码地址为：<a href="https://github.com/thtrieu/darkflow" target="_blank" rel="noopener">https://github.com/thtrieu/darkflow</a></p>
<h3 id="解释2"><a href="#解释2" class="headerlink" title="解释2"></a>解释2</h3><p>在官方提供的Darknet中，YOLOv2的损失函数可以不是和YOLOv1一样的，损失函数可以用下图来进行表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/661.webp" alt></p>
<p>可以看到这个损失函数是相当复杂的，损失函数的定义在Darknet/src/region_layer.c中。对于上面这一堆公式，我们先简单看一下，然后我们在源码中去找到对应部分。这里的$W$和$H$代表的是特征图的高宽，都为13，而$A$指的是Anchor个数，YOLOv2中是5，各个$\lambda$值是各个loss部分的权重系数。我们将损失函数分成3大部分来解释：</p>
<ul>
<li><p>第一部分：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/662.png" alt><br>第一项需要好好解释一下，这个loss是计算background的置信度误差，这也是YOLO系列算法的特色，但是用哪些预测框来预测背景呢？这里需要计算各个预测框和所有的ground  truth之间的IOU值，并且取最大值记作MaxIOU，如果该值小于一定的阈值，YOLOv2论文取了0.6，那么这个预测框就标记为background，需要计算$\lambda_{n o o b j}$这么多倍的损失函数。为什么这个公式可以这样表达呢？因为我们有物体的话，那么$\lambda_{n o o b j}=0$，如果没有物体$\lambda_{\text {noob} j}=1$，我们把这个值带入到下面的公式就可以推出第一项啦！<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/663.webp" alt></p>
</li>
<li><p>第二部分：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/664.png" alt><br>这一部分是计算Anchor boxes和预测框的坐标误差，但是只在前12800个iter计算，这一项应该是促进网络学习到Anchor的形状。</p>
</li>
<li><p>第三部分：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/665.webp" alt><br>这一部分计算的是和ground truth匹配的预测框各部分的损失总和，包括坐标损失，置信度损失以及分类损失。<br><strong>3.1 坐标损失</strong> 这里的匹配原则是指对于某个特定的ground truth，首先要计算其中心点落在哪个cell上，然后计算这个cell的5个先验框和grond  truth的IOU值，计算IOU值的时候不考虑坐标只考虑形状，所以先将Anchor boxes和ground  truth的中心都偏移到同一位置，然后计算出对应的IOU值，IOU值最大的先验框和ground  truth匹配，对应的预测框用来预测这个ground truth。<br><strong>3.2 置信度损失</strong> 在计算obj置信度时， 增加了一项权重系数，也被称为rescore参数，当其为1时，损失是预测框和ground truth的真实IOU值(darknet中采用了这种实现方式)。而对于没有和ground  truth匹配的先验框，除去那些Max_IOU低于阈值的，其它就全部忽略。YOLOv2和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。<br><strong>3.3 分类损失</strong> 这个和YOLOv1一致，没什么好说的了。</p>
</li>
</ul>
<p>我看了一篇讲解YOLOv2损失函数非常好的文章：<a href="https://www.cnblogs.com/YiXiaoZhou/p/7429481.html" target="_blank" rel="noopener">https://www.cnblogs.com/YiXiaoZhou/p/7429481.html</a> 。里面还有一个关键点：</p>
<p>在计算boxes的$w$和$h$误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale x (2 - truth.w x truth.h)（这里和都归一化到(0,1))，这样对于尺度较小的boxes其权重系数会更大一些，可以放大误差，起到和YOLOv1计算平方根相似的效果。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>贴一下YOLOv2在Keras上的复现代码，地址为：<a href="https://github.com/yhcc/yolo2" target="_blank" rel="noopener">https://github.com/yhcc/yolo2</a> 。网络结构如下，可以结合上面可视化图来看：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def darknet(images, n_last_channels&#x3D;425):</span><br><span class="line">    &quot;&quot;&quot;Darknet19 for YOLOv2&quot;&quot;&quot;</span><br><span class="line">    net &#x3D; conv2d(images, 32, 3, 1, name&#x3D;&quot;conv1&quot;)</span><br><span class="line">    net &#x3D; maxpool(net, name&#x3D;&quot;pool1&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 64, 3, 1, name&#x3D;&quot;conv2&quot;)</span><br><span class="line">    net &#x3D; maxpool(net, name&#x3D;&quot;pool2&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 128, 3, 1, name&#x3D;&quot;conv3_1&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 64, 1, name&#x3D;&quot;conv3_2&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 128, 3, 1, name&#x3D;&quot;conv3_3&quot;)</span><br><span class="line">    net &#x3D; maxpool(net, name&#x3D;&quot;pool3&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 256, 3, 1, name&#x3D;&quot;conv4_1&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 128, 1, name&#x3D;&quot;conv4_2&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 256, 3, 1, name&#x3D;&quot;conv4_3&quot;)</span><br><span class="line">    net &#x3D; maxpool(net, name&#x3D;&quot;pool4&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 512, 3, 1, name&#x3D;&quot;conv5_1&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 256, 1, name&#x3D;&quot;conv5_2&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 512, 3, 1, name&#x3D;&quot;conv5_3&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 256, 1, name&#x3D;&quot;conv5_4&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 512, 3, 1, name&#x3D;&quot;conv5_5&quot;)</span><br><span class="line">    shortcut &#x3D; net</span><br><span class="line">    net &#x3D; maxpool(net, name&#x3D;&quot;pool5&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 1024, 3, 1, name&#x3D;&quot;conv6_1&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 512, 1, name&#x3D;&quot;conv6_2&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 1024, 3, 1, name&#x3D;&quot;conv6_3&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 512, 1, name&#x3D;&quot;conv6_4&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 1024, 3, 1, name&#x3D;&quot;conv6_5&quot;)</span><br><span class="line">    # ---------</span><br><span class="line">    net &#x3D; conv2d(net, 1024, 3, 1, name&#x3D;&quot;conv7_1&quot;)</span><br><span class="line">    net &#x3D; conv2d(net, 1024, 3, 1, name&#x3D;&quot;conv7_2&quot;)</span><br><span class="line">    # shortcut</span><br><span class="line">    shortcut &#x3D; conv2d(shortcut, 64, 1, name&#x3D;&quot;conv_shortcut&quot;)</span><br><span class="line">    shortcut &#x3D; reorg(shortcut, 2)</span><br><span class="line">    net &#x3D; tf.concat([shortcut, net], axis&#x3D;-1)</span><br><span class="line">    net &#x3D; conv2d(net, 1024, 3, 1, name&#x3D;&quot;conv8&quot;)</span><br><span class="line">    # detection layer</span><br><span class="line">    net &#x3D; conv2d(net, n_last_channels, 1, batch_normalize&#x3D;0,</span><br><span class="line">                 activation&#x3D;None, use_bias&#x3D;True, name&#x3D;&quot;conv_dec&quot;)</span><br><span class="line">    return net</span><br></pre></td></tr></table></figure>
<p>然后，网络经过介绍的损失函数优化训练以后，对网络输出结果进行解码得到最终的检测结果，这部分代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def decode(detection_feat, feat_sizes&#x3D;(13, 13), num_classes&#x3D;80,</span><br><span class="line">           anchors&#x3D;None):</span><br><span class="line">    &quot;&quot;&quot;decode from the detection feature&quot;&quot;&quot;</span><br><span class="line">    H, W &#x3D; feat_sizes</span><br><span class="line">    num_anchors &#x3D; len(anchors)</span><br><span class="line">    detetion_results &#x3D; tf.reshape(detection_feat, [-1, H * W, num_anchors,</span><br><span class="line">                                        num_classes + 5])</span><br><span class="line"></span><br><span class="line">    bbox_xy &#x3D; tf.nn.sigmoid(detetion_results[:, :, :, 0:2])</span><br><span class="line">    bbox_wh &#x3D; tf.exp(detetion_results[:, :, :, 2:4])</span><br><span class="line">    obj_probs &#x3D; tf.nn.sigmoid(detetion_results[:, :, :, 4])</span><br><span class="line">    class_probs &#x3D; tf.nn.softmax(detetion_results[:, :, :, 5:])</span><br><span class="line"></span><br><span class="line">    anchors &#x3D; tf.constant(anchors, dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line">    height_ind &#x3D; tf.range(H, dtype&#x3D;tf.float32)</span><br><span class="line">    width_ind &#x3D; tf.range(W, dtype&#x3D;tf.float32)</span><br><span class="line">    x_offset, y_offset &#x3D; tf.meshgrid(height_ind, width_ind)</span><br><span class="line">    x_offset &#x3D; tf.reshape(x_offset, [1, -1, 1])</span><br><span class="line">    y_offset &#x3D; tf.reshape(y_offset, [1, -1, 1])</span><br><span class="line"></span><br><span class="line">    # decode</span><br><span class="line">    bbox_x &#x3D; (bbox_xy[:, :, :, 0] + x_offset) &#x2F; W</span><br><span class="line">    bbox_y &#x3D; (bbox_xy[:, :, :, 1] + y_offset) &#x2F; H</span><br><span class="line">    bbox_w &#x3D; bbox_wh[:, :, :, 0] * anchors[:, 0] &#x2F; W * 0.5</span><br><span class="line">    bbox_h &#x3D; bbox_wh[:, :, :, 1] * anchors[:, 1] &#x2F; H * 0.5</span><br><span class="line"></span><br><span class="line">    bboxes &#x3D; tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,</span><br><span class="line">                       bbox_x + bbox_w, bbox_y + bbox_h], axis&#x3D;3)</span><br><span class="line"></span><br><span class="line">    return bboxes, obj_probs, class_probs</span><br></pre></td></tr></table></figure>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>这个损失函数最难的地方应该是YOLOv2利用sigmoid函数计算默认框坐标之后怎么梯度回传，这部分可以看下面的代码(来自Darknet源码)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; box误差函数，计算梯度</span><br><span class="line">float delta_region_box(box truth, float *x, float *biases, int n, int index, int i, int j, int w, int h, float *delta, float scale, int stride)</span><br><span class="line">&#123;</span><br><span class="line">    box pred &#x3D; get_region_box(x, biases, n, index, i, j, w, h, stride);</span><br><span class="line">    float iou &#x3D; box_iou(pred, truth);</span><br><span class="line">   </span><br><span class="line">    &#x2F;&#x2F; 计算ground truth的offsets值</span><br><span class="line">    float tx &#x3D; (truth.x*w - i);</span><br><span class="line">    float ty &#x3D; (truth.y*h - j);</span><br><span class="line">    float tw &#x3D; log(truth.w*w &#x2F; biases[2*n]);</span><br><span class="line">    float th &#x3D; log(truth.h*h &#x2F; biases[2*n + 1]);</span><br><span class="line"></span><br><span class="line">    delta[index + 0*stride] &#x3D; scale * (tx - x[index + 0*stride]);</span><br><span class="line">    delta[index + 1*stride] &#x3D; scale * (ty - x[index + 1*stride]);</span><br><span class="line">    delta[index + 2*stride] &#x3D; scale * (tw - x[index + 2*stride]);</span><br><span class="line">    delta[index + 3*stride] &#x3D; scale * (th - x[index + 3*stride]);</span><br><span class="line">    return iou;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>结合一下前面介绍的公式，这就是一个逆过程。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet BN层代码详解(batchnorm_layer.c)</title>
    <url>/2020/03/01/AlexeyAB-DarkNet-BN%E5%B1%82%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(batchnorm_layer.c)/</url>
    <content><![CDATA[<h2 id="BatchNorm原理"><a href="#BatchNorm原理" class="headerlink" title=" BatchNorm原理"></a><a id="more"></a> BatchNorm原理</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/667.png" alt></p>
<p>这是论文中给出的对BatchNorm的算法流程解释，这篇推文的目的主要是推导和从源码角度解读BatchNorm的前向传播和反向传播，就不关注具体的原理了（实际上是因为BN层的原理非常复杂），我们暂时知道BN层是用来调整数据分布，降低过拟合的就够了。</p>
<h2 id="前向传播推导"><a href="#前向传播推导" class="headerlink" title="前向传播推导"></a>前向传播推导</h2><p>前向传播实际就是将Algorithm1的4个公式转化为编程语言，这里先贴一段CS231N官方提供的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def batchnorm_forward(x, gamma, beta, bn_param):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  Input:</span><br><span class="line">  - x: (N, D)维输入数据</span><br><span class="line">  - gamma: (D,)维尺度变化参数</span><br><span class="line">  - beta: (D,)维尺度变化参数</span><br><span class="line">  - bn_param: Dictionary with the following keys:</span><br><span class="line">    - mode: &#39;train&#39; 或者 &#39;test&#39;</span><br><span class="line">    - eps: 一般取1e-8~1e-4</span><br><span class="line">    - momentum: 计算均值、方差的更新参数</span><br><span class="line">    - running_mean: (D,)动态变化array存储训练集的均值</span><br><span class="line">    - running_var：(D,)动态变化array存储训练集的方差</span><br><span class="line"></span><br><span class="line">  Returns a tuple of:</span><br><span class="line">  - out: 输出y_i（N，D）维</span><br><span class="line">  - cache: 存储反向传播所需数据</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  mode &#x3D; bn_param[&#39;mode&#39;]</span><br><span class="line">  eps &#x3D; bn_param.get(&#39;eps&#39;, 1e-5)</span><br><span class="line">  momentum &#x3D; bn_param.get(&#39;momentum&#39;, 0.9)</span><br><span class="line"></span><br><span class="line">  N, D &#x3D; x.shape</span><br><span class="line">  # 动态变量，存储训练集的均值方差</span><br><span class="line">  running_mean &#x3D; bn_param.get(&#39;running_mean&#39;, np.zeros(D, dtype&#x3D;x.dtype))</span><br><span class="line">  running_var &#x3D; bn_param.get(&#39;running_var&#39;, np.zeros(D, dtype&#x3D;x.dtype))</span><br><span class="line"></span><br><span class="line">  out, cache &#x3D; None, None</span><br><span class="line">  # TRAIN 对每个batch操作</span><br><span class="line">  if mode &#x3D;&#x3D; &#39;train&#39;:</span><br><span class="line">    sample_mean &#x3D; np.mean(x, axis &#x3D; 0)</span><br><span class="line">    sample_var &#x3D; np.var(x, axis &#x3D; 0)</span><br><span class="line">    x_hat &#x3D; (x - sample_mean) &#x2F; np.sqrt(sample_var + eps)</span><br><span class="line">    out &#x3D; gamma * x_hat + beta</span><br><span class="line">    cache &#x3D; (x, gamma, beta, x_hat, sample_mean, sample_var, eps)</span><br><span class="line">    running_mean &#x3D; momentum * running_mean + (1 - momentum) * sample_mean</span><br><span class="line">    running_var &#x3D; momentum * running_var + (1 - momentum) * sample_var</span><br><span class="line">  # TEST：要用整个训练集的均值、方差</span><br><span class="line">  elif mode &#x3D;&#x3D; &#39;test&#39;:</span><br><span class="line">    x_hat &#x3D; (x - running_mean) &#x2F; np.sqrt(running_var + eps)</span><br><span class="line">    out &#x3D; gamma * x_hat + beta</span><br><span class="line">  else:</span><br><span class="line">    raise ValueError(&#39;Invalid forward batchnorm mode &quot;%s&quot;&#39; % mode)</span><br><span class="line"></span><br><span class="line">  bn_param[&#39;running_mean&#39;] &#x3D; running_mean</span><br><span class="line">  bn_param[&#39;running_var&#39;] &#x3D; running_var</span><br><span class="line"></span><br><span class="line">  return out, cache</span><br></pre></td></tr></table></figure>
<p>就是一个公式带入的问题，这里倒是没啥好说的，不过了为了和下面反向传播对比理解，这里我们明确每一个张量的维度：</p>
<ul>
<li><strong>x</strong> shape为(N,D)，可以将N看成batch size,D看成特征图展开为1列的元素个数</li>
<li><strong>gamma</strong> shape为(D,)</li>
<li><strong>beta</strong> shape为(D,)</li>
<li><strong>running_mean</strong> shape为(D,)</li>
<li><strong>running_var</strong> shape为(D,)</li>
</ul>
<p>请特别注意滑动平均(影子变量)这种Trick的引入，目的是为了控制变量更新的速度，防止变量的突然变化对变量的整体影响，这能提高模型的鲁棒性。</p>
<h2 id="反向传播推导"><a href="#反向传播推导" class="headerlink" title="反向传播推导"></a>反向传播推导</h2><p>这才是重点，现在做一些约定：</p>
<ul>
<li>$\delta$为一个Batch所有样本的方差</li>
<li>$\mu$为样本均值</li>
<li>$\hat{x}$为归一化后的样本数据</li>
<li>$y_{i}$为输入样本$x_{i}$经过尺度变化的输出量</li>
<li>$\gamma$和$\beta$为尺度变化系数</li>
<li>$\frac{\partial L}{\partial y}$是上一层的梯度，并假设$x$和$y$都是$(\mathrm{N}, \mathrm{D})$维，即有N个维度为D的样本在BN层的前向传播中$x_{i}$通过$\gamma, \beta, \hat{x}$将$x_{i}$变换为$y_{i}$，那么反向传播则是根据$\frac{\partial L}{\partial y_{i}}$求得$\frac{\partial L}{\partial \gamma}, \frac{\partial L}{\partial \beta}, \frac{\partial L}{\partial x_{i}}$</li>
<li>求解$\frac{\partial L}{\partial \gamma}$  $\frac{\partial L}{\partial \gamma}=\sum_{i} \frac{\partial L}{\partial y_{i}} \frac{\partial y_{i}}{\partial \gamma}=\sum_{i} \frac{\partial L}{\partial y_{i}} \hat{x}$</li>
<li>求解$\frac{\partial L}{\partial \beta}$  $\frac{\partial L}{\partial \beta}=\sum_{i} \frac{\partial L}{\partial y_{i}} \frac{\partial y_{i}}{\partial \beta}=\sum_{i} \frac{\partial L}{\partial y_{i}}$</li>
<li>求解$\frac{\partial L}{\partial x_{i}}$根据论文的公式和链式法则可得下面的等式: $\frac{\partial L}{\partial x_{i}}=\frac{\partial L}{\partial \widehat{x}_{i}} \frac{\partial \widehat{x}_{i}}{\partial x_{i}}+\frac{\partial L}{\partial \sigma} \frac{\partial \sigma}{\partial x_{i}}+\frac{\partial L}{\partial \mu} \frac{\partial \mu}{\partial x_{i}}$我们这里又可以先求$\frac{\partial L}{\partial \hat{x}}$</li>
<li>$\frac{\partial L}{\partial \hat{x}}=\frac{\partial L}{\partial y} \frac{\partial y}{\partial \hat{x}}=\frac{\partial L}{\partial y} \gamma$    (1)</li>
<li>$\frac{\partial L}{\partial \sigma}=\sum_{i} \frac{\partial L}{\partial y_{i}} \frac{\partial y_{i}}{\partial \hat{x}_{i}} \frac{\partial \hat{x}_{i}}{\partial \sigma}=-\frac{1}{2} \sum_{i} \frac{\partial L}{\partial \widehat{x}_{i}}\left(x_{i}-\mu\right)(\sigma+\varepsilon)^{-1.5}$    (2)</li>
<li>$\frac{\partial L}{\partial \mu}=\frac{\partial L}{\partial \hat{x}} \frac{\partial \hat{x}}{\partial \mu}+\frac{\partial L}{\partial \sigma} \frac{\partial \sigma}{\partial \mu}=\sum_{i} \frac{\partial L}{\partial \hat{x}_{i}} \frac{-1}{\sqrt{\sigma+\varepsilon}}+\frac{\partial L}{\partial \sigma} \frac{-2 \Sigma_{i}\left(x_{i}-\mu\right)}{N}$    (3)</li>
<li>有了(1) (2) (3)就可以求出$\frac{\partial L}{\partial x_{i}}$<br>$\frac{\partial L}{\partial x_{i}}=\frac{\partial L}{\partial \widehat{x}_{i}} \frac{\partial \widehat{x}_{i}}{\partial x_{i}}+\frac{\partial L}{\partial \sigma} \frac{\partial \sigma}{\partial x_{i}}+\frac{\partial L}{\partial \mu} \frac{\partial \mu}{\partial x_{i}}=\frac{\partial L}{\partial \hat{x}_{i}} \frac{1}{\sqrt{\sigma+\varepsilon}}+\frac{\partial L}{\partial \sigma} \frac{2\left(x_{i}-\mu\right)}{N}+\frac{\partial L}{\partial \mu} \frac{1}{N}$</li>
</ul>
<p>到这里就推到出了BN层的反向传播公式了，和论文中一样，截取一下论文中的结果图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/668.webp" alt></p>
<p>贴一份CS231N反向传播代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def batchnorm_backward(dout, cache):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  Inputs:</span><br><span class="line">  - dout: 上一层的梯度，维度(N, D)，即 dL&#x2F;dy</span><br><span class="line">  - cache: 所需的中间变量，来自于前向传播</span><br><span class="line"></span><br><span class="line">  Returns a tuple of:</span><br><span class="line">  - dx: (N, D)维的 dL&#x2F;dx</span><br><span class="line">  - dgamma: (D,)维的dL&#x2F;dgamma</span><br><span class="line">  - dbeta: (D,)维的dL&#x2F;dbeta</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">    x, gamma, beta, x_hat, sample_mean, sample_var, eps &#x3D; cache</span><br><span class="line">  N &#x3D; x.shape[0]</span><br><span class="line"></span><br><span class="line">  dgamma &#x3D; np.sum(dout * x_hat, axis &#x3D; 0)</span><br><span class="line">  dbeta &#x3D; np.sum(dout, axis &#x3D; 0)</span><br><span class="line"></span><br><span class="line">  dx_hat &#x3D; dout * gamma</span><br><span class="line">  dsigma &#x3D; -0.5 * np.sum(dx_hat * (x - sample_mean), axis&#x3D;0) * np.power(sample_var + eps, -1.5)</span><br><span class="line">  dmu &#x3D; -np.sum(dx_hat &#x2F; np.sqrt(sample_var + eps), axis&#x3D;0) - 2 * dsigma*np.sum(x-sample_mean, axis&#x3D;0)&#x2F; N</span><br><span class="line">  dx &#x3D; dx_hat &#x2F;np.sqrt(sample_var + eps) + 2.0 * dsigma * (x - sample_mean) &#x2F; N + dmu &#x2F; N</span><br><span class="line"></span><br><span class="line">  return dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h2 id="DarkNet代码详解"><a href="#DarkNet代码详解" class="headerlink" title="DarkNet代码详解"></a>DarkNet代码详解</h2><h3 id="1-构造BN层"><a href="#1-构造BN层" class="headerlink" title="1. 构造BN层"></a>1. 构造BN层</h3><p>构造BN层的代码在<code>src/batchnorm_layer.c</code>中实现，详细代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">layer make_batchnorm_layer(int batch, int w, int h, int c, int train)</span><br><span class="line">&#123;</span><br><span class="line">    fprintf(stderr, &quot;Batch Normalization Layer: %d x %d x %d image\n&quot;, w,h,c);</span><br><span class="line">    layer layer &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">    layer.type &#x3D; BATCHNORM; &#x2F;&#x2F; 网络层的名字</span><br><span class="line">    layer.batch &#x3D; batch; &#x2F;&#x2F;一个batch中包含的图片数</span><br><span class="line">    layer.train &#x3D; train;</span><br><span class="line">    layer.h &#x3D; layer.out_h &#x3D; h;  &#x2F;&#x2F; 当前层的输出高度等于输入高度h</span><br><span class="line">    layer.w &#x3D; layer.out_w &#x3D; w; &#x2F;&#x2F; 当前层的输出宽度等于输入宽度w</span><br><span class="line">    layer.c &#x3D; layer.out_c &#x3D; c; &#x2F;&#x2F; 当前层的输出通道数等于输入通道数</span><br><span class="line"></span><br><span class="line">    layer.n &#x3D; layer.c;</span><br><span class="line">    layer.output &#x3D; (float*)xcalloc(h * w * c * batch, sizeof(float)); &#x2F;&#x2F; layer.output为该层所有的输出（包括mini-batch所有输入图片的输出）</span><br><span class="line">    layer.delta &#x3D; (float*)xcalloc(h * w * c * batch, sizeof(float)); &#x2F;&#x2F;layer.delta 是该层的敏感度图，和输出的维度想同</span><br><span class="line">    layer.inputs &#x3D; w*h*c; &#x2F;&#x2F;mini-batch中每张输入图片的像素元素个数</span><br><span class="line">    layer.outputs &#x3D; layer.inputs; &#x2F;&#x2F; 对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即layer.out_c张特征图）</span><br><span class="line"></span><br><span class="line">    layer.biases &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; BN层特有参数，缩放系数</span><br><span class="line">    layer.bias_updates &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 缩放系数的敏感度图</span><br><span class="line"></span><br><span class="line">    layer.scales &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; BN层特有参数，偏置系数</span><br><span class="line">    layer.scale_updates &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 偏置系数的敏感度图</span><br><span class="line">    int i;</span><br><span class="line">    for(i &#x3D; 0; i &lt; c; ++i)&#123;</span><br><span class="line">        layer.scales[i] &#x3D; 1; &#x2F;&#x2F; 将缩放系数初始化为1</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    layer.mean &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; mean 一个batch中所有图片的均值，分通道求取</span><br><span class="line">    layer.variance &#x3D; (float*)xcalloc(c, sizeof(float));  &#x2F;&#x2F; variance 一个batch中所有图片的方差，分通道求取</span><br><span class="line"></span><br><span class="line">    layer.rolling_mean &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 均值的滑动平均，影子变量</span><br><span class="line">    layer.rolling_variance &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 方差的滑动平均，影子变量</span><br><span class="line"></span><br><span class="line">    layer.forward &#x3D; forward_batchnorm_layer; &#x2F;&#x2F; 前向传播函数</span><br><span class="line">    layer.backward &#x3D; backward_batchnorm_layer; &#x2F;&#x2F; 反向传播函数</span><br><span class="line">    layer.update &#x3D; update_batchnorm_layer;</span><br><span class="line">    ...</span><br><span class="line">    return layer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-前向传播公式实现"><a href="#2-前向传播公式实现" class="headerlink" title="2.前向传播公式实现"></a>2.前向传播公式实现</h3><p>DarkNet中在<code>src/blas.h</code>中实现了前向传播的几个公式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 计算输入数据x的平均值，输出的mean是一个矢量，比如如果x是多张三通道的图片，那么mean的维度就为通道3</span><br><span class="line">** 由于每次训练输入的都是一个batch的图片，因此最终会输出batch张三通道的图片，mean中的第一个元素就是第</span><br><span class="line">** 一个通道上全部batch张输出特征图所有元素的平均值，本函数的用处之一就是batch normalization的第一步了</span><br><span class="line">** x: 包含所有数据，比如l.output，其包含的元素个数为l.batch*l.outputs</span><br><span class="line">** batch: 一个batch中包含的图片张数，即l.batch</span><br><span class="line">** filters: 该层神经网络的滤波器个数，也即该层网络输出图片的通道数（比如对卷积网络来说，就是核的个数了）</span><br><span class="line">** spatial: 该层神经网络每张输出特征图的尺寸，也即等于l.out_w*l.out_h</span><br><span class="line">** mean: 求得的平均值，维度为filters，也即每个滤波器对应有一个均值（每个滤波器会处理所有图片）</span><br><span class="line">** x的内存排布？此处还是结合batchnorm_layer.c中的forward_batch_norm_layer()函数的调用来解释，其中x为l.output，其包含的元素个数为l</span><br><span class="line">** 有l.batch行，每行有l.out_c*l.out_w*l.out_h个元素，每一行又可以分成l.out_c行，l.out_w*l.out_h列，</span><br><span class="line">** 那么l.mean中的每一个元素，是某一个通道上所有batch的输出的平均值</span><br><span class="line">** （比如卷积层，有3个核，那么输出通道有3个，每张输入图片都会输出3张特征图，可以理解每张输出图片是3通道的，</span><br><span class="line">** 若每次输入batch&#x3D;64张图片，那么将会输出64张3通道的图片，而mean中的每个元素就是某个通道上所有64张图片</span><br><span class="line">** 所有元素的平均值，比如第1个通道上，所有64张图片像素平均值）</span><br><span class="line">*&#x2F;</span><br><span class="line">void mean_cpu(float *x, int batch, int filters, int spatial, float *mean)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; scale即是均值中的分母项</span><br><span class="line">    float scale &#x3D; 1.&#x2F;(batch * spatial);</span><br><span class="line">    int i,j,k;</span><br><span class="line">    &#x2F;&#x2F; 外循环次数为filters，也即mean的维度，每次循环将得到一个平均值</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        mean[i] &#x3D; 0;</span><br><span class="line">        &#x2F;&#x2F; 中间循环次数为batch，也即叠加每张输入图片对应的某一通道上的输出</span><br><span class="line">        for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">            &#x2F;&#x2F; 内层循环即叠加一张输出特征图的所有像素值</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                &#x2F;&#x2F; 计算偏移</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                mean[i] +&#x3D; x[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mean[i] *&#x3D; scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">** 计算输入x中每个元素的方差</span><br><span class="line">** 本函数的主要用处应该就是batch normalization的第二步了</span><br><span class="line">** x: 包含所有数据，比如l.output，其包含的元素个数为l.batch*l.outputs</span><br><span class="line">** batch: 一个batch中包含的图片张数，即l.batch</span><br><span class="line">** filters: 该层神经网络的滤波器个数，也即是该网络层输出图片的通道数</span><br><span class="line">** spatial: 该层神经网络每张特征图的尺寸，也即等于l.out_w*l.out_h</span><br><span class="line">** mean: 求得的平均值，维度为filters，也即每个滤波器对应有一个均值（每个滤波器会处理所有图片）</span><br><span class="line">*&#x2F;</span><br><span class="line">void variance_cpu(float *x, float *mean, int batch, int filters, int spatial, float *variance)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; 这里计算方差分母要减去1的原因是无偏估计，可以看：https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20983193</span><br><span class="line">    &#x2F;&#x2F; 事实上，在统计学中，往往采用的方差计算公式都会让分母减1,这时因为所有数据的方差是基于均值这个固定点来计算的，</span><br><span class="line">    &#x2F;&#x2F; 对于有n个数据的样本，在均值固定的情况下，其采样自由度为n-1（只要n-1个数据固定，第n个可以由均值推出）</span><br><span class="line">    float scale &#x3D; 1.&#x2F;(batch * spatial - 1);</span><br><span class="line">    int i,j,k;</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        variance[i] &#x3D; 0;</span><br><span class="line">        for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                &#x2F;&#x2F; 每个元素减去均值求平方</span><br><span class="line">                variance[i] +&#x3D; pow((x[index] - mean[i]), 2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        variance[i] *&#x3D; scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">void normalize_cpu(float *x, float *mean, float *variance, int batch, int filters, int spatial)</span><br><span class="line">&#123;</span><br><span class="line">    int b, f, i;</span><br><span class="line">    for(b &#x3D; 0; b &lt; batch; ++b)&#123;</span><br><span class="line">        for(f &#x3D; 0; f &lt; filters; ++f)&#123;</span><br><span class="line">            for(i &#x3D; 0; i &lt; spatial; ++i)&#123;</span><br><span class="line">                int index &#x3D; b*filters*spatial + f*spatial + i;</span><br><span class="line">                x[index] &#x3D; (x[index] - mean[f])&#x2F;(sqrt(variance[f]) + .000001f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;*</span><br><span class="line">** axpy 是线性代数中的一种基本操作(仿射变换)完成y&#x3D; alpha*x + y操作，其中x,y为矢量，alpha为实数系数，</span><br><span class="line">** 请看: https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;e3f386771c51</span><br><span class="line">** N: X中包含的有效元素个数</span><br><span class="line">** ALPHA: 系数alpha</span><br><span class="line">** X: 参与运算的矢量X</span><br><span class="line">** INCX: 步长(倍数步长)，即x中凡是INCX倍数编号的参与运算</span><br><span class="line">** Y: 参与运算的矢量，也相当于是输出</span><br><span class="line">*&#x2F;</span><br><span class="line">void axpy_cpu(int N, float ALPHA, float *X, int INCX, float *Y, int INCY)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">    for(i &#x3D; 0; i &lt; N; ++i) Y[i*INCY] +&#x3D; ALPHA*X[i*INCX];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void scal_cpu(int N, float ALPHA, float *X, int INCX)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">    for(i &#x3D; 0; i &lt; N; ++i) X[i*INCX] *&#x3D; ALPHA;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-前向传播和反向传播接口函数"><a href="#3-前向传播和反向传播接口函数" class="headerlink" title="3. 前向传播和反向传播接口函数"></a>3. 前向传播和反向传播接口函数</h3><p>DarkNet在<code>src/batchnorm_layer.c</code>中实现了前向传播和反向传播的接口函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; BN层的前向传播函数</span><br><span class="line">void forward_batchnorm_layer(layer l, network net)</span><br><span class="line">&#123;</span><br><span class="line">    if(l.type &#x3D;&#x3D; BATCHNORM) copy_cpu(l.outputs*l.batch, net.input, 1, l.output, 1);</span><br><span class="line">    copy_cpu(l.outputs*l.batch, l.output, 1, l.x, 1);</span><br><span class="line">    &#x2F;&#x2F; 训练阶段</span><br><span class="line">    if(net.train)&#123;</span><br><span class="line">        &#x2F;&#x2F; blas.c中有详细注释，计算输入数据的均值，保存为l.mean</span><br><span class="line">        mean_cpu(l.output, l.batch, l.out_c, l.out_h*l.out_w, l.mean);</span><br><span class="line">        &#x2F;&#x2F; blas.c中有详细注释，计算输入数据的方差，保存为l.variance</span><br><span class="line">        variance_cpu(l.output, l.mean, l.batch, l.out_c, l.out_h*l.out_w, l.variance);</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 计算滑动平均和方差，影子变量，可以参考https:&#x2F;&#x2F;blog.csdn.net&#x2F;just_sort&#x2F;article&#x2F;details&#x2F;100039418</span><br><span class="line">        scal_cpu(l.out_c, .99, l.rolling_mean, 1);</span><br><span class="line">        axpy_cpu(l.out_c, .01, l.mean, 1, l.rolling_mean, 1);</span><br><span class="line">        scal_cpu(l.out_c, .99, l.rolling_variance, 1);</span><br><span class="line">        axpy_cpu(l.out_c, .01, l.variance, 1, l.rolling_variance, 1);</span><br><span class="line">        &#x2F;&#x2F; 减去均值，除以方差得到x^，论文中的第3个公式</span><br><span class="line">        normalize_cpu(l.output, l.mean, l.variance, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">        &#x2F;&#x2F; BN层的输出</span><br><span class="line">        copy_cpu(l.outputs*l.batch, l.output, 1, l.x_norm, 1);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        &#x2F;&#x2F; 测试阶段，直接用滑动变量来计算输出</span><br><span class="line">        normalize_cpu(l.output, l.rolling_mean, l.rolling_variance, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 最后一个公式，对输出进行移位和偏置</span><br><span class="line">    scale_bias(l.output, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    add_bias(l.output, l.biases, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; BN层的反向传播函数</span><br><span class="line">void backward_batchnorm_layer(layer l, network net)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; 如果在测试阶段，均值和方差都可以直接用滑动变量来赋值</span><br><span class="line">    if(!net.train)&#123;</span><br><span class="line">        l.mean &#x3D; l.rolling_mean;</span><br><span class="line">        l.variance &#x3D; l.rolling_variance;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 在卷积层中定义了backward_bias，并有详细注释</span><br><span class="line">    backward_bias(l.bias_updates, l.delta, l.batch, l.out_c, l.out_w*l.out_h);</span><br><span class="line">    &#x2F;&#x2F; 这里是对论文中最后一个公式的缩放系数求梯度更新值</span><br><span class="line">    backward_scale_cpu(l.x_norm, l.delta, l.batch, l.out_c, l.out_w*l.out_h, l.scale_updates);</span><br><span class="line">    &#x2F;&#x2F; 也是在convlution_layer.c中定义的函数，先将敏感度图乘以l.scales</span><br><span class="line">    scale_bias(l.delta, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; 对应了https:&#x2F;&#x2F;blog.csdn.net&#x2F;just_sort&#x2F;article&#x2F;details&#x2F;100039418 中对均值求导数</span><br><span class="line">    mean_delta_cpu(l.delta, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.mean_delta);</span><br><span class="line">    &#x2F;&#x2F; 对应了https:&#x2F;&#x2F;blog.csdn.net&#x2F;just_sort&#x2F;article&#x2F;details&#x2F;100039418 中对方差求导数</span><br><span class="line">    variance_delta_cpu(l.x, l.delta, l.mean, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.variance_delta);</span><br><span class="line">    &#x2F;&#x2F; 计算敏感度图，对应了论文中的最后一部分</span><br><span class="line">    normalize_delta_cpu(l.x, l.mean, l.variance, l.mean_delta, l.variance_delta, l.batch, l.out_c, l.out_w*l.out_h, l.delta);</span><br><span class="line">    if(l.type &#x3D;&#x3D; BATCHNORM) copy_cpu(l.outputs*l.batch, l.delta, 1, net.delta, 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-反向传播函数公式实现"><a href="#4-反向传播函数公式实现" class="headerlink" title="4.反向传播函数公式实现"></a>4.反向传播函数公式实现</h3><p>其中反向传播的函数如下，就是利用推导出的公式来计算：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 这里是对论文中最后一个公式的缩放系数求梯度更新值</span><br><span class="line">&#x2F;&#x2F; x_norm 代表BN层前向传播的输出值</span><br><span class="line">&#x2F;&#x2F; delta 代表上一层的梯度图</span><br><span class="line">&#x2F;&#x2F; batch 为l.batch，即一个batch的图片数</span><br><span class="line">&#x2F;&#x2F; n代表输出通道数，也即是输入通道数</span><br><span class="line">&#x2F;&#x2F; size 代表w * h</span><br><span class="line">&#x2F;&#x2F; scale_updates 代表scale的梯度更新值</span><br><span class="line">&#x2F;&#x2F; y &#x3D; gamma * x + beta</span><br><span class="line">&#x2F;&#x2F; dy &#x2F; d(gamma) &#x3D; x</span><br><span class="line">void backward_scale_cpu(float *x_norm, float *delta, int batch, int n, int size, float *scale_updates)</span><br><span class="line">&#123;</span><br><span class="line">    int i,b,f;</span><br><span class="line">    for(f &#x3D; 0; f &lt; n; ++f)&#123;</span><br><span class="line">        float sum &#x3D; 0;</span><br><span class="line">        for(b &#x3D; 0; b &lt; batch; ++b)&#123;</span><br><span class="line">            for(i &#x3D; 0; i &lt; size; ++i)&#123;</span><br><span class="line">                int index &#x3D; i + size*(f + n*b);</span><br><span class="line">                sum +&#x3D; delta[index] * x_norm[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        scale_updates[f] +&#x3D; sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 对均值求导</span><br><span class="line">&#x2F;&#x2F; 对应了论文中的求导公式3，不过Darknet特殊的点在于是先计算均值的梯度</span><br><span class="line">&#x2F;&#x2F; 这个时候方差是没有梯度的，所以公式3的后半部分为0，也就只保留了公式3的前半部分</span><br><span class="line">&#x2F;&#x2F; 不过我从理论上无法解释这种操作会带来什么影响，但从目标检测来看应该是没有影响的</span><br><span class="line">void mean_delta_cpu(float *delta, float *variance, int batch, int filters, int spatial, float *mean_delta)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    int i,j,k;</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        mean_delta[i] &#x3D; 0;</span><br><span class="line">        for (j &#x3D; 0; j &lt; batch; ++j) &#123;</span><br><span class="line">            for (k &#x3D; 0; k &lt; spatial; ++k) &#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                mean_delta[i] +&#x3D; delta[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mean_delta[i] *&#x3D; (-1.&#x2F;sqrt(variance[i] + .00001f));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 对方差求导</span><br><span class="line">&#x2F;&#x2F; 对应了论文中的求导公式2</span><br><span class="line">void  variance_delta_cpu(float *x, float *delta, float *mean, float *variance, int batch, int filters, int spatial, float *variance_delta)</span><br><span class="line">&#123;</span><br><span class="line">    int i,j,k;</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        variance_delta[i] &#x3D; 0;</span><br><span class="line">        for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                variance_delta[i] +&#x3D; delta[index]*(x[index] - mean[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        variance_delta[i] *&#x3D; -.5 * pow(variance[i] + .00001f, (float)(-3.&#x2F;2.));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 求出BN层的梯度敏感度图</span><br><span class="line">&#x2F;&#x2F; 对应了论文中的求导公式4，即是对x_i求导</span><br><span class="line">void normalize_delta_cpu(float *x, float *mean, float *variance, float *mean_delta, float *variance_delta, int batch, int filters, int spatial, float *delta)</span><br><span class="line">&#123;</span><br><span class="line">    int f, j, k;</span><br><span class="line">    for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">        for(f &#x3D; 0; f &lt; filters; ++f)&#123;</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + f*spatial + k;</span><br><span class="line">                delta[index] &#x3D; delta[index] * 1.&#x2F;(sqrt(variance[f] + .00001f)) + variance_delta[f] * 2. * (x[index] - mean[f]) &#x2F; (spatial * batch) + mean_delta[f]&#x2F;(spatial*batch);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>SSD</title>
    <url>/2020/02/29/SSD/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title=" 摘要"></a><a id="more"></a> 摘要</h2><p>本文提出了仅需要单个卷积神经网络就能完成目标检测的算法，并命名为SSD(Single Shot Detector)。SSD算法将目标框的输出空间离散化为一组在每个特征图位置不同大小和形状的默认框。预测时，网络对位于每个默认框类的物体类别进行打分，并修正默认框位置来更好的匹配物体的位置。此外，SSD网络在不同分辨率的特征图上预测，这样就可以处理大小不同的物体。SSD比那些需要搜索物体候选框的算法简单，因为它完全去除了proposal生成和随后的特征再筛选的过程，把所有的计算封装在一个网络里面。这使得SSD训练起来很容易，可以直接加入到检测系统里面。在PASCAL VOC，COCO,和ILSVRC数据集上的实验也证明，与那些需要object  proposal的算法相比，SSD在保证准确性的同时，速度更快。SSD只需一个完整的框架来训练和测试。在NVIDIA Titan  X对于一个大小是300 x 300的输入图像，SSD在VOC2007测试上的MAP是74.3%，速度是59FPS。对于512 x 512的输入，SSD的MAP是76.9%，比Faster RCNN更准。和其他单阶段的方法比，即便是输入较小的图像，SSD的准确性也会更高。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>目前，目标检测系统基本采用以下的流程：假设物体边框，对每个边框内进行特征再采样，最后使用分类器进行分类。这个流程比较流行，基于Faster-RCNN的方法通过选择性搜索在PASCAL VOC,  COCO和ILSVRC上检测效果都很好。但是这些方法对于嵌入式设备来说计算量过大，甚至需要高端硬件的支持，对于实时系统来说太慢。最快的检测器Faster  RCNN的检测速度也只能到7FPS。人们尝试了很多其他方法来构建更快的检测器，但是增加速度大多以损失检测精度为代价。本文提出了基于目标检测器的网络(object detector)，它不需要为边框进行搜索，但是精度却不降反升。此方法实现了高精度和高速度，在VOC2007  上的测试速度是59FPS，mAP是74.3%；而Faster  R-CNN的mAP是73.2%，速度是7FPS；YOLO的mAP是63.4%，速度的是45FPS。速度的提升得益于去除了边框提议(RPN或者Selective  Search)和随后的特征再采样。使用了一个小卷积滤波器来预测目标分类和边框位置的偏移，对于不同横纵比检测使用不同的滤波器去处理，然后把这些滤波器应用在后面网络阶段的特征图上，这是为了用检测器检测不同比例的图片，这样我们在相对低分辨率的图像上也能获得高精度的输出，还提升了检测速度。本文的贡献如下：</p>
<ul>
<li>提出了SSD算法—-多类别单阶检测器， 要比其它的单阶段检测器（YOLO）快，而且更准确；</li>
<li>SSD的核心部分是，在特征图上应用小卷积滤波器，预测分类得分和一个固定集合的默认边界框的偏移；</li>
<li>为了实现高检测精度，在不同比例的特征图上产生不同的预测，通过纵横比来分开预测；</li>
<li>以上的设计可以端到端训练，精度还高，甚至在低分辨率的图像上效果也不错；</li>
<li>关于速度和精度的试验，主要在PASCAL VOC, COCO 和 ILSVRC数据集上进行，与其它方法进行比较。</li>
</ul>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>SSD基于前馈式卷积神经网络，针对那些方框里的目标检测实例，产生一个固定大小边界框集合和分数，紧接着是一个非极大值抑制步骤来产生最后的检测。网络前半部分是个标准结构(用于高质量图片分类)，成为基网络。然后对网络增加了辅助结构来实现以下特征：</p>
<ul>
<li><p><strong>多比例特征图检测</strong>：在基网络后增加卷积特征层，这些层按大小减少的次序连接，能够进行多尺度预测。</p>
</li>
<li><p><strong>卷积检测预测器</strong>：通过一个卷积滤波器集合，每个新增的特征层可以产生一个预测集合。假设特征层大小为m x n，p个通道，预测参数的基本单元是一个3 x 3 x p的小核，它要么产生一个类别的得分，要么产生一个相对于默认方框的位置偏移。核一共要应用在m x n个位置上，在每个位置上它都有一个输出值。边界框的偏移输出值是相对于默认的位置的。</p>
</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_14-51-11.jpg" alt></p>
<ul>
<li><strong>默认方框和纵横比</strong>：将每个特征图单元(cell) 与默认边界框的集合关联起来，这是对于网络顶层的多特征图来说的。默认方框用卷积的方式覆盖特征图，这样，每个方框对应的单元(cell)是固定的。在每个特征映射单元上，我们预测相对于默认方框形状的偏移，以及每一类别的分数（表明每一个方框中一个类的出现）。在给定的位置有k个框，对于其中的每一个，计算c类类别的分数，和相对于原来默认方框形状的4个偏移。这就一共有(c + 4) x k个滤波器被应用到特征图的每个位置上；对于m x n的特征图，产生(c + 4) x k x m x n个输出。默认方框跟Faster R-CNN中的Anchor类似，但是作者将它们应用到不同分辨率的特征图上时，由于在一些特征图上有不同的默认框形状，这使得算法对不同尺度的目标有较好的探测作用。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSD/640.jfif" alt><br>SSD在训练中只需一张输入图像和图像中每个目标的ground truth边界框信息。在卷积操作中，我们产生一个默认方框的集合，这些方框在每个位置有不同的纵横比，在一些特征图中有不同的比例，如上图所示。对于每个默认方框，预测它形状的偏移和类别的置信度$\left(c_{1}, c_{2}, c_{3}, \dots, c_{p}\right)$。训练时，首先将这些默认方框和 ground truth 边界框对应上。就像图中，作者匹配了2个默认方框，一个是猫，一个是狗，它们被认定为positive, 其余部分被认定为 negative. 模型损失函数是 localization loss(smooth L1) 和 confidence  loss(Softmax) 的加权之和。</li>
</ul>
<h2 id="怎么设置default-boxes？"><a href="#怎么设置default-boxes？" class="headerlink" title="怎么设置default boxes？"></a>怎么设置default boxes？</h2><p>SSD中default box的概念有点类似于Faster R-CNN中的anchor。不同于Faster  R-CNN只在最后一个特征层取anchor，SSD在多个特征层上取default box，可以得到不同尺度的default  box。在特征图的每个单元上取不同宽高比的default  box,一般宽高比在{1，2，3，1/2，1/3}中选取，有时还会额外增加一个宽高比为1但具有特殊尺度的box。上面那张图展示了在8 x 8的feature map和4 x 4的feature map上的每个单元取4个不同的default box。原文对于300 x 300的输入，分别在conv4_3，conv7，conv8_2，conv9_2，conv10_2，conv11_2的特征图上的每个单元取4，6，6，6，4，4个default  box.  由于以上特征图的大小分别是38x38，19x19，10x10，5x5，3x3，1x1，所以一共得到38 x 38 x 4 + 19 x 19 x 6 + 10 x 10 x 6 + 5 x 5 x 6 + 3 x 3 x 4 + 1 x 1 x 4 = 8732个default box.对一张300x300的图片输入网络将会针对这8732个default  box预测8732个边界框。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>训练SSD和训练其他使用区域提议检测器的主要区别是，ground  truth信息在固定检测器输出的情况下需要指定到特定的输出。这样，损失函数和反向传播就可以端到端的应用。训练需要选择默认方框的集合，检测比例，以及hard negative mining（也就是不存在目标的样本集合）和数据增强策略。</p>
<ul>
<li><p><strong>Matching strategy</strong>， 在训练中，需要决定哪个默认框匹配一个 ground truth，由此来训练网络。对于每个从默认方框（不同位置，不同纵横比，不同比例上）中选择的 ground truth 边界框，开始时，根据最高的 jaccard overlap 来匹配 ground truth  边界框和默认方框（与MultiBox一样）。实际操作中，与 MultiBox 不同，当它们的 jaccard  overlap高于阈值0.5时，作者就将默认方框认为 ground  truth。这简化了学习问题，使得网络可以对多个重叠的默认方框预测得到高分，而不是仅挑选一个重合度最高的方框。</p>
</li>
<li><p><strong>Training objective</strong>， SSD的训练目标来自于MultiBox，但是作者将之扩展成可处理多目标分类的问题。$x_{i j}^{p}=1,0$表示匹配第$i$个默认方框和$p$类别第$j$个ground truth边界框。这种匹配策略会出现$\sum_{i} x_{i, j}^{p}&gt;=1$。整体的目标损失函数是定位损失加分类损失： $L(x, c, l, g)=\frac{1}{N}\left(L_{c o n f}(x, c)+\alpha L_{l o c}(x, l, g)\right)$，其中$N$是匹配默认框的个数，如果$N=0$，loss设为0。定位损失是预测边界框$l$和真值边界框参数$g$的Smooth L1 loss。与Faster R-CNN类似，对默认边界框$d$中心$(cx, cy)$的偏移量进行回归，$w$是宽，$h$是高。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSD/641.webp" alt><br>分类损失是多个类别置信度的 softmax loss：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSD/642.webp" alt><br>$\alpha$在交叉验证中设为1。</p>
</li>
<li><p><strong>Choosing scales and aspect ratio for default boxes</strong>，为了处理不同的目标比例，有些方法是把图片处理成不同的大小，然后结合不同大小图片的结果。但是在一个网络中利用多个不同层产生的特征图来预测也能产生类似的结果，所有比例的目标还可以共享参数。前面的研究已经证明使用底层的特征图可以提升语义分割质量，因为底层能捕捉到输入图像中的细节信息。受这些方法启发，本文使用了特征图中的高层和低层特征来进行目标检测。网络中不同层级的特征图会有不同的感受野，在SSD中，默认框不一定要和每层中的实际感受野对应。假设我们要用$m$个特征图来预测，每个特征图的默认框尺度计算如下：$s_{k}=s_{\min }+\frac{s_{\max }-s_{\min }}{m-1}(k-1, k \in[1, m])$ ，其中$s_{m i n}$是0.2，$s_{m a x}$是0.9，意味着最低的层的尺度是0.2，最高的层的尺度是0.9，中间层正常间隔分布。对于默认框，使用不同的高宽比，$a_{r} \in 1,2,3, \frac{1}{2}, \frac{1}{3}$。可以计算每个默认框的宽度$w_{k}^{a}=s_{k} \sqrt{a}_{r}$和高度$h_{k}^{a}=s_{k} / \sqrt{a_{r}}$。对于宽高比是1的情况，增加一个尺度是$s_{k}^{\prime}=\sqrt{s_{k} s_{k+1}}$的默认框，这样就在每个特征图位置有6个默认框。将框的中心设为$\left(\frac{i+0.5}{\left|f_{k}\right|}, \frac{j+0.5}{\left|f_{k}\right|}\right)$，其中$\left|f_{k}\right|$是第$k$个正方形特征图的大小。结合诸多特征图的不同位置下所有不同尺度和宽高比的默认框，就有了一个预测结果的集合，覆盖不同大小和形状的输入对象。例如图一中，那条狗与特征图中4 x 4的默认框匹配，但是不和任何8 x 8特征图中的默认框匹配，因为这些默认框有着不同的尺度，与狗的默认框不匹配，因此在训练中被认为是negative。</p>
</li>
<li><p><strong>Hard Negative Mining</strong> ，匹配步骤后，默认框中的大多数都是negative，尤其是候选框个数众多的时候。这就导致 positive 和 negative 训练样本不平衡。这些 negative 样本不全用，而是对于每一个默认框，通过最高置信度损失来对它们进行排序，选择最高的几个，这样negative 和 positive的比例最多是3:1。这样训练更稳定也更快。</p>
</li>
<li><p><strong>Data augmentation</strong> 为了让模型对不同的输入大小和形状更鲁棒，每张训练图片都通过以下步骤随机选择：(1)使用整张原始图片输入；(2)选择图片中的一块，与物体最低的  jaccard overlap 值为0.1, 0.3, 0.5, 0.7,  0.9；(3)随机采样某一块。采样区块的大小在原图片[0.1,1]之间，高宽比介于0.5和2之间。保留真值边界框中的重叠部分，如果它的中心在采样区块内。在采样步骤后，每个采样区块缩放到固定大小，以0.5的概率来水平翻转。</p>
</li>
</ul>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在PSCAL VOC2012上面：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSD/643.webp" alt></p>
<p>在COCO数据集上面：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSD/644.webp" alt></p>
<p>速度和精度的整体对比：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSD/645.png" alt></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>SSD优势是速度比较快，整个过程只需要一步，首先在图片不同位置按照不同尺度和宽高比进行密集抽样，然后利用CNN提取特征后直接进行分类与回归，所以速度比较快，但均匀密集采样会造成正负样本不均衡的情况使得训练比较困难，导致模型准确度有所降低。另外，SSD对小目标的检测没有大目标好，因为随着网络的加深，在高层特征图中小目标的信息丢失掉了，适当增大输入图片的尺寸可以提升小目标的检测效果。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet池化层代码详解(maxpool_layer.c)</title>
    <url>/2020/02/29/AlexeyAB-DarkNet%E6%B1%A0%E5%8C%96%E5%B1%82%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(maxpool_layer.c)/</url>
    <content><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title=" 原理"></a><a id="more"></a> 原理</h2><p>为了图文并茂的解释这个层，我们首先来说一下池化层的原理，池化层分为最大池化以及平均池化。最大池化可以用下图表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/666.png" alt></p>
<p>可以看到最大池化层需要记录池化输出特征图的每个值是由原始特征图中哪个值得来的，也就是需要额外记录一个最大值在原图的中的索引。而平均池化只需要将上面的求最大值的操作换成求平均的操作即可，因为是平均操作所以就没必要记录索引了。</p>
<h2 id="池化层的构造"><a href="#池化层的构造" class="headerlink" title="池化层的构造"></a>池化层的构造</h2><p>池化层的构造由<code>make_maxpool_layer</code>函数实现，虽然名字是构造<code>maxpool_layer</code>，但其实现也考虑了平均池化，也就是说通过参数设置可以将池化层变成平均池化。这一函数的详细讲解请看如下代码，为了美观，我去掉了一些无关代码，完整代码请到github查看。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 构建最大&#x2F;平均池化层</span><br><span class="line">** batch: 该层输入中一个batch所含有的图片张数，等于net.batch</span><br><span class="line">** h,w,c: 该层输入图片的高度，宽度与通道数</span><br><span class="line">** size: 池化核的大小</span><br><span class="line">** stride: 滑动步长</span><br><span class="line">** padding: 四周补0长度</span><br><span class="line">返回: 最大&#x2F;平均池化层l</span><br><span class="line">*&#x2F;</span><br><span class="line">maxpool_layer make_maxpool_layer(int batch, int h, int w, int c, int size, int stride_x, int stride_y, int padding, int maxpool_depth, int out_channels, int antialiasing, int avgpool, int train)</span><br><span class="line">&#123;</span><br><span class="line">    maxpool_layer l &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">	&#x2F;&#x2F;层类别</span><br><span class="line">    l.avgpool &#x3D; avgpool;</span><br><span class="line">    if (avgpool) l.type &#x3D; LOCAL_AVGPOOL;</span><br><span class="line">    else l.type &#x3D; MAXPOOL;</span><br><span class="line">    l.train &#x3D; train;</span><br><span class="line"></span><br><span class="line">    const int blur_stride_x &#x3D; stride_x;</span><br><span class="line">    const int blur_stride_y &#x3D; stride_y;</span><br><span class="line">    l.antialiasing &#x3D; antialiasing;</span><br><span class="line">    if (antialiasing) &#123;</span><br><span class="line">        stride_x &#x3D; stride_y &#x3D; l.stride &#x3D; l.stride_x &#x3D; l.stride_y &#x3D; 1; &#x2F;&#x2F; use stride&#x3D;1 in host-layer</span><br><span class="line">    &#125;</span><br><span class="line">    l.batch &#x3D; batch;&#x2F;&#x2F;一个batch中包含的图片数</span><br><span class="line">    l.h &#x3D; h; &#x2F;&#x2F;输入图片的高度</span><br><span class="line">    l.w &#x3D; w; &#x2F;&#x2F;输入图片的宽度</span><br><span class="line">    l.c &#x3D; c; &#x2F;&#x2F;输入图片的通道数</span><br><span class="line">    l.pad &#x3D; padding; &#x2F;&#x2F; 补0的个数</span><br><span class="line">    l.maxpool_depth &#x3D; maxpool_depth; &#x2F;&#x2F;池化层每隔l.maxpool_depth执行一次pool操作</span><br><span class="line">    l.out_channels &#x3D; out_channels; &#x2F;&#x2F;输出图片的通道数</span><br><span class="line">    if (maxpool_depth) &#123;</span><br><span class="line">        l.out_c &#x3D; out_channels;</span><br><span class="line">        l.out_w &#x3D; l.w;</span><br><span class="line">        l.out_h &#x3D; l.h;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        l.out_w &#x3D; (w + padding - size) &#x2F; stride_x + 1; &#x2F;&#x2F;输出图片的宽度</span><br><span class="line">        l.out_h &#x3D; (h + padding - size) &#x2F; stride_y + 1; &#x2F;&#x2F;输出图片的高度</span><br><span class="line">        l.out_c &#x3D; c; &#x2F;&#x2F;输出图片的通道数</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F;</span><br><span class="line">    l.outputs &#x3D; l.out_h * l.out_w * l.out_c; &#x2F;&#x2F;池化化层对应一张输入图片的输出元素个数</span><br><span class="line">    l.inputs &#x3D; h*w*c; &#x2F;&#x2F;池化层</span><br><span class="line">    l.size &#x3D; size; &#x2F;&#x2F;池化层池化窗口大小</span><br><span class="line">    l.stride &#x3D; stride_x; &#x2F;&#x2F;池化层步幅</span><br><span class="line">    l.stride_x &#x3D; stride_x; &#x2F;&#x2F;在x方向上的池化层步幅</span><br><span class="line">    l.stride_y &#x3D; stride_y; &#x2F;&#x2F;在y方向上的池化层步幅</span><br><span class="line">    int output_size &#x3D; l.out_h * l.out_w * l.out_c * batch; &#x2F;&#x2F; 池化层所有输出的元素个数（包含整个batch的）</span><br><span class="line"></span><br><span class="line">    if (train) &#123;</span><br><span class="line">		&#x2F;&#x2F; 训练的时候，用于保存每个最大池化窗口内的最大值对应的索引，方便之后的反向传播</span><br><span class="line">		&#x2F;&#x2F; 如果是平均池化层就不用了</span><br><span class="line">        if (!avgpool) l.indexes &#x3D; (int*)xcalloc(output_size, sizeof(int));</span><br><span class="line">		&#x2F;&#x2F;池化层的误差项</span><br><span class="line">        l.delta &#x3D; (float*)xcalloc(output_size, sizeof(float));</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F;池化层的所有输出(包含整个batch的)</span><br><span class="line">    l.output &#x3D; (float*)xcalloc(output_size, sizeof(float));</span><br><span class="line">    if (avgpool) &#123;</span><br><span class="line">		&#x2F;&#x2F;平均池化层的前向传播和反向传播</span><br><span class="line">        l.forward &#x3D; forward_local_avgpool_layer;</span><br><span class="line">        l.backward &#x3D; backward_local_avgpool_layer;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">		&#x2F;&#x2F;最大池化层的前向传播和反向传播</span><br><span class="line">        l.forward &#x3D; forward_maxpool_layer;</span><br><span class="line">        l.backward &#x3D; backward_maxpool_layer;</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F; GPU上和CPU上的操作类似</span><br><span class="line">#ifdef GPU</span><br><span class="line">    if (avgpool) &#123;</span><br><span class="line">        l.forward_gpu &#x3D; forward_local_avgpool_layer_gpu;</span><br><span class="line">        l.backward_gpu &#x3D; backward_local_avgpool_layer_gpu;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        l.forward_gpu &#x3D; forward_maxpool_layer_gpu;</span><br><span class="line">        l.backward_gpu &#x3D; backward_maxpool_layer_gpu;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (train) &#123;</span><br><span class="line">        if (!avgpool) l.indexes_gpu &#x3D; cuda_make_int_array(output_size);</span><br><span class="line">        l.delta_gpu &#x3D; cuda_make_array(l.delta, output_size);</span><br><span class="line">    &#125;</span><br><span class="line">    l.output_gpu  &#x3D; cuda_make_array(l.output, output_size);</span><br><span class="line">    create_maxpool_cudnn_tensors(&amp;l);</span><br><span class="line">    if (avgpool) cudnn_local_avgpool_setup(&amp;l);</span><br><span class="line">    else cudnn_maxpool_setup(&amp;l);</span><br><span class="line"></span><br><span class="line">#endif  &#x2F;&#x2F; GPU</span><br><span class="line">    &#x2F;&#x2F;计算池化层的参数量，以BFLOPs为单位，这是AlexeyAB DarkNet新增的</span><br><span class="line">	l.bflops &#x3D; (l.size*l.size*l.c * l.out_h*l.out_w) &#x2F; 1000000000.;</span><br><span class="line">    return l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最大池化层的前向传播"><a href="#最大池化层的前向传播" class="headerlink" title="最大池化层的前向传播"></a>最大池化层的前向传播</h2><p>AlexeyAB DarkNet的池化层和原始的DarkNet的池化层最大的不同在于新增了一个<code>l.maxpool_depth</code>参数，如果这个参数不为0，那么池化层需要每隔<code>l.out_channels</code>个特征图执行最大池化，注意这个参数只对最大池化有效。池化层的前向传播函数为<code>forward_maxpool_layer</code>，详细解释如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 池化层的前向传播函数</span><br><span class="line">** l: 当前层(最大池化层&#x2F;平均池化层)</span><br><span class="line">** net: 整个网络结构</span><br><span class="line">** 最大池化层处理图像的方式与卷积层类似，也是将最大池化核在图像</span><br><span class="line">** 平面上按照指定的跨度移动，并取对应池化核区域中最大元素值为对应输出元素。</span><br><span class="line">** 最大池化层没有训练参数（没有权重以及偏置），因此，相对与卷积来说，</span><br><span class="line">** 其前向（以及下面的反向）过程比较简单，实现上也是非常直接，不需要什么技巧。</span><br><span class="line">** 但需要注意AlexeyAB DarkNet在原始的代码上改动比较多，具体注释如下。</span><br><span class="line">*&#x2F;</span><br><span class="line">void forward_maxpool_layer(const maxpool_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F;如果l.maxpool_depth参数生效，执行下面的前向传播过程</span><br><span class="line">    if (l.maxpool_depth)</span><br><span class="line">    &#123;</span><br><span class="line">        int b, i, j, k, g;</span><br><span class="line">		&#x2F;&#x2F; 遍历batch中每一张输入图片，计算得到与每一张输入图片具有l.maxpool_depth个通道的输出图</span><br><span class="line">        for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;</span><br><span class="line">			&#x2F;&#x2F;openmp优化</span><br><span class="line">			&#x2F;&#x2F;外层循环遍历特征图的长</span><br><span class="line">            #pragma omp parallel for</span><br><span class="line">            for (i &#x3D; 0; i &lt; l.h; ++i) &#123;</span><br><span class="line">				&#x2F;&#x2F;中层循环遍历特征图的宽</span><br><span class="line">                for (j &#x3D; 0; j &lt; l.w; ++j) &#123;</span><br><span class="line">					&#x2F;&#x2F;内层循环遍历特征图的输出通道</span><br><span class="line">                    for (g &#x3D; 0; g &lt; l.out_c; ++g)</span><br><span class="line">                    &#123;</span><br><span class="line">						&#x2F;&#x2F;out_index为输出图中的索引</span><br><span class="line">                        int out_index &#x3D; j + l.w*(i + l.h*(g + l.out_c*b));</span><br><span class="line">                        float max &#x3D; -FLT_MAX;</span><br><span class="line">                        int max_i &#x3D; -1;</span><br><span class="line">						&#x2F;&#x2F;如上所述，每隔l.out_c个通道执行一次最大池化操作</span><br><span class="line">                        for (k &#x3D; g; k &lt; l.c; k +&#x3D; l.out_c)</span><br><span class="line">                        &#123;</span><br><span class="line">                            int in_index &#x3D; j + l.w*(i + l.h*(k + l.c*b));</span><br><span class="line">                            float val &#x3D; state.input[in_index];</span><br><span class="line">                            &#x2F;&#x2F;记录最大池化的索引</span><br><span class="line">                            max_i &#x3D; (val &gt; max) ? in_index : max_i;</span><br><span class="line">                            max &#x3D; (val &gt; max) ? val : max;</span><br><span class="line">                        &#125;</span><br><span class="line">                        l.output[out_index] &#x3D; max;</span><br><span class="line">                        if (l.indexes) l.indexes[out_index] &#x3D; max_i;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if (!state.train &amp;&amp; l.stride_x &#x3D;&#x3D; l.stride_y) &#123;</span><br><span class="line">		&#x2F;&#x2F;前向推理并且x和y方向的步幅相同的情况下，使用avx指令集优化Pool层的前向传播</span><br><span class="line">        forward_maxpool_layer_avx(state.input, l.output, l.indexes, l.size, l.w, l.h, l.out_w, l.out_h, l.c, l.pad, l.stride, l.batch);</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">        int b, i, j, k, m, n;</span><br><span class="line">		&#x2F;&#x2F; 初始偏移设定为四周补0长度的负值</span><br><span class="line">        int w_offset &#x3D; -l.pad &#x2F; 2;</span><br><span class="line">        int h_offset &#x3D; -l.pad &#x2F; 2;</span><br><span class="line">		&#x2F;&#x2F; 获取当前层的输出尺寸</span><br><span class="line">        int h &#x3D; l.out_h;</span><br><span class="line">        int w &#x3D; l.out_w;</span><br><span class="line">		&#x2F;&#x2F; 获取当前层输入图像的通道数，为什么是输入通道数？不应该为输出通道数吗？</span><br><span class="line">        &#x2F;&#x2F; 实际二者没有区别，对于最大池化层来说，输入有多少通道，输出就有多少通道！</span><br><span class="line">		&#x2F;&#x2F; 注意上面如果maxpool_depth有值，那么输出通道数就和输入通道数不一样了。</span><br><span class="line">        int c &#x3D; l.c;</span><br><span class="line">		&#x2F;&#x2F; 遍历batch中每一张输入图片，计算得到与每一张输入图片具有相同通道的输出图</span><br><span class="line">        for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;</span><br><span class="line">			&#x2F;&#x2F; 对于每张输入图片，将得到通道数一样的输出图，以输出图为基准，按输出图通道，行，列依次遍历</span><br><span class="line">			&#x2F;&#x2F; （这对应图像在l.output的存储方式，每张图片按行铺排成一大行，然后图片与图片之间再并成一行）。</span><br><span class="line">			&#x2F;&#x2F; 以输出图为基准进行遍历，最终循环的总次数刚好覆盖池化核在输入图片不同位置进行池化操作。</span><br><span class="line">            for (k &#x3D; 0; k &lt; c; ++k) &#123;</span><br><span class="line">                for (i &#x3D; 0; i &lt; h; ++i) &#123;</span><br><span class="line">                    for (j &#x3D; 0; j &lt; w; ++j) &#123;</span><br><span class="line">						&#x2F;&#x2F; out_index为输出图中的索引：out_index &#x3D; b * c * w * h + k * w * h + h * w + w，展开写可能更为清晰些</span><br><span class="line">                        int out_index &#x3D; j + w*(i + h*(k + c*b));</span><br><span class="line">                        float max &#x3D; -FLT_MAX;</span><br><span class="line">                        int max_i &#x3D; -1;</span><br><span class="line">						&#x2F;&#x2F; 下面两个循环回到了输入图片，计算得到的cur_h以及cur_w都是在当前层所有输入元素的索引，内外循环的目的是</span><br><span class="line">                        &#x2F;&#x2F; 找寻输入图像中，以(h_offset + i*l.stride, w_offset + j*l.stride)为左上起点，尺寸为l.size池化区域中的</span><br><span class="line">                        &#x2F;&#x2F;最大元素值max及其在所有输入元素中的索引max_i</span><br><span class="line">                        for (n &#x3D; 0; n &lt; l.size; ++n) &#123;</span><br><span class="line">                            for (m &#x3D; 0; m &lt; l.size; ++m) &#123;</span><br><span class="line">								&#x2F;&#x2F;cur_h, cur_w是在所有输入图像的第k通道的cur_h行与cur_w列，index是在所有输入图像元素中的总索引</span><br><span class="line">                                int cur_h &#x3D; h_offset + i*l.stride_y + n;</span><br><span class="line">                                int cur_w &#x3D; w_offset + j*l.stride_x + m;</span><br><span class="line">                                int index &#x3D; cur_w + l.w*(cur_h + l.h*(k + b*l.c));</span><br><span class="line">								&#x2F;&#x2F; 边界检查：正常情况下，是不会越界的，但是如果有补0操作，就会越界了，这里的处理方式是直接让这些元素值为-FLT_MAX</span><br><span class="line">                                int valid &#x3D; (cur_h &gt;&#x3D; 0 &amp;&amp; cur_h &lt; l.h &amp;&amp;</span><br><span class="line">                                    cur_w &gt;&#x3D; 0 &amp;&amp; cur_w &lt; l.w);</span><br><span class="line">								&#x2F;&#x2F; 记录这个池化区域中最大的元素及其在所有输入元素中的总索引</span><br><span class="line">                                float val &#x3D; (valid !&#x3D; 0) ? state.input[index] : -FLT_MAX;</span><br><span class="line">                                max_i &#x3D; (val &gt; max) ? index : max_i;</span><br><span class="line">                                max &#x3D; (val &gt; max) ? val : max;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">						&#x2F;&#x2F; 由此得到最大池化层每一个输出元素值及其在所有输入元素中的总索引。</span><br><span class="line">						&#x2F;&#x2F; 为什么需要记录每个输出元素值对应在输入元素中的总索引呢？因为在下面的反向过程中需要用到，在计算当前最大池化层上一层网络的敏感度时，</span><br><span class="line">						&#x2F;&#x2F; 需要该索引明确当前层的每个元素究竟是取上一层输出（也即上前层输入）的哪一个元素的值，具体见下面backward_maxpool_layer()函数的注释。</span><br><span class="line">                        l.output[out_index] &#x3D; max;</span><br><span class="line">                        if (l.indexes) l.indexes[out_index] &#x3D; max_i;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (l.antialiasing) &#123;</span><br><span class="line">        network_state s &#x3D; &#123; 0 &#125;;</span><br><span class="line">        s.train &#x3D; state.train;</span><br><span class="line">        s.workspace &#x3D; state.workspace;</span><br><span class="line">        s.net &#x3D; state.net;</span><br><span class="line">        s.input &#x3D; l.output;</span><br><span class="line">        forward_convolutional_layer(*(l.input_layer), s);</span><br><span class="line">        &#x2F;&#x2F;simple_copy_ongpu(l.outputs*l.batch, l.output, l.input_antialiasing);</span><br><span class="line">        memcpy(l.output, l.input_layer-&gt;output, l.input_layer-&gt;outputs * l.input_layer-&gt;batch * sizeof(float));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最大池化层的反向传播"><a href="#最大池化层的反向传播" class="headerlink" title="最大池化层的反向传播"></a>最大池化层的反向传播</h2><p>池化层的反向传播由<code>backward_maxpool_layer</code>实现，反向传播实际上比前向传播更加简单，你可以停下来想想为什么，再看我下面的详细解释。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 最大池化层反向传播函数</span><br><span class="line">** l: 当前最大池化层</span><br><span class="line">** state: 整个网络</span><br><span class="line">** 说明：这个函数看上去很简单，比起backward_convolutional_layer()少了很多，这都是有原因的。实际上，在darknet中，不管是什么层，</span><br><span class="line">**      其反向传播函数都会先后做两件事：1）计算当前层的敏感度图l.delta、权重更新值以及偏置更新值；2）计算上一层的敏感度图net.delta（部分计算，</span><br><span class="line">**      要完成计算得等到真正到了这一层再说）。而这里，显然没有第一步，只有第二步，而且很简单，这是为什么呢？首先回答为什么没有第一步。注意当前层l是最大池化层，</span><br><span class="line">**      最大池化层没有训练参数，说的再直白一点就是没有激活函数，或者认为激活函数就是f(x)&#x3D;x，所以激活函数对于加权输入的导数其实就是1,</span><br><span class="line">**      正如在backward_convolutional_layer()注释的那样，每一层的反向传播函数的第一步是将之前（就是下一层计算得到的，注意过程是反向的）</span><br><span class="line">**      未计算完得到的l.delta乘以激活函数对加权输入的导数，以最终得到当前层的敏感度图，而对于最大池化层来说，每一个输出对于加权输入的导数值都是1,</span><br><span class="line">**      同时并没有权重及偏置这些需要训练的参数，自然不再需要第一步；对于第二步为什么会如此简单。请看代码注释。</span><br><span class="line">*&#x2F;</span><br><span class="line">void backward_maxpool_layer(const maxpool_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">	&#x2F;&#x2F;获取当前最大池化层l的输出尺寸h,w</span><br><span class="line">    int h &#x3D; l.out_h;</span><br><span class="line">    int w &#x3D; l.out_w;</span><br><span class="line">	&#x2F;&#x2F;获取当前层输入&#x2F;输出通道数</span><br><span class="line">    int c &#x3D; l.out_c;</span><br><span class="line">	&#x2F;&#x2F; 计算上一层的敏感度图（未计算完全，还差一个环节，这个环节等真正反向到了那层再执行，但是其实已经完全计算了，因为池化层无参数）</span><br><span class="line">    &#x2F;&#x2F; 循环总次数为当前层输出总元素个数（包含所有输入图片的输出，即维度为l.out_h * l.out_w * l.c * l.batch，注意此处l.c&#x3D;&#x3D;l.out_c）</span><br><span class="line">    &#x2F;&#x2F; 对于上一层输出中的很多元素的导数值为0,而对最大值元素，其导数值为1；再乘以当前层的敏感度图，导数值为0的还是为0,导数值为1则就等于当前层的敏感度值。</span><br><span class="line">    &#x2F;&#x2F; 以输出图总元素个数进行遍历，刚好可以找出上一层输出中所有真正起作用（在某个池化区域中充当了最大元素值）也即敏感度值不为0的元素，而那些没有起作用的元素，</span><br><span class="line">    &#x2F;&#x2F; 可以不用理会，保持其初始值0就可以了。</span><br><span class="line">    #pragma omp parallel for &#x2F;&#x2F;openmp优化</span><br><span class="line">    for(i &#x3D; 0; i &lt; h*w*c*l.batch; ++i)&#123;</span><br><span class="line">        int index &#x3D; l.indexes[i];</span><br><span class="line">		&#x2F;&#x2F; 遍历的基准是以当前层的输出元素为基准的，l.indexes记录了当前层每一个输出元素与上一层哪一个输出元素有真正联系（也即上一层对应池化核区域中最大值元素的索引），</span><br><span class="line">        &#x2F;&#x2F; 所以index是上一层中所有输出元素的索引，且该元素在当前层某个池化域中充当了最大值元素，这个元素的敏感度值将直接传承当前层对应元素的敏感度值。</span><br><span class="line">        &#x2F;&#x2F; 而net.delta中，剩下没有被index按索引访问到的元素，就是那些没有真正起到作用的元素，这些元素的敏感度值为0（net.delta已经在前向时将所有元素值初始化为0）</span><br><span class="line">        &#x2F;&#x2F; 至于为什么要用+&#x3D;运算符，原因有两个，和卷积类似：一是池化核由于跨度较小，导致有重叠区域；二是batch中有多张图片，需要将所有图片的影响加起来。</span><br><span class="line">        state.delta[index] +&#x3D; l.delta[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="平均池化层的前向传播和反向传播"><a href="#平均池化层的前向传播和反向传播" class="headerlink" title="平均池化层的前向传播和反向传播"></a>平均池化层的前向传播和反向传播</h2><p>刚才已经讲到了，最大池化以及平均池化整理是非常类似的，只是把最大的算术操作换成平均，然后平均池化层的反向传播就完成了，具体的代码可以去github项目中查看。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv2</title>
    <url>/2020/02/28/YOLOv2/</url>
    <content><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title=" 原理"></a><a id="more"></a> 原理</h2><p>YOLOv1作为One-Stage目标检测算法的开山之作，速度快是它最大的优势。但我们知道，YOLOv1的定位不够准，并且召回率低。为了提升定位准确度，提高召回率，YOLOv2在YOLOv1的基础上进行了改进。具体的改进方法如图Fig1所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/640.webp" alt></p>
<p>可以看到YOLOv2通过增加一些Trick使得v1的map值从63.4提高到了78.6，说明了YOLOv2改进方法的有效性。接下来我们就分析一下这些改进方法。</p>
<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p>这个应该不用多说了，YOLOv2在每个卷积层后面增加了BN层，去掉全连接的dropout。使用BN策略将map值提高了2%。</p>
<h2 id="高分辨率"><a href="#高分辨率" class="headerlink" title="高分辨率"></a>高分辨率</h2><p>当前大多数目标检测网络都喜欢使用主流分类网络如VGG,ResNet来做Backbone，而这些网络大多是在ImageNet上训练的，而分辨率的大小必然会影响到模型在测试集上的表现。所以，YOLOv2将输入的分辨率提升到$448 \times 448$，同时，为了使网络适应高分辨率，YOLOv2先在ImageNet上以$448 \times 448$的分辨率对网络进行10个epoch的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2将map值提高了约4%。</p>
<h2 id="基于卷积的Anchor机制"><a href="#基于卷积的Anchor机制" class="headerlink" title="基于卷积的Anchor机制"></a>基于卷积的Anchor机制</h2><p>YOLOv1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLOv2去掉了YOLOv1中的全连接层，使用Anchor  Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLOv2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416x416的输入，模型下采样的总步长为32，最后得到13x13的特征图，然后对13x13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。使用anchor boxes之后，YOLOv2可以预测13x13x5=845个边界框，模型的召回率由原来的81%提升到88%，mAP由原来的69.5%降低到69.2%.召回率提升了7%，准确率下降了0.3%。这里我们和SSD以及Faster-RCNN做个对比，Faster  RCNN输入大小为1000*600时的boxes数量大概是6000，在SSD300中boxes数量是8732。显然增加box数量是为了提高object的定位准确率。</p>
<h2 id="维度聚类"><a href="#维度聚类" class="headerlink" title="维度聚类"></a>维度聚类</h2><p>在Faster-RCNN中，Anchor都是手动设定的，YOLOv2使用k-means聚类算法对训练集中的边界框做了聚类分析，尝试找到合适尺寸的Anchor。另外作者发现如果采用标准的k-means聚类，在box的尺寸比较大的时候其误差也更大，而我们希望的是误差和box的尺寸没有太大关系。所以通过IOU定义了如下的距离函数，使得误差和box的大小无关：</p>
<script type="math/tex; mode=display">
d(\text {box }, \text { centroid })=1-I O U(\text { box }, \text { centroid })</script><p>Fig2展示了聚类的簇的个数和IOU之间的关系，两条曲线分别代表了VOC和COCO数据集的测试结果。最后结合不同的K值对召回率的影响，论文选择了K=5，Figure2中右边的示意图是选出来的5个box的大小，这里紫色和黑色也是分别表示两个不同的数据集，可以看出其基本形状是类似的。而且发现聚类的结果和手动设置的anchor box大小差别显著。聚类的结果中多是高瘦的box，而矮胖的box数量较少，这也比较符合数据集中目标的视觉效果。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/641.png" alt></p>
<p>在结果测试时，YOLOv2采用的5种Anchor可以达到的Avg  IOU是61，而Faster-RCNN采用9种Anchor达到的平均IOU是60.9，也即是说本文仅仅选取5种Anchor就可以达到Faster-RCNN中9种Anchor的效果。如Table1所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/642.png" alt></p>
<h2 id="新Backbone-Darknet-19"><a href="#新Backbone-Darknet-19" class="headerlink" title="新Backbone:Darknet-19"></a>新Backbone:Darknet-19</h2><p>YOLOv2采用Darknet-19，其网络结构如下图所示，包括19个卷积层和5个max pooling层，主要采用$3 \times 3$卷积和$1 \times 1$卷积，这里$1 \times 1$卷积可以压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用BN层以加快模型收敛同时防止过拟合。最终采用global avg pool 做预测。采用YOLOv2，模型的mAP值没有显著提升，但计算量减少了。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/643.webp" alt></p>
<h2 id="直接位置预测"><a href="#直接位置预测" class="headerlink" title="直接位置预测"></a>直接位置预测</h2><p>YOLOv2在引入Anchor的时候碰到第2个问题：模型不稳定，尤其是训练刚开始阶段。论文任务这种不稳定主要来自box的(x,y)预测值。我们知道在Faster-RCNN中，是通过预测下图中的$t_x$和$t_y$来得到(x,y)值，也就是预测的是offset。另外关于文中的这个公式，这个地方应该把后面的减号改成加号，这样才能符合公式下面的example。这里$\boldsymbol{x}_{\boldsymbol{a}}$和$\boldsymbol{y}_{\boldsymbol{a}}$是anchor的坐标，$\boldsymbol{w}_{\boldsymbol{a}}$和$\boldsymbol{h}_{\boldsymbol{a}}$是anchor的size，$\boldsymbol{x}$和$\boldsymbol{y}$是坐标的预测值，$\boldsymbol{t}_{\boldsymbol{x}}$和$\boldsymbol{t}_{\boldsymbol{y}}$是偏移量。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/644.png" alt></p>
<p>例子翻译过来是：当预测时$t_{x}=1$，就会把box向右边移动一定距离（具体为anchor box的宽度），预测时$t_{x}=-1$，就会把box向左边移动相同的距离。这个公式没有任何限制，使得无论在什么位置进行预测，任何anchor boxes可以在图像中任意一点结束，模型随机初始化后，需要花很长一段时间才能稳定预测敏感的物体位置。.</p>
<p>注意，高能来了！！！分析了原因之后，YOLOv2没有采用直接预测offset的方法，还是沿用了YOLO算法中直接预测相对于grid  cell的坐标位置的方式。前面提到网络在最后一个卷积层输出13*13大小的特征图，然后每个cell预测5个bounding  box，然后每个bounding box预测5个值：$t_{x}, t_{y}, t_{w}, t_{h}$和$t_o$（这里的$t_{o}$类似YOLOv1中的confidence）。$t_x$和$t_{y}$经过sigmoid函数处理后范围在0到1之间，这样的归一化处理使得模型训练更加稳定。$c_x$和$c_{y}$表示一个cell和图像左上角的横纵距离。$p_w$和$p_h$表示bounding box的宽高，这样$b_x$和$b_y$就是$c_x$和$c_y$这个cell附近的anchor来预测$t_x$和$t<br>_y$得到的结果。如Fig3所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/645.webp" alt></p>
<p>其中，$c_x$和$c_{y}$表示grid cell与图像左上角的横纵坐标距离，黑色虚线框是bounding box，蓝色矩形框就是最终预测的结果。注意，上图右边里面的$\delta\left(t_{x}\right)$可以理解为$s t_{x}$，$\delta\left(t_{y}\right)$可以理解为$s t_{y}$。每一个输出的bounding box是针对于一个特定的anchor，anchor其实是bounding box的width及height的一个参考。$p_{w}$和$p_{h}$是某个anchor box的宽和高，一个格子的$c_{x}$和$c_{y}$单位都是1，$\delta\left(t_{x}\right)$，$\delta\left(t_{y}\right)$是相对于某个格子左上角的偏移量。</p>
<h2 id="细粒度特征"><a href="#细粒度特征" class="headerlink" title="细粒度特征"></a>细粒度特征</h2><p>YOLOv2提取Darknet-19最后一个max pool层的输入，得到26x26x512的特征图。经过1x1x64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass  through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%。这个过程可以在下面的YOLOv2的结构图中看得很清楚：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/646.webp" alt></p>
<h2 id="多尺度训练"><a href="#多尺度训练" class="headerlink" title="多尺度训练"></a><strong>多尺度训练</strong></h2><p>YOLOv2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数{320,352,…,608}。采用Multi-Scale Training,  可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p>
<p>这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。在小尺寸图片检测中，YOLOv2成绩很好，输入为228 x 228的时候，帧率达到90FPS，mAP几乎和Faster  R-CNN的水准相同。使得其在低性能GPU、高帧率视频、多路视频场景中更加适用。在大尺寸图片检测中，YOLOv2达到了SOAT结果，VOC2007 上mAP为78.6%，仍然高于平均水准，下图是YOLOv2和其他网络的精度对比：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/647.jfif" alt></p>
<p>速度对比：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv2/648.webp" alt></p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>YOLOv2的训练主要包括三个阶段。第一阶段：作者使用Darknet-19在标准1000类的ImageNet上训练了160次，用的随机梯度下降法，starting learning rate 为0.1，polynomial rate decay 为4，weight decay为0.0005  ，momentum 为0.9。训练的时候仍然使用了很多常见的数据扩充方法（data augmentation），包括random crops,  rotations, and hue, saturation, and exposure  shifts。（这些训练参数是基于darknet框架，和caffe不尽相同）初始的224 <em> 224训练后，作者把分辨率上调到了448 </em>  448，然后又训练了10次，学习率调整到了0.001。高分辨率下训练的分类网络在top-1准确率76.5%，top-5准确率93.3%。</p>
<p>第二个阶段：分类网络训练完后，就该训练检测网络了，作者去掉了原网络最后一个卷积层，转而增加了三个3 <em> 3 </em> 1024的卷积层（可参考darknet中cfg文件），并且在每一个上述卷积层后面跟一个1 <em>  1的卷积层，输出维度是检测所需的数量。对于VOC数据集，预测5种boxes大小，每个box包含5个坐标值和20个类别，所以总共是5 </em>  （5+20）= 125个输出维度。同时也添加了转移层（passthrough layer ），从最后那个3 <em> 3 </em>  512的卷积层连到倒数第二层，使模型有了细粒度特征。作者的检测模型以0.001的初始学习率训练了160次，在60次和90次的时候，学习率减为原来的十分之一。其他的方面，weight decay为0.0005，momentum为0.9，依然使用了类似于Faster-RCNN和SSD的数据扩充（data  augmentation）策略。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>YOLOv2借鉴了很多其它目标检测方法的一些技巧，如Faster R-CNN的anchor boxes,  SSD中的多尺度检测。除此之外，YOLOv2在网络设计上做了很多tricks,使它能在保证速度的同时提高检测准确率，Multi-Scale  Training更使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOV3损失函数代码详解(yolo_layer.c)</title>
    <url>/2020/02/28/YOLOV3%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(yolo_layer.c)/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>YOLOV3的损失函数在YOLOV2的基础上，用多个独立的逻辑回归损失代替了YOLOV2里面的softmax损失，然后去掉了对Anchor在前12800次训练轮次中的回归损失，也即是YOLOV2损失函数的第二项。另外新增了一个ignore_thresh参数来忽略一些和GT box的IOU大于ignore_thresh的预测框的objectness损失。除了以上细节，其它部分和YOLOV2的处理类似。</p>
<h2 id="AlexeyAB的一些更新"><a href="#AlexeyAB的一些更新" class="headerlink" title="AlexeyAB的一些更新"></a>AlexeyAB的一些更新</h2><p>除了上面提到的相对于YOLOV2一些基础改动，AlexeyAB大神在目标框回归过程中新增了IOU/GIOU/DIOU/CIOU Loss，并且在分类过程中新增了Focal Loss，方便大家在自己的数据集上进行试验，预祝涨点。</p>
<h2 id="代码解析步骤"><a href="#代码解析步骤" class="headerlink" title="代码解析步骤"></a>代码解析步骤</h2><h3 id="yolo-层"><a href="#yolo-层" class="headerlink" title="[yolo]层"></a>[yolo]层</h3><p>YOLOV3使用[yolo] 层来计算损失函数以及预测分类和边界框回归，前面经过 darknet-53 的基础网络提取特征，又经过一些特征融合，就得到了3个不同尺度的 yolo 层，分别预测大、中、小物体。主要代码在<code>/src/yolo_layer.c</code>。cfg文件的配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[yolo]</span><br><span class="line">mask &#x3D; 0,1,2  #该层预测哪个规模的框，0,1,2表示预测小物体</span><br><span class="line">anchors &#x3D; 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326 </span><br><span class="line">classes&#x3D;80</span><br><span class="line">num&#x3D;9</span><br><span class="line">jitter&#x3D;.3</span><br><span class="line">ignore_thresh &#x3D; .7</span><br><span class="line">truth_thresh &#x3D; 1</span><br><span class="line">random&#x3D;1</span><br></pre></td></tr></table></figure></p>
<h3 id="make-yolo-layer-完成-yolo-层初始化操作"><a href="#make-yolo-layer-完成-yolo-层初始化操作" class="headerlink" title="make_yolo_layer 完成 yolo 层初始化操作"></a>make_yolo_layer 完成 yolo 层初始化操作</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 构造YOLOV3的yolo层</span><br><span class="line">&#x2F;&#x2F; batch 一个batch中包含图片的张数</span><br><span class="line">&#x2F;&#x2F; w 输入图片的宽度</span><br><span class="line">&#x2F;&#x2F; h 输入图片的高度</span><br><span class="line">&#x2F;&#x2F; n 一个cell预测多少个bbox</span><br><span class="line">&#x2F;&#x2F; total total Anchor bbox的数目</span><br><span class="line">&#x2F;&#x2F; mask 使用的是0,1,2 还是</span><br><span class="line">&#x2F;&#x2F; classes 网络需要识别的物体类别数</span><br><span class="line">layer make_yolo_layer(int batch, int w, int h, int n, int total, int *mask, int classes, int max_boxes)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">    layer l &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">    l.type &#x3D; YOLO; &#x2F;&#x2F;层类别</span><br><span class="line"></span><br><span class="line">    l.n &#x3D; n; &#x2F;&#x2F;一个cell预测多少个bbox</span><br><span class="line">    l.total &#x3D; total; &#x2F;&#x2F;anchors的数目，为9</span><br><span class="line">    l.batch &#x3D; batch;&#x2F;&#x2F; 一个batch包含图片的张数</span><br><span class="line">    l.h &#x3D; h; &#x2F;&#x2F; 输入图片的宽度</span><br><span class="line">    l.w &#x3D; w; &#x2F;&#x2F; 输入图片的高度</span><br><span class="line">    l.c &#x3D; n*(classes + 4 + 1); &#x2F;&#x2F; 输入图片的通道数, 3*(20 + 5)</span><br><span class="line">    l.out_w &#x3D; l.w;&#x2F;&#x2F; 输出图片的宽度</span><br><span class="line">    l.out_h &#x3D; l.h;&#x2F;&#x2F; 输出图片的高度</span><br><span class="line">    l.out_c &#x3D; l.c;&#x2F;&#x2F; 输出图片的通道数</span><br><span class="line">    l.classes &#x3D; classes;&#x2F;&#x2F;目标类别数</span><br><span class="line">    l.cost &#x3D; (float*)xcalloc(1, sizeof(float)); &#x2F;&#x2F;yolo层总的损失</span><br><span class="line">    l.biases &#x3D; (float*)xcalloc(total * 2, sizeof(float)); &#x2F;&#x2F;存储bbox的Anchor box的[w,h]</span><br><span class="line">    if(mask) l.mask &#x3D; mask; &#x2F;&#x2F;yolov3有mask传入</span><br><span class="line">    else&#123;</span><br><span class="line">        l.mask &#x3D; (int*)xcalloc(n, sizeof(int));</span><br><span class="line">        for(i &#x3D; 0; i &lt; n; ++i)&#123;</span><br><span class="line">            l.mask[i] &#x3D; i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F;存储bbox的Anchor box的[w,h]的更新值</span><br><span class="line">    l.bias_updates &#x3D; (float*)xcalloc(n * 2, sizeof(float));</span><br><span class="line">	&#x2F;&#x2F; 一张训练图片经过yolo层后得到的输出元素个数（等于网格数*每个网格预测的矩形框数*每个矩形框的参数个数）</span><br><span class="line">    l.outputs &#x3D; h*w*n*(classes + 4 + 1);</span><br><span class="line">	&#x2F;&#x2F;一张训练图片输入到yolo层的元素个数（注意是一张图片，对于yolo_layer，输入和输出的元素个数相等）</span><br><span class="line">    l.inputs &#x3D; l.outputs;</span><br><span class="line">	&#x2F;&#x2F;每张图片含有的真实矩形框参数的个数（max_boxes表示一张图片中最多有max_boxes个ground truth矩形框，每个真实矩形框有</span><br><span class="line">    &#x2F;&#x2F;5个参数，包括x,y,w,h四个定位参数，以及物体类别）,注意max_boxes是darknet程序内写死的，实际上每张图片可能</span><br><span class="line">    &#x2F;&#x2F;并没有max_boxes个真实矩形框，也能没有这么多参数，但为了保持一致性，还是会留着这么大的存储空间，只是其中的</span><br><span class="line">    &#x2F;&#x2F;值为空而已.</span><br><span class="line">    l.max_boxes &#x3D; max_boxes;</span><br><span class="line">	&#x2F;&#x2F; GT: max_boxes*(4+1) 存储max_boxes个bbox的信息，这里是假设图片中GT bbox的数量是</span><br><span class="line">	&#x2F;&#x2F;小于max_boxes的，这里是写死的；此处与yolov1是不同的</span><br><span class="line">    l.truths &#x3D; l.max_boxes*(4 + 1);    &#x2F;&#x2F; 90*(4 + 1);</span><br><span class="line">	&#x2F;&#x2F; yolo层误差项(包含整个batch的)</span><br><span class="line">    l.delta &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float));</span><br><span class="line">	&#x2F;&#x2F;yolo层所有输出（包含整个batch的）</span><br><span class="line">    &#x2F;&#x2F;yolo的输出维度是l.out_w*l.out_h，等于输出的维度，输出的通道数为l.out_c，也即是输入的通道数，具体为：n*(classes+coords+1)</span><br><span class="line">	&#x2F;&#x2F;YOLO检测模型将图片分成S*S个网格，每个网格又预测B个矩形框，最后输出的就是这些网格中包含的所有矩形框的信息</span><br><span class="line">    l.output &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float));</span><br><span class="line">	&#x2F;&#x2F; 存储bbox的Anchor box的[w,h]的初始化,在src&#x2F;parse.c中parse_yolo函数会加载cfg中Anchor尺寸</span><br><span class="line">    for(i &#x3D; 0; i &lt; total*2; ++i)&#123;</span><br><span class="line">        l.biases[i] &#x3D; .5;</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F; yolo层的前向传播</span><br><span class="line">    l.forward &#x3D; forward_yolo_layer;</span><br><span class="line">	&#x2F;&#x2F; yolo层的反向传播</span><br><span class="line">    l.backward &#x3D; backward_yolo_layer;</span><br><span class="line">#ifdef GPU</span><br><span class="line">    l.forward_gpu &#x3D; forward_yolo_layer_gpu;</span><br><span class="line">    l.backward_gpu &#x3D; backward_yolo_layer_gpu;</span><br><span class="line">    l.output_gpu &#x3D; cuda_make_array(l.output, batch*l.outputs);</span><br><span class="line">    l.delta_gpu &#x3D; cuda_make_array(l.delta, batch*l.outputs);</span><br><span class="line"></span><br><span class="line">    free(l.output);</span><br><span class="line">    if (cudaSuccess &#x3D;&#x3D; cudaHostAlloc(&amp;l.output, batch*l.outputs*sizeof(float), cudaHostRegisterMapped)) l.output_pinned &#x3D; 1;</span><br><span class="line">    else &#123;</span><br><span class="line">        cudaGetLastError(); &#x2F;&#x2F; reset CUDA-error</span><br><span class="line">        l.output &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    free(l.delta);</span><br><span class="line">    if (cudaSuccess &#x3D;&#x3D; cudaHostAlloc(&amp;l.delta, batch*l.outputs*sizeof(float), cudaHostRegisterMapped)) l.delta_pinned &#x3D; 1;</span><br><span class="line">    else &#123;</span><br><span class="line">        cudaGetLastError(); &#x2F;&#x2F; reset CUDA-error</span><br><span class="line">        l.delta &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float));</span><br><span class="line">    &#125;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    fprintf(stderr, &quot;yolo\n&quot;);</span><br><span class="line">    srand(time(0));</span><br><span class="line"></span><br><span class="line">    return l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="get-yolo-box-获得预测的边界框"><a href="#get-yolo-box-获得预测的边界框" class="headerlink" title="get_yolo_box 获得预测的边界框"></a>get_yolo_box 获得预测的边界框</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;获取某个矩形框的4个定位信息，即根据输入的矩形框索引从l.output中获取该矩形框的定位信息x,y,w,h</span><br><span class="line">&#x2F;&#x2F;x  yolo_layer的输出，即l.output，包含所有batch预测得到的矩形框信息</span><br><span class="line">&#x2F;&#x2F;biases 表示Anchor框的长和宽</span><br><span class="line">&#x2F;&#x2F;index 矩形框的首地址（索引，矩形框中存储的首个参数x在l.output中的索引）</span><br><span class="line">&#x2F;&#x2F;i 第几行（yolo_layer维度为l.out_w*l.out_c）</span><br><span class="line">&#x2F;&#x2F;j 第几列</span><br><span class="line">&#x2F;&#x2F;lw 特征图的宽度</span><br><span class="line">&#x2F;&#x2F;lh 特征图的高度</span><br><span class="line">&#x2F;&#x2F;w 输入图像的宽度</span><br><span class="line">&#x2F;&#x2F;h 输入图像的高度</span><br><span class="line">&#x2F;&#x2F;stride 不同的特征图具有不同的步长(即是两个grid cell之间跨的像素个数不同)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;biases中存储的是预定以的anchor box的宽和高（输入图尺度），(lw,lh)是yolo层输入的特征图尺度，</span><br><span class="line">&#x2F;&#x2F;(w,h)是整个网络输入图尺度，get_yolo_box()函数利用了论文截图中的公式，而且把结果分别利用特征</span><br><span class="line">&#x2F;&#x2F;图宽高和输入图宽高做了归一化。既然这个机制是用来限制回归，避免预测很远的目标，那么这个预测</span><br><span class="line">&#x2F;&#x2F;范围是多大呢？(b.x,by)最小是(i,j),最大是(i+1,x+1)，即中心点在特征图上最多一定一个像素（假设</span><br><span class="line">&#x2F;&#x2F;输入图下采样n得到特征图，特征图中一个像素对应输入图的n个像素）(b.w,b.h)最大是(2.7 * anchor.w,</span><br><span class="line">&#x2F;&#x2F;2.7 * anchor.h),最小就是(anchor.w,anchor.h)，这是在输入图尺寸下的值。</span><br><span class="line"></span><br><span class="line">box get_yolo_box(float *x, float *biases, int n, int index, int i, int j, int lw, int lh, int w, int h, int stride)</span><br><span class="line">&#123;</span><br><span class="line">    box b;</span><br><span class="line">    &#x2F;&#x2F; ln - natural logarithm (base &#x3D; e)</span><br><span class="line">    &#x2F;&#x2F; x&#96; &#x3D; t.x * lw - i;   &#x2F;&#x2F; x &#x3D; ln(x&#96;&#x2F;(1-x&#96;))   &#x2F;&#x2F; x - output of previous conv-layer</span><br><span class="line">    &#x2F;&#x2F; y&#96; &#x3D; t.y * lh - i;   &#x2F;&#x2F; y &#x3D; ln(y&#96;&#x2F;(1-y&#96;))   &#x2F;&#x2F; y - output of previous conv-layer</span><br><span class="line">                            &#x2F;&#x2F; w &#x3D; ln(t.w * net.w &#x2F; anchors_w); &#x2F;&#x2F; w - output of previous conv-layer</span><br><span class="line">                            &#x2F;&#x2F; h &#x3D; ln(t.h * net.h &#x2F; anchors_h); &#x2F;&#x2F; h - output of previous conv-layer</span><br><span class="line">    b.x &#x3D; (i + x[index + 0*stride]) &#x2F; lw;</span><br><span class="line">    b.y &#x3D; (j + x[index + 1*stride]) &#x2F; lh;</span><br><span class="line">    b.w &#x3D; exp(x[index + 2*stride]) * biases[2*n]   &#x2F; w;</span><br><span class="line">    b.h &#x3D; exp(x[index + 3*stride]) * biases[2*n+1] &#x2F; h;</span><br><span class="line">    return b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="delta-yolo-box-计算预测边界框的误差"><a href="#delta-yolo-box-计算预测边界框的误差" class="headerlink" title="delta_yolo_box 计算预测边界框的误差"></a>delta_yolo_box 计算预测边界框的误差</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;调用方式：delta_yolo_box(truth, l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, state.net.w, state.net.h, l.delta, (2 - truth.w*truth.h), l.w*l.h, l.iou_normalizer * class_multiplier, l.iou_loss, 1, l.max_delta);</span><br><span class="line">&#x2F;&#x2F; 计算预测边界框的误差</span><br><span class="line">ious delta_yolo_box(box truth, float *x, float *biases, int n, int index, int i, int j, int lw, int lh, int w, int h, float *delta, float scale, int stride, float iou_normalizer, IOU_LOSS iou_loss, int accumulate, int max_delta)</span><br><span class="line">&#123;</span><br><span class="line">    ious all_ious &#x3D; &#123; 0 &#125;;</span><br><span class="line">    &#x2F;&#x2F; i - step in layer width</span><br><span class="line">    &#x2F;&#x2F; j - step in layer height</span><br><span class="line">    &#x2F;&#x2F;  Returns a box in absolute coordinates</span><br><span class="line">	&#x2F;&#x2F; 获得第j*w+i个cell的第n个bbox在当前特征图的[x,y,w,h]</span><br><span class="line">    box pred &#x3D; get_yolo_box(x, biases, n, index, i, j, lw, lh, w, h, stride);</span><br><span class="line">	&#x2F;&#x2F;iou</span><br><span class="line">    all_ious.iou &#x3D; box_iou(pred, truth);</span><br><span class="line">	&#x2F;&#x2F;giou</span><br><span class="line">    all_ious.giou &#x3D; box_giou(pred, truth);</span><br><span class="line">	&#x2F;&#x2F;diou</span><br><span class="line">    all_ious.diou &#x3D; box_diou(pred, truth);</span><br><span class="line">	&#x2F;&#x2F;ciou</span><br><span class="line">    all_ious.ciou &#x3D; box_ciou(pred, truth);</span><br><span class="line">    &#x2F;&#x2F; avoid nan in dx_box_iou</span><br><span class="line">	</span><br><span class="line">    if (pred.w &#x3D;&#x3D; 0) &#123; pred.w &#x3D; 1.0; &#125;</span><br><span class="line">    if (pred.h &#x3D;&#x3D; 0) &#123; pred.h &#x3D; 1.0; &#125;</span><br><span class="line">    if (iou_loss &#x3D;&#x3D; MSE)    &#x2F;&#x2F; old loss</span><br><span class="line">    &#123;</span><br><span class="line">		&#x2F;&#x2F; 计算GT bbox的tx, ty, tw, th</span><br><span class="line">        float tx &#x3D; (truth.x*lw - i); &#x2F;&#x2F;和预测值匹配</span><br><span class="line">        float ty &#x3D; (truth.y*lh - j);</span><br><span class="line">        float tw &#x3D; log(truth.w*w &#x2F; biases[2 * n]); &#x2F;&#x2F;log 使大框和小框的误差影响接近</span><br><span class="line">        float th &#x3D; log(truth.h*h &#x2F; biases[2 * n + 1]);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; accumulate delta</span><br><span class="line">		&#x2F;&#x2F;计算tx, ty, tw, th的梯度</span><br><span class="line">        delta[index + 0 * stride] +&#x3D; scale * (tx - x[index + 0 * stride]) * iou_normalizer;  &#x2F;&#x2F;计算误差 delta，乘了权重系数 scale&#x3D;(2-truth.w*truth.h)</span><br><span class="line">        delta[index + 1 * stride] +&#x3D; scale * (ty - x[index + 1 * stride]) * iou_normalizer;</span><br><span class="line">        delta[index + 2 * stride] +&#x3D; scale * (tw - x[index + 2 * stride]) * iou_normalizer;</span><br><span class="line">        delta[index + 3 * stride] +&#x3D; scale * (th - x[index + 3 * stride]) * iou_normalizer;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        &#x2F;&#x2F; https:&#x2F;&#x2F;github.com&#x2F;generalized-iou&#x2F;g-darknet</span><br><span class="line">        &#x2F;&#x2F; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1902.09630v2</span><br><span class="line">        &#x2F;&#x2F; https:&#x2F;&#x2F;giou.stanford.edu&#x2F;</span><br><span class="line">        all_ious.dx_iou &#x3D; dx_box_iou(pred, truth, iou_loss);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; jacobian^t (transpose)</span><br><span class="line">        &#x2F;&#x2F;float dx &#x3D; (all_ious.dx_iou.dl + all_ious.dx_iou.dr);</span><br><span class="line">        &#x2F;&#x2F;float dy &#x3D; (all_ious.dx_iou.dt + all_ious.dx_iou.db);</span><br><span class="line">        &#x2F;&#x2F;float dw &#x3D; ((-0.5 * all_ious.dx_iou.dl) + (0.5 * all_ious.dx_iou.dr));</span><br><span class="line">        &#x2F;&#x2F;float dh &#x3D; ((-0.5 * all_ious.dx_iou.dt) + (0.5 * all_ious.dx_iou.db));</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; jacobian^t (transpose)</span><br><span class="line">        float dx &#x3D; all_ious.dx_iou.dt;</span><br><span class="line">        float dy &#x3D; all_ious.dx_iou.db;</span><br><span class="line">        float dw &#x3D; all_ious.dx_iou.dl;</span><br><span class="line">        float dh &#x3D; all_ious.dx_iou.dr;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; predict exponential, apply gradient of e^delta_t ONLY for w,h</span><br><span class="line">        dw *&#x3D; exp(x[index + 2 * stride]);</span><br><span class="line">        dh *&#x3D; exp(x[index + 3 * stride]);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; normalize iou weight</span><br><span class="line">        dx *&#x3D; iou_normalizer;</span><br><span class="line">        dy *&#x3D; iou_normalizer;</span><br><span class="line">        dw *&#x3D; iou_normalizer;</span><br><span class="line">        dh *&#x3D; iou_normalizer;</span><br><span class="line"></span><br><span class="line">        dx &#x3D; fix_nan_inf(dx);</span><br><span class="line">        dy &#x3D; fix_nan_inf(dy);</span><br><span class="line">        dw &#x3D; fix_nan_inf(dw);</span><br><span class="line">        dh &#x3D; fix_nan_inf(dh);</span><br><span class="line"></span><br><span class="line">        dx &#x3D; clip_value(dx, max_delta);</span><br><span class="line">        dy &#x3D; clip_value(dy, max_delta);</span><br><span class="line">        dw &#x3D; clip_value(dw, max_delta);</span><br><span class="line">        dh &#x3D; clip_value(dh, max_delta);</span><br><span class="line"></span><br><span class="line">        if (!accumulate) &#123;</span><br><span class="line">            delta[index + 0 * stride] &#x3D; 0;</span><br><span class="line">            delta[index + 1 * stride] &#x3D; 0;</span><br><span class="line">            delta[index + 2 * stride] &#x3D; 0;</span><br><span class="line">            delta[index + 3 * stride] &#x3D; 0;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; accumulate delta</span><br><span class="line">        delta[index + 0 * stride] +&#x3D; dx;</span><br><span class="line">        delta[index + 1 * stride] +&#x3D; dy;</span><br><span class="line">        delta[index + 2 * stride] +&#x3D; dw;</span><br><span class="line">        delta[index + 3 * stride] +&#x3D; dh;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;返回梯度</span><br><span class="line">    return all_ious;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="delta-yolo-class-计算类别误差"><a href="#delta-yolo-class-计算类别误差" class="headerlink" title="delta_yolo_class 计算类别误差"></a>delta_yolo_class 计算类别误差</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;计算类别误差</span><br><span class="line">void delta_yolo_class(float *output, float *delta, int index, int class_id, int classes, int stride, float *avg_cat, int focal_loss, float label_smooth_eps, float *classes_multipliers)</span><br><span class="line">&#123;</span><br><span class="line">    int n;</span><br><span class="line">    if (delta[index + stride*class_id])&#123; &#x2F;&#x2F;应该不会进入这个判断，因为 delta[index] 初值为0</span><br><span class="line">        delta[index + stride*class_id] &#x3D; (1 - label_smooth_eps) - output[index + stride*class_id];</span><br><span class="line">        if (classes_multipliers) delta[index + stride*class_id] *&#x3D; classes_multipliers[class_id];</span><br><span class="line">        if(avg_cat) *avg_cat +&#x3D; output[index + stride*class_id];</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; Focal loss</span><br><span class="line">    if (focal_loss) &#123;</span><br><span class="line">        &#x2F;&#x2F; Focal Loss</span><br><span class="line">        float alpha &#x3D; 0.5;    &#x2F;&#x2F; 0.25 or 0.5</span><br><span class="line">        &#x2F;&#x2F;float gamma &#x3D; 2;    &#x2F;&#x2F; hardcoded in many places of the grad-formula</span><br><span class="line"></span><br><span class="line">        int ti &#x3D; index + stride*class_id;</span><br><span class="line">        float pt &#x3D; output[ti] + 0.000000000000001F;</span><br><span class="line">        &#x2F;&#x2F; http:&#x2F;&#x2F;fooplot.com&#x2F;#W3sidHlwZSI6MCwiZXEiOiItKDEteCkqKDIqeCpsb2coeCkreC0xKSIsImNvbG9yIjoiIzAwMDAwMCJ9LHsidHlwZSI6MTAwMH1d</span><br><span class="line">        float grad &#x3D; -(1 - pt) * (2 * pt*logf(pt) + pt - 1);    &#x2F;&#x2F; http:&#x2F;&#x2F;blog.csdn.net&#x2F;linmingan&#x2F;article&#x2F;details&#x2F;77885832</span><br><span class="line">        &#x2F;&#x2F;float grad &#x3D; (1 - pt) * (2 * pt*logf(pt) + pt - 1);    &#x2F;&#x2F; https:&#x2F;&#x2F;github.com&#x2F;unsky&#x2F;focal-loss</span><br><span class="line"></span><br><span class="line">        for (n &#x3D; 0; n &lt; classes; ++n) &#123; &#x2F;&#x2F;对所有类别，如果预测正确，则误差为 1-predict，否则为 0-predict</span><br><span class="line">            delta[index + stride*n] &#x3D; (((n &#x3D;&#x3D; class_id) ? 1 : 0) - output[index + stride*n]);</span><br><span class="line"></span><br><span class="line">            delta[index + stride*n] *&#x3D; alpha*grad;</span><br><span class="line"></span><br><span class="line">            if (n &#x3D;&#x3D; class_id &amp;&amp; avg_cat) *avg_cat +&#x3D; output[index + stride*n];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        &#x2F;&#x2F; default</span><br><span class="line">        for (n &#x3D; 0; n &lt; classes; ++n) &#123;</span><br><span class="line">            delta[index + stride*n] &#x3D; ((n &#x3D;&#x3D; class_id) ? (1 - label_smooth_eps) : (0 + label_smooth_eps&#x2F;classes)) - output[index + stride*n];</span><br><span class="line">            if (classes_multipliers &amp;&amp; n &#x3D;&#x3D; class_id) delta[index + stride*class_id] *&#x3D; classes_multipliers[class_id];</span><br><span class="line">            if (n &#x3D;&#x3D; class_id &amp;&amp; avg_cat) *avg_cat +&#x3D; output[index + stride*n];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="entry-index-得到指针偏移量，即入口需要的索引"><a href="#entry-index-得到指针偏移量，即入口需要的索引" class="headerlink" title="entry_index 得到指针偏移量，即入口需要的索引"></a>entry_index 得到指针偏移量，即入口需要的索引</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * @brief 计算某个矩形框中某个参数在l.output中的索引。一个矩形框包含了x,y,w,h,c,C1,C2...,Cn信息，</span><br><span class="line"> *        前四个用于定位，第五个为矩形框含有物体的置信度信息c，即矩形框中存在物体的概率为多大，而C1到Cn</span><br><span class="line"> *        为矩形框中所包含的物体分别属于这n类物体的概率。本函数负责获取该矩形框首个定位信息也即x值在</span><br><span class="line"> *        l.output中索引、获取该矩形框置信度信息c在l.output中的索引、获取该矩形框分类所属概率的首个</span><br><span class="line"> *        概率也即C1值的索引，具体是获取矩形框哪个参数的索引，取决于输入参数entry的值，这些在</span><br><span class="line"> *        forward_region_layer()函数中都有用到，由于l.output的存储方式，当entry&#x3D;0时，就是获取矩形框</span><br><span class="line"> *        x参数在l.output中的索引；当entry&#x3D;4时，就是获取矩形框置信度信息c在l.output中的索引；当</span><br><span class="line"> *        entry&#x3D;5时，就是获取矩形框首个所属概率C1在l.output中的索引，具体可以参考forward_region_layer()</span><br><span class="line"> *        中调用本函数时的注释.</span><br><span class="line"> * @param l 当前region_layer</span><br><span class="line"> * @param batch 当前照片是整个batch中的第几张，因为l.output中包含整个batch的输出，所以要定位某张训练图片</span><br><span class="line"> *              输出的众多网格中的某个矩形框，当然需要该参数.</span><br><span class="line"> * @param location 这个参数，说实话，感觉像个鸡肋参数，函数中用这个参数获取n和loc的值，这个n就是表示网格中</span><br><span class="line"> *                 的第几个预测矩形框（比如每个网格预测5个矩形框，那么n取值范围就是从0~4，loc就是某个</span><br><span class="line"> *                 通道上的元素偏移（region_layer输出的通道数为l.out_c &#x3D; (classes + coords + 1)，</span><br><span class="line"> *                 这样说可能没有说明白，这都与l.output的存储结构相关，见下面详细注释以及其他说明。总之，</span><br><span class="line"> *                 查看一下调用本函数的父函数forward_region_layer()就知道了，可以直接输入n和j*l.w+i的，</span><br><span class="line"> *                 没有必要输入location，这样还得重新计算一次n和loc.</span><br><span class="line"> * @param entry 切入点偏移系数，关于这个参数，就又要扯到l.output的存储结构了，见下面详细注释以及其他说明.</span><br><span class="line"> * @details l.output这个参数的存储内容以及存储方式已经在多个地方说明了，再多的文字都不及图文说明，此处再</span><br><span class="line"> *          简要罗嗦几句，更为具体的参考图文说明。l.output中存储了整个batch的训练输出，每张训练图片都会输出</span><br><span class="line"> *          l.out_w*l.out_h个网格，每个网格会预测l.n个矩形框，每个矩形框含有l.classes+l.coords+1个参数，</span><br><span class="line"> *          而最后一层的输出通道数为l.n*(l.classes+l.coords+1)，可以想象下最终输出的三维张量是个什么样子的。</span><br><span class="line"> *          展成一维数组存储时，l.output可以首先分成batch个大段，每个大段存储了一张训练图片的所有输出；进一步细分，</span><br><span class="line"> *          取其中第一大段分析，该大段中存储了第一张训练图片所有输出网格预测的矩形框信息，每个网格预测了l.n个矩形框，</span><br><span class="line"> *          存储时，l.n个矩形框是分开存储的，也就是先存储所有网格中的第一个矩形框，而后存储所有网格中的第二个矩形框，</span><br><span class="line"> *          依次类推，如果每个网格中预测5个矩形框，则可以继续把这一大段分成5个中段。继续细分，5个中段中取第</span><br><span class="line"> *          一个中段来分析，这个中段中按行（有l.out_w*l.out_h个网格，按行存储）依次存储了这张训练图片所有输出网格中</span><br><span class="line"> *          的第一个矩形框信息，要注意的是，这个中段存储的顺序并不是挨个挨个存储每个矩形框的所有信息，</span><br><span class="line"> *          而是先存储所有矩形框的x，而后是所有的y,然后是所有的w,再是h，c，最后的的概率数组也是拆分进行存储，</span><br><span class="line"> *          并不是一下子存储完一个矩形框所有类的概率，而是先存储所有网格所属第一类的概率，再存储所属第二类的概率，</span><br><span class="line"> *          具体来说这一中段首先存储了l.out_w*l.out_h个x，然后是l.out_w*l.out_c个y，依次下去，</span><br><span class="line"> *          最后是l.out_w*l.out_h个C1（属于第一类的概率，用C1表示，下面类似），l.out_w*l.outh个C2,...,</span><br><span class="line"> *          l.out_w*l.out_c*Cn（假设共有n类），所以可以继续将中段分成几个小段，依次为x,y,w,h,c,C1,C2,...Cn</span><br><span class="line"> *          小段，每小段的长度都为l.out_w*l.out_c.</span><br><span class="line"> *          现在回过来看本函数的输入参数，batch就是大段的偏移数（从第几个大段开始，对应是第几张训练图片），</span><br><span class="line"> *          由location计算得到的n就是中段的偏移数（从第几个中段开始，对应是第几个矩形框），</span><br><span class="line"> *          entry就是小段的偏移数（从几个小段开始，对应具体是那种参数，x,c还是C1），而loc则是最后的定位，</span><br><span class="line"> *          前面确定好第几大段中的第几中段中的第几小段的首地址，loc就是从该首地址往后数loc个元素，得到最终定位</span><br><span class="line"> *          某个具体参数（x或c或C1）的索引值，比如l.output中存储的数据如下所示（这里假设只存了一张训练图片的输出，</span><br><span class="line"> *          因此batch只能为0；并假设l.out_w&#x3D;l.out_h&#x3D;2,l.classes&#x3D;2）：</span><br><span class="line"> *          xxxxyyyywwwwhhhhccccC1C1C1C1C2C2C2C2-#-xxxxyyyywwwwhhhhccccC1C1C1C1C2C2C2C2，</span><br><span class="line"> *          n&#x3D;0则定位到-#-左边的首地址（表示每个网格预测的第一个矩形框），n&#x3D;1则定位到-#-右边的首地址（表示每个网格预测的第二个矩形框）</span><br><span class="line"> *          entry&#x3D;0,loc&#x3D;0获取的是x的索引，且获取的是第一个x也即l.out_w*l.out_h个网格中第一个网格中第一个矩形框x参数的索引；</span><br><span class="line"> *          entry&#x3D;4,loc&#x3D;1获取的是c的索引，且获取的是第二个c也即l.out_w*l.out_h个网格中第二个网格中第一个矩形框c参数的索引；</span><br><span class="line"> *          entry&#x3D;5,loc&#x3D;2获取的是C1的索引，且获取的是第三个C1也即l.out_w*l.out_h个网格中第三个网格中第一个矩形框C1参数的索引；</span><br><span class="line"> *          如果要获取第一个网格中第一个矩形框w参数的索引呢？如果已经获取了其x值的索引，显然用x的索引加上3*l.out_w*l.out_h即可获取到，</span><br><span class="line"> *          这正是delta_region_box()函数的做法；</span><br><span class="line"> *          如果要获取第三个网格中第一个矩形框C2参数的索引呢？如果已经获取了其C1值的索引，显然用C1的索引加上l.out_w*l.out_h即可获取到，</span><br><span class="line"> *          这正是delta_region_class()函数中的做法；</span><br><span class="line"> *          由上可知，entry&#x3D;0时,即偏移0个小段，是获取x的索引；entry&#x3D;4,是获取自信度信息c的索引；entry&#x3D;5，是获取C1的索引.</span><br><span class="line"> *          l.output的存储方式大致就是这样，个人觉得说的已经很清楚了，但可视化效果终究不如图文说明～</span><br><span class="line">*&#x2F;</span><br><span class="line">static int entry_index(layer l, int batch, int location, int entry)</span><br><span class="line">&#123;</span><br><span class="line">    int n &#x3D;   location &#x2F; (l.w*l.h);</span><br><span class="line">    int loc &#x3D; location % (l.w*l.h);</span><br><span class="line">    return batch*l.outputs + n*l.w*l.h*(4+l.classes+1) + entry*l.w*l.h + loc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="forward-yolo-layer-前向传播函数"><a href="#forward-yolo-layer-前向传播函数" class="headerlink" title="forward_yolo_layer 前向传播函数"></a>forward_yolo_layer 前向传播函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;前向传播</span><br><span class="line">void forward_yolo_layer(const layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int i, j, b, t, n;</span><br><span class="line">	&#x2F;&#x2F;将层输入直接拷贝到层输出</span><br><span class="line">    memcpy(l.output, state.input, l.outputs*l.batch * sizeof(float));</span><br><span class="line">   &#x2F;&#x2F;在 cpu 里，把预测输出的 x,y,confidence 和80种类别都 sigmoid 激活，确保值在0~1</span><br><span class="line">#ifndef GPU</span><br><span class="line">    for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;</span><br><span class="line">        for (n &#x3D; 0; n &lt; l.n; ++n) &#123;</span><br><span class="line">			&#x2F;&#x2F; 获取第b个batch开始的index</span><br><span class="line">            int index &#x3D; entry_index(l, b, n*l.w*l.h, 0);</span><br><span class="line">			&#x2F;&#x2F; 对预测的tx,ty进行逻辑回归预测,</span><br><span class="line">            activate_array(l.output + index, 2 * l.w*l.h, LOGISTIC);        &#x2F;&#x2F; x,y,</span><br><span class="line">            scal_add_cpu(2 * l.w*l.h, l.scale_x_y, -0.5*(l.scale_x_y - 1), l.output + index, 1);    &#x2F;&#x2F; scale x,y</span><br><span class="line">            &#x2F;&#x2F; 获取第b个batch confidence开始的index</span><br><span class="line">			index &#x3D; entry_index(l, b, n*l.w*l.h, 4);</span><br><span class="line">			&#x2F;&#x2F; 对预测的confidence以及class进行逻辑回归</span><br><span class="line">            activate_array(l.output + index, (1 + l.classes)*l.w*l.h, LOGISTIC);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; delta is zeroed</span><br><span class="line">	&#x2F;&#x2F;将yolo层的误差项进行初始化(包含整个batch的)</span><br><span class="line">    memset(l.delta, 0, l.outputs * l.batch * sizeof(float));</span><br><span class="line">	&#x2F;&#x2F; inference阶段,到此结束</span><br><span class="line">    if (!state.train) return;</span><br><span class="line">    &#x2F;&#x2F;float avg_iou &#x3D; 0;</span><br><span class="line">    float tot_iou &#x3D; 0; &#x2F;&#x2F;总的IoU（Intersection over Union）</span><br><span class="line">    float tot_giou &#x3D; 0;</span><br><span class="line">    float tot_diou &#x3D; 0;</span><br><span class="line">    float tot_ciou &#x3D; 0;</span><br><span class="line">    float tot_iou_loss &#x3D; 0;</span><br><span class="line">    float tot_giou_loss &#x3D; 0;</span><br><span class="line">    float tot_diou_loss &#x3D; 0;</span><br><span class="line">    float tot_ciou_loss &#x3D; 0;</span><br><span class="line">    float recall &#x3D; 0;</span><br><span class="line">    float recall75 &#x3D; 0;</span><br><span class="line">    float avg_cat &#x3D; 0;</span><br><span class="line">    float avg_obj &#x3D; 0;</span><br><span class="line">    float avg_anyobj &#x3D; 0;</span><br><span class="line">    int count &#x3D; 0;</span><br><span class="line">    int class_count &#x3D; 0;</span><br><span class="line">    *(l.cost) &#x3D; 0; &#x2F;&#x2F; yolo层的总损失初始化为0</span><br><span class="line">    for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;&#x2F;&#x2F; 遍历batch中的每一张图片</span><br><span class="line">        for (j &#x3D; 0; j &lt; l.h; ++j) &#123;</span><br><span class="line">            for (i &#x3D; 0; i &lt; l.w; ++i) &#123;&#x2F;&#x2F; 遍历每个cell, 当前cell编号[j, i]</span><br><span class="line">                for (n &#x3D; 0; n &lt; l.n; ++n) &#123;&#x2F;&#x2F; 遍历每一个bbox, 当前bbox编号 [n]</span><br><span class="line">					&#x2F;&#x2F; 在这里与yolov2 reorg层是相似的, 获得第j*w+i个cell第n个bbox的index</span><br><span class="line">                    int box_index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 0);</span><br><span class="line">					&#x2F;&#x2F; 计算第j*w+i个cell第n个bbox在当前特征图上的相对位置[x,y],在网络输入图片上的相对宽度,高度[w,h]</span><br><span class="line">                    box pred &#x3D; get_yolo_box(l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, state.net.w, state.net.h, l.w*l.h);</span><br><span class="line">                    float best_match_iou &#x3D; 0;</span><br><span class="line">                    int best_match_t &#x3D; 0;</span><br><span class="line">                    float best_iou &#x3D; 0; &#x2F;&#x2F; 保存最大iou</span><br><span class="line">                    int best_t &#x3D; 0;&#x2F;&#x2F; 保存最大iou的bbox id</span><br><span class="line">                    for (t &#x3D; 0; t &lt; l.max_boxes; ++t) &#123;&#x2F;&#x2F; 遍历每一个GT bbox</span><br><span class="line">						&#x2F;&#x2F; 将第t个bbox由float数组转bbox结构体,方便计算iou</span><br><span class="line">                        box truth &#x3D; float_to_box_stride(state.truth + t*(4 + 1) + b*l.truths, 1);</span><br><span class="line">						&#x2F;&#x2F;获取第t个bbox的类别，检查是否有标注错误</span><br><span class="line">                        int class_id &#x3D; state.truth[t*(4 + 1) + b*l.truths + 4];</span><br><span class="line">                        if (class_id &gt;&#x3D; l.classes) &#123;</span><br><span class="line">                            printf(&quot; Warning: in txt-labels class_id&#x3D;%d &gt;&#x3D; classes&#x3D;%d in cfg-file. In txt-labels class_id should be [from 0 to %d] \n&quot;, class_id, l.classes, l.classes - 1);</span><br><span class="line">                            printf(&quot; truth.x &#x3D; %f, truth.y &#x3D; %f, truth.w &#x3D; %f, truth.h &#x3D; %f, class_id &#x3D; %d \n&quot;, truth.x, truth.y, truth.w, truth.h, class_id);</span><br><span class="line">                            getchar();</span><br><span class="line">                            continue; &#x2F;&#x2F; if label contains class_id more than number of classes in the cfg-file</span><br><span class="line">                        &#125;</span><br><span class="line">						&#x2F;&#x2F; 如果x坐标为0则取消,因为yolov3这里定义了max_boxes个bbox</span><br><span class="line">                        if (!truth.x) break;  &#x2F;&#x2F; continue;</span><br><span class="line"></span><br><span class="line">                        int class_index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 4 + 1);&#x2F;&#x2F;预测bbox 类别s下标</span><br><span class="line">                        int obj_index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 4); &#x2F;&#x2F;预测bbox objectness下标</span><br><span class="line">                        float objectness &#x3D; l.output[obj_index]; &#x2F;&#x2F;预测bbox object置信度</span><br><span class="line">						&#x2F;&#x2F;获得预测bbox 的类别信息，如果某个类别的概率超过0.25返回1</span><br><span class="line">                        int class_id_match &#x3D; compare_yolo_class(l.output, l.classes, class_index, l.w*l.h, objectness, class_id, 0.25f);</span><br><span class="line"></span><br><span class="line">                        float iou &#x3D; box_iou(pred, truth); &#x2F;&#x2F; 计算pred bbox与第t个GT bbox之间的iou</span><br><span class="line">						&#x2F;&#x2F; 这个地方和原始的DarkNet实现不太一样，多了一个class_id_match&#x3D;1的限制，即预测bbox的置信度必须大于0.25</span><br><span class="line">                        if (iou &gt; best_match_iou &amp;&amp; class_id_match &#x3D;&#x3D; 1) &#123;</span><br><span class="line">                            best_match_iou &#x3D; iou;</span><br><span class="line">                            best_match_t &#x3D; t;</span><br><span class="line">                        &#125;</span><br><span class="line">                        if (iou &gt; best_iou) &#123;</span><br><span class="line">                            best_iou &#x3D; iou; &#x2F;&#x2F; 记录iou最大的iou</span><br><span class="line">                            best_t &#x3D; t; &#x2F;&#x2F; 记录该GT bbox的编号t</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">					&#x2F;&#x2F; 在这里与yolov2 reorg层是相似的, 获得第j*w+i个cell第n个bbox的confidence</span><br><span class="line">                    int obj_index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 4);</span><br><span class="line">					&#x2F;&#x2F; 统计pred bbox的confidence</span><br><span class="line">                    avg_anyobj +&#x3D; l.output[obj_index];</span><br><span class="line">					 &#x2F;&#x2F; 与yolov1相似,先将所有pred bbox都当做noobject, 计算其confidence梯度，不过这里多了一个平衡系数</span><br><span class="line">                    l.delta[obj_index] &#x3D; l.cls_normalizer * (0 - l.output[obj_index]);</span><br><span class="line">					&#x2F;&#x2F; best_iou大于阈值则说明pred box有物体,在yolov3中正样本阈值ignore_thresh&#x3D;.5</span><br><span class="line">                    if (best_match_iou &gt; l.ignore_thresh) &#123;</span><br><span class="line">                        l.delta[obj_index] &#x3D; 0;</span><br><span class="line">                    &#125;</span><br><span class="line">					&#x2F;&#x2F; pred bbox为完全预测正确样本,在yolov3完全预测正确样本的阈值truth_thresh&#x3D;1.</span><br><span class="line">					&#x2F;&#x2F;这个参数在cfg文件中，值为1，这个条件语句永远不可能成立</span><br><span class="line">                    if (best_iou &gt; l.truth_thresh) &#123;</span><br><span class="line">						&#x2F;&#x2F; 作者在YOLOV3论文中的第4节提到了这部分。</span><br><span class="line">						&#x2F;&#x2F; 作者尝试Faster-RCNN中提到的双IOU策略，当Anchor与GT的IoU大于0.7时，该Anchor被算作正样本</span><br><span class="line">						&#x2F;&#x2F;计入损失中，但训练过程中并没有产生好的结果，所以最后放弃了。</span><br><span class="line">                        l.delta[obj_index] &#x3D; l.cls_normalizer * (1 - l.output[obj_index]);</span><br><span class="line">						 &#x2F;&#x2F; 获得best_iou对应GT bbox的class的index</span><br><span class="line">                        int class_id &#x3D; state.truth[best_t*(4 + 1) + b*l.truths + 4];</span><br><span class="line">						&#x2F;&#x2F;yolov3 yolo层中map&#x3D;0, 不参与计算</span><br><span class="line">                        if (l.map) class_id &#x3D; l.map[class_id];</span><br><span class="line">						&#x2F;&#x2F; 获得best_iou对应pred bbox的class的index</span><br><span class="line">                        int class_index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 4 + 1);</span><br><span class="line">                        delta_yolo_class(l.output, l.delta, class_index, class_id, l.classes, l.w*l.h, 0, l.focal_loss, l.label_smooth_eps, l.classes_multipliers);</span><br><span class="line">                        box truth &#x3D; float_to_box_stride(state.truth + best_t*(4 + 1) + b*l.truths, 1);</span><br><span class="line">                        const float class_multiplier &#x3D; (l.classes_multipliers) ? l.classes_multipliers[class_id] : 1.0f;</span><br><span class="line">                        &#x2F;&#x2F; 计算pred bbox的[x,y,w,h]的梯度</span><br><span class="line">						delta_yolo_box(truth, l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, state.net.w, state.net.h, l.delta, (2 - truth.w*truth.h), l.w*l.h, l.iou_normalizer * class_multiplier, l.iou_loss, 1, l.max_delta);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        for (t &#x3D; 0; t &lt; l.max_boxes; ++t) &#123;</span><br><span class="line">			&#x2F;&#x2F;遍历每一个GT box</span><br><span class="line">			&#x2F;&#x2F; 将第t个bbox由float数组转bbox结构体,方便计算iou</span><br><span class="line">            box truth &#x3D; float_to_box_stride(state.truth + t*(4 + 1) + b*l.truths, 1);</span><br><span class="line">            if (truth.x &lt; 0 || truth.y &lt; 0 || truth.x &gt; 1 || truth.y &gt; 1 || truth.w &lt; 0 || truth.h &lt; 0) &#123;</span><br><span class="line">                char buff[256];</span><br><span class="line">                printf(&quot; Wrong label: truth.x &#x3D; %f, truth.y &#x3D; %f, truth.w &#x3D; %f, truth.h &#x3D; %f \n&quot;, truth.x, truth.y, truth.w, truth.h);</span><br><span class="line">                sprintf(buff, &quot;echo \&quot;Wrong label: truth.x &#x3D; %f, truth.y &#x3D; %f, truth.w &#x3D; %f, truth.h &#x3D; %f\&quot; &gt;&gt; bad_label.list&quot;,</span><br><span class="line">                    truth.x, truth.y, truth.w, truth.h);</span><br><span class="line">                system(buff);</span><br><span class="line">            &#125;</span><br><span class="line">            int class_id &#x3D; state.truth[t*(4 + 1) + b*l.truths + 4];</span><br><span class="line">            if (class_id &gt;&#x3D; l.classes) continue; &#x2F;&#x2F; if label contains class_id more than number of classes in the cfg-file</span><br><span class="line"></span><br><span class="line">            if (!truth.x) break;  &#x2F;&#x2F; 如果x坐标为0则取消，因为yolov3定义了max_boxes个bbox,可能实际上没那么多</span><br><span class="line">            float best_iou &#x3D; 0; &#x2F;&#x2F;保存最大的IOU</span><br><span class="line">            int best_n &#x3D; 0; &#x2F;&#x2F;保存最大IOU的bbox index</span><br><span class="line">            i &#x3D; (truth.x * l.w); &#x2F;&#x2F; 获得当前t个GT bbox所在的cell</span><br><span class="line">            j &#x3D; (truth.y * l.h);</span><br><span class="line">            box truth_shift &#x3D; truth;</span><br><span class="line">            truth_shift.x &#x3D; truth_shift.y &#x3D; 0; &#x2F;&#x2F;将truth_shift的box位置移动到0,0</span><br><span class="line">            for (n &#x3D; 0; n &lt; l.total; ++n) &#123; &#x2F;&#x2F; 遍历每一个anchor bbox找到与GT bbox最大的IOU</span><br><span class="line">                box pred &#x3D; &#123; 0 &#125;;</span><br><span class="line">                pred.w &#x3D; l.biases[2 * n] &#x2F; state.net.w; &#x2F;&#x2F; 计算pred bbox的w在相对整张输入图片的位置</span><br><span class="line">                pred.h &#x3D; l.biases[2 * n + 1] &#x2F; state.net.h; &#x2F;&#x2F; 计算pred bbox的h在相对整张输入图片的位置</span><br><span class="line">                float iou &#x3D; box_iou(pred, truth_shift); &#x2F;&#x2F; 计算GT box truth_shift 与 预测bbox pred二者之间的IOU</span><br><span class="line">                if (iou &gt; best_iou) &#123;</span><br><span class="line">                    best_iou &#x3D; iou;&#x2F;&#x2F; 记录最大的IOU</span><br><span class="line">                    best_n &#x3D; n;&#x2F;&#x2F; 以及记录该bbox的编号n</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F; 上面记录bbox的编号,是否由该层Anchor预测的</span><br><span class="line">            int mask_n &#x3D; int_index(l.mask, best_n, l.n);</span><br><span class="line">            if (mask_n &gt;&#x3D; 0) &#123;</span><br><span class="line">                int class_id &#x3D; state.truth[t*(4 + 1) + b*l.truths + 4];</span><br><span class="line">                if (l.map) class_id &#x3D; l.map[class_id];</span><br><span class="line">				&#x2F;&#x2F; 获得best_iou对应anchor box的index</span><br><span class="line">                int box_index &#x3D; entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 0);</span><br><span class="line">				&#x2F;&#x2F;这个参数是用来控制样本数量不均衡的，即Focal Loss中的alpha</span><br><span class="line">                const float class_multiplier &#x3D; (l.classes_multipliers) ? l.classes_multipliers[class_id] : 1.0f;</span><br><span class="line">				&#x2F;&#x2F; 计算best_iou对应Anchor bbox的[x,y,w,h]的梯度</span><br><span class="line">                ious all_ious &#x3D; delta_yolo_box(truth, l.output, l.biases, best_n, box_index, i, j, l.w, l.h, state.net.w, state.net.h, l.delta, (2 - truth.w*truth.h), l.w*l.h, l.iou_normalizer * class_multiplier, l.iou_loss, 1, l.max_delta);</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F; 下面的都是模板检测最新的工作，metricl learning，包括IOU&#x2F;GIOU&#x2F;DIOU&#x2F;CIOU Loss</span><br><span class="line">                &#x2F;&#x2F; range is 0 &lt;&#x3D; 1</span><br><span class="line">                tot_iou +&#x3D; all_ious.iou;</span><br><span class="line">                tot_iou_loss +&#x3D; 1 - all_ious.iou;</span><br><span class="line">                &#x2F;&#x2F; range is -1 &lt;&#x3D; giou &lt;&#x3D; 1</span><br><span class="line">                tot_giou +&#x3D; all_ious.giou;</span><br><span class="line">                tot_giou_loss +&#x3D; 1 - all_ious.giou;</span><br><span class="line"></span><br><span class="line">                tot_diou +&#x3D; all_ious.diou;</span><br><span class="line">                tot_diou_loss +&#x3D; 1 - all_ious.diou;</span><br><span class="line"></span><br><span class="line">                tot_ciou +&#x3D; all_ious.ciou;</span><br><span class="line">                tot_ciou_loss +&#x3D; 1 - all_ious.ciou;</span><br><span class="line">				&#x2F;&#x2F; 获得best_iou对应anchor box的confidence的index</span><br><span class="line">                int obj_index &#x3D; entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 4);</span><br><span class="line">				&#x2F;&#x2F;统计confidence</span><br><span class="line">                avg_obj +&#x3D; l.output[obj_index];</span><br><span class="line">				&#x2F;&#x2F; 计算confidence的梯度</span><br><span class="line">                l.delta[obj_index] &#x3D; class_multiplier * l.cls_normalizer * (1 - l.output[obj_index]);</span><br><span class="line">				&#x2F;&#x2F; 获得best_iou对应GT box的class的index</span><br><span class="line">                int class_index &#x3D; entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 4 + 1);</span><br><span class="line">				&#x2F;&#x2F; 获得best_iou对应anchor box的class的index</span><br><span class="line">                delta_yolo_class(l.output, l.delta, class_index, class_id, l.classes, l.w*l.h, &amp;avg_cat, l.focal_loss, l.label_smooth_eps, l.classes_multipliers);</span><br><span class="line"></span><br><span class="line">                ++count;</span><br><span class="line">                ++class_count;</span><br><span class="line">                if (all_ious.iou &gt; .5) recall +&#x3D; 1;</span><br><span class="line">                if (all_ious.iou &gt; .75) recall75 +&#x3D; 1;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F;下面这个过程和上面一样，不过多约束了一个iou_thresh</span><br><span class="line">            &#x2F;&#x2F; iou_thresh</span><br><span class="line">            for (n &#x3D; 0; n &lt; l.total; ++n) &#123;</span><br><span class="line">                int mask_n &#x3D; int_index(l.mask, n, l.n);</span><br><span class="line">                if (mask_n &gt;&#x3D; 0 &amp;&amp; n !&#x3D; best_n &amp;&amp; l.iou_thresh &lt; 1.0f) &#123;</span><br><span class="line">                    box pred &#x3D; &#123; 0 &#125;;</span><br><span class="line">                    pred.w &#x3D; l.biases[2 * n] &#x2F; state.net.w;</span><br><span class="line">                    pred.h &#x3D; l.biases[2 * n + 1] &#x2F; state.net.h;</span><br><span class="line">                    float iou &#x3D; box_iou(pred, truth_shift);</span><br><span class="line">                    &#x2F;&#x2F; iou, n</span><br><span class="line"></span><br><span class="line">                    if (iou &gt; l.iou_thresh) &#123;</span><br><span class="line">                        int class_id &#x3D; state.truth[t*(4 + 1) + b*l.truths + 4];</span><br><span class="line">                        if (l.map) class_id &#x3D; l.map[class_id];</span><br><span class="line"></span><br><span class="line">                        int box_index &#x3D; entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 0);</span><br><span class="line">                        const float class_multiplier &#x3D; (l.classes_multipliers) ? l.classes_multipliers[class_id] : 1.0f;</span><br><span class="line">                        ious all_ious &#x3D; delta_yolo_box(truth, l.output, l.biases, n, box_index, i, j, l.w, l.h, state.net.w, state.net.h, l.delta, (2 - truth.w*truth.h), l.w*l.h, l.iou_normalizer * class_multiplier, l.iou_loss, 1, l.max_delta);</span><br><span class="line"></span><br><span class="line">                        &#x2F;&#x2F; range is 0 &lt;&#x3D; 1</span><br><span class="line">                        tot_iou +&#x3D; all_ious.iou;</span><br><span class="line">                        tot_iou_loss +&#x3D; 1 - all_ious.iou;</span><br><span class="line">                        &#x2F;&#x2F; range is -1 &lt;&#x3D; giou &lt;&#x3D; 1</span><br><span class="line">                        tot_giou +&#x3D; all_ious.giou;</span><br><span class="line">                        tot_giou_loss +&#x3D; 1 - all_ious.giou;</span><br><span class="line"></span><br><span class="line">                        tot_diou +&#x3D; all_ious.diou;</span><br><span class="line">                        tot_diou_loss +&#x3D; 1 - all_ious.diou;</span><br><span class="line"></span><br><span class="line">                        tot_ciou +&#x3D; all_ious.ciou;</span><br><span class="line">                        tot_ciou_loss +&#x3D; 1 - all_ious.ciou;</span><br><span class="line"></span><br><span class="line">                        int obj_index &#x3D; entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 4);</span><br><span class="line">                        avg_obj +&#x3D; l.output[obj_index];</span><br><span class="line">                        l.delta[obj_index] &#x3D; class_multiplier * l.cls_normalizer * (1 - l.output[obj_index]);</span><br><span class="line"></span><br><span class="line">                        int class_index &#x3D; entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 4 + 1);</span><br><span class="line">                        delta_yolo_class(l.output, l.delta, class_index, class_id, l.classes, l.w*l.h, &amp;avg_cat, l.focal_loss, l.label_smooth_eps, l.classes_multipliers);</span><br><span class="line"></span><br><span class="line">                        ++count;</span><br><span class="line">                        ++class_count;</span><br><span class="line">                        if (all_ious.iou &gt; .5) recall +&#x3D; 1;</span><br><span class="line">                        if (all_ious.iou &gt; .75) recall75 +&#x3D; 1;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; averages the deltas obtained by the function: delta_yolo_box()_accumulate</span><br><span class="line">        for (j &#x3D; 0; j &lt; l.h; ++j) &#123;</span><br><span class="line">            for (i &#x3D; 0; i &lt; l.w; ++i) &#123;</span><br><span class="line">                for (n &#x3D; 0; n &lt; l.n; ++n) &#123;</span><br><span class="line">					&#x2F;&#x2F; 在这里与yolov2 reorg层是相似的, 获得第j*w+i个cell第n个bbox的index</span><br><span class="line">                    int box_index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 0);</span><br><span class="line">					&#x2F;&#x2F;获得第j*w+i个cell第n个bbox的类别</span><br><span class="line">                    int class_index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 4 + 1);</span><br><span class="line">					&#x2F;&#x2F;特征图的大小</span><br><span class="line">                    const int stride &#x3D; l.w*l.h;</span><br><span class="line">					&#x2F;&#x2F;对梯度进行平均</span><br><span class="line">                    averages_yolo_deltas(class_index, box_index, stride, l.classes, l.delta);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;*(l.cost) &#x3D; pow(mag_array(l.delta, l.outputs * l.batch), 2);</span><br><span class="line">    &#x2F;&#x2F;printf(&quot;Region %d Avg IOU: %f, Class: %f, Obj: %f, No Obj: %f, .5R: %f, .75R: %f,  count: %d\n&quot;, state.index, avg_iou &#x2F; count, avg_cat &#x2F; class_count, avg_obj &#x2F; count, avg_anyobj &#x2F; (l.w*l.h*l.n*l.batch), recall &#x2F; count, recall75 &#x2F; count, count);</span><br><span class="line"></span><br><span class="line">    int stride &#x3D; l.w*l.h;</span><br><span class="line">    float* no_iou_loss_delta &#x3D; (float *)calloc(l.batch * l.outputs, sizeof(float));</span><br><span class="line">    memcpy(no_iou_loss_delta, l.delta, l.batch * l.outputs * sizeof(float));</span><br><span class="line">    for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;</span><br><span class="line">        for (j &#x3D; 0; j &lt; l.h; ++j) &#123;</span><br><span class="line">            for (i &#x3D; 0; i &lt; l.w; ++i) &#123;</span><br><span class="line">                for (n &#x3D; 0; n &lt; l.n; ++n) &#123;</span><br><span class="line">					&#x2F;&#x2F;yolov3如果使用的是iou loss，也就是metric learning的方式，那么x,y,w,h的loss可以设置为0</span><br><span class="line">                    int index &#x3D; entry_index(l, b, n*l.w*l.h + j*l.w + i, 0);</span><br><span class="line">                    no_iou_loss_delta[index + 0 * stride] &#x3D; 0;</span><br><span class="line">                    no_iou_loss_delta[index + 1 * stride] &#x3D; 0;</span><br><span class="line">                    no_iou_loss_delta[index + 2 * stride] &#x3D; 0;</span><br><span class="line">                    no_iou_loss_delta[index + 3 * stride] &#x3D; 0;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F;计算所有的分类loss</span><br><span class="line">    float classification_loss &#x3D; l.cls_normalizer * pow(mag_array(no_iou_loss_delta, l.outputs * l.batch), 2);</span><br><span class="line">    free(no_iou_loss_delta);</span><br><span class="line">	&#x2F;&#x2F;计算总的loss</span><br><span class="line">    float loss &#x3D; pow(mag_array(l.delta, l.outputs * l.batch), 2);</span><br><span class="line">	&#x2F;&#x2F;计算回归loss</span><br><span class="line">    float iou_loss &#x3D; loss - classification_loss;</span><br><span class="line"></span><br><span class="line">    float avg_iou_loss &#x3D; 0;</span><br><span class="line">    &#x2F;&#x2F; gIOU loss + MSE (objectness) loss</span><br><span class="line">    if (l.iou_loss &#x3D;&#x3D; MSE) &#123;</span><br><span class="line">        *(l.cost) &#x3D; pow(mag_array(l.delta, l.outputs * l.batch), 2);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        &#x2F;&#x2F; Always compute classification loss both for iou + cls loss and for logging with mse loss</span><br><span class="line">        &#x2F;&#x2F; TODO: remove IOU loss fields before computing MSE on class</span><br><span class="line">        &#x2F;&#x2F;   probably split into two arrays</span><br><span class="line">        if (l.iou_loss &#x3D;&#x3D; GIOU) &#123;</span><br><span class="line">            avg_iou_loss &#x3D; count &gt; 0 ? l.iou_normalizer * (tot_giou_loss &#x2F; count) : 0;</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">			&#x2F;&#x2F;count代表目标个数</span><br><span class="line">            avg_iou_loss &#x3D; count &gt; 0 ? l.iou_normalizer * (tot_iou_loss &#x2F; count) : 0;</span><br><span class="line">        &#125;</span><br><span class="line">        *(l.cost) &#x3D; avg_iou_loss + classification_loss;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    loss &#x2F;&#x3D; l.batch;</span><br><span class="line">    classification_loss &#x2F;&#x3D; l.batch;</span><br><span class="line">    iou_loss &#x2F;&#x3D; l.batch;</span><br><span class="line"></span><br><span class="line">    printf(&quot;v3 (%s loss, Normalizer: (iou: %f, cls: %f) Region %d Avg (IOU: %f, GIOU: %f), Class: %f, Obj: %f, No Obj: %f, .5R: %f, .75R: %f, count: %d, loss &#x3D; %f, class_loss &#x3D; %f, iou_loss &#x3D; %f\n&quot;,</span><br><span class="line">        (l.iou_loss &#x3D;&#x3D; MSE ? &quot;mse&quot; : (l.iou_loss &#x3D;&#x3D; GIOU ? &quot;giou&quot; : &quot;iou&quot;)), l.iou_normalizer, l.cls_normalizer, state.index, tot_iou &#x2F; count, tot_giou &#x2F; count, avg_cat &#x2F; class_count, avg_obj &#x2F; count, avg_anyobj &#x2F; (l.w*l.h*l.n*l.batch), recall &#x2F; count, recall75 &#x2F; count, count,</span><br><span class="line">        loss, classification_loss, iou_loss);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="backward-yolo-layer-误差反向传播"><a href="#backward-yolo-layer-误差反向传播" class="headerlink" title="backward_yolo_layer 误差反向传播"></a>backward_yolo_layer 误差反向传播</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;误差反向传播</span><br><span class="line">void backward_yolo_layer(const layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F;直接把 l.delta 拷贝给上一层的 delta。注意 net.delta 指向 prev_layer.delta。</span><br><span class="line">   axpy_cpu(l.batch*l.inputs, 1, l.delta, 1, state.delta, 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="correct-yolo-boxes-调整预测-box-中心和大小"><a href="#correct-yolo-boxes-调整预测-box-中心和大小" class="headerlink" title="correct_yolo_boxes 调整预测 box 中心和大小"></a>correct_yolo_boxes 调整预测 box 中心和大小</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;调整预测 box 中心和大小</span><br><span class="line"></span><br><span class="line">void correct_yolo_boxes(detection *dets, int n, int w, int h, int netw, int neth, int relative, int letter)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F;w 和 h 是输入图片的尺寸，netw 和 neth 是网络输入尺寸</span><br><span class="line">    int i;</span><br><span class="line">    &#x2F;&#x2F; network height (or width)</span><br><span class="line">    int new_w &#x3D; 0;</span><br><span class="line">    &#x2F;&#x2F; network height (or width)</span><br><span class="line">    int new_h &#x3D; 0;</span><br><span class="line">    &#x2F;&#x2F; Compute scale given image w,h vs network w,h</span><br><span class="line">    &#x2F;&#x2F; I think this &quot;rotates&quot; the image to match network to input image w&#x2F;h ratio</span><br><span class="line">    &#x2F;&#x2F; new_h and new_w are really just network width and height</span><br><span class="line">    if (letter) &#123;</span><br><span class="line">        if (((float)netw &#x2F; w) &lt; ((float)neth &#x2F; h)) &#123; &#x2F;&#x2F;新图片尺寸</span><br><span class="line">            new_w &#x3D; netw;</span><br><span class="line">            new_h &#x3D; (h * netw) &#x2F; w;</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">            new_h &#x3D; neth;</span><br><span class="line">            new_w &#x3D; (w * neth) &#x2F; h;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        new_w &#x3D; netw;</span><br><span class="line">        new_h &#x3D; neth;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; difference between network width and &quot;rotated&quot; width</span><br><span class="line">    float deltaw &#x3D; netw - new_w;</span><br><span class="line">    &#x2F;&#x2F; difference between network height and &quot;rotated&quot; height</span><br><span class="line">    float deltah &#x3D; neth - new_h;</span><br><span class="line">    &#x2F;&#x2F; ratio between rotated network width and network width</span><br><span class="line">    float ratiow &#x3D; (float)new_w &#x2F; netw;</span><br><span class="line">    &#x2F;&#x2F; ratio between rotated network width and network width</span><br><span class="line">    float ratioh &#x3D; (float)new_h &#x2F; neth;</span><br><span class="line">    for (i &#x3D; 0; i &lt; n; ++i) &#123; &#x2F;&#x2F;调整 box 相对新图片尺寸的位置</span><br><span class="line"></span><br><span class="line">        box b &#x3D; dets[i].bbox;</span><br><span class="line">        &#x2F;&#x2F; x &#x3D; ( x - (deltaw&#x2F;2)&#x2F;netw ) &#x2F; ratiow;</span><br><span class="line">        &#x2F;&#x2F;   x - [(1&#x2F;2 the difference of the network width and rotated width) &#x2F; (network width)]</span><br><span class="line">        b.x &#x3D; (b.x - deltaw &#x2F; 2. &#x2F; netw) &#x2F; ratiow;</span><br><span class="line">        b.y &#x3D; (b.y - deltah &#x2F; 2. &#x2F; neth) &#x2F; ratioh;</span><br><span class="line">        &#x2F;&#x2F; scale to match rotation of incoming image</span><br><span class="line">        b.w *&#x3D; 1 &#x2F; ratiow;</span><br><span class="line">        b.h *&#x3D; 1 &#x2F; ratioh;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; relative seems to always be &#x3D;&#x3D; 1, I don&#39;t think we hit this condition, ever.</span><br><span class="line">        if (!relative) &#123;</span><br><span class="line">            b.x *&#x3D; w;</span><br><span class="line">            b.w *&#x3D; w;</span><br><span class="line">            b.y *&#x3D; h;</span><br><span class="line">            b.h *&#x3D; h;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        dets[i].bbox &#x3D; b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="yolo-num-detections-预测输出中置信度超过阈值的-box-个数"><a href="#yolo-num-detections-预测输出中置信度超过阈值的-box-个数" class="headerlink" title="yolo_num_detections 预测输出中置信度超过阈值的 box 个数"></a>yolo_num_detections 预测输出中置信度超过阈值的 box 个数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;预测输出中置信度超过阈值的 box 个数</span><br><span class="line">int yolo_num_detections(layer l, float thresh)</span><br><span class="line">&#123;</span><br><span class="line">    int i, n;</span><br><span class="line">    int count &#x3D; 0;</span><br><span class="line">    for (i &#x3D; 0; i &lt; l.w*l.h; ++i)&#123;</span><br><span class="line">        for(n &#x3D; 0; n &lt; l.n; ++n)&#123;</span><br><span class="line">			&#x2F;&#x2F;&#x2F;&#x2F;获得置信度偏移位置</span><br><span class="line">            int obj_index  &#x3D; entry_index(l, 0, n*l.w*l.h + i, 4);</span><br><span class="line">			&#x2F;&#x2F;置信度超过阈值</span><br><span class="line">            if(l.output[obj_index] &gt; thresh)&#123;</span><br><span class="line">                ++count;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="get-yolo-detections-获得预测输出中超过阈值的-box"><a href="#get-yolo-detections-获得预测输出中超过阈值的-box" class="headerlink" title="get_yolo_detections 获得预测输出中超过阈值的 box"></a>get_yolo_detections 获得预测输出中超过阈值的 box</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;获得预测输出中超过阈值的 box</span><br><span class="line">int get_yolo_detections(layer l, int w, int h, int netw, int neth, float thresh, int *map, int relative, detection *dets, int letter)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F;printf(&quot;\n l.batch &#x3D; %d, l.w &#x3D; %d, l.h &#x3D; %d, l.n &#x3D; %d \n&quot;, l.batch, l.w, l.h, l.n);</span><br><span class="line">    int i,j,n;</span><br><span class="line">    float *predictions &#x3D; l.output;</span><br><span class="line">    &#x2F;&#x2F; This snippet below is not necessary</span><br><span class="line">    &#x2F;&#x2F; Need to comment it in order to batch processing &gt;&#x3D; 2 images</span><br><span class="line">    &#x2F;&#x2F;if (l.batch &#x3D;&#x3D; 2) avg_flipped_yolo(l);</span><br><span class="line">    int count &#x3D; 0;</span><br><span class="line">    for (i &#x3D; 0; i &lt; l.w*l.h; ++i)&#123;</span><br><span class="line">        int row &#x3D; i &#x2F; l.w;</span><br><span class="line">        int col &#x3D; i % l.w;</span><br><span class="line">        for(n &#x3D; 0; n &lt; l.n; ++n)&#123;</span><br><span class="line">            int obj_index  &#x3D; entry_index(l, 0, n*l.w*l.h + i, 4);</span><br><span class="line">            float objectness &#x3D; predictions[obj_index]; &#x2F;&#x2F;置信度</span><br><span class="line">            &#x2F;&#x2F;if(objectness &lt;&#x3D; thresh) continue;    &#x2F;&#x2F; incorrect behavior for Nan values</span><br><span class="line">            if (objectness &gt; thresh) &#123;</span><br><span class="line">                &#x2F;&#x2F;printf(&quot;\n objectness &#x3D; %f, thresh &#x3D; %f, i &#x3D; %d, n &#x3D; %d \n&quot;, objectness, thresh, i, n);</span><br><span class="line">                int box_index &#x3D; entry_index(l, 0, n*l.w*l.h + i, 0);</span><br><span class="line">                dets[count].bbox &#x3D; get_yolo_box(predictions, l.biases, l.mask[n], box_index, col, row, l.w, l.h, netw, neth, l.w*l.h);</span><br><span class="line">                dets[count].objectness &#x3D; objectness;</span><br><span class="line">                dets[count].classes &#x3D; l.classes;</span><br><span class="line">                for (j &#x3D; 0; j &lt; l.classes; ++j) &#123;</span><br><span class="line">                    int class_index &#x3D; entry_index(l, 0, n*l.w*l.h + i, 4 + 1 + j);</span><br><span class="line">                    float prob &#x3D; objectness*predictions[class_index];&#x2F;&#x2F;置信度 x 类别概率</span><br><span class="line">                    dets[count].prob[j] &#x3D; (prob &gt; thresh) ? prob : 0;&#x2F;&#x2F;小于阈值则概率置0</span><br><span class="line">                &#125;</span><br><span class="line">                ++count;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    correct_yolo_boxes(dets, count, w, h, netw, neth, relative, letter);&#x2F;&#x2F;调整 box 大小</span><br><span class="line">    return count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv1</title>
    <url>/2020/02/27/YOLOv1/</url>
    <content><![CDATA[<h2 id="创新点"><a href="#创新点" class="headerlink" title=" 创新点"></a><a id="more"></a> 创新点</h2><ul>
<li>将整张图作为网络的输入，直接在输出层回归bounding box的位置和所属类别。</li>
<li>速度快，One-Stage检测算法开山之作。</li>
</ul>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>回顾YOLO之前的目标检测算法，都是基于产生大量可能包含物体的先验框，然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区域再进行检测的方法虽然有较高的精度，但速度非常慢。YOLO直接将目标检测堪称一个回归问题进行处理，将候选区和检测两个阶段合二为一。YOLO的检测过程如下所示：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/640.webp" alt><br>事实上，YOLO并没有真正的去掉候选区，而是直接将输入图片划分成7 x 7=49个网格，每个网格预测两个边界框，一共预测49 x 2=98个边界框。可以近似理解为在输入图片上粗略的选取98个候选区，这98个候选区覆盖了图片的整个区域，进而用回归预测这98个候选框对应的边界框。</p>
<h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2><p>YOLO将输入图像划分为S x S的栅格，每个栅格负责检测中心落在该栅格中的物体。每一个栅格预测B个bounding boxes，以及这些bounding boxes的confidence scores。这个confidence  scores反映了模型对于这个栅格的预测：该栅格是否含有物体，以及这个box的坐标预测的有多准。公式定义如下： </p>
<script type="math/tex; mode=display">
confidence =\operatorname{Pr}(\text { Object }) * I O U_{\text {pred}}^{\text {truth}}</script><p>如果这个栅格中不存在一个object，则confidence score应该为0。相反，confidence score则为预测框与真实框框之间的交并比。YOLO对每个bounding  box有5个predictions：x，y，w，h和 confidence。坐标x，y代表了预测的bounding  box的中心与栅格边界的相对值。坐标w，h代表了预测的bounding  box的width、height相对于整幅图像width,height的比例。confidence就是预测的bounding  box和ground truth box的IOU值。每一个栅格还要预测C个conditional class probability（条件类别概率）：$Pr(Class_i|Object)$。即在一个栅格包含一个Object的前提下，它属于某个类的概率。我们只为每个栅格预测一组（C个）类概率，而不考虑框B的数量。如图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/641.webp" alt><br>它将图像划分为S × S网格，并且每个网格单元预测B个边界框，对这些框的置信度以及C类概率。这些预测值被编码为S × S × (B × 5 + C)张量。为了评估PASCAL VOC上的YOLO，我们使用S = 7，B = 2。PASCAL VOC有20个标记类，因此C = 20。我们的最终预测是7 × 7 × 30张量。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>我们将此模型作为卷积神经网络实施并在PASCAL  VOC检测数据集上进行评估。网络的初始卷积层从图像中提取特征，而全连接的层预测输出概率和坐标。YOLO网络借鉴了GoogLeNet分类网络结构。不同的是，YOLO未使用inception  module，而是使用1 x 1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3 x 3卷积层简单替代。完整的网络结构如图所示，最终的输出结果是一个7 x 7 x 30的张量。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/642.webp" alt></p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>首先利用ImageNet 1000-class的分类任务数据集Pretrain卷积层。使用上述网络中的前20 个卷积层，加上一个 average-pooling  layer，最后加一个全连接层，作为 Pretrain 的网络。训练大约一周的时间，使得在ImageNet  2012的验证数据集Top-5的精度达到 88%，这个结果跟 GoogleNet 的效果相当。</p>
<p>将Pretrain的结果的前20层卷积层应用到Detection中，并加入剩下的4个卷积层及2个全连接。同时为了获取更精细化的结果，将输入图像的分辨率由 224<em> 224 提升到 448</em> 448。将所有的预测结果都归一化到 0~1, 使用 Leaky RELU 作为激活函数。Leaky  RELU的公式如下：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/643.png" alt></p>
<p>Leaky RELU可以解决RELU的梯度消失问题。</p>
<p>损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡。简单的全部采用了sum-squared error loss来做这件事会有以下不足：</p>
<ul>
<li>8维的localization error和20维的classification error同等重要显然是不合理的。</li>
<li>如果一些栅格中没有object（一幅图中这种栅格很多），那么就会将这些栅格中的bounding box的confidence置为0，相比于较少的有object的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。</li>
</ul>
<p>为了解决这些问题，YOLO的损失函数的定义如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180511204751564.png" alt></p>
<p>YOLO的损失函数更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为 λcoord ，在pascal VOC训练中取5（上图蓝色框）。 对没有object的bbox的confidence loss，赋予小的loss weight，记为 λnoobj ，在pascal VOC训练中取0.5（上图橙色框）。 有object的bbox的confidence loss (上图红色框) 和类别的loss （上图紫色框）的loss weight正常取1。对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏相同的尺寸对IOU的影响更大。而sum-square error loss中对同样的偏移loss是一样。为了缓和这个问题，作者用了一个巧妙的办法，就是将box的width和height取平方根代替原本的height和width。如下图：small bbox的横轴值的偏移与big bbox一样都为0.1，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。即放大了small boxx的误差，使网络更注重于小尺度bbox的误差。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/644.webp" alt></p>
<p>在 YOLO中，每个栅格预测多个bounding box，但在网络模型的训练中，希望每一个物体最后由一个bounding box  predictor来负责预测。因此，当前哪一个predictor预测的bounding box与ground truth  box的IOU最大，这个predictor就负责predict  object。这会使得每个predictor可以专门的负责特定的物体检测。随着训练的进行，每一个predictor对特定的物体尺寸、长宽比的物体的类别的预测会越来越好。</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>测试的时候，每个网格预测的class信息$\left(\operatorname{Pr}\left(\text {Class}_{i} | \text {Object}\right)\right)$和bounding box预测的confidence信息$\left(\operatorname{Pr}(\text { Object }) * I O U_{\text {pred }}^{\text {truth }}\right)$相乘，就得到每个bounding box的class-specific confidence score。</p>
<script type="math/tex; mode=display">
P_r(Class_i|Object) ∗ P_r(Object) ∗ IOU^{truth}_{pred} = P_r(Class_i) ∗ IOU^{truth}_{pred}</script><ul>
<li><p>等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bounding box预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/645.webp" alt></p>
</li>
<li><p>对每一个网格的每一个bbox执行同样操作：7 x 7 x 2 = 98 bbox （每个bbox既有对应的class信息又有坐标信息）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/646.webp" alt></p>
</li>
<li><p>得到每个bbox的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/647.webp" alt></p>
<p>NMS的过程如下：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/648.gif" alt></p>
</li>
</ul>
<h2 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>就像在训练中一样，图像的检测只需要一个网络评估。在PASCAL VOC上，网络预测每个图像的98个边界框和每个框的类概率。YOLO在测试时间速度非常快，因为它只需要一个网络预测，而不像基于分类器的方法，所以速度很快。</li>
<li>速度快，YOLO将物体检测作为回归问题进行求解，整个检测网络pipeline简单。在titan x GPU上，在保证检测准确率的前提下（63.4% mAP，VOC 2007 test set），可以达到45fps的检测速度。</li>
<li>背景误检率低。YOLO在训练和推理过程中能看到整张图像的整体信息，而基于region proposal的物体检测方法（如rcnn/fast  rcnn），在检测过程中，只看到候选框内的局部图像信息。因此，若当图像背景（非物体）中的部分数据被包含在候选框中送入检测网络进行检测时，容易被误检测成物体。测试证明，YOLO对于背景图像的误检率低于fast rcnn误检率的一半。</li>
<li>通用性强。YOLO对于艺术类作品中的物体检测同样适用。它对非自然图像物体的检测率远远高于DPM和RCNN系列检测方法。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>每个 grid cell 只预测一个 类别的 Bounding Boxes，而且最后只取置信度最大的那个 Box。这就导致如果多个不同物体(或者同类物体的不同实体)的中心落在同一个网格中，会造成漏检。</li>
<li>预测的 Box 对于尺度的变化比较敏感，在尺度上的泛化能力比较差。</li>
<li>识别物体位置精准性差。</li>
<li>召回率低。</li>
</ul>
<h3 id="和其它算法对比"><a href="#和其它算法对比" class="headerlink" title="和其它算法对比"></a>和其它算法对比</h3><p>Table1给出了YOLO与其他物体检测方法，在检测速度和准确性方面的比较结果（使用VOC 2007数据集）。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/649.png" alt></p>
<p>论文中，作者还给出了YOLO与Fast RCNN在各方面的识别误差比例，如Table4所示。YOLO对背景内容的误判率（4.75%）比Fast  RCNN的误判率（13.6%）低很多。但是YOLO的定位准确率较差，占总误差比例的19.0%，而Fast RCNN仅为8.6%。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/YOLOv1/650.png" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>YOLOv1最大的开创性贡献在于将物体检测作为一个回归问题进行求解，输入图像经过一次inference，便能得到图像中所有物体的位置和其所属类别及相应的置信概率。而rcnn/fast rcnn/faster rcnn将检测结果分为两部分求解：物体类别（分类问题），物体位置即bounding  box（回归问题），所以YOLO的目标检测速度很快。</li>
<li>YOLO仍然是一个速度换精度的算法，目标检测的精度不如RCNN</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOV2损失函数代码详解(region_layer.c)</title>
    <url>/2020/02/27/YOLOV2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(region_layer.c)/</url>
    <content><![CDATA[<h2 id="YOLOV2损失函数"><a href="#YOLOV2损失函数" class="headerlink" title=" YOLOV2损失函数"></a><a id="more"></a> YOLOV2损失函数</h2><p>YOLOV2对每个预测<code>box</code>的<code>[x,y]</code>，<code>confidence</code>进行逻辑回归，类别进行<code>softmax</code>回归；</p>
<p>在Darknet中，损失函数可以用下图来进行表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/661.webp" alt></p>
<p>可以看到这个损失函数是相当复杂的，损失函数的定义在Darknet/src/region_layer.c中。对于上面这一堆公式，我们先简单看一下，然后我们在源码中去找到对应部分。这里的$W$和$H$代表的是特征图的高宽，都为13，而$A$指的是Anchor个数，YOLOv2中是5，各个$\lambda$值是各个loss部分的权重系数。我们将损失函数分成3大部分来解释：</p>
<ul>
<li><p>第一部分：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/662.png" alt><br>第一项需要好好解释一下，这个loss是计算background的置信度误差，这也是YOLO系列算法的特色，但是用哪些预测框来预测背景呢？这里需要计算各个预测框和所有的ground  truth之间的IOU值，并且取最大值记作MaxIOU，如果该值小于一定的阈值，YOLOv2论文取了0.6，那么这个预测框就标记为background，需要计算$\lambda_{n o o b j}$这么多倍的损失函数。为什么这个公式可以这样表达呢？因为我们有物体的话，那么$\lambda_{n o o b j}=0$，如果没有物体$\lambda_{\text {noob} j}=1$，我们把这个值带入到下面的公式就可以推出第一项啦！<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/663.webp" alt></p>
</li>
<li><p>第二部分：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/664.png" alt><br>这一部分是计算Anchor boxes和预测框的坐标误差，但是只在前12800个iter计算，这一项应该是促进网络学习到Anchor的形状。</p>
</li>
<li><p>第三部分：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/665.webp" alt><br>这一部分计算的是和ground truth匹配的预测框各部分的损失总和，包括坐标损失，置信度损失以及分类损失。<br><strong>3.1 坐标损失</strong> 这里的匹配原则是指对于某个特定的ground truth，首先要计算其中心点落在哪个cell上，然后计算这个cell的5个先验框和grond  truth的IOU值，计算IOU值的时候不考虑坐标只考虑形状，所以先将Anchor boxes和ground  truth的中心都偏移到同一位置，然后计算出对应的IOU值，IOU值最大的先验框和ground  truth匹配，对应的预测框用来预测这个ground truth。<br><strong>3.2 置信度损失</strong> 在计算obj置信度时， 增加了一项权重系数，也被称为rescore参数，当其为1时，损失是预测框和ground truth的真实IOU值(darknet中采用了这种实现方式)。而对于没有和ground  truth匹配的先验框，除去那些Max_IOU低于阈值的，其它就全部忽略。YOLOv2和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。<br><strong>3.3 分类损失</strong> 这个和YOLOv1一致，没什么好说的了。</p>
</li>
</ul>
<p>我看了一篇讲解YOLOv2损失函数非常好的文章：<a href="https://www.cnblogs.com/YiXiaoZhou/p/7429481.html" target="_blank" rel="noopener">https://www.cnblogs.com/YiXiaoZhou/p/7429481.html</a> 。里面还有一个关键点：</p>
<p>在计算boxes的$w$和$h$误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale x (2 - truth.w x truth.h)（这里和都归一化到(0,1))，这样对于尺度较小的boxes其权重系数会更大一些，可以放大误差，起到和YOLOv1计算平方根相似的效果。</p>
<h2 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#define DOABS 1</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 构建YOLOv2 region_layer层</span><br><span class="line">&#x2F;&#x2F; batch 一个batch中包含的图片数</span><br><span class="line">&#x2F;&#x2F; w 输入特征图的宽度</span><br><span class="line">&#x2F;&#x2F; h 输入特征图的高度</span><br><span class="line">&#x2F;&#x2F; n 一个cell预测多少个bbox</span><br><span class="line">&#x2F;&#x2F; classes 网络需要识别的物体类别数</span><br><span class="line">&#x2F;&#x2F; coord 一个bbox包含的[x,y,w,h]</span><br><span class="line">region_layer make_region_layer(int batch, int w, int h, int n, int classes, int coords, int max_boxes)</span><br><span class="line">&#123;</span><br><span class="line">    region_layer l &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">    l.type &#x3D; REGION; &#x2F;&#x2F;层类别</span><br><span class="line">	&#x2F;&#x2F; 这些变量都可以参考darknet.h中的注释</span><br><span class="line">    l.n &#x3D; n; &#x2F;&#x2F;一个cell中预测多少个box</span><br><span class="line">    l.batch &#x3D; batch; &#x2F;&#x2F;一个batch中包含的图片数</span><br><span class="line">    l.h &#x3D; h; &#x2F;&#x2F;输入图片的宽度</span><br><span class="line">    l.w &#x3D; w; &#x2F;&#x2F;输入图片的宽度</span><br><span class="line">    l.classes &#x3D; classes; &#x2F;&#x2F;网络需要识别的物体类数</span><br><span class="line">    l.coords &#x3D; coords; &#x2F;&#x2F;定位一个物体所需的参数个数（一般值为4,包括矩形中心点坐标x,y以及长宽w,h）</span><br><span class="line">    l.cost &#x3D; (float*)xcalloc(1, sizeof(float)); &#x2F;&#x2F;目标函数值，为单精度浮点型指针</span><br><span class="line">    l.biases &#x3D; (float*)xcalloc(n * 2, sizeof(float));</span><br><span class="line">    l.bias_updates &#x3D; (float*)xcalloc(n * 2, sizeof(float));</span><br><span class="line">    l.outputs &#x3D; h*w*n*(classes + coords + 1);  &#x2F;&#x2F;一张训练图片经过region_layer层后得到的输出元素个数（等于网格数*每个网格预测的矩形框数*每个矩形框的参数个数）</span><br><span class="line">    l.inputs &#x3D; l.outputs;   &#x2F;&#x2F;一张训练图片输入到reigon_layer层的元素个数（注意是一张图片，对于region_layer，输入和输出的元素个数相等）</span><br><span class="line">    &#x2F;&#x2F;每张图片含有的真实矩形框参数的个数（max_boxes表示一张图片中最多有max_boxes个ground truth矩形框，每个真实矩形框有</span><br><span class="line">    &#x2F;&#x2F;5个参数，包括x,y,w,h四个定位参数，以及物体类别）,注意max_boxes是darknet程序内写死的，实际上每张图片可能</span><br><span class="line">    &#x2F;&#x2F;并没有max_boxes个真实矩形框，也能没有这么多参数，但为了保持一致性，还是会留着这么大的存储空间，只是其中的</span><br><span class="line">    &#x2F;&#x2F;值为空而已.</span><br><span class="line">	l.max_boxes &#x3D; max_boxes;</span><br><span class="line">	&#x2F;&#x2F; GT: max_boxes*(4+1) 存储max_boxes个bbox的信息，这里是假设图片中GT bbox的数量是</span><br><span class="line">	&#x2F;&#x2F;小于max_boxes的，这里是写死的；此处与yolov1是不同的</span><br><span class="line">    l.truths &#x3D; max_boxes*(5);</span><br><span class="line">	&#x2F;&#x2F;  region层误差项（包含整个batch的）</span><br><span class="line">    l.delta &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float));</span><br><span class="line">	&#x2F;&#x2F; region层所有输出（包含整个batch的）</span><br><span class="line">    &#x2F;&#x2F;region_layer的输出维度是l.out_w*l.out_h，等于输出的维度，输出的通道数为l.out_c，也即是输入的通道数，具体为：n*(classes+coords+1)</span><br><span class="line">	&#x2F;&#x2F;YOLO检测模型将图片分成S*S个网格，每个网格又预测B个矩形框，最后一层输出的就是这些网格中包含的所有矩形框的信息</span><br><span class="line">	l.output &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float));</span><br><span class="line">    int i;</span><br><span class="line">	&#x2F;&#x2F;存储bbox的Anchor box的[w,h]的初始化,在src&#x2F;parse.c中parse_yolo函数会加载cfg中Anchor尺寸</span><br><span class="line">    for(i &#x3D; 0; i &lt; n*2; ++i)&#123;</span><br><span class="line">        l.biases[i] &#x3D; .5;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    l.forward &#x3D; forward_region_layer;</span><br><span class="line">    l.backward &#x3D; backward_region_layer;</span><br><span class="line">#ifdef GPU</span><br><span class="line">    l.forward_gpu &#x3D; forward_region_layer_gpu;</span><br><span class="line">    l.backward_gpu &#x3D; backward_region_layer_gpu;</span><br><span class="line">    l.output_gpu &#x3D; cuda_make_array(l.output, batch*l.outputs);</span><br><span class="line">    l.delta_gpu &#x3D; cuda_make_array(l.delta, batch*l.outputs);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    fprintf(stderr, &quot;detection\n&quot;);</span><br><span class="line">    srand(time(0));</span><br><span class="line"></span><br><span class="line">    return l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void resize_region_layer(layer *l, int w, int h)</span><br><span class="line">&#123;</span><br><span class="line">#ifdef GPU</span><br><span class="line">    int old_w &#x3D; l-&gt;w;</span><br><span class="line">    int old_h &#x3D; l-&gt;h;</span><br><span class="line">#endif</span><br><span class="line">    l-&gt;w &#x3D; w;</span><br><span class="line">    l-&gt;h &#x3D; h;</span><br><span class="line"></span><br><span class="line">    l-&gt;outputs &#x3D; h*w*l-&gt;n*(l-&gt;classes + l-&gt;coords + 1);</span><br><span class="line">    l-&gt;inputs &#x3D; l-&gt;outputs;</span><br><span class="line"></span><br><span class="line">    l-&gt;output &#x3D; (float*)xrealloc(l-&gt;output, l-&gt;batch * l-&gt;outputs * sizeof(float));</span><br><span class="line">    l-&gt;delta &#x3D; (float*)xrealloc(l-&gt;delta, l-&gt;batch * l-&gt;outputs * sizeof(float));</span><br><span class="line"></span><br><span class="line">#ifdef GPU</span><br><span class="line">    if (old_w &lt; w || old_h &lt; h) &#123;</span><br><span class="line">        cuda_free(l-&gt;delta_gpu);</span><br><span class="line">        cuda_free(l-&gt;output_gpu);</span><br><span class="line"></span><br><span class="line">        l-&gt;delta_gpu &#x3D; cuda_make_array(l-&gt;delta, l-&gt;batch*l-&gt;outputs);</span><br><span class="line">        l-&gt;output_gpu &#x3D; cuda_make_array(l-&gt;output, l-&gt;batch*l-&gt;outputs);</span><br><span class="line">    &#125;</span><br><span class="line">#endif</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;获取某个矩形框的4个定位信息，即根据输入的矩形框索引从l.output中获取该矩形框的定位信息x,y,w,h</span><br><span class="line">&#x2F;&#x2F;x  region_layer的输出，即l.output，包含所有batch预测得到的矩形框信息</span><br><span class="line">&#x2F;&#x2F;biases 表示Anchor框的长和宽</span><br><span class="line">&#x2F;&#x2F;index 矩形框的首地址（索引，矩形框中存储的首个参数x在l.output中的索引）</span><br><span class="line">&#x2F;&#x2F;i 第几行（region_layer维度为l.out_w*l.out_c）</span><br><span class="line">&#x2F;&#x2F;j 第几列</span><br><span class="line">&#x2F;&#x2F;w 特征图的宽度</span><br><span class="line">&#x2F;&#x2F;h 特征图的高度</span><br><span class="line">box get_region_box(float *x, float *biases, int n, int index, int i, int j, int w, int h)</span><br><span class="line">&#123;</span><br><span class="line">    box b;</span><br><span class="line">    b.x &#x3D; (i + logistic_activate(x[index + 0])) &#x2F; w;</span><br><span class="line">    b.y &#x3D; (j + logistic_activate(x[index + 1])) &#x2F; h;</span><br><span class="line">    b.w &#x3D; exp(x[index + 2]) * biases[2*n];</span><br><span class="line">    b.h &#x3D; exp(x[index + 3]) * biases[2*n+1];</span><br><span class="line">    if(DOABS)&#123;</span><br><span class="line">        b.w &#x3D; exp(x[index + 2]) * biases[2*n]   &#x2F; w;</span><br><span class="line">        b.h &#x3D; exp(x[index + 3]) * biases[2*n+1] &#x2F; h;</span><br><span class="line">    &#125;</span><br><span class="line">    return b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float delta_region_box(box truth, float *x, float *biases, int n, int index, int i, int j, int w, int h, float *delta, float scale)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F; 获得第j*w+i个cell第n个bbox在当前特征图上位置和宽高</span><br><span class="line">    box pred &#x3D; get_region_box(x, biases, n, index, i, j, w, h);</span><br><span class="line">	&#x2F;&#x2F; 计算pred bbox 与 GT bbox的IOU【前12800GT boox为当前cell第n个bbox的Anchor】</span><br><span class="line">    float iou &#x3D; box_iou(pred, truth);</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 计算GT bbox的tx,ty,tw,th</span><br><span class="line">    float tx &#x3D; (truth.x*w - i);</span><br><span class="line">    float ty &#x3D; (truth.y*h - j);</span><br><span class="line">    float tw &#x3D; log(truth.w &#x2F; biases[2*n]);</span><br><span class="line">    float th &#x3D; log(truth.h &#x2F; biases[2*n + 1]);</span><br><span class="line">    if(DOABS)&#123;</span><br><span class="line">        tw &#x3D; log(truth.w*w &#x2F; biases[2*n]);</span><br><span class="line">        th &#x3D; log(truth.h*h &#x2F; biases[2*n + 1]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 计算tx,ty,tw,th梯度</span><br><span class="line">    delta[index + 0] &#x3D; scale * (tx - logistic_activate(x[index + 0])) * logistic_gradient(logistic_activate(x[index + 0]));</span><br><span class="line">    delta[index + 1] &#x3D; scale * (ty - logistic_activate(x[index + 1])) * logistic_gradient(logistic_activate(x[index + 1]));</span><br><span class="line">    delta[index + 2] &#x3D; scale * (tw - x[index + 2]);</span><br><span class="line">    delta[index + 3] &#x3D; scale * (th - x[index + 3]);</span><br><span class="line">    return iou;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void delta_region_class(float *output, float *delta, int index, int class_id, int classes, tree *hier, float scale, float *avg_cat, int focal_loss)</span><br><span class="line">&#123;</span><br><span class="line">    int i, n;</span><br><span class="line">    if(hier)&#123; &#x2F;&#x2F; 在yolov2 中region层, 此部分不参与计算【这是在yolo9000才参与计算】</span><br><span class="line">        float pred &#x3D; 1;</span><br><span class="line">        while(class_id &gt;&#x3D; 0)&#123;</span><br><span class="line">            pred *&#x3D; output[index + class_id];</span><br><span class="line">            int g &#x3D; hier-&gt;group[class_id];</span><br><span class="line">            int offset &#x3D; hier-&gt;group_offset[g];</span><br><span class="line">            for(i &#x3D; 0; i &lt; hier-&gt;group_size[g]; ++i)&#123;</span><br><span class="line">                delta[index + offset + i] &#x3D; scale * (0 - output[index + offset + i]);</span><br><span class="line">            &#125;</span><br><span class="line">            delta[index + class_id] &#x3D; scale * (1 - output[index + class_id]);</span><br><span class="line"></span><br><span class="line">            class_id &#x3D; hier-&gt;parent[class_id];</span><br><span class="line">        &#125;</span><br><span class="line">        *avg_cat +&#x3D; pred;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        &#x2F;&#x2F; Focal loss</span><br><span class="line">        if (focal_loss) &#123; &#x2F;&#x2F;如果使用focal loss</span><br><span class="line">            &#x2F;&#x2F; Focal Loss</span><br><span class="line">            float alpha &#x3D; 0.5;    &#x2F;&#x2F; 0.25 or 0.5</span><br><span class="line">            &#x2F;&#x2F;float gamma &#x3D; 2;    &#x2F;&#x2F; hardcoded in many places of the grad-formula</span><br><span class="line"></span><br><span class="line">            int ti &#x3D; index + class_id;</span><br><span class="line">            float pt &#x3D; output[ti] + 0.000000000000001F;</span><br><span class="line">            &#x2F;&#x2F; http:&#x2F;&#x2F;fooplot.com&#x2F;#W3sidHlwZSI6MCwiZXEiOiItKDEteCkqKDIqeCpsb2coeCkreC0xKSIsImNvbG9yIjoiIzAwMDAwMCJ9LHsidHlwZSI6MTAwMH1d</span><br><span class="line">            float grad &#x3D; -(1 - pt) * (2 * pt*logf(pt) + pt - 1);    &#x2F;&#x2F; http:&#x2F;&#x2F;blog.csdn.net&#x2F;linmingan&#x2F;article&#x2F;details&#x2F;77885832</span><br><span class="line">            &#x2F;&#x2F;float grad &#x3D; (1 - pt) * (2 * pt*logf(pt) + pt - 1);    &#x2F;&#x2F; https:&#x2F;&#x2F;github.com&#x2F;unsky&#x2F;focal-loss</span><br><span class="line"></span><br><span class="line">            for (n &#x3D; 0; n &lt; classes; ++n) &#123;</span><br><span class="line">				&#x2F;&#x2F; focal loss的梯度</span><br><span class="line">                delta[index + n] &#x3D; scale * (((n &#x3D;&#x3D; class_id) ? 1 : 0) - output[index + n]);</span><br><span class="line"></span><br><span class="line">                delta[index + n] *&#x3D; alpha*grad;</span><br><span class="line"></span><br><span class="line">                if (n &#x3D;&#x3D; class_id) *avg_cat +&#x3D; output[index + n];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">            &#x2F;&#x2F; default</span><br><span class="line">            for (n &#x3D; 0; n &lt; classes; ++n) &#123;</span><br><span class="line">				&#x2F;&#x2F; 计算类别损失的梯度, 反向传递到误差项l.delta中, 在yolo v2中scale&#x3D;1</span><br><span class="line">                delta[index + n] &#x3D; scale * (((n &#x3D;&#x3D; class_id) ? 1 : 0) - output[index + n]);</span><br><span class="line">                if (n &#x3D;&#x3D; class_id) *avg_cat +&#x3D; output[index + n];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">float logit(float x)</span><br><span class="line">&#123;</span><br><span class="line">    return log(x&#x2F;(1.-x));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float tisnan(float x)</span><br><span class="line">&#123;</span><br><span class="line">    return (x !&#x3D; x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @brief 计算某个矩形框中某个参数在l.output中的索引。一个矩形框包含了x,y,w,h,c,C1,C2...,Cn信息，</span><br><span class="line"> *        前四个用于定位，第五个为矩形框含有物体的置信度信息c，即矩形框中存在物体的概率为多大，而C1到Cn</span><br><span class="line"> *        为矩形框中所包含的物体分别属于这n类物体的概率。本函数负责获取该矩形框首个定位信息也即x值在</span><br><span class="line"> *        l.output中索引、获取该矩形框置信度信息c在l.output中的索引、获取该矩形框分类所属概率的首个</span><br><span class="line"> *        概率也即C1值的索引，具体是获取矩形框哪个参数的索引，取决于输入参数entry的值，这些在</span><br><span class="line"> *        forward_region_layer()函数中都有用到，由于l.output的存储方式，当entry&#x3D;0时，就是获取矩形框</span><br><span class="line"> *        x参数在l.output中的索引；当entry&#x3D;4时，就是获取矩形框置信度信息c在l.output中的索引；当</span><br><span class="line"> *        entry&#x3D;5时，就是获取矩形框首个所属概率C1在l.output中的索引，具体可以参考forward_region_layer()</span><br><span class="line"> *        中调用本函数时的注释.</span><br><span class="line"> * @param l 当前region_layer</span><br><span class="line"> * @param batch 当前照片是整个batch中的第几张，因为l.output中包含整个batch的输出，所以要定位某张训练图片</span><br><span class="line"> *              输出的众多网格中的某个矩形框，当然需要该参数.</span><br><span class="line"> * @param location 这个参数，说实话，感觉像个鸡肋参数，函数中用这个参数获取n和loc的值，这个n就是表示网格中</span><br><span class="line"> *                 的第几个预测矩形框（比如每个网格预测5个矩形框，那么n取值范围就是从0~4，loc就是某个</span><br><span class="line"> *                 通道上的元素偏移（region_layer输出的通道数为l.out_c &#x3D; (classes + coords + 1)，</span><br><span class="line"> *                 这样说可能没有说明白，这都与l.output的存储结构相关，见下面详细注释以及其他说明。总之，</span><br><span class="line"> *                 查看一下调用本函数的父函数forward_region_layer()就知道了，可以直接输入n和j*l.w+i的，</span><br><span class="line"> *                 没有必要输入location，这样还得重新计算一次n和loc.</span><br><span class="line"> * @param entry 切入点偏移系数，关于这个参数，就又要扯到l.output的存储结构了，见下面详细注释以及其他说明.</span><br><span class="line"> * @details l.output这个参数的存储内容以及存储方式已经在多个地方说明了，再多的文字都不及图文说明，此处再</span><br><span class="line"> *          简要罗嗦几句，更为具体的参考图文说明。l.output中存储了整个batch的训练输出，每张训练图片都会输出</span><br><span class="line"> *          l.out_w*l.out_h个网格，每个网格会预测l.n个矩形框，每个矩形框含有l.classes+l.coords+1个参数，</span><br><span class="line"> *          而最后一层的输出通道数为l.n*(l.classes+l.coords+1)，可以想象下最终输出的三维张量是个什么样子的。</span><br><span class="line"> *          展成一维数组存储时，l.output可以首先分成batch个大段，每个大段存储了一张训练图片的所有输出；进一步细分，</span><br><span class="line"> *          取其中第一大段分析，该大段中存储了第一张训练图片所有输出网格预测的矩形框信息，每个网格预测了l.n个矩形框，</span><br><span class="line"> *          存储时，l.n个矩形框是分开存储的，也就是先存储所有网格中的第一个矩形框，而后存储所有网格中的第二个矩形框，</span><br><span class="line"> *          依次类推，如果每个网格中预测5个矩形框，则可以继续把这一大段分成5个中段。继续细分，5个中段中取第</span><br><span class="line"> *          一个中段来分析，这个中段中按行（有l.out_w*l.out_h个网格，按行存储）依次存储了这张训练图片所有输出网格中</span><br><span class="line"> *          的第一个矩形框信息，要注意的是，这个中段存储的顺序并不是挨个挨个存储每个矩形框的所有信息，</span><br><span class="line"> *          而是先存储所有矩形框的x，而后是所有的y,然后是所有的w,再是h，c，最后的的概率数组也是拆分进行存储，</span><br><span class="line"> *          并不是一下子存储完一个矩形框所有类的概率，而是先存储所有网格所属第一类的概率，再存储所属第二类的概率，</span><br><span class="line"> *          具体来说这一中段首先存储了l.out_w*l.out_h个x，然后是l.out_w*l.out_c个y，依次下去，</span><br><span class="line"> *          最后是l.out_w*l.out_h个C1（属于第一类的概率，用C1表示，下面类似），l.out_w*l.outh个C2,...,</span><br><span class="line"> *          l.out_w*l.out_c*Cn（假设共有n类），所以可以继续将中段分成几个小段，依次为x,y,w,h,c,C1,C2,...Cn</span><br><span class="line"> *          小段，每小段的长度都为l.out_w*l.out_c.</span><br><span class="line"> *          现在回过来看本函数的输入参数，batch就是大段的偏移数（从第几个大段开始，对应是第几张训练图片），</span><br><span class="line"> *          由location计算得到的n就是中段的偏移数（从第几个中段开始，对应是第几个矩形框），</span><br><span class="line"> *          entry就是小段的偏移数（从几个小段开始，对应具体是那种参数，x,c还是C1），而loc则是最后的定位，</span><br><span class="line"> *          前面确定好第几大段中的第几中段中的第几小段的首地址，loc就是从该首地址往后数loc个元素，得到最终定位</span><br><span class="line"> *          某个具体参数（x或c或C1）的索引值，比如l.output中存储的数据如下所示（这里假设只存了一张训练图片的输出，</span><br><span class="line"> *          因此batch只能为0；并假设l.out_w&#x3D;l.out_h&#x3D;2,l.classes&#x3D;2）：</span><br><span class="line"> *          xxxxyyyywwwwhhhhccccC1C1C1C1C2C2C2C2-#-xxxxyyyywwwwhhhhccccC1C1C1C1C2C2C2C2，</span><br><span class="line"> *          n&#x3D;0则定位到-#-左边的首地址（表示每个网格预测的第一个矩形框），n&#x3D;1则定位到-#-右边的首地址（表示每个网格预测的第二个矩形框）</span><br><span class="line"> *          entry&#x3D;0,loc&#x3D;0获取的是x的索引，且获取的是第一个x也即l.out_w*l.out_h个网格中第一个网格中第一个矩形框x参数的索引；</span><br><span class="line"> *          entry&#x3D;4,loc&#x3D;1获取的是c的索引，且获取的是第二个c也即l.out_w*l.out_h个网格中第二个网格中第一个矩形框c参数的索引；</span><br><span class="line"> *          entry&#x3D;5,loc&#x3D;2获取的是C1的索引，且获取的是第三个C1也即l.out_w*l.out_h个网格中第三个网格中第一个矩形框C1参数的索引；</span><br><span class="line"> *          如果要获取第一个网格中第一个矩形框w参数的索引呢？如果已经获取了其x值的索引，显然用x的索引加上3*l.out_w*l.out_h即可获取到，</span><br><span class="line"> *          这正是delta_region_box()函数的做法；</span><br><span class="line"> *          如果要获取第三个网格中第一个矩形框C2参数的索引呢？如果已经获取了其C1值的索引，显然用C1的索引加上l.out_w*l.out_h即可获取到，</span><br><span class="line"> *          这正是delta_region_class()函数中的做法；</span><br><span class="line"> *          由上可知，entry&#x3D;0时,即偏移0个小段，是获取x的索引；entry&#x3D;4,是获取自信度信息c的索引；entry&#x3D;5，是获取C1的索引.</span><br><span class="line"> *          l.output的存储方式大致就是这样，个人觉得说的已经很清楚了，但可视化效果终究不如图文说明～</span><br><span class="line">*&#x2F;</span><br><span class="line">static int entry_index(layer l, int batch, int location, int entry)</span><br><span class="line">&#123;</span><br><span class="line">    int n &#x3D; location &#x2F; (l.w*l.h);</span><br><span class="line">    int loc &#x3D; location % (l.w*l.h);</span><br><span class="line">    return batch*l.outputs + n*l.w*l.h*(l.coords + l.classes + 1) + entry*l.w*l.h + loc;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void softmax_tree(float *input, int batch, int inputs, float temp, tree *hierarchy, float *output);</span><br><span class="line">&#x2F;&#x2F;本函数多次调用了entry_index()函数，且使用的参数不尽相同，尤其是最后一个参数，通过最后一个参数，</span><br><span class="line">&#x2F;&#x2F;可以确定出region_layer输出l.output的数据存储方式。为方便叙述，假设本层输出参数l.w &#x3D; 2, l.h&#x3D; 3,</span><br><span class="line">&#x2F;&#x2F;l.n &#x3D; 2, l.classes &#x3D; 2, l.coords &#x3D; 4, l.c &#x3D; l.n * (l.coords + l.classes + 1) &#x3D; 21,</span><br><span class="line">&#x2F;&#x2F;l.output中存储了所有矩形框的信息参数，每个矩形框包括4条定位信息参数x,y,w,h，一条置信度（confidience）</span><br><span class="line">&#x2F;&#x2F;参数c，以及所有类别的概率C1,C2（本例中，假设就只有两个类别，l.classes&#x3D;2），那么一张样本图片最终会有</span><br><span class="line">&#x2F;&#x2F;l.w*l.h*l.n个矩形框（l.w*l.h即为最终图像划分层网格的个数，每个网格预测l.n个矩形框），那么</span><br><span class="line">&#x2F;&#x2F;l.output中存储的元素个数共有l.w*l.h*l.n*(l.coords + 1 + l.classes)，这些元素全部拉伸成一维数组</span><br><span class="line">&#x2F;&#x2F;的形式存储在l.output中，存储的顺序为：</span><br><span class="line">&#x2F;&#x2F;xxxxxx-yyyyyy-wwwwww-hhhhhh-cccccc-C1C1C1C1C1C1C2C2C2C2C2C2-##-xxxxxx-yyyyyy-wwwwww-hhhhhh-cccccc-C1C2C1C2C1C2C1C2C1C2C1C2</span><br><span class="line">&#x2F;&#x2F;文字说明如下：-##-隔开分成两段，左右分别是代表所有网格的第1个box和第2个box（因为l.n&#x3D;2，表示每个网格预测两个box）</span><br><span class="line">&#x2F;&#x2F;总共有l.w*l.h个网格，且存储时，把所有网格的x,y,w,h,c信息聚到一起再拼接起来，因此xxxxxx及其他信息都有l.w*l.h&#x3D;6个，</span><br><span class="line">&#x2F;&#x2F;因为每个有l.classes个物体类别，而且也是和xywh一样，每一类都集中存储，先存储l.w*l.h&#x3D;6个C1类，而后存储6个C2类，</span><br><span class="line">&#x2F;&#x2F;置信度参数c表示的是该矩形框内存在物体的概率，而C1，C2分别表示矩形框内存在物体时属于物体1和物体2的概率，</span><br><span class="line">&#x2F;&#x2F;因此c*C1即得矩形框内存在物体1的概率，c*C2即得矩形框内存在物体2的概率</span><br><span class="line">void forward_region_layer(const region_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int i,j,b,t,n;</span><br><span class="line">    int size &#x3D; l.coords + l.classes + 1;</span><br><span class="line">	&#x2F;&#x2F;内存拷贝, l.output &#x3D; state.input</span><br><span class="line">    memcpy(l.output, state.input, l.outputs*l.batch*sizeof(float));</span><br><span class="line">	&#x2F;&#x2F;这个#ifndef预编译指令没有必要用的，因为forward_region_layer()函数本身就对应没有定义gpu版的，所以肯定会执行其中的语句</span><br><span class="line">    #ifndef GPU</span><br><span class="line">    flatten(l.output, l.w*l.h, size*l.n, l.batch, 1);</span><br><span class="line">    #endif</span><br><span class="line">    for (b &#x3D; 0; b &lt; l.batch; ++b)&#123;</span><br><span class="line">        for(i &#x3D; 0; i &lt; l.h*l.w*l.n; ++i)&#123;</span><br><span class="line">            int index &#x3D; size*i + b*l.outputs;</span><br><span class="line">			&#x2F;&#x2F; 对confidence进行逻辑回归</span><br><span class="line">            l.output[index + 4] &#x3D; logistic_activate(l.output[index + 4]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#ifndef GPU</span><br><span class="line">    if (l.softmax_tree)&#123;</span><br><span class="line">        for (b &#x3D; 0; b &lt; l.batch; ++b)&#123;</span><br><span class="line">            for(i &#x3D; 0; i &lt; l.h*l.w*l.n; ++i)&#123;</span><br><span class="line">                int index &#x3D; size*i + b*l.outputs;</span><br><span class="line">                softmax_tree(l.output + index + 5, 1, 0, 1, l.softmax_tree, l.output + index + 5);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; else if (l.softmax)&#123;</span><br><span class="line">        for (b &#x3D; 0; b &lt; l.batch; ++b)&#123;</span><br><span class="line">            for(i &#x3D; 0; i &lt; l.h*l.w*l.n; ++i)&#123;</span><br><span class="line">                int index &#x3D; size*i + b*l.outputs;</span><br><span class="line">				&#x2F;&#x2F; l.softmax 对class进行softmax回归</span><br><span class="line">                softmax(l.output + index + 5, l.classes, 1, l.output + index + 5, 1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">#endif</span><br><span class="line">	&#x2F;&#x2F; inference阶段，则到此结束</span><br><span class="line">    if(!state.train) return;</span><br><span class="line">	&#x2F;&#x2F; 将reorg层的误差项进行初始化（包含整个batch的）</span><br><span class="line">    memset(l.delta, 0, l.outputs * l.batch * sizeof(float));</span><br><span class="line">    float avg_iou &#x3D; 0; &#x2F;&#x2F;平均IoU（Intersection over Union）</span><br><span class="line">    float recall &#x3D; 0; &#x2F;&#x2F;召回率</span><br><span class="line">    float avg_cat &#x3D; 0;</span><br><span class="line">    float avg_obj &#x3D; 0;</span><br><span class="line">    float avg_anyobj &#x3D; 0; &#x2F;&#x2F;一张训练图片所有预测矩形框的平均置信度（矩形框中含有物体的概率），该参数没有实际用处，仅用于输出打印</span><br><span class="line">    int count &#x3D; 0;</span><br><span class="line">    int class_count &#x3D; 0;</span><br><span class="line">	&#x2F;&#x2F; region层的总损失初始化为0</span><br><span class="line">    *(l.cost) &#x3D; 0;</span><br><span class="line">	&#x2F;&#x2F; 遍历batch中每一张图片</span><br><span class="line">    for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;</span><br><span class="line">        if(l.softmax_tree)&#123; &#x2F;&#x2F;【这是在yolo9000才参与计算】</span><br><span class="line">            int onlyclass_id &#x3D; 0;</span><br><span class="line">			&#x2F;&#x2F; 循环max_boxes次，每张图片固定处理max_boxes个矩形框</span><br><span class="line">            for(t &#x3D; 0; t &lt; l.max_boxes; ++t)&#123;</span><br><span class="line">				&#x2F;&#x2F; 通过移位来获取每一个真实矩形框的信息，net.truth存储了网络吞入的所有图片的真实矩形框信息（一次吞入一个batch的训练图片），</span><br><span class="line">                &#x2F;&#x2F; net.truth作为这一个大数组的首地址，l.truths参数是每一张图片含有的真实值参数个数（可参考layer.h中的truths参数中的注释），</span><br><span class="line">                &#x2F;&#x2F; b是batch中已经处理完图片的图片的张数，5是每个真实矩形框需要5个参数值（也即每条矩形框真值有5个参数），t是本张图片已经处理</span><br><span class="line">                &#x2F;&#x2F; 过的矩形框的个数（每张图片最多处理max_boxes个矩形框），明白了上面的参数之后对于下面的移位获取对应矩形框真实值的代码就不难了</span><br><span class="line">                box truth &#x3D; float_to_box(state.truth + t*5 + b*l.truths);</span><br><span class="line">                &#x2F;&#x2F; 这个if语句是用来判断一下是否有读到真实矩形框值（每个矩形框有5个参数,float_to_box只读取其中的4个定位参数，</span><br><span class="line">                &#x2F;&#x2F; 只要验证x的值不为0,那肯定是4个参数值都读取到了，要么全部读取到了，要么一个也没有），另外，因为程序中写死了每张图片处理max_boxes个矩形框，</span><br><span class="line">                &#x2F;&#x2F; 那么有些图片没有这么多矩形框，就会出现没有读到的情况。</span><br><span class="line">				if(!truth.x) break; &#x2F;&#x2F; continue;</span><br><span class="line">				&#x2F;&#x2F;float_to_box()中没有读取矩形框中包含的物体类别编号的信息，就在此处获取。（darknet中，物体类别标签值为编号，</span><br><span class="line">				&#x2F;&#x2F;每一个类别都有一个编号值，这些物体具体的字符名称存储在一个文件中，如data&#x2F;*.names文件，其所在行数就是其编号值）</span><br><span class="line">                int class_id &#x3D; state.truth[t*5 + b*l.truths + 4];</span><br><span class="line">                float maxp &#x3D; 0;</span><br><span class="line">                int maxi &#x3D; 0;</span><br><span class="line">                if(truth.x &gt; 100000 &amp;&amp; truth.y &gt; 100000)&#123;</span><br><span class="line">                    for(n &#x3D; 0; n &lt; l.n*l.w*l.h; ++n)&#123;</span><br><span class="line">                        int index &#x3D; size*n + b*l.outputs + 5;</span><br><span class="line">                        float scale &#x3D;  l.output[index-1];</span><br><span class="line">                        float p &#x3D; scale*get_hierarchy_probability(l.output + index, l.softmax_tree, class_id);</span><br><span class="line">                        if(p &gt; maxp)&#123;</span><br><span class="line">                            maxp &#x3D; p;</span><br><span class="line">                            maxi &#x3D; n;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    int index &#x3D; size*maxi + b*l.outputs + 5;</span><br><span class="line">                    delta_region_class(l.output, l.delta, index, class_id, l.classes, l.softmax_tree, l.class_scale, &amp;avg_cat, l.focal_loss);</span><br><span class="line">                    ++class_count;</span><br><span class="line">                    onlyclass_id &#x3D; 1;</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            if(onlyclass_id) continue;</span><br><span class="line">        &#125;</span><br><span class="line">		</span><br><span class="line">        for (j &#x3D; 0; j &lt; l.h; ++j) &#123;</span><br><span class="line">            for (i &#x3D; 0; i &lt; l.w; ++i) &#123; &#x2F;&#x2F; 遍历每个cell, 当前cell编号为[j, i]</span><br><span class="line">                for (n &#x3D; 0; n &lt; l.n; ++n) &#123; &#x2F;&#x2F; 遍历每个bbox，当前bbox编号为[n]</span><br><span class="line">					&#x2F;&#x2F;根据i,j,n计算该矩形框的索引，实际是矩形框中存储的x参数在l.output中的索引，矩形框中包含多个参数，</span><br><span class="line">					&#x2F;&#x2F;x是其存储的首个参数，所以也可以说是获取该矩形框的首地址。</span><br><span class="line">                    int index &#x3D; size*(j*l.w*l.n + i*l.n + n) + b*l.outputs;</span><br><span class="line">					&#x2F;&#x2F; 根据矩形框的索引，获取矩形框的定位信息</span><br><span class="line">                    box pred &#x3D; get_region_box(l.output, l.biases, n, index, i, j, l.w, l.h);</span><br><span class="line">                    &#x2F;&#x2F; 最高IoU，赋初值0</span><br><span class="line">					float best_iou &#x3D; 0;</span><br><span class="line">                    int best_class_id &#x3D; -1;</span><br><span class="line">					&#x2F;&#x2F; 遍历每一个GT bbox</span><br><span class="line">                    for(t &#x3D; 0; t &lt; l.max_boxes; ++t)&#123;</span><br><span class="line">						&#x2F;&#x2F;将第t个bbox由float数组转bbox结构体,方便计算IOU</span><br><span class="line">                        box truth &#x3D; float_to_box(state.truth + t*5 + b*l.truths);</span><br><span class="line">						&#x2F;&#x2F;获取第t个bbox的物体类别</span><br><span class="line">                        int class_id &#x3D; state.truth[t * 5 + b*l.truths + 4];</span><br><span class="line">                        if (class_id &gt;&#x3D; l.classes) continue; &#x2F;&#x2F; if label contains class_id more than number of classes in the cfg-file</span><br><span class="line">                        if(!truth.x) break; &#x2F;&#x2F; continue;</span><br><span class="line">						&#x2F;&#x2F; 计算pred与第t个GT之间的IOU</span><br><span class="line">                        float iou &#x3D; box_iou(pred, truth);</span><br><span class="line">                        if (iou &gt; best_iou) &#123;</span><br><span class="line">                            best_class_id &#x3D; state.truth[t*5 + b*l.truths + 4];</span><br><span class="line">                            best_iou &#x3D; iou; &#x2F;&#x2F; 最大IOU更新</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">					&#x2F;&#x2F; 统计有目标的概率</span><br><span class="line">                    avg_anyobj +&#x3D; l.output[index + 4];</span><br><span class="line">					&#x2F;&#x2F; 与yolov1相似, 先将所有pred bbox都当做noobject，计算其confidence损失梯度</span><br><span class="line">                    l.delta[index + 4] &#x3D; l.noobject_scale * ((0 - l.output[index + 4]) * logistic_gradient(l.output[index + 4]));</span><br><span class="line">                    &#x2F;&#x2F; 在yolov2中并没有执行</span><br><span class="line">					if(l.classfix &#x3D;&#x3D; -1) l.delta[index + 4] &#x3D; l.noobject_scale * ((best_iou - l.output[index + 4]) * logistic_gradient(l.output[index + 4]));</span><br><span class="line">                    else&#123;</span><br><span class="line">						&#x2F;&#x2F; best_iou大于阈值则说明有object, 在yolo v2中阈值为0.6</span><br><span class="line">                        if (best_iou &gt; l.thresh) &#123;</span><br><span class="line">                            l.delta[index + 4] &#x3D; 0;</span><br><span class="line">                            if(l.classfix &gt; 0)&#123;</span><br><span class="line">                                delta_region_class(l.output, l.delta, index + 5, best_class_id, l.classes, l.softmax_tree, l.class_scale*(l.classfix &#x3D;&#x3D; 2 ? l.output[index + 4] : 1), &amp;avg_cat, l.focal_loss);</span><br><span class="line">                                ++class_count;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">					&#x2F;&#x2F; net.seen 保存当前是训练第多少张图片</span><br><span class="line">                    if(*(state.net.seen) &lt; 12800)&#123;</span><br><span class="line">						&#x2F;&#x2F; 对于训练阶段的前12800张图片,GT bbox 直接用了anchor box</span><br><span class="line">                        box truth &#x3D; &#123;0&#125;; &#x2F;&#x2F; 计算第[j, i]cell, 第n个bbox的anchor bbox</span><br><span class="line">                        truth.x &#x3D; (i + .5)&#x2F;l.w; &#x2F;&#x2F; +0.5是因为x位于几何中心, 然后计算x相对整张特征图的位置</span><br><span class="line">                        truth.y &#x3D; (j + .5)&#x2F;l.h;</span><br><span class="line">                        truth.w &#x3D; l.biases[2*n];</span><br><span class="line">                        truth.h &#x3D; l.biases[2*n+1];</span><br><span class="line">                        if(DOABS)&#123;</span><br><span class="line">                            truth.w &#x3D; l.biases[2*n]&#x2F;l.w;</span><br><span class="line">                            truth.h &#x3D; l.biases[2*n+1]&#x2F;l.h;</span><br><span class="line">                        &#125;</span><br><span class="line">						&#x2F;&#x2F; 将pred bbox的tx,ty,tw,th和上面的truth box的差值反向传递到l.detla</span><br><span class="line">                        delta_region_box(truth, l.output, l.biases, n, index, i, j, l.w, l.h, l.delta, .01);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">		&#x2F;&#x2F; 遍历每一个GT bbox</span><br><span class="line">        for(t &#x3D; 0; t &lt; l.max_boxes; ++t)&#123;</span><br><span class="line">			&#x2F;&#x2F; 将第t个bbox由float数组转bbox结构体,方便计算IOU</span><br><span class="line">            box truth &#x3D; float_to_box(state.truth + t*5 + b*l.truths);</span><br><span class="line">			</span><br><span class="line">            int class_id &#x3D; state.truth[t * 5 + b*l.truths + 4];</span><br><span class="line">            if (class_id &gt;&#x3D; l.classes) &#123;</span><br><span class="line">                printf(&quot; Warning: in txt-labels class_id&#x3D;%d &gt;&#x3D; classes&#x3D;%d in cfg-file. In txt-labels class_id should be [from 0 to %d] \n&quot;, class_id, l.classes, l.classes-1);</span><br><span class="line">                getchar();</span><br><span class="line">                continue; &#x2F;&#x2F; if label contains class_id more than number of classes in the cfg-file</span><br><span class="line">            &#125;</span><br><span class="line">			&#x2F;&#x2F; 如果x坐标为0则取消, 因为yolov2这里定义了30 bbox, 可能实际上没有bbox</span><br><span class="line">            if(!truth.x) break; &#x2F;&#x2F; continue;</span><br><span class="line">            float best_iou &#x3D; 0; &#x2F;&#x2F; 保存最大IOU</span><br><span class="line">            int best_index &#x3D; 0;&#x2F;&#x2F; 保存最大IOU的bbox index</span><br><span class="line">            int best_n &#x3D; 0;</span><br><span class="line">            i &#x3D; (truth.x * l.w); &#x2F;&#x2F; 获得当前第t个GT bbox所在cell</span><br><span class="line">            j &#x3D; (truth.y * l.h);</span><br><span class="line">            &#x2F;&#x2F;printf(&quot;%d %f %d %f\n&quot;, i, truth.x*l.w, j, truth.y*l.h);</span><br><span class="line">            box truth_shift &#x3D; truth; &#x2F;&#x2F; 将truth_shift的box移动到0,0</span><br><span class="line">            truth_shift.x &#x3D; 0;</span><br><span class="line">            truth_shift.y &#x3D; 0;</span><br><span class="line">            &#x2F;&#x2F;printf(&quot;index %d %d\n&quot;,i, j);</span><br><span class="line">            for(n &#x3D; 0; n &lt; l.n; ++n)&#123; &#x2F;&#x2F; 遍历cell[j,i]所在的n个预测bbox</span><br><span class="line">				&#x2F;&#x2F; 获得第j*w+i个cell第n个bbox的index</span><br><span class="line">                int index &#x3D; size*(j*l.w*l.n + i*l.n + n) + b*l.outputs;</span><br><span class="line">				&#x2F;&#x2F; 获得第j*w+i个cell第n个bbox在当前特征图上位置和宽高</span><br><span class="line">                box pred &#x3D; get_region_box(l.output, l.biases, n, index, i, j, l.w, l.h);</span><br><span class="line">                if(l.bias_match)&#123;&#x2F;&#x2F; yolov2 reorg层 bias_match &#x3D; 1</span><br><span class="line">                    pred.w &#x3D; l.biases[2*n];</span><br><span class="line">                    pred.h &#x3D; l.biases[2*n+1];</span><br><span class="line">                    if(DOABS)&#123;</span><br><span class="line">                        pred.w &#x3D; l.biases[2*n]&#x2F;l.w; &#x2F;&#x2F; 然后计算pred box的w相对整张特征图的位置</span><br><span class="line">                        pred.h &#x3D; l.biases[2*n+1]&#x2F;l.h;  &#x2F;&#x2F; 然后计算pred box的h相对整张特征图的位置</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                &#x2F;&#x2F;printf(&quot;pred: (%f, %f) %f x %f\n&quot;, pred.x, pred.y, pred.w, pred.h);</span><br><span class="line">                pred.x &#x3D; 0; &#x2F;&#x2F; 将预测的bbox移动到0,0</span><br><span class="line">                pred.y &#x3D; 0;</span><br><span class="line">                float iou &#x3D; box_iou(pred, truth_shift); &#x2F;&#x2F; 计算GT box truth_shift 与 预测bbox pred 二者之间的IOU</span><br><span class="line">                if (iou &gt; best_iou)&#123;</span><br><span class="line">                    best_index &#x3D; index;  &#x2F;&#x2F; 记录best_iou对应bbox的index</span><br><span class="line">                    best_iou &#x3D; iou; &#x2F;&#x2F; 记录IOU最大的IOU</span><br><span class="line">                    best_n &#x3D; n; &#x2F;&#x2F; 以及记录该bbox的编号n</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F;printf(&quot;%d %f (%f, %f) %f x %f\n&quot;, best_n, best_iou, truth.x, truth.y, truth.w, truth.h);</span><br><span class="line">			&#x2F;&#x2F; 计算获得best_iou的pred bbox 与 GT bbox之间的真实iou, 之前best_iou是方便计算,以及加速,</span><br><span class="line">            &#x2F;&#x2F; 同时完成坐标损失的反向传递</span><br><span class="line">            float iou &#x3D; delta_region_box(truth, l.output, l.biases, best_n, best_index, i, j, l.w, l.h, l.delta, l.coord_scale);</span><br><span class="line">            &#x2F;&#x2F; 如果iou大于0.5, recall ++;</span><br><span class="line">			if(iou &gt; .5) recall +&#x3D; 1;</span><br><span class="line">            avg_iou +&#x3D; iou;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;l.delta[best_index + 4] &#x3D; iou - l.output[best_index + 4];</span><br><span class="line">			&#x2F;&#x2F; 统计有目标的概率</span><br><span class="line">            avg_obj +&#x3D; l.output[best_index + 4];</span><br><span class="line">			&#x2F;&#x2F; 与yolov1相似, 该pred bbox中是有object，计算其confidence损失梯度; object_scale &#x3D; 5</span><br><span class="line">            l.delta[best_index + 4] &#x3D; l.object_scale * (1 - l.output[best_index + 4]) * logistic_gradient(l.output[best_index + 4]);</span><br><span class="line">            if (l.rescore) &#123; &#x2F;&#x2F; yolov2 reorg层中rescore &#x3D; 1, 参于计算</span><br><span class="line">				&#x2F;&#x2F;定义了rescore表示同时对confidence score进行回归</span><br><span class="line">				&#x2F;&#x2F; 该pred bbox中是有object，计算其confidence损失梯度的方法发生变化; object_scale &#x3D; 5,</span><br><span class="line">				l.delta[best_index + 4] &#x3D; l.object_scale * (iou - l.output[best_index + 4]) * logistic_gradient(l.output[best_index + 4]);</span><br><span class="line">            &#125;</span><br><span class="line">			&#x2F;&#x2F; yolov2 reorg层中map &#x3D; 0, 不参与计算 【这是在yolo9000才参与计算】</span><br><span class="line">            if (l.map) class_id &#x3D; l.map[class_id];</span><br><span class="line">			 &#x2F;&#x2F; 与yolov1相似, 该pred bbox中是有object，计算其class损失梯度; class_scale &#x3D; 1</span><br><span class="line">            delta_region_class(l.output, l.delta, best_index + 5, class_id, l.classes, l.softmax_tree, l.class_scale, &amp;avg_cat, l.focal_loss);</span><br><span class="line">            ++count;&#x2F;&#x2F; 正样本个数+1</span><br><span class="line">            ++class_count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;printf(&quot;\n&quot;);</span><br><span class="line">    #ifndef GPU</span><br><span class="line">    flatten(l.delta, l.w*l.h, size*l.n, l.batch, 0);</span><br><span class="line">    #endif</span><br><span class="line">    *(l.cost) &#x3D; pow(mag_array(l.delta, l.outputs * l.batch), 2);</span><br><span class="line">    printf(&quot;Region Avg IOU: %f, Class: %f, Obj: %f, No Obj: %f, Avg Recall: %f,  count: %d\n&quot;, avg_iou&#x2F;count, avg_cat&#x2F;class_count, avg_obj&#x2F;count, avg_anyobj&#x2F;(l.w*l.h*l.n*l.batch), recall&#x2F;count, count);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster-RCNN</title>
    <url>/2020/02/26/Faster-RCNN/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>我们知道RCNN和Fast-RCNN都是双阶段的算法，依赖于候选框搜索算法。而搜索算法是很慢的，这就导致这两个算法不能实时。基于这个重大缺点，Faster-RCNN算法问世。</p>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><p>Fast-RCNN仍依赖于搜索候选框方法，其中以Selective  Search为主。在Fast-RCNN给出的时间测试结果中，一张图片需要2.3s的前向推理时间，其中2s用于生成2000个ROI。可以看到整个算法的时间消耗几乎都在区域候选框搜索这个步骤了，如果我们能去掉候选框搜索这个过程是不是实时有希望了？Faster-RCNN就干了这件事，论文提出在内部使用深层网络代替候选区域。新的候选区域网络(RPN)在生成ROI的效率大大提升，一张图片只需要10毫秒！！！</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>Faster-RCNN的网络结构如下图表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Faster%20RCNN/640.png" alt></p>
<p>我们可以发现除了添加一个RPN网络之外，其他地方和Fast-RCNN是完全一致的。引用知乎上看到的一张更详细的网络结构如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Faster%20RCNN/641.jfif" alt></p>
<h2 id="RPN网络"><a href="#RPN网络" class="headerlink" title="RPN网络"></a>RPN网络</h2><p>RPN网络将第一个卷积网络(backbone，如VGG16,ResNet)的输出特征图作为输入。它在特征图上滑动一个$3 \times 3$的卷积核，以使用卷积网络构建与类别无关的候选区域（候选框建议网络只用关心建议出来的框是否包含物体，而不用关系那个物体是哪一类的），我们将RPN产生的每个框叫做Anchor。</p>
<p>这里这样说肯定还是比较模糊，我引用一张训练时候的RPN的结构图然后固定输入分辨率和backbone为VGG16来解释一下。下面这张图是RPN架构：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Faster%20RCNN/642.jfif" alt></p>
<p>我们可以看到anchor的数量是和Feature Map的大小相关，对于特征图中的每一个位置，RPN会做$k$次预测。因此，在这里对每个像素，RPN将输出$4 \times k$个坐标和$2 \times k$个得分。然后由于使用了VGG16做Backbone，所以输入到RPN的特征图大小是原图的$H, W$的$\frac{1}{16}$。对于一个$512 \times 62 \times 37$的feature map，有$62 \times 37 \times 9$约等于20000个anchor。也就是对一张图片，有20000个左右的anchor。这里可以看到RPN的高明之处，一张图片20000个候选框就是猜也能猜得七七八八。但是并不是20000个框我们都需要，我们只需要选取其中的256个。具体的选取规则如下：</p>
<ul>
<li>对于每一个Ground Truth Bounding Box，选择和它IOU最高的一个anchor作为正样本。</li>
<li>对于剩下的anchor，选择和任意一个Ground Truth Bounding Box 的IOU大于0.7的anchor作为正样本，正样本的数目不超过128个。</li>
<li>负样本直接选择和Ground Truth Bounding Box 的IOU&lt;0.3的anchor。正负样本的总数保证为256个。</li>
</ul>
<p>RPN在产生正负样本训练的时候，还会产生ROIs作为Faster-RCNN(ROI-Head)的训练样本。RPN生成ROIs的过程（网络结构图中的ProposalCreator）如下：</p>
<ul>
<li>对于每张图片，利用它的feature map， 计算 (H/16)× (W/16)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。</li>
<li>选取概率较大的12000个anchor</li>
<li>利用回归的位置参数，修正这12000个anchor的位置，得到RoIs</li>
<li>利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的2000个RoIs</li>
</ul>
<p>在前向推理阶段，12000和2000分别变为6000和3000以提高速度，这个过程不需要反向传播，所以更容易实现。</p>
<p>最后RPN的输出维度是$2000 \times 4$或者$300 \times 4$的tensor。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在RPN网络中，对于每个Anchor，它们对应的gt_label（就是筛选到这个Anchor的那个ground truth框的label）要么是1要么是0，1代表前景，0代表背景。而gt_loc则是由4个位置参数$(t x, t y, t w, t h)$组成，这样比直接回归坐标更好。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Faster%20RCNN/643.webp" alt></p>
<p>计算分类用的是交叉熵损失，而计算回归损失用的是SmoothL1Loss。在计算回归损失的时候只统计前景的损失，忽略背景的损失。</p>
<p>网络在最后对每一个框都有两种损失，即物体属于哪一类的分类损失(21类，加了个背景)，位置在哪的回归损失。所以整个Faster-RCNN的损失是这4个损失之和。网络的目标就是最小化这四个损失之和。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>上面讲了，RPN会产生大约2000个ROIs，这2000个ROIs并不都拿去训练，而是利用Proposal Target Creator选择128个ROIs用以训练。选择的规则如下：</p>
<ul>
<li>RoIs和gt_bboxes 的IoU大于0.5的，选择一些（比如32个）</li>
<li>选择 RoIs和gt_bboxes的IoU小于等于0（或者0.1）的选择一些（比如 128-32=96个）作为负样本</li>
</ul>
<p>同时为了便于训练，对选择出的128个ROIs的对应的ground truth  bbox的坐标进行标准化处理，即减去均值除以标准差。对于分类问题,直接利用交叉熵损失。而对于位置的回归损失，一样采用Smooth_L1Loss，只不过只对正样本计算损失。而且是只对正样本中的这个类别4个参数计算损失。举例来说:</p>
<ul>
<li>一个RoI在经过FC 84后会输出一个84维的loc 向量。如果这个RoI是负样本,则这84维向量不参与计算 L1_Loss。</li>
<li>如果这个RoI是正样本,属于label K,那么它的第 K×4，K×4+1，K×4+2， K×4+3 这4个数参与计算损失，其余的不参与计算损失。</li>
</ul>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的时候保留大约300个ROIs，对每一个计算概率，并利用位置参数调整候选框的位置。最后用NMS筛一遍，就得到结果了。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOV1损失函数代码详解(detection_layer.c)</title>
    <url>/2020/02/26/YOLOV1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(detection_layer.c)/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>灵魂拷问，你真的懂YOLOV1的损失函数吗？进一步，懂了损失函数，你清楚它的反向求导过程吗？为了解决这俩问题，本文就结合DarkNet中的YOLOV1的损失函数代码实现(在<code>src/detection_layer.c</code>中)来帮助你理解。</p>
<h2 id="损失函数公式"><a href="#损失函数公式" class="headerlink" title="损失函数公式"></a>损失函数公式</h2><p>YOLOV1的损失函数就是这样，不做过多解释了。需要注意的一个点是，在反向传播求导的时候，各个变量的梯度其实应该都有一个<strong>系数2</strong>的，但是代码中全部都省掉了，这对整个优化过程其实是没有影响的。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/660.png" alt></p>
<h2 id="代码详细解析"><a href="#代码详细解析" class="headerlink" title="代码详细解析"></a>代码详细解析</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 构建detection层，yolov1中最后一层</span><br><span class="line"> * @param batch 一个batch包含图片的张数</span><br><span class="line"> * @param inputs detection层一张输入图片元素个数</span><br><span class="line"> * @param n yolov1一个grid cell预测bbox的数量 2</span><br><span class="line"> * @param side &#x2F;&#x2F; grid cell的大小 7</span><br><span class="line"> * @param classes yolov1 预测类的个数</span><br><span class="line"> * @param coords 一个bbox包含的坐标数量 4</span><br><span class="line"> * @param rescore</span><br><span class="line"> * @return</span><br><span class="line"> *&#x2F;</span><br><span class="line">detection_layer make_detection_layer(int batch, int inputs, int n, int side, int classes, int coords, int rescore)</span><br><span class="line">&#123;</span><br><span class="line">    detection_layer l &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">    l.type &#x3D; DETECTION;</span><br><span class="line">	&#x2F;&#x2F; 这些变量都可以参考darknet.h中的注释</span><br><span class="line">    l.n &#x3D; n; &#x2F;&#x2F;一个cell中预测多少个box</span><br><span class="line">    l.batch &#x3D; batch; &#x2F;&#x2F;一个batch中包含图片的张数</span><br><span class="line">    l.inputs &#x3D; inputs; &#x2F;&#x2F;detection层一张输入图片的元素个数</span><br><span class="line">    l.classes &#x3D; classes; &#x2F;&#x2F;类别数</span><br><span class="line">    l.coords &#x3D; coords; &#x2F;&#x2F;一个bbox包含的坐标数量</span><br><span class="line">    l.rescore &#x3D; rescore;</span><br><span class="line">    l.side &#x3D; side; &#x2F;&#x2F;grid cell的大小 7</span><br><span class="line">    l.w &#x3D; side; &#x2F;&#x2F;grid cell的宽度</span><br><span class="line">    l.h &#x3D; side; &#x2F;&#x2F;grid cell的高度</span><br><span class="line">    assert(side*side*((1 + l.coords)*l.n + l.classes) &#x3D;&#x3D; inputs); &#x2F;&#x2F;7*7*(1 + 4) * 2 + 30 ) &#x3D; 7*7*30</span><br><span class="line">    l.cost &#x3D; (float*)xcalloc(1, sizeof(float)); &#x2F;&#x2F;detection层的总损失</span><br><span class="line">    l.outputs &#x3D; l.inputs; &#x2F;&#x2F;detection层对应输入图片的输出元素个数，detection层不改变输入输出大小</span><br><span class="line">    l.truths &#x3D; l.side*l.side*(1+l.coords+l.classes); &#x2F;&#x2F;GT:7*7*(1+4+20) 只有一个bbox和置信度</span><br><span class="line">    l.output &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float)); &#x2F;&#x2F; detection层所有输出（包含整个batch的）</span><br><span class="line">    l.delta &#x3D; (float*)xcalloc(batch * l.outputs, sizeof(float)); &#x2F;&#x2F;detection层误差项（包含整个batch的）</span><br><span class="line"></span><br><span class="line">    l.forward &#x3D; forward_detection_layer; &#x2F;&#x2F;前向传播</span><br><span class="line">    l.backward &#x3D; backward_detection_layer; &#x2F;&#x2F;反向传播</span><br><span class="line">#ifdef GPU</span><br><span class="line">    l.forward_gpu &#x3D; forward_detection_layer_gpu;</span><br><span class="line">    l.backward_gpu &#x3D; backward_detection_layer_gpu;</span><br><span class="line">    l.output_gpu &#x3D; cuda_make_array(l.output, batch*l.outputs);</span><br><span class="line">    l.delta_gpu &#x3D; cuda_make_array(l.delta, batch*l.outputs);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    fprintf(stderr, &quot;Detection Layer\n&quot;);</span><br><span class="line">    srand(time(0));</span><br><span class="line"></span><br><span class="line">    return l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * detection层前向传播函数</span><br><span class="line"> * @param l 当前detection层</span><br><span class="line"> * @param net 整个网络</span><br><span class="line"> *&#x2F;</span><br><span class="line">void forward_detection_layer(const detection_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int locations &#x3D; l.side*l.side; &#x2F;&#x2F;grid cell的数量7*7&#x3D;49</span><br><span class="line">    int i,j;</span><br><span class="line">    memcpy(l.output, state.input, l.outputs*l.batch*sizeof(float));</span><br><span class="line">    &#x2F;&#x2F;if(l.reorg) reorg(l.output, l.w*l.h, size*l.n, l.batch, 1);</span><br><span class="line">    int b;</span><br><span class="line">    if (l.softmax)&#123; &#x2F;&#x2F;yolo v1这里为0，并没有使用</span><br><span class="line">        for(b &#x3D; 0; b &lt; l.batch; ++b)&#123;</span><br><span class="line">            int index &#x3D; b*l.inputs;</span><br><span class="line">            for (i &#x3D; 0; i &lt; locations; ++i) &#123;</span><br><span class="line">                int offset &#x3D; i*l.classes;</span><br><span class="line">                softmax(l.output + index + offset, l.classes, 1,</span><br><span class="line">                        l.output + index + offset, 1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if(state.train)&#123;</span><br><span class="line">        float avg_iou &#x3D; 0;</span><br><span class="line">        float avg_cat &#x3D; 0;</span><br><span class="line">        float avg_allcat &#x3D; 0;</span><br><span class="line">        float avg_obj &#x3D; 0;</span><br><span class="line">        float avg_anyobj &#x3D; 0;</span><br><span class="line">        int count &#x3D; 0;</span><br><span class="line">        *(l.cost) &#x3D; 0; &#x2F;&#x2F;detection层的总损失</span><br><span class="line">        int size &#x3D; l.inputs * l.batch; &#x2F;&#x2F;误差项的个数</span><br><span class="line">        memset(l.delta, 0, size * sizeof(float)); &#x2F;&#x2F;误差项初始化</span><br><span class="line">        for (b &#x3D; 0; b &lt; l.batch; ++b)&#123;</span><br><span class="line">            int index &#x3D; b*l.inputs; &#x2F;&#x2F;第b个batch的起始位置</span><br><span class="line">            for (i &#x3D; 0; i &lt; locations; ++i) &#123; &#x2F;&#x2F;第i个grid cell，一共有7*7个</span><br><span class="line">                int truth_index &#x3D; (b*locations + i)*(1+l.coords+l.classes); &#x2F;&#x2F;获取第i个grid cell的bbox的GT</span><br><span class="line">                int is_obj &#x3D; state.truth[truth_index]; &#x2F;&#x2F;获取第i个grid cell是否包含物体</span><br><span class="line">                for (j &#x3D; 0; j &lt; l.n; ++j) &#123; &#x2F;&#x2F; 获取yolov1 第i个grid cell预测的两个bbox，与GT比较</span><br><span class="line">                    int p_index &#x3D; index + locations*l.classes + i*l.n + j; &#x2F;&#x2F; 获取第j个预测的bbox起始位置</span><br><span class="line">                    l.delta[p_index] &#x3D; l.noobject_scale*(0 - l.output[p_index]); &#x2F;&#x2F; bbox中不含object的置信度误差项， noobject_scale&#x3D;0.5 Loss 1-4(1-4指的是公式)</span><br><span class="line">                    *(l.cost) +&#x3D; l.noobject_scale*pow(l.output[p_index], 2); &#x2F;&#x2F;第i个grid cell中第j个预测bbox中，不含object的置信度损失计算，Loss 1-4</span><br><span class="line">                    avg_anyobj +&#x3D; l.output[p_index]; &#x2F;&#x2F;bbox中不含object的置信度求和</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                int best_index &#x3D; -1;</span><br><span class="line">                float best_iou &#x3D; 0;</span><br><span class="line">                float best_rmse &#x3D; 20; &#x2F;&#x2F;best bbox的rmse阈值</span><br><span class="line"> </span><br><span class="line">                if (!is_obj)&#123; &#x2F;&#x2F; 当前第i个grid cell, 第j个bbox不含object, 则loss计算完成</span><br><span class="line">                    continue;</span><br><span class="line">                &#125;</span><br><span class="line">				&#x2F;&#x2F; 当前第i个grid cell, 第j个bbox含有object，继续计算坐标预测损失Loss 1-1,1-2，confidence预测损失Loss 1-3，类别预测损失Loss 1-5</span><br><span class="line">                int class_index &#x3D; index + i*l.classes;&#x2F;&#x2F; 获取第i个grid cell的classes起始位置</span><br><span class="line">                for(j &#x3D; 0; j &lt; l.classes; ++j) &#123;</span><br><span class="line">					&#x2F;&#x2F;第i个grid cell预测分类误差项</span><br><span class="line">                    l.delta[class_index+j] &#x3D; l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]); &#x2F;&#x2F; 第i个grid cell预测分类误差项</span><br><span class="line">                    *(l.cost) +&#x3D; l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2); &#x2F;&#x2F; 类别预测损失计算， Loss 1-5</span><br><span class="line">                    if(state.truth[truth_index + 1 + j]) avg_cat +&#x3D; l.output[class_index+j]; &#x2F;&#x2F; GT对应的grid cell预测分类值求和</span><br><span class="line">                    avg_allcat +&#x3D; l.output[class_index+j]; &#x2F;&#x2F; 所有grid cell预测分类值求和</span><br><span class="line">                &#125;</span><br><span class="line">				&#x2F;&#x2F; 获取第i个grid cell, GT BBOX的[x, y, w, h], float_to_box 第一个参数是bbox起始位置</span><br><span class="line">                box truth &#x3D; float_to_box(state.truth + truth_index + 1 + l.classes);</span><br><span class="line">                truth.x &#x2F;&#x3D; l.side;</span><br><span class="line">                truth.y &#x2F;&#x3D; l.side;</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F;坐标预测损失计算 Loss 1-1, 1-2</span><br><span class="line">				&#x2F;&#x2F; 找到第i个grid cell的best bbox</span><br><span class="line">                for(j &#x3D; 0; j &lt; l.n; ++j)&#123;</span><br><span class="line">					&#x2F;&#x2F;第i个grid cell预测第j个bbox的起始位置</span><br><span class="line">                    int box_index &#x3D; index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;</span><br><span class="line">                    box out &#x3D; float_to_box(l.output + box_index); &#x2F;&#x2F; 获取预测bbox的[x,y,w,h]</span><br><span class="line">					&#x2F;&#x2F;yolo v1 直接回归的是 7*x, 所以与GT bbox 计算IOU, 需要先除以7</span><br><span class="line">                    out.x &#x2F;&#x3D; l.side;</span><br><span class="line">                    out.y &#x2F;&#x3D; l.side;</span><br><span class="line"></span><br><span class="line">                    if (l.sqrt)&#123;</span><br><span class="line">						&#x2F;&#x2F;yolo v1直接回归的sqrt(w), 所以与GT bbox 计算IOU前，需要pow一下</span><br><span class="line">                        out.w &#x3D; out.w*out.w;</span><br><span class="line">                        out.h &#x3D; out.h*out.h;</span><br><span class="line">                    &#125;</span><br><span class="line">					&#x2F;&#x2F;计算预测bbox与 GT bbox之间的IOU</span><br><span class="line">                    float iou  &#x3D; box_iou(out, truth);</span><br><span class="line">                    &#x2F;&#x2F;iou &#x3D; 0;</span><br><span class="line">					&#x2F;&#x2F;计算预测bbox的[x,y]与GT bbox的[x,y]之间的均方差损失 Loss 1-1</span><br><span class="line">                    float rmse &#x3D; box_rmse(out, truth);</span><br><span class="line">					&#x2F;&#x2F; 找到第i个grid cell预测最大的那个bbox</span><br><span class="line">                    if(best_iou &gt; 0 || iou &gt; 0)&#123;</span><br><span class="line">                        if(iou &gt; best_iou)&#123;</span><br><span class="line">                            best_iou &#x3D; iou;</span><br><span class="line">                            best_index &#x3D; j;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;else&#123; &#x2F;&#x2F; 均方差最小的</span><br><span class="line">                        if(rmse &lt; best_rmse)&#123;</span><br><span class="line">                            best_rmse &#x3D; rmse;</span><br><span class="line">                            best_index &#x3D; j;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">				&#x2F;&#x2F; 强制指定一个bbox</span><br><span class="line">                if(l.forced)&#123;</span><br><span class="line">					&#x2F;&#x2F; GT bbox w*h &lt; 0.1,强制最好的bbox index是1</span><br><span class="line">                    if(truth.w*truth.h &lt; .1)&#123;</span><br><span class="line">                        best_index &#x3D; 1;</span><br><span class="line">                    &#125;else&#123;</span><br><span class="line">                        best_index &#x3D; 0;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">				&#x2F;&#x2F;随机选择最佳bbox</span><br><span class="line">                if(l.random &amp;&amp; *(state.net.seen) &lt; 64000)&#123;</span><br><span class="line">                    best_index &#x3D; rand()%l.n;</span><br><span class="line">                &#125;</span><br><span class="line">				</span><br><span class="line">				&#x2F;&#x2F; 模型预测的bbox起始位置</span><br><span class="line">                int box_index &#x3D; index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;</span><br><span class="line">                int tbox_index &#x3D; truth_index + 1 + l.classes;</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F; 获取最佳bbox的[x, y, w, h]</span><br><span class="line">                box out &#x3D; float_to_box(l.output + box_index);</span><br><span class="line">                out.x &#x2F;&#x3D; l.side; &#x2F;&#x2F; 归一化x</span><br><span class="line">                out.y &#x2F;&#x3D; l.side;</span><br><span class="line">                if (l.sqrt) &#123; &#x2F;&#x2F; yolo v1直接回归的sqrt(w), 所以与GT bbox 计算IOU前，需要pow一下</span><br><span class="line">                    out.w &#x3D; out.w*out.w;</span><br><span class="line">                    out.h &#x3D; out.h*out.h;</span><br><span class="line">                &#125;</span><br><span class="line">                float iou  &#x3D; box_iou(out, truth); &#x2F;&#x2F; 计算二者IOU</span><br><span class="line"></span><br><span class="line">                &#x2F;&#x2F;printf(&quot;%d,&quot;, best_index);</span><br><span class="line">				&#x2F;&#x2F; 获取第i个grid cell，best bbox的起始位置</span><br><span class="line">                int p_index &#x3D; index + locations*l.classes + i*l.n + best_index;</span><br><span class="line">				&#x2F;&#x2F; 减去之前计算不含object的confidence预测损失</span><br><span class="line">                *(l.cost) -&#x3D; l.noobject_scale * pow(l.output[p_index], 2);</span><br><span class="line">				&#x2F;&#x2F; 含有object的confidence的预测损失</span><br><span class="line">                *(l.cost) +&#x3D; l.object_scale * pow(1-l.output[p_index], 2);</span><br><span class="line">				&#x2F;&#x2F; bbox中含object的置信度求和</span><br><span class="line">                avg_obj +&#x3D; l.output[p_index];</span><br><span class="line">				&#x2F;&#x2F; 第i个含有object的那个best bbox,grid cell预测分类误差项</span><br><span class="line">                l.delta[p_index] &#x3D; l.object_scale * (1.-l.output[p_index]);</span><br><span class="line">				&#x2F;&#x2F;yolo v1这里为0，并没有使用</span><br><span class="line">                if(l.rescore)&#123;</span><br><span class="line">                    l.delta[p_index] &#x3D; l.object_scale * (iou - l.output[p_index]);</span><br><span class="line">                &#125;</span><br><span class="line">				&#x2F;&#x2F; 第i个grid cell的x对应误差项计算</span><br><span class="line">                l.delta[box_index+0] &#x3D; l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]);</span><br><span class="line">				&#x2F;&#x2F; 第i个grid cell的y对应误差项计算</span><br><span class="line">                l.delta[box_index+1] &#x3D; l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]);</span><br><span class="line">				&#x2F;&#x2F; 第i个grid cell的w对应误差项计算</span><br><span class="line">                l.delta[box_index+2] &#x3D; l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]);</span><br><span class="line">				&#x2F;&#x2F; 第i个grid cell的h对应误差项计算</span><br><span class="line">                l.delta[box_index+3] &#x3D; l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]);</span><br><span class="line">                if(l.sqrt)&#123;</span><br><span class="line">					&#x2F;&#x2F; Loss 1-2计算, GT bbox需要开根号</span><br><span class="line">                    l.delta[box_index+2] &#x3D; l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]);</span><br><span class="line">                    l.delta[box_index+3] &#x3D; l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                *(l.cost) +&#x3D; pow(1-iou, 2);</span><br><span class="line">                avg_iou +&#x3D; iou; &#x2F;&#x2F; 包含object的grid cell，best bbox 与 GT bbox的IOU求和</span><br><span class="line">                ++count;  &#x2F;&#x2F; 训练阶段，截止到本batch的训练完，包含object总数量</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; &#x2F;&#x2F; 一个batch中所有图片处理完</span><br><span class="line"></span><br><span class="line">        if(0)&#123;</span><br><span class="line">            float* costs &#x3D; (float*)xcalloc(l.batch * locations * l.n, sizeof(float));</span><br><span class="line">            for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;</span><br><span class="line">                int index &#x3D; b*l.inputs;</span><br><span class="line">                for (i &#x3D; 0; i &lt; locations; ++i) &#123;</span><br><span class="line">                    for (j &#x3D; 0; j &lt; l.n; ++j) &#123;</span><br><span class="line">                        int p_index &#x3D; index + locations*l.classes + i*l.n + j;</span><br><span class="line">                        costs[b*locations*l.n + i*l.n + j] &#x3D; l.delta[p_index]*l.delta[p_index];</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            int indexes[100];</span><br><span class="line">            top_k(costs, l.batch*locations*l.n, 100, indexes);</span><br><span class="line">            float cutoff &#x3D; costs[indexes[99]];</span><br><span class="line">            for (b &#x3D; 0; b &lt; l.batch; ++b) &#123;</span><br><span class="line">                int index &#x3D; b*l.inputs;</span><br><span class="line">                for (i &#x3D; 0; i &lt; locations; ++i) &#123;</span><br><span class="line">                    for (j &#x3D; 0; j &lt; l.n; ++j) &#123;</span><br><span class="line">                        int p_index &#x3D; index + locations*l.classes + i*l.n + j;</span><br><span class="line">                        if (l.delta[p_index]*l.delta[p_index] &lt; cutoff) l.delta[p_index] &#x3D; 0;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            free(costs);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F;一个batch的总损失计算</span><br><span class="line">        *(l.cost) &#x3D; pow(mag_array(l.delta, l.outputs * l.batch), 2); &#x2F;&#x2F; 一个batch的总损失计算</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        printf(&quot;Detection Avg IOU: %f, Pos Cat: %f, All Cat: %f, Pos Obj: %f, Any Obj: %f, count: %d\n&quot;, avg_iou&#x2F;count, avg_cat&#x2F;count, avg_allcat&#x2F;(count*l.classes), avg_obj&#x2F;count, avg_anyobj&#x2F;(l.batch*locations*l.n), count);</span><br><span class="line">        &#x2F;&#x2F;if(l.reorg) reorg(l.delta, l.w*l.h, size*l.n, l.batch, 0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * detection层反向传播函数</span><br><span class="line"> * @param l 当前detection层</span><br><span class="line"> * @param net 整个网络</span><br><span class="line"> *&#x2F;</span><br><span class="line">void backward_detection_layer(const detection_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    axpy_cpu(l.batch*l.inputs, 1, l.delta, 1, state.delta, 1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * yolo v1 Infence 阶段，解析7*7*30</span><br><span class="line"> * @param l 当前detection层</span><br><span class="line"> * @param w 输入图片的宽度</span><br><span class="line"> * @param h 输入图片的高度</span><br><span class="line"> * @param thresh confidence阈值</span><br><span class="line"> * @param dets 用于保存结果</span><br><span class="line"> *&#x2F;</span><br><span class="line">void get_detection_detections(layer l, int w, int h, float thresh, detection *dets)</span><br><span class="line">&#123;</span><br><span class="line">	int i, j, n;</span><br><span class="line">	float *predictions &#x3D; l.output;</span><br><span class="line">	&#x2F;&#x2F;int per_cell &#x3D; 5*num+classes;</span><br><span class="line">	for (i &#x3D; 0; i &lt; l.side*l.side; ++i) &#123;</span><br><span class="line">		int row &#x3D; i &#x2F; l.side; &#x2F;&#x2F;获取grid cell的行号</span><br><span class="line">		int col &#x3D; i % l.side; &#x2F;&#x2F;获取grid cell的列号</span><br><span class="line">		for (n &#x3D; 0; n &lt; l.n; ++n) &#123; &#x2F;&#x2F;遍历两个box</span><br><span class="line">			int index &#x3D; i*l.n + n;</span><br><span class="line">			int p_index &#x3D; l.side*l.side*l.classes + i*l.n + n;</span><br><span class="line">			float scale &#x3D; predictions[p_index];</span><br><span class="line">			int box_index &#x3D; l.side*l.side*(l.classes + l.n) + (i*l.n + n) * 4;</span><br><span class="line">			box b;</span><br><span class="line">			b.x &#x3D; (predictions[box_index + 0] + col) &#x2F; l.side * w; &#x2F;&#x2F; 坐标转换为真实值</span><br><span class="line">			b.y &#x3D; (predictions[box_index + 1] + row) &#x2F; l.side * h;</span><br><span class="line">			b.w &#x3D; pow(predictions[box_index + 2], (l.sqrt ? 2 : 1)) * w;</span><br><span class="line">			b.h &#x3D; pow(predictions[box_index + 3], (l.sqrt ? 2 : 1)) * h;</span><br><span class="line">			dets[index].bbox &#x3D; b;</span><br><span class="line">			dets[index].objectness &#x3D; scale; &#x2F;&#x2F; 保存框置信度得分</span><br><span class="line">			for (j &#x3D; 0; j &lt; l.classes; ++j) &#123;</span><br><span class="line">				int class_index &#x3D; i*l.classes;</span><br><span class="line">				float prob &#x3D; scale*predictions[class_index + j]; &#x2F;&#x2F; 类别置信度得分&#x3D;条件类别概率×框置信度得分</span><br><span class="line">				dets[index].prob[j] &#x3D; (prob &gt; thresh) ? prob : 0; &#x2F;&#x2F; 低于阈值一律置为0</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows下VSCode使用SSH连接报Bad owner or permissions on C:\\Users\\Administrator/.ssh/config错误问题解决</title>
    <url>/2020/02/25/Windows%E4%B8%8BVSCode%E4%BD%BF%E7%94%A8SSH%E8%BF%9E%E6%8E%A5%E6%8A%A5Bad-owner-or-permissions-on-CUsersAdministrator.sshconfig%E9%94%99%E8%AF%AF%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title=" 问题描述"></a><a id="more"></a> 问题描述</h2><p>在 Windows 系统下的 VSCode 安装 <strong>Remote - SSH</strong> 扩展后，使用扩展配置 SSH 并进行远程连接，可能会发生 <strong>Bad owner or permissions on C:\Users\Administrator/.ssh/config</strong> 错误，造成无法进行 SSH 远程连接的问题。</p>
<p>原因是由于使用 <strong>Remote - SSH</strong> 扩展所依赖的 <strong>Remote - SSH: Editing Configuration Files</strong> 扩展编辑了 <strong>C:\Users\Administrator.ssh\config</strong> 文件后，此文件的权限发生了改变：<br> <img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Bad%20owner%20or%20permissions%20on/image-2fd80730.png.jfif" alt></p>
<p>如上图所示，编辑了 <strong>%USER_HOME%.ssh\config</strong> 文件后，不但在 VSCode 中由于配置文件权限问题而无法进行 SSH 远程连接，就连使用系统的 <strong>PowerShell</strong> 进行 SSH 连接时也会报此错误，而把此配置文件删除后，使用  <strong>PowerShell</strong> 即可正常进行远程连接。但 VSCode 的 SSH 连接又依赖此配置文件，所以就产生了冲突，要么只有 <strong>PowerShell</strong> 能用，要么就都不能用。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><ol>
<li><p>在 GitHub 上下载 <a href="https://github.com/PowerShell/openssh-portable" target="_blank" rel="noopener"><strong>openssh-portable</strong></a> 项目，其 Git 命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/PowerShell/openssh-portable.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载完成后进入 <strong>openssh-portable</strong> 项目中的 <a href="https://github.com/PowerShell/openssh-portable/tree/latestw_all/contrib/win32/openssh" target="_blank" rel="noopener"><code>contrib\win32\openssh</code></a> 目录，在此目录中打开 PowerShell 命令行，执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">.\FixUserFilePermissions.ps1 -Confirm:$false</span><br></pre></td></tr></table></figure>
<ul>
<li><p>执行此命令时若提示错误 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">无法加载文件 FixUserFil ePermissions.ps1，因为在此系统上禁止运行脚本</span><br></pre></td></tr></table></figure>
<p>则先执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Set-ExecutionPolicy RemoteSigned</span><br></pre></td></tr></table></figure>
<p>然后输入 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Y</span><br></pre></td></tr></table></figure>
<p> 回车确认后再重新执行，执行完毕后可以再执行<code>Set-ExecutionPolicy RemoteSigned</code>输入 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">N</span><br></pre></td></tr></table></figure>
<p> 恢复默认配置</p>
</li>
</ul>
</li>
<li><p>操作完成后，在 VSCode 中编辑 <strong>C:\Users\Administrator.ssh\config</strong> 文件将不会影响此文件的权限，在 VSCode 和 PowerShell 中均可正常进行 SSH 远程连接：</p>
<ul>
<li><p>在 VSCode 中通过 Remote - SSH 进行 SSH 远程连接：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Bad%20owner%20or%20permissions%20on/image-c923ced8.png" alt></p>
</li>
<li><p>在 PowerShell 中进行 SSH 远程连接：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Bad%20owner%20or%20permissions%20on/93310FA6D4F64FA7BF7BEBE7357A79E6-cf86116a.jpeg" alt></p>
</li>
</ul>
</li>
</ol>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><p>在其他位置建好config文件，并在插件设置中添加路径即可</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Bad%20owner%20or%20permissions%20on/20200212142410779.png" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Fast RCNN</title>
    <url>/2020/02/24/Fast-RCNN/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>我们知道RCNN需要把每一个可能有目标的候选框搜索出来，然后把每个候选框传入CNN提取特征，每一张图片要产生大约2K个候选框，而每个框对应的图像都要传入CNN，这个时间开销肯定是很难承受的。基于RCNN这个致命问题，Fast-RCNN出现了。</p>
<h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><p>Fast-RCNN是在SPPNet和RCNN的基础上进行改进的。SPPNet的主要贡献是在整张图像上计算全局特征图，然后对于特定的proposal，只需要在全局特征图上取出对应坐标的特征图就可以了。但SPPNet仍然需要将特征保存在磁盘中，速度还是很慢。结合RCNN的思想，论文提出直接将候选框区域应用于特征图，并使用ROI Pooling将其转化为固定大小的特征图，最后再连接两个并行的分类头和回归头完成检测任务。整个算法可以用下面的图来表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-7569280b566d0e58.png" alt></p>
<h2 id="贡献-amp-创新点"><a href="#贡献-amp-创新点" class="headerlink" title="贡献&amp;创新点"></a>贡献&amp;创新点</h2><ul>
<li>Fast-RCNN 只对整个图像进行一次特征提取，避免R-CNN的上千次特征提取。</li>
<li>使用ROI Pooling层替换最后一层的Max Pooling层，巧妙避免RCNN中的将每个候选框Resize到固定大小的操作。</li>
<li>Fast RCNN在网络的尾部采用并行的全连接层，可同时输出分类结果和窗口回归结果，实现了端到端的多任务训练，且不需要额外的特征存储空间(在R-CNN中特征需要保存到磁盘，以供SVM和线性回归器训练)。</li>
<li>使用SVD矩阵分解算法对网络末端并行的全连接层进行分解，加速运算。</li>
</ul>
<h2 id="ROI-Pooling层"><a href="#ROI-Pooling层" class="headerlink" title="ROI Pooling层"></a>ROI Pooling层</h2><p>Fast-RCNN的核心是ROI池化层，它的作用是输入特征图的大小不定，但输出大小固定的输出特征图。而什么是ROI呢？ROI就是经过区域建议算法(Selective Search)生成的框经过卷积神经网络网络提取特征后的特征图上的区域，每一个ROI对应了原图的一个区域建议框，只有大小变化了，相对位置没有发生改变。这个过程可以用下图表示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Fast%20RCNN/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200224140858.jpg" alt></p>
<p>ROI Pooling层的输入有特征图和ROIs，特征图是经过CNN提取后的结果，ROIs表示Selective Search的结果，形状为$N \times 5 \times 1 \times 1$，其中$N$代表ROI的个数，5代表$x, y, w, h$。这里需要注意的是，坐标系的参数是针对原图的。</p>
<h2 id="ROI-Pooling的具体操作"><a href="#ROI-Pooling的具体操作" class="headerlink" title="ROI Pooling的具体操作"></a>ROI Pooling的具体操作</h2><ul>
<li>根据输入图片，将ROI映射到特征图对应位置（映射规则就是直接把各个坐标除以“输入图片和特征图大小的比值”）</li>
<li>将映射后的区域划分为相同大小的sections，其中sections代表输出维度，例如7。</li>
<li>对每个sections进行最大池化操作。</li>
</ul>
<p>最后上传一张经典动态图片，更好的表示这个过程：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Fast%20RCNN/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200224140920.gif" alt></p>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><p>Fast-RCNN的作者rgbirshick依然给出了源码，有兴趣可以读一下：<a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/fast-rcnn</a></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>Fast RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>RCNN</title>
    <url>/2020/02/24/RCNN/</url>
    <content><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title=" 背景介绍"></a><a id="more"></a> 背景介绍</h2><h3 id="什么是目标检测"><a href="#什么是目标检测" class="headerlink" title="什么是目标检测"></a>什么是目标检测</h3><p>所谓目标检测就是在一张图像中找到我们关注的目标，并确定它的类别和位置，这是计算机视觉领域最核心的问题之一。由于各类目标不同的外观，颜色，大小以及在成像时光照，遮挡等具有挑战性的问题，目标检测一直处于不断的优化和研究中。</p>
<h3 id="目标检测算法分类"><a href="#目标检测算法分类" class="headerlink" title="目标检测算法分类"></a>目标检测算法分类</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180509095302426.png" alt></p>
<p>这张甘特图已经说明了目标检测算法主要分为两类，即：</p>
<ul>
<li>Two Stage目标检测算法。这类算法都是先进行区域候选框生成，就是找到一个可能包含物体的预选框，再通过卷积神经网络进行分类和回归修正，常见算法有R-CNN，SPP-Net，Fast-RCNN，Faster-RCNN和R-FCN等。</li>
<li>One Stage目标检测算法。这类算法不使用候选框生成，直接在网络中提取特征来预测物体的分类和位置。常见的One-Stage算法有：YOLO系列，SSD，RetinaNet。</li>
</ul>
<h2 id="RCNN算法"><a href="#RCNN算法" class="headerlink" title="RCNN算法"></a>RCNN算法</h2><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><p>RCNN是第一个使用卷积神经网络来对目标候选框提取特征的目标检测算法。同时，RCNN使用了微调(finetune)的技术，使用大数据集上训练好的分类模型的前几层做backbone，进行更有效的特征提取。</p>
<h3 id="RCNN总览"><a href="#RCNN总览" class="headerlink" title="RCNN总览"></a>RCNN总览</h3><p>看下图：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_09-57-14.jpg" alt></p>
<p>首先，R-CNN是将传统图像算法和深度学习技术结合起来的结构，第一部分是需要候选框区域建议，这里一般使用Selective Search的方法提取出候选框，然后再传入CNN做特征提取及分类，后面还借助了机器学习算法做回归修正。</p>
<h3 id="RCNN算法步骤"><a href="#RCNN算法步骤" class="headerlink" title="RCNN算法步骤"></a>RCNN算法步骤</h3><ul>
<li>选择一个预训练 （pre-trained）神经网络（如AlexNet、VGG）。</li>
<li>重新训练全连接层。使用需要检测的目标重新训练（re-train）最后全连接层（connected layer）。即是fintune技术的应用。</li>
<li><p>生成候选框。利用Selective Search算法提取所有的Proposals，一张图片大概产生2000张，然后将图片规整化固定大小，使得其满足CNN的输入要求，最后将feature map存到磁盘(是的，你没有看错，RCNN要把提取到的特征存储到磁盘)，这个过程可以用下图表示：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/RCNN/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200224135208.jpg" alt></p>
</li>
<li><p>利用feature map训练SVM来对目标和背景进行分类，这里每一个类一个二元SVM。</p>
</li>
<li><p>训练线性回归器修正目标的位置，如下图所示：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/RCNN/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200224135251.jpg" alt></p>
</li>
</ul>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/RCNN/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200224135256.jpg" alt></p>
<p>RCNN成为了当时目标检测领域的SOAT算法，虽然现在很少有人使用到了，但论文的思想我们仍可以借鉴。任何事情都要经历一个从无到有的过程。</p>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><p>rgbirshick大神，也就是RCNN作者，提供了源码，链接如下：<a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/rcnn</a></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet卷积层的反向传播解析</title>
    <url>/2020/02/24/AlexeyAB-DarkNet%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>前面已经详细讲解了卷积层的前向传播过程，大致思路就是使用im2col方法对数据进行重排，然后利用sgemm算法计算出结果，反向传播实际上就是前向传播的逆过程，我们一起来分析一下源码吧。</p>
<h2 id="反向传播解析"><a href="#反向传播解析" class="headerlink" title="反向传播解析"></a>反向传播解析</h2><ul>
<li>首先调用<code>gradient_array()</code>计算当前层<code>l</code>所有输出元素关于加权输入的导数值（也即激活函数关于输入的导数值），并乘上上一次调用<code>backward_convolutional_layer()</code>还没计算完的<code>l.delta</code>，得到当前层最终的敏感度图。这部分的代码如下：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 计算激活函数对加权输入的导数，并乘以delta，得到当前层最终的delta（敏感度图）</span><br><span class="line">** 输入：x    当前层的所有输出（维度为l.batch * l.out_c * l.out_w * l.out_h）</span><br><span class="line">**      n    l.output的维度，即为l.batch * l.out_c * l.out_w * l.out_h（包含整个batch的）</span><br><span class="line">**      ACTIVATION    激活函数类型</span><br><span class="line">**      delta     当前层敏感度图（与当前成输出x维度一样）</span><br><span class="line">** 说明1：该函数不但计算了激活函数对于加权输入的导数，还将该导数乘以了之前完成大部分计算的敏感度图delta（对应元素相乘），因此调用改函数之后，将得到该层最终的敏感度图</span><br><span class="line">** 说明2：这里直接利用输出值求激活函数关于输入的导数值是因为神经网络中所使用的绝大部分激活函数，其关于输入的导数值都可以描述为输出值的函数表达式，</span><br><span class="line">          比如对于Sigmoid激活函数（记作f(x)），其导数值为f(x)&#39;&#x3D;f(x)*(1-f(x)),因此如果给出y&#x3D;f(x)，那么f(x)&#39;&#x3D;y*(1-y)，只需要输出值y就可以了，不需要输入x的值，</span><br><span class="line">          （暂时不确定darknet中有没有使用特殊的激活函数，以致于必须要输入值才能够求出导数值，在activiation.c文件中，有几个激活函数暂时没看懂，也没在网上查到）。</span><br><span class="line">** 说明3：关于l.delta的初值，可能你有注意到在看某一类型网络层的时候，比如卷积层中的backward_convolutional_layer()函数，没有发现在此之前对l.delta赋初值的语句，</span><br><span class="line">**        只是用calloc为其动态分配了内存，这样的l.delta其所有元素的值都为0,那么这里使用*&#x3D;运算符得到的值将恒为0。是的，如果只看某一层，或者说某一类型的层，的确有这个疑惑，</span><br><span class="line">**        但是整个网络是有很多层的，且有多种类型，一般来说，不会以卷积层为最后一层，而回以COST或者REGION为最后一层，这些层中，会对l.delta赋初值，又由于l.delta是由后</span><br><span class="line">**        网前逐层传播的，因此，当反向运行到某一层时，l.delta的值将都不会为0.</span><br><span class="line">*&#x2F;</span><br><span class="line">void gradient_array(const float *x, const int n, const ACTIVATION a, float *delta)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">    #pragma omp parallel for</span><br><span class="line">    for(i &#x3D; 0; i &lt; n; ++i)&#123;</span><br><span class="line">        delta[i] *&#x3D; gradient(x[i], a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><code>gradient</code>函数的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 根据不同的激活函数求取对输入的梯度（导数）</span><br><span class="line">** 输入：x    激活函数接收的输入值</span><br><span class="line">**      a    激活函数类型，包括的激活函数类型见activations.h中枚举类型ACTIVATION的定义</span><br><span class="line">** 输出：激活函数关于输入x的导数值</span><br><span class="line">*&#x2F;</span><br><span class="line">float gradient(float x, ACTIVATION a)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F; 以下分别求取各种激活函数对输入的导数值，详见各个导数求取函数的内部注释</span><br><span class="line">    switch(a)&#123;</span><br><span class="line">        case LINEAR:</span><br><span class="line">            return linear_gradient(x);</span><br><span class="line">        case LOGISTIC:</span><br><span class="line">            return logistic_gradient(x);</span><br><span class="line">        case LOGGY:</span><br><span class="line">            return loggy_gradient(x);</span><br><span class="line">        case RELU:</span><br><span class="line">            return relu_gradient(x);</span><br><span class="line">        case NORM_CHAN:</span><br><span class="line">            &#x2F;&#x2F;return relu_gradient(x);</span><br><span class="line">        case NORM_CHAN_SOFTMAX_MAXVAL:</span><br><span class="line">            &#x2F;&#x2F;...</span><br><span class="line">        case NORM_CHAN_SOFTMAX:</span><br><span class="line">            printf(&quot; Error: should be used custom NORM_CHAN or NORM_CHAN_SOFTMAX-function for gradient \n&quot;);</span><br><span class="line">            exit(0);</span><br><span class="line">            return 0;</span><br><span class="line">        case ELU:</span><br><span class="line">            return elu_gradient(x);</span><br><span class="line">        case SELU:</span><br><span class="line">            return selu_gradient(x);</span><br><span class="line">        case RELIE:</span><br><span class="line">            return relie_gradient(x);</span><br><span class="line">        case RAMP:</span><br><span class="line">            return ramp_gradient(x);</span><br><span class="line">        case LEAKY:</span><br><span class="line">            return leaky_gradient(x);</span><br><span class="line">        case TANH:</span><br><span class="line">            return tanh_gradient(x);</span><br><span class="line">        case PLSE:</span><br><span class="line">            return plse_gradient(x);</span><br><span class="line">        case STAIR:</span><br><span class="line">            return stair_gradient(x);</span><br><span class="line">        case HARDTAN:</span><br><span class="line">            return hardtan_gradient(x);</span><br><span class="line">        case LHTAN:</span><br><span class="line">            return lhtan_gradient(x);</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>然后，如果网络进行了BN，则调用backward_batchnorm_layer，否则直接调用  backward_bias()计算当前层所有卷积核的偏置更新值。backward_batchnorm_layer之后会单独讲，这里来看看backward_bias()的实现。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 计算每个卷积核的偏置更新值，所谓偏置更新值，就是bias &#x3D; bias - alpha * bias_update中的bias_update</span><br><span class="line">** 输入：bias_updates     当前层所有偏置的更新值，维度为l.n（即当前层卷积核的个数）</span><br><span class="line">**      delta            当前层的敏感度图（即l.delta）</span><br><span class="line">**      batch            一个batch含有的图片张数（即l.batch）</span><br><span class="line">**      n                当前层卷积核个数（即l.n）</span><br><span class="line">**      k                当前层输入特征图尺寸（即l.out_w*l.out_h）</span><br><span class="line">** 原理：当前层的敏感度图l.delta是误差函数对加权输入的导数，也就是偏置更新值，只是其中每l.out_w*l.out_h个元素都对应同一个</span><br><span class="line">**      偏置，因此需要将其加起来，得到的和就是误差函数对当前层各偏置的导数（l.delta的维度为l.batch*l.n*l.out_h*l.out_w,</span><br><span class="line">**      可理解成共有l.batch行，每行有l.n*l.out_h*l.out_w列，而这一大行又可以理解成有l.n，l.out_h*l.out_w列，这每一小行就</span><br><span class="line">**      对应同一个卷积核也即同一个偏置）</span><br><span class="line">*&#x2F;</span><br><span class="line">void backward_bias(float *bias_updates, float *delta, int batch, int n, int size)</span><br><span class="line">&#123;</span><br><span class="line">    int i,b;</span><br><span class="line">	&#x2F;&#x2F; 遍历batch中每张输入图片</span><br><span class="line">    &#x2F;&#x2F; 注意，最后的偏置更新值是所有输入图片的总和（多张图片无非就是重复一张图片的操作，求和即可）。</span><br><span class="line">    &#x2F;&#x2F; 总之：一个卷积核对应一个偏置更新值，该偏置更新值等于batch中所有输入图片累积的偏置更新值，</span><br><span class="line">    &#x2F;&#x2F; 而每张图片也需要进行偏置更新值求和（因为每个卷积核在每张图片多个位置做了卷积运算，这都对偏置更新值有贡献）以得到每张图片的总偏置更新值。</span><br><span class="line">    for(b &#x3D; 0; b &lt; batch; ++b)&#123;</span><br><span class="line">        for(i &#x3D; 0; i &lt; n; ++i)&#123;</span><br><span class="line">            bias_updates[i] +&#x3D; sum_array(delta+size*(i+b*n), size);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>接下来依次调用im2col_cpu()，gemm_nt()函数计算当前层权重系数更新值；如果上一层的delta已经动态分配了内存，则依次调用gemm_tn(), col2im_cpu()计算上一层的敏感度图（并未完成所有计算，还差一个步骤）。整个反向传播的核心函数解释如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 卷积神经网络反向传播核心函数</span><br><span class="line">** 主要流程：1） 调用gradient_array()计算当前层l所有输出元素关于加权输入的导数值（也即激活函数关于输入的导数值），</span><br><span class="line">**             并乘上上一次调用backward_convolutional_layer()还没计算完的l.delta，得到当前层最终的敏感度图；</span><br><span class="line">**          2） 如果网络进行了BN，则backward_batchnorm_layer。</span><br><span class="line">**          3） 如果网络没有进行BN，则直接调用 backward_bias()计算当前层所有卷积核的偏置更新值；</span><br><span class="line">**          4） 依次调用im2col_cpu()，gemm_nt()函数计算当前层权重系数更新值；</span><br><span class="line">**          5） 如果上一层的delta已经动态分配了内存，则依次调用gemm_tn(), col2im_cpu()计算上一层的敏感度图（并未完成所有计算，还差一个步骤）；</span><br><span class="line">** 强调：每次调用本函数会计算完成当前层的敏感度计算，同时计算当前层的偏置、权重更新值，除此之外，还会计算上一层的敏感度图，但是要注意的是，</span><br><span class="line">**      并没有完全计算完，还差一步：乘上激活函数对加权输入的导数值。这一步在下一次调用本函数时完成。</span><br><span class="line">*&#x2F;</span><br><span class="line">void backward_convolutional_layer(convolutional_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int i, j;</span><br><span class="line">	&#x2F;&#x2F; 卷积核个数，考虑到分组卷积</span><br><span class="line">    int m &#x3D; l.n &#x2F; l.groups;</span><br><span class="line">	&#x2F;&#x2F; 每一个卷积核元素个数（包括l.c（l.c为该层网络接受的输入图片的通道数）个通道上的卷积核元素个数总数，比如卷积核尺寸为3*3,</span><br><span class="line">    &#x2F;&#x2F; 输入图片有3个通道，因为要同时作用于输入的3个通道上，所以实际上这个卷积核是一个立体的，共有3*3*3&#x3D;27个元素，这些元素都是要训练的参数），同样需要考虑分组数</span><br><span class="line">    int n &#x3D; l.size*l.size*l.c &#x2F; l.groups;</span><br><span class="line">	&#x2F;&#x2F; 每张输出特征图的元素个数：out_w，out_h是输出特征图的宽高</span><br><span class="line">    int k &#x3D; l.out_w*l.out_h;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 计算当前层激活函数对加权输入的导数值并乘以l.delta相应元素，从而彻底完成当前层敏感度图的计算，得到当前层的敏感度图l.delta。</span><br><span class="line">    &#x2F;&#x2F; l.output存储了该层网络的所有输出：该层网络接受一个batch的输入图片，其中每张图片经卷积处理后得到的特征图尺寸为：l.out_w,l.out_h，</span><br><span class="line">    &#x2F;&#x2F; 该层卷积网络共有l.n个卷积核，因此一张输入图片共输出l.n张宽高为l.out_w,l.out_h的特征图（l.output为一张图所有输出特征图的总元素个数），</span><br><span class="line">    &#x2F;&#x2F; 所以所有输入图片也即l.output中的总元素个数为：l.n*l.out_w*l.out_h*l.batch；</span><br><span class="line">    &#x2F;&#x2F; l.activation为该卷积层的激活函数类型，l.delta就是gradient_array()函数计算得到的l.output中每一个元素关于激活函数函数输入的导数值，</span><br><span class="line">    &#x2F;&#x2F; 注意，这里直接利用输出值求得激活函数关于输入的导数值是因为神经网络中所使用的绝大部分激活函数关于输入的导数值都可以描述为输出值的函数表达式，</span><br><span class="line">    &#x2F;&#x2F; 比如对于Sigmoid激活函数（记作f(x)），其导数值为f(x)&#39;&#x3D;f(x)*(1-f(x)),因此如果给出y&#x3D;f(x)，那么f(x)&#39;&#x3D;y*(1-y)，只需要输出值y就可以了，不需要输入x的值，</span><br><span class="line">    &#x2F;&#x2F; （暂时不确定darknet中有没有使用特殊的激活函数，以致于必须要输入值才能够求出导数值，在activiation.c文件中，有几个激活函数暂时没看懂，也没在网上查到）。</span><br><span class="line">    &#x2F;&#x2F; l.delta是一个一维数组，长度为l.batch * l.outputs（其中l.outputs &#x3D; l.out_h * l.out_w * l.out_c），在make_convolutional_layer()动态分配内存；</span><br><span class="line">    &#x2F;&#x2F; 再强调一次：gradient_array()不单单是完成激活函数对输入的求导运算，还完成计算当前层敏感度图的最后一步：l.delta中每个元素乘以激活函数对输入的导数（注意gradient_arry中使用的是*&#x3D;运算符）。</span><br><span class="line">    &#x2F;&#x2F; 每次调用backward_convolutional_laye时，都会完成当前层敏感度图的计算，同时会计算上一层的敏感度图，但对于上一层，其敏感度图并没有完全计算完成，还差一步，</span><br><span class="line">    &#x2F;&#x2F; 需要等到下一次调用backward_convolutional_layer()时来完成，诚如col2im_cpu()中注释一样。</span><br><span class="line">    if (l.activation &#x3D;&#x3D; SWISH) gradient_array_swish(l.output, l.outputs*l.batch, l.activation_input, l.delta);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; MISH) gradient_array_mish(l.outputs*l.batch, l.activation_input, l.delta);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX || l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX_MAXVAL) gradient_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.delta);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN) gradient_array_normalize_channels(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.delta);</span><br><span class="line">    else gradient_array(l.output, l.outputs*l.batch, l.activation, l.delta);</span><br><span class="line"></span><br><span class="line">    if (l.batch_normalize) &#123;</span><br><span class="line">		&#x2F;&#x2F; 之后单独讲BN层的前向和反向传播</span><br><span class="line">        backward_batchnorm_layer(l, state);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">		&#x2F;&#x2F; 计算偏置的更新值：每个卷积核都有一个偏置，偏置的更新值也即误差函数对偏置的导数，这个导数的计算很简单，实际所有的导数已经求完了，都存储在l.delta中，</span><br><span class="line">        &#x2F;&#x2F; 接下来只需把l.delta中对应同一个卷积核的项加起来就可以（卷积核在图像上逐行逐列跨步移动做卷积，每个位置处都有一个输出，共有l.out_w*l.out_h个，</span><br><span class="line">        &#x2F;&#x2F; 这些输出都与同一个偏置关联，因此将l.delta中对应同一个卷积核的项加起来即得误差函数对这个偏置的导数）</span><br><span class="line">        backward_bias(l.bias_updates, l.delta, l.batch, l.n, k);</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F; 遍历batch中的每张照片，对于l.delta来说，每张照片是分开存的，因此其维度会达到：l.batch*l.n*l.out_w*l.out_h，</span><br><span class="line">    &#x2F;&#x2F; 对于l.weights,l.weight_updates以及上面提到的l.bias,l.bias_updates，是将所有照片对应元素叠加起来</span><br><span class="line">    &#x2F;&#x2F; （循环的过程就是叠加的过程，注意gemm()这系列函数含有叠加效果，不是覆盖输入C的值，而是叠加到之前的C上），</span><br><span class="line">    &#x2F;&#x2F; 因此l.weights与l.weight_updates维度为l.n*l.size*l.size，l.bias与l.bias_updates的维度为l.h，都与l.batch无关</span><br><span class="line">    for (i &#x3D; 0; i &lt; l.batch; ++i) &#123;</span><br><span class="line">        for (j &#x3D; 0; j &lt; l.groups; ++j) &#123;</span><br><span class="line">			float *a &#x3D; l.delta + (i*l.groups + j)*m*k;</span><br><span class="line">			&#x2F;&#x2F; net.workspace的元素个数为所有层中最大的l.workspace_size（在make_convolutional_layer()计算得到workspace_size的大小，在parse_network_cfg()中动态分配内存，此值对应未使用gpu时的情况）,</span><br><span class="line">			&#x2F;&#x2F; net.workspace充当一个临时工作空间的作用，存储临时所需要的计算参数，比如每层单张图片重排后的结果（这些参数马上就会参与卷积运算），一旦用完，就会被马上更新（因此该变量的值的更新频率比较大）</span><br><span class="line">            float *b &#x3D; state.workspace;</span><br><span class="line">			</span><br><span class="line">            float *c &#x3D; l.weight_updates + j*l.nweights &#x2F; l.groups;</span><br><span class="line">			&#x2F;&#x2F; 进入本函数之前，在backward_network()函数中，已经将net.input赋值为prev.output，也即若当前层为第l层，net.input此时已经是第l-1层的输出</span><br><span class="line">			&#x2F;&#x2F; 注意反向传播从后往前来看</span><br><span class="line">            float *im &#x3D; state.input + (i*l.groups + j)* (l.c &#x2F; l.groups)*l.h*l.w;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;im2col_cpu(im, l.c &#x2F; l.groups, l.h, l.w, l.size, l.stride, l.pad, b);</span><br><span class="line">			&#x2F;&#x2F; 下面两步：im2col_cpu()与gemm()是为了计算当前层的权重更新值（其实也就是误差函数对当前层权重的导数）</span><br><span class="line">			&#x2F;&#x2F; 将多通道二维图像net.input变成按一定存储规则排列的数组b，以方便、高效地进行矩阵（卷积）计算，详细查看该函数注释（比较复杂），</span><br><span class="line">			&#x2F;&#x2F; im2col_cpu_ext每次仅处理net.input（包含整个batch）中的一张输入图片（对于第一层，则就是读入的图片，对于之后的层，这些图片都是上一层的输出，通道数等于上一层卷积核个数）。</span><br><span class="line">			&#x2F;&#x2F; 最终重排的b为l.c * l.size * l.size行，l.out_h * l.out_w列。</span><br><span class="line">			&#x2F;&#x2F; 你会发现在前向forward_convolutional_layer()函数中，也为每层的输入进行了重排，但是很遗憾的是，并没有一个l.workspace把每一层的重排结果保存下来，而是统一存储到net.workspace中，</span><br><span class="line">			&#x2F;&#x2F; 并被不断擦除更新，那为什么不保存呢？保存下来不是省掉一大笔额外重复计算开销？原因有两个：1）net.workspace中只存储了一张输入图片的重排结果，所以重排下张图片时，马上就会被擦除，</span><br><span class="line">			&#x2F;&#x2F; 当然你可能会想，那为什么不弄一个l.worspaces将每层所有输入图片的结果保存呢？这引出第二个原因；2）计算成本是降低了，但存储空间需求急剧增加，想想每一层都有l.batch张图，且每张都是多通道的，</span><br><span class="line">			&#x2F;&#x2F; 重排后其元素个数还会增多，这个存储量搁谁都受不了，如果一个batch有128张图，输入图片尺寸为400*400，3通道，网络有16层（假设每层输入输出尺寸及通道数都一样），那么单单为了存储这些重排结果，</span><br><span class="line">			&#x2F;&#x2F; 就需要128*400*400*3*16*4&#x2F;1024&#x2F;1024&#x2F;1024 &#x3D; 3.66G，所以为了权衡，只能重复计算！</span><br><span class="line">            im2col_cpu_ext(</span><br><span class="line">                im,                 &#x2F;&#x2F; input</span><br><span class="line">                l.c &#x2F; l.groups,     &#x2F;&#x2F; input channels</span><br><span class="line">                l.h, l.w,           &#x2F;&#x2F; input size (h, w)</span><br><span class="line">                l.size, l.size,     &#x2F;&#x2F; kernel size (h, w)</span><br><span class="line">                l.pad, l.pad,       &#x2F;&#x2F; padding (h, w)</span><br><span class="line">                l.stride_y, l.stride_x, &#x2F;&#x2F; stride (h, w)</span><br><span class="line">                l.dilation, l.dilation, &#x2F;&#x2F; dilation (h, w)</span><br><span class="line">                b);                 &#x2F;&#x2F; output</span><br><span class="line">			&#x2F;&#x2F; 下面计算当前层的权重更新值，所谓权重更新值就是weight &#x3D; weight - alpha * weight_update中的weight_update，</span><br><span class="line">			&#x2F;&#x2F; 权重更新值等于当前层敏感度图中每个元素乘以相应的像素值，因为一个权重跟当前层多个输出有关联（权值共享，即卷积核在图像中跨步移动做卷积，每个位置卷积得到的值</span><br><span class="line">			&#x2F;&#x2F; 都与该权值相关），所以对每一个权重更新值来说，需要在l.delta中找出所有与之相关的敏感度，乘以相应像素值，再求和，具体实现的方式依靠im2col_cpu()与gemm_nt()完成。</span><br><span class="line">			&#x2F;&#x2F; （backward_convolutional_layer整个函数的代码非常重要，仅靠文字没有公式与图表辅助说明可能很难说清，所以这部分更为清晰详细的说明，请参考个人博客！）</span><br><span class="line">			&#x2F;&#x2F; GEneral Matrix to Matrix Multiplication</span><br><span class="line">			&#x2F;&#x2F; 此处在im2col_cpu操作基础上，利用矩阵乘法c&#x3D;alpha*a*b+beta*c完成对图像卷积的操作；</span><br><span class="line">			&#x2F;&#x2F; 0表示不对输入a进行转置，1表示对输入b进行转置；</span><br><span class="line">			&#x2F;&#x2F; m是输入a,c的行数，具体含义为卷积核的个数(l.n)；</span><br><span class="line">			&#x2F;&#x2F; n是输入b,c的列数，具体含义为每个卷积核元素个数乘以输入图像的通道数(l.size*l.size*l.c)；</span><br><span class="line">			&#x2F;&#x2F; k是输入a的列数也是b的行数，具体含义为每个输出特征图的元素个数（l.out_w*l.out_h）；</span><br><span class="line">			&#x2F;&#x2F; a,b,c即为三个参与运算的矩阵（用一维数组存储）,alpha&#x3D;beta&#x3D;1为常系数；</span><br><span class="line">			&#x2F;&#x2F; a为l.delta的一大行。l.delta为本层所有输出元素（包含整个batch中每张图片的所有输出特征图）关于加权输入的导数（即激活函数的导数值）集合,</span><br><span class="line">			&#x2F;&#x2F; 元素个数为l.batch * l.out_h * l.out_w * l.out_c（l.out_c &#x3D; l.n），按行存储，共有l.batch行，l.out_c * l.out_h * l.out_w列，</span><br><span class="line">			&#x2F;&#x2F; 即l.delta中每行包含一张图的所有输出图，故这么一大行，又可以视作有l.out_c（l.out_c&#x3D;l.n）小行，l.out_h*l*out_w小列，而一次循环就是处理l.delta的一大行，</span><br><span class="line">			&#x2F;&#x2F; 故可以将a视作l.out_c行，l.out_h*l*out_w列的矩阵；</span><br><span class="line">			&#x2F;&#x2F; b为单张输入图像经过im2col_cpu重排后的图像数据；</span><br><span class="line">			&#x2F;&#x2F; c为输出，按行存储，可视作有l.n行，l.c*l.size*l.size列（l.c是输入图像的通道数，l.n是卷积核个数），</span><br><span class="line">			&#x2F;&#x2F; 即c就是所谓的误差项（输出关于加权输入的导数），或者敏感度（强烈推荐：https:&#x2F;&#x2F;www.zybuluo.com&#x2F;hanbingtao&#x2F;note&#x2F;485480）（一个核有l.c*l.size*l.size个权重，共有l.n个核）。</span><br><span class="line">			&#x2F;&#x2F; 由上可知：</span><br><span class="line">			&#x2F;&#x2F; a: (l.out_c) * (l.out_h*l*out_w)</span><br><span class="line">			&#x2F;&#x2F; b: (l.c * l.size * l.size) * (l.out_h * l.out_w)</span><br><span class="line">			&#x2F;&#x2F; c: (l.n) * (l.c*l.size*l.size)（注意：l.n &#x3D; l.out_c）</span><br><span class="line">			&#x2F;&#x2F; 故要进行a * b + c计算，必须对b进行转置（否则行列不匹配），因故调用gemm_nt()函数</span><br><span class="line">            gemm(0, 1, m, n, k, 1, a, k, b, k, 1, c, n);</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; 接下来，用当前层的敏感度图l.delta以及权重l.weights（还未更新）来获取上一层网络的敏感度图，BP算法的主要流程就是依靠这种层与层之间敏感度反向递推传播关系来实现。</span><br><span class="line">			&#x2F;&#x2F; 在network.c的backward_network()中，会从最后一层网络往前遍循环历至第一层，而每次开始遍历某一层网络之前，都会更新net.input为这一层网络前一层的输出，即prev.output,</span><br><span class="line">			&#x2F;&#x2F; 同时更新net.delta为prev.delta，因此，这里的net.delta是当前层前一层的敏感度图。</span><br><span class="line">			&#x2F;&#x2F; 已经强调很多次了，再说一次：下面得到的上一层的敏感度并不完整，完整的敏感度图是损失函数对上一层的加权输入的导数，</span><br><span class="line">			&#x2F;&#x2F; 而这里得到的敏感度图是损失函数对上一层输出值的导数，还差乘以一个输出值也即激活函数对加权输入的导数。</span><br><span class="line">            if (state.delta) &#123;</span><br><span class="line">				&#x2F;&#x2F; 当前层还未更新的权重</span><br><span class="line">                a &#x3D; l.weights + j*l.nweights &#x2F; l.groups;</span><br><span class="line">				</span><br><span class="line">				&#x2F;&#x2F; 每次循环仅处理一张输入图，注意移位（l.delta的维度为l.batch * l.out_c * l.out_w * l.out_h）（注意l.n &#x3D; l.out_c，另外提一下，对整个网络来说，每一层的l.batch其实都是一样的）</span><br><span class="line">                b &#x3D; l.delta + (i*l.groups + j)*m*k;</span><br><span class="line">				</span><br><span class="line">				&#x2F;&#x2F; net.workspace和上面一样，还是一张输入图片的重排，不同的是，此处我们只需要这个容器，而里面存储的值我们并不需要，在后面的处理过程中，</span><br><span class="line">				&#x2F;&#x2F; 会将其中存储的值一一覆盖掉（尺寸维持不变，还是(l.c * l.size * l.size) * (l.out_h * l.out_w）</span><br><span class="line">                c &#x3D; state.workspace;</span><br><span class="line">				</span><br><span class="line">				 &#x2F;&#x2F; 相比上一个gemm，此处的a对应上一个的c,b对应上一个的a，c对应上一个的b，即此处a,b,c的行列分别为：</span><br><span class="line">				&#x2F;&#x2F; a: (l.n) * (l.c*l.size*l.size)，表示当前层所有权重系数</span><br><span class="line">				&#x2F;&#x2F; b: (l.out_c) * (l.out_h*l*out_w)（注意：l.n &#x3D; l.out_c），表示当前层的敏感度图</span><br><span class="line">				&#x2F;&#x2F; c: (l.c * l.size * l.size) * (l.out_h * l.out_w)，表示上一层的敏感度图（其元素个数等于上一层网络单张输入图片的所有输出元素个数），</span><br><span class="line">				&#x2F;&#x2F; 此时要完成a * b + c计算，必须对a进行转置（否则行列不匹配），因故调用gemm_tn()函数。</span><br><span class="line">				&#x2F;&#x2F; 此操作含义是用：用当前层还未更新的权重值对敏感度图做卷积，得到包含上一层所有敏感度信息的矩阵，但这不是上一层最终的敏感度图，</span><br><span class="line">				&#x2F;&#x2F; 因为此时的c，也即net.workspace的尺寸为(l.c * l.size * l.size) * (l.out_h * l.out_w)，明显不是上一层的输出尺寸l.c*l.w*l.h，</span><br><span class="line">				&#x2F;&#x2F; 接下来还需要调用col2im_cpu()函数将其恢复至l.c*l.w*l.h（可视为l.c行，l.w*l.h列），这才是上一层的敏感度图（实际还差一个环节，</span><br><span class="line">				&#x2F;&#x2F; 这个环节需要等到下一次调用backward_convolutional_layer()才完成：将net.delta中每个元素乘以激活函数对加权输入的导数值）。</span><br><span class="line">				&#x2F;&#x2F; 完成gemm这一步，如col2im_cpu()中注释，是考虑了多个卷积核导致的一对多关系（上一层的一个输出元素会流入到下一层多个输出元素中），</span><br><span class="line">				&#x2F;&#x2F; 接下来调用col2im_cpu()则是考虑卷积核重叠（步长较小）导致的一对多关系。</span><br><span class="line">                gemm(1, 0, n, k, m, 1, a, n, b, k, 0, c, k);</span><br><span class="line"></span><br><span class="line">                &#x2F;&#x2F;col2im_cpu(state.workspace, l.c &#x2F; l.groups, l.h, l.w, l.size, l.stride,</span><br><span class="line">                &#x2F;&#x2F;     l.pad, state.delta + (i*l.groups + j)*l.c &#x2F; l.groups*l.h*l.w);</span><br><span class="line">				</span><br><span class="line">				&#x2F;&#x2F; 对c也即state.workspace进行重排，得到的结果存储在state.delta中，每次循环只会处理一张输入图片，因此，此处只会得到一张输入图产生的敏感图（注意net.delta的移位）,</span><br><span class="line">				&#x2F;&#x2F; 整个循环结束后，net.delta的总尺寸为l.batch * l.h * l.w * l.c，这就是上一层网络整个batch的敏感度图，可视为有l.batch行，l.h*l.w*l.c列，</span><br><span class="line">				&#x2F;&#x2F; 每行存储了一张输入图片所有输出特征图的敏感度</span><br><span class="line">				&#x2F;&#x2F; col2im_cpu()函数中会调用col2im_add_pixel()函数，该函数中使用了+&#x3D;运算符，也即该函数要求输入的net.delta的初始值为0,而在gradient_array()中注释到l.delta的元素是不为0（也不能为0）的，</span><br><span class="line">				&#x2F;&#x2F; 看上去是矛盾的，实则不然，gradient_array()使用的l.delta是当前层的敏感度图，而在col2im_cpu()使用的net.delta是上一层的敏感度图，正如gradient_array()中所注释的，</span><br><span class="line">				&#x2F;&#x2F; 当前层l.delta之所以不为0,是因为从后面层反向传播过来的，对于上一层，显然还没有反向传播到那，因此net.delta的初始值都是为0的（注意，每一层在构建时，就为其delta动态分配了内存，</span><br><span class="line">				&#x2F;&#x2F; 且在前向传播时，为每一层的delta都赋值为0,可以参考network.c中forward_network()函数）</span><br><span class="line">                col2im_cpu_ext(</span><br><span class="line">                    state.workspace,        &#x2F;&#x2F; input</span><br><span class="line">                    l.c &#x2F; l.groups,         &#x2F;&#x2F; input channels (h, w)</span><br><span class="line">                    l.h, l.w,               &#x2F;&#x2F; input size (h, w)</span><br><span class="line">                    l.size, l.size,         &#x2F;&#x2F; kernel size (h, w)</span><br><span class="line">                    l.pad, l.pad,           &#x2F;&#x2F; padding (h, w)</span><br><span class="line">                    l.stride_y, l.stride_x,     &#x2F;&#x2F; stride (h, w)</span><br><span class="line">                    l.dilation, l.dilation, &#x2F;&#x2F; dilation (h, w)</span><br><span class="line">                    state.delta + (i*l.groups + j)* (l.c &#x2F; l.groups)*l.h*l.w); &#x2F;&#x2F; output (delta)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="col2im函数解析"><a href="#col2im函数解析" class="headerlink" title="col2im函数解析"></a>col2im函数解析</h2><p>col2im函数是im2col的逆过程，代码在<code>src/col2im.c</code>中实现，具体如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 注释来自https:&#x2F;&#x2F;github.com&#x2F;hgpvision&#x2F;darknet&#x2F;blob&#x2F;master&#x2F;src&#x2F;col2im.c</span><br><span class="line">&#x2F;*</span><br><span class="line">*  将输入图像im的channel通道上的第row行，col列像素灰度值加上val（直接修改im的值，因此im相当于是返回值）</span><br><span class="line">** 输入：im         输入图像</span><br><span class="line">**       channels   输入图像的im通道数（这个参数没用。。。）</span><br><span class="line">**       height     输入图像im的高度（行）</span><br><span class="line">**       width      输入图像im的宽度（列）</span><br><span class="line">**       row        需要加上val的像素所在的行数（补零之后的行数，因此需要先减去pad才能得到真正在im中的行数）</span><br><span class="line">**       col        需要加上val的像素所在的列数（补零之后的列数，因此需要先减去pad才能得到真正在im中的列数）</span><br><span class="line">**       channel    需要加上val的像素所在的通道数</span><br><span class="line">**       pad        四周补0长度</span><br><span class="line">**       val        像素灰度添加值</span><br><span class="line">*&#x2F;</span><br><span class="line">void col2im_add_pixel(float *im, int height, int width, int channels,</span><br><span class="line">                        int row, int col, int channel, int pad, float val)</span><br><span class="line">&#123;</span><br><span class="line">    row -&#x3D; pad;</span><br><span class="line">    col -&#x3D; pad;</span><br><span class="line"></span><br><span class="line">    if (row &lt; 0 || col &lt; 0 ||</span><br><span class="line">        row &gt;&#x3D; height || col &gt;&#x3D; width) return;</span><br><span class="line">    im[col + width*(row + height*channel)] +&#x3D; val;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F;This one might be too, can&#39;t remember.</span><br><span class="line">&#x2F;&#x2F; 注释来自：https:&#x2F;&#x2F;github.com&#x2F;hgpvision&#x2F;darknet&#x2F;blob&#x2F;master&#x2F;src&#x2F;col2im.c</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">** 此函数与im2col_cpu()函数的流程相反，目地是将im2col_cpu()函数重排得到的图片data_col恢复至正常的图像矩阵排列，并与data_im相加，最终data_im相当于是输出值，</span><br><span class="line">** 要注意的是，data_im的尺寸是在函数外确定的，且并没有显示的将data_col转为一个与data_im尺寸相同的矩阵，而是将其中元素直接加在data_im对应元素上（data_im初始所有元素值都为0）。</span><br><span class="line">** 得到的data_im尺寸为l.c*l.h*l.w，即为当前层的输入图像尺寸，上一层的输出图像尺寸，按行存储，可视为l.c行，l.h*l.w列，即其中每行对应一张输出特征图的敏感度图（实际上这还不是最终的敏感度，</span><br><span class="line">** 还差一个环节：乘以激活函数对加权输入的导数，这将在下一次调用backward_convolutional_laye时完成）。</span><br><span class="line">**</span><br><span class="line">** 举个例子：第L-1层每张输入图片（本例子只分析单张输入图片）的输出为5*5*3（3为输出通道数），第L层共有2个卷积核，每个卷积核的尺寸为3*3，stride &#x3D; 2,</span><br><span class="line">**         第L-1层的输出是第L层的输入，第L层的2个卷积核同时对上一层3个通道的输出做卷积运算，为了做到这一点，需要调用im2col_cpu()函数将</span><br><span class="line">**         上一层的输出，也就是本层的输入重排为27行4列的图，也就是由5*5*3变换至27*4，你会发现总的元素个数变多了（75增多到了98），</span><br><span class="line">**         这是因为卷积核stride&#x3D;2,小于卷积核的尺寸3,因此卷积在两个连续位置做卷积，会有重叠部分，而im2col_cpu()函数为了便于卷积运算，完全将其</span><br><span class="line">**         铺排开来，并没有在空间上避免重复元素，因此像素元素会增多。此外，之所以是27行，是因为卷积核尺寸为3*3，而上一层的输出即本层输入有3个通道，</span><br><span class="line">**         为了同时给3个通道做卷积运算，需要将3个通道上的输入一起考虑，即得到3*3*3行，4列是因为对于对于5*5的图像，使用3*3的卷积核，stride&#x3D;2的卷积跨度，</span><br><span class="line">**         最终会得到2*2的特征图，也就是4个元素。除了调用im2col_cpu()对输入图像做重排，相应的，也要将所有卷积核重排成一个2*27的矩阵，为什么是2呢？</span><br><span class="line">**         因为有两个卷积核，为了做到同时将两个卷积核作用到输入图像上，需要将两个核合到一个矩阵中，每个核对应一行，因此有2行，那为什么是27呢？每个核</span><br><span class="line">**         元素个数不是3*3&#x3D;9吗？是的，但是考虑到要同时作用到3个通道上，所以实际一个卷积核有9*3&#x3D;27个元素。综述，得到2*27的卷积核矩阵与27*4的输入图像矩阵，</span><br><span class="line">**         两个矩阵相乘，即可完成将2个卷积核同时作用于3通道的输入图像上（非常方便，不枉前面非这么大劲的重排！），最终得到2*4的矩阵，这2*4矩阵又代表这什么呢？</span><br><span class="line">**         2代表这有两个输出图（对应2个卷积核，即l.out_c&#x3D;2），每个输出图占一行，4代表这每个输出图元素有4个（前面说了，每个卷积核会得到2*2的特征图，即l.out_h&#x3D;l.out_w&#x3D;2）。这个例子说到这，只说完了</span><br><span class="line">**         前向传播部分，可以看出im2col_cpu()这个函数的重要性。而此处的col2im_cpu()是一个逆过程，主要用于反向传播中，由L层的敏感度图(sensitivity map，</span><br><span class="line">**         可能每个地方叫的不一样，此处参考博客：https:&#x2F;&#x2F;www.zybuluo.com&#x2F;hanbingtao&#x2F;note&#x2F;485480)反向求得第L-1层的敏感度图。顺承上面的例子，第L-1层的输出</span><br><span class="line">**         是一个5*5*3（l.w&#x3D;l.h&#x3D;5,l.c&#x3D;3）的矩阵，也就是敏感度图的维度为5*5*3（每个输出元素，对应一个敏感度值），第L层的输出是一个2*4的矩阵，敏感度图的维度为2*4，假设已经计算得到了</span><br><span class="line">**         第L层的2*4的敏感度图，那么现在的问题是，如何由第L层的2*4敏感度图以及2个卷积核（2*27）反向获取第L-1层的敏感度图呢？上面给的博客链接给出了一种很好的求解方式，</span><br><span class="line">**         但darknet并不是这样做的，为什么？因为前面有im2col_cpu()，im2col_cpu()函数中的重排方式，使得我们不再需要博客中提到的将sensitivity map还原为步长为1的sensitivity map，</span><br><span class="line">**         只需再使用col2im_cpu()就可以了！过程是怎样的呢，看backward_convolutional_layer()函数中if(net.delta)中的语句就知道了，此处仅讨论col2im_cpu()的过程，</span><br><span class="line">**         在backward_convolutional_layer()已经得到了data_col，这个矩阵含有了所有的第L-1层敏感度的信息，但遗憾的是，不能直接用，需要整理，因为此时的data_col还是一个</span><br><span class="line">**         27*4的矩阵，而我们知道第L-1层的敏感度图是一个5*5*3的矩阵，如何将一个27*4的矩阵变换至一个5*5*3的矩阵是本函数要完成的工作，前面说到27*4元素个数多于5*5*3,</span><br><span class="line">**         很显然要从27*4变换至5*5*3，肯定会将某些元素相加合并（下面col2im_add_pixel()函数就是干这个的），具体怎样，先不说，先来看看输入参数都代表什么意思吧：</span><br><span class="line">** 输入：data_col    backward_convolutional_layer()中计算得到的包含上一层所有敏感度信息的矩阵，行数为l.n*l.size*l.size（l代表本层&#x2F;当前层），列数为l.out_h*l.out_w（对于本例子，行数为27,列数为4,上一层为第L-1层，本层是第L层）</span><br><span class="line">**       channels    当前层输入图像的通道数（对于本例子，为3）</span><br><span class="line">**       height      当前层输入图像的行数（对于本例子，为5）</span><br><span class="line">**       width       当前层输入图像的列数（对于本例子，为5）</span><br><span class="line">**       ksize       当前层卷积核尺寸（对于本例子，为3）</span><br><span class="line">**       stride      当前层卷积跨度（对于本例子，为2）</span><br><span class="line">**       pad         当前层对输入图像做卷积时四周补0的长度</span><br><span class="line">**       data_im     经col2im_cpu()重排恢复之后得到的输出矩阵，也即上一层的敏感度图，尺寸为l.c * l.h * l.w（刚好就是上一层的输出当前层输入的尺寸，对于本例子，5行5列3通道），</span><br><span class="line">**                   注意data_im的尺寸，是在本函数之外就已经确定的，不是在本函数内部计算出来的，这与im2col_cpu()不同，im2col_cpu()计算得到的data_col的尺寸都是在函数内部计算得到的，</span><br><span class="line">**                   并不是事先指定的。也就是说，col2im_cpu()函数完成的是指定尺寸的输入矩阵往指定尺寸的输出矩阵的转换。</span><br><span class="line">** 原理：原理比较复杂，很难用文字叙述，博客：https:&#x2F;&#x2F;www.zybuluo.com&#x2F;hanbingtao&#x2F;note&#x2F;485480中基本原理说得很详细了，但是此处的实现与博客中并不一样，所以具体实现的原理此处简要叙述一下，具体见个人博客。</span><br><span class="line">**      第L-1层得到l.h*l.w*l.c输出，也是第L层的输入，经L层卷积及激活函数处理之后，得到l.out_h*l.out_w*l.out_c的输出，也就是由l.h*l.w*l.c--&gt;l.out_h*l.out_w*l.out_c，</span><br><span class="line">**      由于第L层有多个卷积核，所以第L-1层中的一个输出元素会流入到第L层多个输出中，除此之外，由于卷积核之间的重叠，也导致部分元素流入到第L层的多个输出中，这两种情况，都导致第L-1层中的某个敏感度会与第L层多个输出有关，</span><br><span class="line">**      为清晰，还是用上面的例子来解释，第L-1层得到5*5*3(3*25)的输出，第L层得到2*2*2（2*4）的输出，在backward_convolutional_layer()已经计算得到的data_col实际是27*2矩阵与2*4矩阵相乘的结果，</span><br><span class="line">**      为方便，我们记27*2的矩阵为a，记2*4矩阵为b，那么a中一行（2个元素）与b中一列（2个元素）相乘对应这什么呢？对应第一情况，因为有两个卷积核，使得L-1中一个输出至少与L层中两个输出有关系，经此矩阵相乘，得到27*4的矩阵，</span><br><span class="line">**      已经考虑了第一种情况（27*4这个矩阵中的每一个元素都是两个卷积核影响结果的求和），那么接下来的就是要考虑第二种情况：卷积核重叠导致的一对多关系，具体做法就是将data_col中对应相同像素的值相加，这是由</span><br><span class="line">**      im2col_cpu()函数决定的（可以配合im2col_cpu()来理解），因为im2col_cpu()将这些重叠元素也铺陈保存在data_col中，所以接下来，只要按照im2col_cpu()逆向将这些重叠元素的影响叠加就可以了，</span><br><span class="line">**      大致就是这个思路，具体的实现细节可能得见个人博客了（这段写的有点罗嗦～）。</span><br><span class="line">**</span><br><span class="line">*&#x2F;</span><br><span class="line">void col2im_cpu(float* data_col,</span><br><span class="line">         int channels,  int height,  int width,</span><br><span class="line">         int ksize,  int stride, int pad, float* data_im)</span><br><span class="line">&#123;</span><br><span class="line">    int c,h,w;</span><br><span class="line">	&#x2F;&#x2F; 当前层输出图的尺寸（对于上面的例子，height_col&#x3D;2,width_col&#x3D;2）</span><br><span class="line">    int height_col &#x3D; (height + 2*pad - ksize) &#x2F; stride + 1;</span><br><span class="line">    int width_col &#x3D; (width + 2*pad - ksize) &#x2F; stride + 1;</span><br><span class="line">	&#x2F;&#x2F; 当前层每个卷积核在所有输入图像通道上的总元素个数（对于上面的例子，channels_col&#x3D;3*3*3&#x3D;27）</span><br><span class="line">    &#x2F;&#x2F; 注意channels_col实际是data_col的行数</span><br><span class="line">    int channels_col &#x3D; channels * ksize * ksize;</span><br><span class="line">	&#x2F;&#x2F; 开始遍历：外循环遍历data_col的每一行（对于上面的例子，data_col共27行）</span><br><span class="line">    for (c &#x3D; 0; c &lt; channels_col; ++c) &#123;</span><br><span class="line">		&#x2F;&#x2F; 列偏移，卷积核是一个二维矩阵，并按行存储在一维数组中，利用求余运算获取对应在卷积核中的列数，比如对于</span><br><span class="line">        &#x2F;&#x2F; 3*3的卷积核，当c&#x3D;0时，显然在第一列，当c&#x3D;5时，显然在第2列，当c&#x3D;9时，在第二通道上的卷积核的第一列</span><br><span class="line">        int w_offset &#x3D; c % ksize;</span><br><span class="line">		&#x2F;&#x2F; 行偏移，卷积核是一个二维的矩阵，且是按行（卷积核所有行并成一行）存储在一维数组中的，</span><br><span class="line">        &#x2F;&#x2F; 比如对于3*3的卷积核，处理3通道的图像，那么一个卷积核具有27个元素，每9个元素对应一个通道上的卷积核（互为一样），</span><br><span class="line">        &#x2F;&#x2F; 每当c为3的倍数，就意味着卷积核换了一行，h_offset取值为0,1,2</span><br><span class="line">        int h_offset &#x3D; (c &#x2F; ksize) % ksize;</span><br><span class="line">		&#x2F;&#x2F; 通道偏移，channels_col是多通道的卷积核并在一起的，比如对于3通道，3*3卷积核，每过9个元素就要换一通道数，</span><br><span class="line">        &#x2F;&#x2F; 当c&#x3D;0~8时，c_im&#x3D;0;c&#x3D;9~17时，c_im&#x3D;1;c&#x3D;18~26时，c_im&#x3D;2</span><br><span class="line">        &#x2F;&#x2F; c_im是data_im的通道数（即上一层输出当前层输入的通道数），对于上面的例子，c_im取值为0,1,2</span><br><span class="line">        int c_im &#x3D; c &#x2F; ksize &#x2F; ksize;</span><br><span class="line">		&#x2F;&#x2F; 中循环与内循环和起来刚好遍历data_col的每一行（对于上面的例子，data_col的列数为4,height_col*width_col&#x3D;4）</span><br><span class="line">        for (h &#x3D; 0; h &lt; height_col; ++h) &#123;</span><br><span class="line">            for (w &#x3D; 0; w &lt; width_col; ++w) &#123;</span><br><span class="line">				&#x2F;&#x2F; 获取在输出data_im中的行数im_row与列数im_col</span><br><span class="line">                &#x2F;&#x2F; 由上面可知，对于3*3的卷积核，h_offset取值为0,1,2,当h_offset&#x3D;0时，会提取出所有与卷积核第一行元素进行运算的像素，</span><br><span class="line">                &#x2F;&#x2F; 依次类推；加上h*stride是对卷积核进行行移位操作，比如卷积核从图像(0,0)位置开始做卷积，那么最先开始涉及(0,0)~(3,3)</span><br><span class="line">                &#x2F;&#x2F; 之间的像素值，若stride&#x3D;2，那么卷积核进行行移位一次时，下一行的卷积操作是从元素(2,0)（2为图像行号，0为列号）开始</span><br><span class="line">                int im_row &#x3D; h_offset + h * stride;</span><br><span class="line">				&#x2F;&#x2F; 对于3*3的卷积核，w_offset取值也为0,1,2，当w_offset取1时，会提取出所有与卷积核中第2列元素进行运算的像素，</span><br><span class="line">                &#x2F;&#x2F; 实际在做卷积操作时，卷积核对图像逐行扫描做卷积，加上w*stride就是为了做列移位，</span><br><span class="line">                &#x2F;&#x2F; 比如前一次卷积其实像素元素为(0,0)，若stride&#x3D;2,那么下次卷积元素起始像素位置为(0,2)（0为行号，2为列号）</span><br><span class="line">                int im_col &#x3D; w_offset + w * stride;</span><br><span class="line">				&#x2F;&#x2F; 计算在输出data_im中的索引号</span><br><span class="line">                &#x2F;&#x2F; 对于上面的例子，im_row的取值范围为0~4,im_col从0~4，c从0~2（其中h_offset从0~2,w_offset从0~2, h从0~1,w从0~1）</span><br><span class="line">                &#x2F;&#x2F; 输出的data_im的尺寸为l.c * l.h * lw，对于上面的例子，为3*5*5,因此，im_row,im_col,c的取值范围刚好填满data_im</span><br><span class="line"></span><br><span class="line">                &#x2F;&#x2F; 获取data_col中索引为col_index的元素，对于上面的例子，data_col为27*4行，按行存储</span><br><span class="line">                &#x2F;&#x2F; col_index &#x3D; c * height_col * width_col + h * width_col + w逐行读取data_col中的每一个元素。</span><br><span class="line">                &#x2F;&#x2F; 相同的im_row,im_col与c_im可能会对应多个不同的col_index，这就是卷积核重叠带来的影响，处理的方式是将这些val都加起来，</span><br><span class="line">                &#x2F;&#x2F; 存在data_im的第im_row - pad行，第im_col - pad列（c_im通道上）中。</span><br><span class="line">                &#x2F;&#x2F; 比如上面的例子，上面的例子，如果固定im_row &#x3D; 0, im_col &#x3D;2, c_im &#x3D;0，由c_im &#x3D; 0可以知道c在0~8之间，由im_row&#x3D;0,可以确定h &#x3D; 0, h_offset &#x3D;0，</span><br><span class="line">                &#x2F;&#x2F; 可以得到两组：1)w_offset &#x3D; 0, w &#x3D; 1; 2) w_offset &#x3D; 2, w &#x3D;0，第一组，则可以完全定下：c&#x3D;0,h&#x3D;0,w&#x3D;1，此时col_index&#x3D;1，由第二组，可完全定下：c&#x3D;2,h&#x3D;0,w&#x3D;0，</span><br><span class="line">                &#x2F;&#x2F; 此时col_index &#x3D; 2*2*2&#x3D;8</span><br><span class="line">                int col_index &#x3D; (c * height_col + h) * width_col + w;</span><br><span class="line">                float val &#x3D; data_col[col_index];</span><br><span class="line">				&#x2F;&#x2F; 从data_im找出c_im通道上第im_row - pad行im_col - pad列处的像素，使其加上val</span><br><span class="line">                &#x2F;&#x2F; height, width, channels都是上一层输出即当前层输入图像的尺寸，也是data_im的尺寸（对于本例子，三者的值分别为5,5,3）,</span><br><span class="line">                &#x2F;&#x2F; im_row - pad,im_col - pad,c_im都是某一具体元素在data_im中的行数与列数与通道数（因为im_row与im_col是根据卷积过程计算的，</span><br><span class="line">                &#x2F;&#x2F; 所以im_col和im_row中实际还包含了补零长度pad，需要减去之后，才是原本的没有补零矩阵data_im中的行列号）</span><br><span class="line">                col2im_add_pixel(data_im, height, width, channels,</span><br><span class="line">                        im_row, im_col, c_im, pad, val);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h2><p>上面介绍的反向传播可以用下图来表示，更容易理解。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/659.webp" alt></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu配置v2ray详细教程</title>
    <url>/2020/02/23/Ubuntu%E9%85%8D%E7%BD%AEv2ray%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title=" 简介"></a><a id="more"></a> 简介</h2><p>🌟Linux/Windows/macOS 跨平台 v2ray GUI 🔨 使用 c++ 编写，支持订阅，扫描二维码，支持自定义路由编辑 🌟。使用 Qt 框架的跨平台 v2ray 客户端。支持 Windows, Linux, macOS。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul>
<li><p>下载<code>V2ray</code>客户端，这里以最简单的<code>AppImage</code>文件为例,下载链接：<br><a href="https://github.com/Qv2ray/Qv2ray/releases/download/v1.99.6/Qv2ray-refs.tags.v1.99.6-linux.AppImage" target="_blank" rel="noopener">https://github.com/Qv2ray/Qv2ray/releases/download/v1.99.6/Qv2ray-refs.tags.v1.99.6-linux.AppImage</a><br>或者打开网站：<a href="https://github.com/Qv2ray/Qv2ray/releases/tag/v1.99.6" target="_blank" rel="noopener">https://github.com/Qv2ray/Qv2ray/releases/tag/v1.99.6</a> 选择下图文件<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580651760-1.png" alt><br><strong>注意：建议下载1.99.6及以上版本，其它版本可能出现找不到openssl库。</strong></p>
</li>
<li><p>下载核心文件，下载链接：<br><a href="https://github.com/v2ray/v2ray-core/releases/download/v4.22.1/v2ray-linux-64.zip" target="_blank" rel="noopener">https://github.com/v2ray/v2ray-core/releases/download/v4.22.1/v2ray-linux-64.zip</a><br>或者打开网站：<a href="https://github.com/v2ray/v2ray-core/releases/" target="_blank" rel="noopener">https://github.com/v2ray/v2ray-core/releases/</a> 选择下图文件<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580651801-2.png" alt></p>
</li>
<li><p>进入v2ray下载的根目录，执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod +x .&#x2F;Qv2ray-refs.tags.v1.99.6-linux.AppImage</span><br></pre></td></tr></table></figure>
</li>
<li><p>仍然在v2ray根目录下打开终端，输入以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;Qv2ray-refs.tags.v1.99.6-linux.AppImage</span><br></pre></td></tr></table></figure>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2></li>
</ul>
<p>执行后会出现主界面，点击首选项</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580652261-3-1580634671283.png" alt></p>
<p>在常规设置里面按照图示操作,最后点击ok保存：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580652339-4.png" alt></p>
<p>回到主界面，点击订阅：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580652363-5.png" alt></p>
<p>然后按照下图要求填入相应内容,然后点击ok：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580655824-6.png" alt></p>
<p>进入网站-&gt;个人中心，按照下图说明复制链接到上图中：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580652405-7.png" alt></p>
<p>将软件的代理模式打开，如下图所示；</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580653297-8.png" alt></p>
<p>一切准备好后点击主界面的连接，开始科学上网</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580653343-9.png" alt></p>
<p>墙外的世界在欢迎你!</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2ray/1580653648-tzyy_2020-02-02_22-27-02.png" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet卷积层的前向传播解析</title>
    <url>/2020/02/23/AlexeyAB-DarkNet%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>今天来介绍一下DarkNet中卷积层的前向传播和反向传播的实现，卷积层是卷积神经网络中的核心组件，了解它的底层代码实现对我们理解卷积神经网络以及优化卷积神经网络都有一些帮助。</p>
<h2 id="卷积层的构造"><a href="#卷积层的构造" class="headerlink" title="卷积层的构造"></a>卷积层的构造</h2><p>卷积层的构造主要在<code>src/convolutional_layer.c</code>中的<code>make_convolutional_layer</code>中进行实现，下面给出部分核心代码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** batch 每个batch含有的图片数</span><br><span class="line">** step</span><br><span class="line">** h 图像高度(行数)</span><br><span class="line">** w 图像宽度(列数)</span><br><span class="line">** c 输入图像通道数</span><br><span class="line">** n 卷积核个数</span><br><span class="line">** groups 分组数</span><br><span class="line">** size 卷积核尺寸</span><br><span class="line">** stride 步长</span><br><span class="line">** dilation 空洞卷积空洞率</span><br><span class="line">** padding 四周补0长度</span><br><span class="line">** activation 激活函数类别</span><br><span class="line">** batch_normalize 是否进行BN</span><br><span class="line">** binary 是否对权重进行二值化</span><br><span class="line">** xnor 是否对权重以及输入进行二值化</span><br><span class="line">** adam 优化方式</span><br><span class="line">** use_bin_output</span><br><span class="line">** index 分组卷积的时候分组索引</span><br><span class="line">** antialiasing 抗锯齿标志，如果为真强行设置所有的步长为1</span><br><span class="line">** share_layer 标志参数，表示这一个卷积层是否和其它卷积层共享权重</span><br><span class="line">** assisted_excitation</span><br><span class="line">** deform 暂时不知道</span><br><span class="line">** train 标志参数，是否在训练</span><br><span class="line">*&#x2F;</span><br><span class="line">convolutional_layer make_convolutional_layer(int batch, int steps, int h, int w, int c, int n, int groups, int size, int stride_x, int stride_y, int dilation, int padding, ACTIVATION activation,</span><br><span class="line"> int batch_normalize, int binary, int xnor, int adam, int use_bin_output, int index, int antialiasing, convolutional_layer *share_layer, int assisted_excitation, int deform, int train)</span><br><span class="line">&#123;</span><br><span class="line">    int total_batch &#x3D; batch*steps;</span><br><span class="line">    int i;</span><br><span class="line">    convolutional_layer l &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">    l.type &#x3D; CONVOLUTIONAL;</span><br><span class="line">    l.train &#x3D; train;</span><br><span class="line"></span><br><span class="line">    if (xnor) groups &#x3D; 1;   &#x2F;&#x2F;对于二值网络，不能使用分组卷积</span><br><span class="line">    if (groups &lt; 1) groups &#x3D; 1;</span><br><span class="line"></span><br><span class="line">    const int blur_stride_x &#x3D; stride_x;</span><br><span class="line">    const int blur_stride_y &#x3D; stride_y;</span><br><span class="line">    l.antialiasing &#x3D; antialiasing;</span><br><span class="line">    if (antialiasing) &#123;</span><br><span class="line">        stride_x &#x3D; stride_y &#x3D; l.stride &#x3D; l.stride_x &#x3D; l.stride_y &#x3D; 1; &#x2F;&#x2F; use stride&#x3D;1 in host-layer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    l.deform &#x3D; deform;</span><br><span class="line">    l.assisted_excitation &#x3D; assisted_excitation;</span><br><span class="line">    l.share_layer &#x3D; share_layer;</span><br><span class="line">    l.index &#x3D; index;</span><br><span class="line">    l.h &#x3D; h;</span><br><span class="line">    l.w &#x3D; w;</span><br><span class="line">    l.c &#x3D; c;</span><br><span class="line">    l.groups &#x3D; groups;</span><br><span class="line">    l.n &#x3D; n;</span><br><span class="line">    l.binary &#x3D; binary;</span><br><span class="line">    l.xnor &#x3D; xnor;</span><br><span class="line">    l.use_bin_output &#x3D; use_bin_output;</span><br><span class="line">    l.batch &#x3D; batch;</span><br><span class="line">    l.steps &#x3D; steps;</span><br><span class="line">    l.stride &#x3D; stride_x;</span><br><span class="line">    l.stride_x &#x3D; stride_x;</span><br><span class="line">    l.stride_y &#x3D; stride_y;</span><br><span class="line">    l.dilation &#x3D; dilation;</span><br><span class="line">    l.size &#x3D; size;</span><br><span class="line">    l.pad &#x3D; padding;</span><br><span class="line">    l.batch_normalize &#x3D; batch_normalize;</span><br><span class="line">    l.learning_rate_scale &#x3D; 1;</span><br><span class="line">	&#x2F;&#x2F; 该卷积层总的权重元素个数（权重元素个数等于输入数据的通道数&#x2F;分组数*卷积核个数*卷积核的二维尺寸，注意因为每一个卷积核是同时作用于输入数据</span><br><span class="line">    &#x2F;&#x2F; 的多个通道上的，因此实际上卷积核是三维的，包括两个维度的平面尺寸，以及输入数据通道数这个维度，每个通道上的卷积核参数都是独立的训练参数）</span><br><span class="line">    l.nweights &#x3D; (c &#x2F; groups) * n * size * size;</span><br><span class="line">	&#x2F;&#x2F; 如果是共享卷积层，可以直接用共享的卷积层来赋值（猜测是有预训练权重的时候可以直接赋值）</span><br><span class="line">    if (l.share_layer) &#123;</span><br><span class="line">        if (l.size !&#x3D; l.share_layer-&gt;size || l.nweights !&#x3D; l.share_layer-&gt;nweights || l.c !&#x3D; l.share_layer-&gt;c || l.n !&#x3D; l.share_layer-&gt;n) &#123;</span><br><span class="line">            printf(&quot;Layer size, nweights, channels or filters don&#39;t match for the share_layer&quot;);</span><br><span class="line">            getchar();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        l.weights &#x3D; l.share_layer-&gt;weights;</span><br><span class="line">        l.weight_updates &#x3D; l.share_layer-&gt;weight_updates;</span><br><span class="line"></span><br><span class="line">        l.biases &#x3D; l.share_layer-&gt;biases;</span><br><span class="line">        l.bias_updates &#x3D; l.share_layer-&gt;bias_updates;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">		&#x2F;&#x2F; 该卷积层总的权重元素(卷积核元素)个数&#x3D;输入图像通道数 &#x2F; 分组数*卷积核个数*卷积核尺寸</span><br><span class="line">        l.weights &#x3D; (float*)xcalloc(l.nweights, sizeof(float));</span><br><span class="line">		&#x2F;&#x2F; bias就是Wx+b中的b（上面的weights就是W），有多少个卷积核，就有多少个b（与W的个数一一对应，每个W的元素个数为c*size*size）</span><br><span class="line">        l.biases &#x3D; (float*)xcalloc(n, sizeof(float));</span><br><span class="line">		&#x2F;&#x2F; 训练期间，需要执行反向传播</span><br><span class="line">        if (train) &#123;</span><br><span class="line">			&#x2F;&#x2F; 敏感图和特征图的尺寸应该是一样的</span><br><span class="line">            l.weight_updates &#x3D; (float*)xcalloc(l.nweights, sizeof(float));</span><br><span class="line">			&#x2F;&#x2F; bias的敏感图，维度和bias一致</span><br><span class="line">            l.bias_updates &#x3D; (float*)xcalloc(n, sizeof(float));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; float scale &#x3D; 1.&#x2F;sqrt(size*size*c);</span><br><span class="line">	&#x2F;&#x2F; 初始化权重：缩放因子*标准正态分布随机数，缩放因子等于sqrt(2.&#x2F;(size*size*c))，随机初始化</span><br><span class="line">    &#x2F;&#x2F; 此处初始化权重为正态分布，而在全连接层make_connected_layer()中初始化权重是均匀分布的。</span><br><span class="line">    &#x2F;&#x2F; TODO：个人感觉，这里应该加一个if条件语句：if(weightfile)，因为如果导入了预训练权重文件，就没有必要这样初始化了（事实上在detector.c的train_detector()函数中，</span><br><span class="line">    &#x2F;&#x2F; 紧接着parse_network_cfg()函数之后，就添加了if(weightfile)语句判断是否导入权重系数文件，如果导入了权重系数文件，也许这里初始化的值也会覆盖掉，</span><br><span class="line">    &#x2F;&#x2F; 总之这里的权重初始化的处理方式还是值得思考的，也许更好的方式是应该设置专门的函数进行权重的初始化，同时偏置也是，不过这里似乎没有考虑偏置的初始化，在make_connected_layer()中倒是有。。。）</span><br><span class="line">    float scale &#x3D; sqrt(2.&#x2F;(size*size*c&#x2F;groups));</span><br><span class="line">    for(i &#x3D; 0; i &lt; l.nweights; ++i) l.weights[i] &#x3D; scale*rand_uniform(-1, 1);   &#x2F;&#x2F; rand_normal();</span><br><span class="line">	&#x2F;&#x2F; 根据该层输入图像的尺寸、卷积核尺寸以及跨度计算输出特征图的宽度和高度</span><br><span class="line">    int out_h &#x3D; convolutional_out_height(l);</span><br><span class="line">    int out_w &#x3D; convolutional_out_width(l);</span><br><span class="line">	&#x2F;&#x2F; 输出图像高度</span><br><span class="line">    l.out_h &#x3D; out_h;</span><br><span class="line">	&#x2F;&#x2F; 输出图像宽度	</span><br><span class="line">    l.out_w &#x3D; out_w;</span><br><span class="line">	&#x2F;&#x2F; 输出图像通道数(等于卷积核个数,有多少个卷积核，最终就得到多少张特征图，每张特征图是一个通道)</span><br><span class="line">    l.out_c &#x3D; n;</span><br><span class="line">    l.outputs &#x3D; l.out_h * l.out_w * l.out_c; &#x2F;&#x2F; 对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即l.out_c张特征图）</span><br><span class="line">    l.inputs &#x3D; l.w * l.h * l.c; &#x2F;&#x2F; mini-batch中每张输入图片的像素元素个数</span><br><span class="line">    l.activation &#x3D; activation;</span><br><span class="line"></span><br><span class="line">    l.output &#x3D; (float*)xcalloc(total_batch*l.outputs, sizeof(float)); &#x2F;&#x2F; l.output为该层所有的输出（包括mini-batch所有输入图片的输出）</span><br><span class="line">#ifndef GPU</span><br><span class="line">    if (train) l.delta &#x3D; (float*)xcalloc(total_batch*l.outputs, sizeof(float));  &#x2F;&#x2F; l.delta 该层的敏感度图，和输出的维度想同</span><br><span class="line">#endif  &#x2F;&#x2F; not GPU</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 卷积层三种指针函数，对应三种计算：前向，反向，更新</span><br><span class="line">    l.forward &#x3D; forward_convolutional_layer;</span><br><span class="line">    l.backward &#x3D; backward_convolutional_layer;</span><br><span class="line">    l.update &#x3D; update_convolutional_layer;</span><br></pre></td></tr></table></figure>
<h2 id="卷积层前向传播的代码解析"><a href="#卷积层前向传播的代码解析" class="headerlink" title="卷积层前向传播的代码解析"></a>卷积层前向传播的代码解析</h2><p>代码在<code>src/convolutional_layer.c</code>中，注释如下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 卷积层的前向传播核心代码</span><br><span class="line">void forward_convolutional_layer(convolutional_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int out_h &#x3D; convolutional_out_height(l);</span><br><span class="line">    int out_w &#x3D; convolutional_out_width(l);</span><br><span class="line">    int i, j;</span><br><span class="line">	&#x2F;&#x2F; l.outputs &#x3D; l.out_h * l.out_w * l.out_c在make各网络层函数中赋值（比如make_convolutional_layer()），</span><br><span class="line">    &#x2F;&#x2F; 对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即l.out_c张特征图）</span><br><span class="line">    &#x2F;&#x2F; 初始化输出l.output全为0.0；输入l.outputs*l.batch为输出的总元素个数，其中l.outputs为batch</span><br><span class="line">    &#x2F;&#x2F; 中一个输入对应的输出的所有元素的个数，l.batch为一个batch输入包含的图片张数；0表示初始化所有输出为0；</span><br><span class="line">    fill_cpu(l.outputs*l.batch, 0, l.output, 1);</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 是否进行二值化操作</span><br><span class="line">    if (l.xnor &amp;&amp; (!l.align_bit_weights || state.train)) &#123;</span><br><span class="line">        if (!l.align_bit_weights || state.train) &#123;</span><br><span class="line">            binarize_weights(l.weights, l.n, l.nweights, l.binary_weights);</span><br><span class="line">            &#x2F;&#x2F;printf(&quot;\n binarize_weights l.align_bit_weights &#x3D; %p \n&quot;, l.align_bit_weights);</span><br><span class="line">        &#125;</span><br><span class="line">        swap_binary(&amp;l);</span><br><span class="line">        binarize_cpu(state.input, l.c*l.h*l.w*l.batch, l.binary_input);</span><br><span class="line">        state.input &#x3D; l.binary_input;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int m &#x3D; l.n &#x2F; l.groups; &#x2F;&#x2F; 该层的卷积核个数</span><br><span class="line">    int k &#x3D; l.size*l.size*l.c &#x2F; l.groups; &#x2F;&#x2F; 该层每个卷积核的参数元素个数</span><br><span class="line">    int n &#x3D; out_h*out_w; &#x2F;&#x2F; 该层每个特征图的尺寸(元素个数)</span><br><span class="line"></span><br><span class="line">    static int u &#x3D; 0;</span><br><span class="line">    u++;</span><br><span class="line">    &#x2F;&#x2F; 该循环即为卷积计算核心代码：所有卷积核对batch中每张图片进行卷积运算</span><br><span class="line">    &#x2F;&#x2F; 每次循环处理一张输入图片（所有卷积核对batch中一张图片做卷积运算）</span><br><span class="line">    for(i &#x3D; 0; i &lt; l.batch; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">		&#x2F;&#x2F; 该循环是为了处理分组卷积</span><br><span class="line">        for (j &#x3D; 0; j &lt; l.groups; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">			&#x2F;&#x2F; 当前组卷积核(也即权重)，元素个数为l.n*l.c&#x2F;l.groups*l.size*l.size,</span><br><span class="line">            &#x2F;&#x2F; 共有l.n行，l.c&#x2F;l.gropus,l.c*l.size*l.size列</span><br><span class="line">            float *a &#x3D; l.weights +j*l.nweights &#x2F; l.groups;</span><br><span class="line">			&#x2F;&#x2F; 对输入图像进行重排之后的图像数据，所以内存空间申请为网络中最大占用内存</span><br><span class="line">            float *b &#x3D; state.workspace;</span><br><span class="line">			&#x2F;&#x2F; 存储一张输入图片（多通道）当前组的输出特征图（输入图片是多通道的，输出</span><br><span class="line">            &#x2F;&#x2F; 图片也是多通道的，有多少组卷积核就有多少组通道，每个分组后的卷积核得到一张特征图即为一个通道）</span><br><span class="line">            &#x2F;&#x2F; 这里似乎有点拗口，可以看下分组卷积原理。</span><br><span class="line">            float *c &#x3D; l.output +(i*l.groups + j)*n*m;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;gemm(0,0,m,n,k,1,a,k,b,n,1,c,n);</span><br><span class="line">            &#x2F;&#x2F;gemm_nn_custom(m, n, k, 1, a, k, b, n, c, n);</span><br><span class="line">			&#x2F;&#x2F;二值网络，特殊处理，里面还有一些优化，细节很多，这里暂时不管二值网络这部分，把注意力先放在普通卷积层的计算上</span><br><span class="line">            if (l.xnor &amp;&amp; l.align_bit_weights &amp;&amp; !state.train &amp;&amp; l.stride_x &#x3D;&#x3D; l.stride_y)</span><br><span class="line">            &#123;</span><br><span class="line">                memset(b, 0, l.bit_align*l.size*l.size*l.c * sizeof(float));</span><br><span class="line"></span><br><span class="line">                if (l.c % 32 &#x3D;&#x3D; 0)</span><br><span class="line">                &#123;</span><br><span class="line">                    &#x2F;&#x2F;printf(&quot; l.index &#x3D; %d - new XNOR \n&quot;, l.index);</span><br><span class="line"></span><br><span class="line">                    int ldb_align &#x3D; l.lda_align;</span><br><span class="line">                    size_t new_ldb &#x3D; k + (ldb_align - k%ldb_align); &#x2F;&#x2F; (k &#x2F; 8 + 1) * 8;</span><br><span class="line">                    &#x2F;&#x2F;size_t t_intput_size &#x3D; new_ldb * l.bit_align;&#x2F;&#x2F; n;</span><br><span class="line">                    &#x2F;&#x2F;size_t t_bit_input_size &#x3D; t_intput_size &#x2F; 8;&#x2F;&#x2F; +1;</span><br><span class="line"></span><br><span class="line">                    int re_packed_input_size &#x3D; l.c * l.w * l.h;</span><br><span class="line">                    memset(state.workspace, 0, re_packed_input_size * sizeof(float));</span><br><span class="line"></span><br><span class="line">                    const size_t new_c &#x3D; l.c &#x2F; 32;</span><br><span class="line">                    size_t in_re_packed_input_size &#x3D; new_c * l.w * l.h + 1;</span><br><span class="line">                    memset(l.bin_re_packed_input, 0, in_re_packed_input_size * sizeof(uint32_t));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;float *re_packed_input &#x3D; calloc(l.c * l.w * l.h, sizeof(float));</span><br><span class="line">                    &#x2F;&#x2F;uint32_t *bin_re_packed_input &#x3D; calloc(new_c * l.w * l.h + 1, sizeof(uint32_t));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; float32x4 by channel (as in cuDNN)</span><br><span class="line">                    repack_input(state.input, state.workspace, l.w, l.h, l.c);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; 32 x floats -&gt; 1 x uint32_t</span><br><span class="line">                    float_to_bit(state.workspace, (unsigned char *)l.bin_re_packed_input, l.c * l.w * l.h);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;free(re_packed_input);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; slow - convolution the packed inputs and weights: float x 32 by channel (as in cuDNN)</span><br><span class="line">                    &#x2F;&#x2F;convolution_repacked((uint32_t *)bin_re_packed_input, (uint32_t *)l.align_bit_weights, l.output,</span><br><span class="line">                    &#x2F;&#x2F;    l.w, l.h, l.c, l.n, l.size, l.pad, l.new_lda, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; &#x2F;&#x2F; then exit from if()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    im2col_cpu_custom((float *)l.bin_re_packed_input, new_c, l.h, l.w, l.size, l.stride, l.pad, state.workspace);</span><br><span class="line">                    &#x2F;&#x2F;im2col_cpu((float *)bin_re_packed_input, new_c, l.h, l.w, l.size, l.stride, l.pad, b);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;free(bin_re_packed_input);</span><br><span class="line"></span><br><span class="line">                    int new_k &#x3D; l.size*l.size*l.c &#x2F; 32;</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; good for (l.c &#x3D;&#x3D; 64)</span><br><span class="line">                    &#x2F;&#x2F;gemm_nn_bin_32bit_packed(m, n, new_k, 1,</span><br><span class="line">                    &#x2F;&#x2F;    l.align_bit_weights, l.new_lda&#x2F;32,</span><br><span class="line">                    &#x2F;&#x2F;    b, n,</span><br><span class="line">                    &#x2F;&#x2F;    c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; &#x2F;&#x2F; then exit from if()</span><br><span class="line"></span><br><span class="line">                    transpose_uint32((uint32_t *)state.workspace, (uint32_t*)l.t_bit_input, new_k, n, n, new_ldb);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; the main GEMM function</span><br><span class="line">                    gemm_nn_custom_bin_mean_transposed(m, n, k, 1, (unsigned char*)l.align_bit_weights, new_ldb, (unsigned char*)l.t_bit_input, new_ldb, c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; &#x2F;&#x2F; alternative GEMM</span><br><span class="line">                    &#x2F;&#x2F;gemm_nn_bin_transposed_32bit_packed(m, n, new_k, 1,</span><br><span class="line">                    &#x2F;&#x2F;    l.align_bit_weights, l.new_lda&#x2F;32,</span><br><span class="line">                    &#x2F;&#x2F;    t_bit_input, new_ldb &#x2F; 32,</span><br><span class="line">                    &#x2F;&#x2F;    c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;free(t_bit_input);</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">                else</span><br><span class="line">                &#123; &#x2F;&#x2F; else (l.c % 32 !&#x3D; 0)</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;--------------------------------------------------------</span><br><span class="line">                    &#x2F;&#x2F;printf(&quot; l.index &#x3D; %d - old XNOR \n&quot;, l.index);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;im2col_cpu_custom_align(state.input, l.c, l.h, l.w, l.size, l.stride, l.pad, b, l.bit_align);</span><br><span class="line">                    im2col_cpu_custom_bin(state.input, l.c, l.h, l.w, l.size, l.stride, l.pad, state.workspace, l.bit_align);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;size_t output_size &#x3D; l.outputs;</span><br><span class="line">                    &#x2F;&#x2F;float *count_output &#x3D; calloc(output_size, sizeof(float));</span><br><span class="line">                    &#x2F;&#x2F;size_t bit_output_size &#x3D; output_size &#x2F; 8 + 1;</span><br><span class="line">                    &#x2F;&#x2F;char *bit_output &#x3D; calloc(bit_output_size, sizeof(char));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;size_t intput_size &#x3D; n * k; &#x2F;&#x2F; (out_h*out_w) X (l.size*l.size*l.c) : after im2col()</span><br><span class="line">                    &#x2F;&#x2F;size_t bit_input_size &#x3D; intput_size &#x2F; 8 + 1;</span><br><span class="line">                    &#x2F;&#x2F;char *bit_input &#x3D; calloc(bit_input_size, sizeof(char));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;size_t weights_size &#x3D; k * m; &#x2F;&#x2F;l.size*l.size*l.c*l.n; &#x2F;&#x2F; l.nweights</span><br><span class="line">                    &#x2F;&#x2F;size_t bit_weights_size &#x3D; weights_size &#x2F; 8 + 1;</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;char *bit_weights &#x3D; calloc(bit_weights_size, sizeof(char));</span><br><span class="line">                    &#x2F;&#x2F;float *mean_arr &#x3D; calloc(l.n, sizeof(float));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; transpose B from NxK to KxN (x-axis (ldb &#x3D; l.size*l.size*l.c) - should be multiple of 8 bits)</span><br><span class="line">                    &#123;</span><br><span class="line">                        &#x2F;&#x2F;size_t ldb_align &#x3D; 256; &#x2F;&#x2F; 256 bit for AVX2</span><br><span class="line">                        int ldb_align &#x3D; l.lda_align;</span><br><span class="line">                        size_t new_ldb &#x3D; k + (ldb_align - k%ldb_align);</span><br><span class="line">                        size_t t_intput_size &#x3D; binary_transpose_align_input(k, n, state.workspace, &amp;l.t_bit_input, ldb_align, l.bit_align);</span><br><span class="line"></span><br><span class="line">                        &#x2F;&#x2F; 5x times faster than gemm()-float32</span><br><span class="line">                        gemm_nn_custom_bin_mean_transposed(m, n, k, 1, (unsigned char*)l.align_bit_weights, new_ldb, (unsigned char*)l.t_bit_input, new_ldb, c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                        &#x2F;&#x2F;gemm_nn_custom_bin_mean_transposed(m, n, k, 1, bit_weights, k, t_bit_input, new_ldb, c, n, mean_arr);</span><br><span class="line"></span><br><span class="line">                        &#x2F;&#x2F;free(t_input);</span><br><span class="line">                        &#x2F;&#x2F;free(t_bit_input);</span><br><span class="line">                        &#x2F;&#x2F;&#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                add_bias(l.output, l.biases, l.batch, l.n, out_h*out_w);</span><br><span class="line"></span><br><span class="line">                &#x2F;&#x2F;activate_array(l.output, m*n*l.batch, l.activation);</span><br><span class="line">                if (l.activation &#x3D;&#x3D; SWISH) activate_array_swish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; MISH) activate_array_mish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; NORM_CHAN) activate_array_normalize_channels(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 0);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX_MAXVAL) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 1);</span><br><span class="line">                else activate_array_cpu_custom(l.output, m*n*l.batch, l.activation);</span><br><span class="line">                return;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                &#x2F;&#x2F;printf(&quot; l.index &#x3D; %d - FP32 \n&quot;, l.index);</span><br><span class="line">				&#x2F;&#x2F; 由于有分组卷积，所以获取属于当前组的输入im并按一定存储规则排列的数组b，</span><br><span class="line">				&#x2F;&#x2F; 以方便、高效地进行矩阵（卷积）计算，详细查看该函数注释（比较复杂）</span><br><span class="line">				&#x2F;&#x2F; 这里的im实际上只加载了一张图片的数据</span><br><span class="line">				&#x2F;&#x2F; 关于im2col的原理我会讲</span><br><span class="line">                float *im &#x3D; state.input + (i*l.groups + j)*(l.c &#x2F; l.groups)*l.h*l.w;</span><br><span class="line">				&#x2F;&#x2F; 如果这里卷积核尺寸为1，是不需要改变内存排布方式</span><br><span class="line">                if (l.size &#x3D;&#x3D; 1) &#123;</span><br><span class="line">                    b &#x3D; im;</span><br><span class="line">                &#125;</span><br><span class="line">                else &#123;</span><br><span class="line">                    &#x2F;&#x2F;im2col_cpu(im, l.c &#x2F; l.groups, l.h, l.w, l.size, l.stride, l.pad, b);</span><br><span class="line">					&#x2F;&#x2F; 将多通道二维图像im变成按一定存储规则排列的数组b，</span><br><span class="line">					&#x2F;&#x2F; 以方便、高效地进行矩阵（卷积）计算，详细查看该函数注释（比较复杂）</span><br><span class="line">					&#x2F;&#x2F; 进行重排，l.c&#x2F;groups为每张图片的通道数分组，l.h为每张图片的高度，l.w为每张图片的宽度，l.size为卷积核尺寸，l.stride为步长</span><br><span class="line">					&#x2F;&#x2F; 得到的b为一张图片重排后的结果，也是按行存储的一维数组（共有l.c&#x2F;l.groups*l.size*l.size行，l.out_w*l.out_h列）</span><br><span class="line">                    im2col_cpu_ext(im,   &#x2F;&#x2F; input</span><br><span class="line">                        l.c &#x2F; l.groups,     &#x2F;&#x2F; input channels</span><br><span class="line">                        l.h, l.w,           &#x2F;&#x2F; input size (h, w)</span><br><span class="line">                        l.size, l.size,     &#x2F;&#x2F; kernel size (h, w)</span><br><span class="line">                        l.pad, l.pad,       &#x2F;&#x2F; padding (h, w)</span><br><span class="line">                        l.stride_y, l.stride_x, &#x2F;&#x2F; stride (h, w)</span><br><span class="line">                        l.dilation, l.dilation, &#x2F;&#x2F; dilation (h, w)</span><br><span class="line">                        b);                 &#x2F;&#x2F; output</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">				&#x2F;&#x2F; 此处在im2col_cpu操作基础上，利用矩阵乘法c&#x3D;alpha*a*b+beta*c完成对图像卷积的操作</span><br><span class="line">				&#x2F;&#x2F; 0,0表示不对输入a,b进行转置，</span><br><span class="line">				&#x2F;&#x2F; m是输入a,c的行数，具体含义为每个卷积核的个数，</span><br><span class="line">				&#x2F;&#x2F; n是输入b,c的列数，具体含义为每个输出特征图的元素个数(out_h*out_w)，</span><br><span class="line">				&#x2F;&#x2F; k是输入a的列数也是b的行数，具体含义为卷积核元素个数乘以输入图像的通道数除以分组数（l.size*l.size*l.c&#x2F;l.groups），</span><br><span class="line">				&#x2F;&#x2F; a,b,c即为三个参与运算的矩阵（用一维数组存储）,alpha&#x3D;beta&#x3D;1为常系数，</span><br><span class="line">				&#x2F;&#x2F; a为所有卷积核集合,元素个数为l.n*l.c&#x2F;l.groups*l.size*l.size，按行存储，共有l*n行，l.c&#x2F;l.groups*l.size*l.size列，</span><br><span class="line">				&#x2F;&#x2F; 即a中每行代表一个可以作用在3通道上的卷积核，</span><br><span class="line">				&#x2F;&#x2F; b为一张输入图像经过im2col_cpu重排后的图像数据（共有l.c&#x2F;l.group*l.size*l.size行，l.out_w*l.out_h列），</span><br><span class="line">				&#x2F;&#x2F; c为gemm()计算得到的值，包含一张输入图片得到的所有输出特征图（每个卷积核得到一张特征图），c中一行代表一张特征图，</span><br><span class="line">				&#x2F;&#x2F; 各特征图铺排开成一行后，再将所有特征图并成一大行，存储在c中，因此c可视作有l.n行，l.out_h*l.out_w列。</span><br><span class="line">				&#x2F;&#x2F; 详细查看该函数注释（比较复杂）</span><br><span class="line">                gemm(0, 0, m, n, k, 1, a, k, b, n, 1, c, n);</span><br><span class="line">                &#x2F;&#x2F; bit-count to float</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F;c +&#x3D; n*m;</span><br><span class="line">            &#x2F;&#x2F;state.input +&#x3D; l.c*l.h*l.w;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F; 如果卷积层使用了BatchNorm，那么执行forward_batchnorm，如果没有，则添加偏置</span><br><span class="line">    if(l.batch_normalize)&#123;</span><br><span class="line">        forward_batchnorm_layer(l, state);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        add_bias(l.output, l.biases, l.batch, l.n, out_h*out_w);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;activate_array(l.output, m*n*l.batch, l.activation);</span><br><span class="line">	&#x2F;&#x2F; 使用不同的激活函数</span><br><span class="line">    if (l.activation &#x3D;&#x3D; SWISH) activate_array_swish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; MISH) activate_array_mish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN) activate_array_normalize_channels(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 0);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX_MAXVAL) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 1);</span><br><span class="line">    else activate_array_cpu_custom(l.output, l.outputs*l.batch, l.activation);</span><br><span class="line">	&#x2F;&#x2F; 二值网络，前向传播结束之后转回float</span><br><span class="line">    if(l.binary || l.xnor) swap_binary(&amp;l);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;visualize_convolutional_layer(l, &quot;conv_visual&quot;, NULL);</span><br><span class="line">    &#x2F;&#x2F;wait_until_press_key_cv();</span><br><span class="line">	&#x2F;&#x2F; 暂时不懂</span><br><span class="line">    if(l.assisted_excitation &amp;&amp; state.train) assisted_excitation_forward(l, state);</span><br><span class="line">	&#x2F;&#x2F; 暂时不懂</span><br><span class="line">    if (l.antialiasing) &#123;</span><br><span class="line">        network_state s &#x3D; &#123; 0 &#125;;</span><br><span class="line">        s.train &#x3D; state.train;</span><br><span class="line">        s.workspace &#x3D; state.workspace;</span><br><span class="line">        s.net &#x3D; state.net;</span><br><span class="line">        s.input &#x3D; l.output;</span><br><span class="line">        forward_convolutional_layer(*(l.input_layer), s);</span><br><span class="line">        &#x2F;&#x2F;simple_copy_ongpu(l.outputs*l.batch, l.output, l.input_antialiasing);</span><br><span class="line">        memcpy(l.output, l.input_layer-&gt;output, l.input_layer-&gt;outputs * l.input_layer-&gt;batch * sizeof(float));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="im2col解析"><a href="#im2col解析" class="headerlink" title="im2col解析"></a>im2col解析</h2><p>从上面的代码可以知道，卷积层的前向传播核心点是im2col操作还有sgemm矩阵计算方法对使用im2col进行重排后的数据进行计算。现在来解析一下im2col算法，sgemm算法就是im2col运行后直接调用即可，就不细讲了。</p>
<p>这里考虑到结合图片更容易理解im2col的思想，我利用CSDN Tiger-Gao博主的图描述一下。首先，我们把一个单通道的长宽均为4的图片通过im2col重新排布后会变成什么样呢？看下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/649.webp" alt></p>
<p>来具体看一下变化过程：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/650.webp" alt></p>
<p>这是单通道的变化过程，那么多通道的呢？首先来看原图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/651.png" alt></p>
<p>多通道的im2col的过程，是首先im2col第一通道，然后再im2col第二通道，最后im2col第三通道。各通道im2col的数据在内存中也是连续存储的。看下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/652.webp" alt></p>
<p>这是原图经过im2col的变化，那么kernel呢？看原图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/653.webp" alt></p>
<p>kernel的通道数据在内存中也是连续存储的。所以上面的kernel图像经过im2col算法后可以表示为下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/654.webp" alt></p>
<p>那么我们是如何得到前向传播的结果呢？在DarkNet中和Caffe的实现方式一样，都是Kernel*Img，即是在矩阵乘法中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">M&#x3D;1 </span><br><span class="line">N&#x3D;output_h * output_w</span><br><span class="line">K&#x3D;input_channels * kernel_h * kernel_w</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/655.webp" alt></p>
<p>图像数据是连续存储，因此输出图像也可以如下图所示【output_h x output_w】=【2 x 2】：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/656.png" alt></p>
<p>对于多通道图像过程类似：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/657.webp" alt></p>
<p>同样，多个输出通道图像的数据是连续存储，因此输出图像也可以如下图所示【output_channels x output_h x output_w】=【3 x 2 x 2】</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/658.webp" alt></p>
<p>im2col算法的实现在<code>src/im2col.c</code>中，即<code>im2col_cpu</code>函数。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 从输入的多通道数组im（存储图像数据）中获取指定行，列，通道数处的元素值</span><br><span class="line">** im:  函数的输入，所有的数据存成一个一维数组</span><br><span class="line">** height: 每一个通道的高度(即是输入图像的真正高度，补0之前)</span><br><span class="line">** width: 每一个通道的宽度(即是输入图像的真正宽度，补0之前)</span><br><span class="line">** channles：输入通道数</span><br><span class="line">** row: 要提取的元素所在的行(padding之后的行数)</span><br><span class="line">** col: 要提取的元素所在的列(padding之后的列数)</span><br><span class="line">** channel: 要提取的元素所在的通道</span><br><span class="line">** pad: 图像上下左右补0的个数，四周是一样的</span><br><span class="line">** 返回im中channel通道，row-pad行,col-pad列处的元素值</span><br><span class="line">** 在im中并没有存储补0的元素值，因此height，width都是没有补0时输入图像真正的高、宽；</span><br><span class="line">** 而row与col则是补0之后，元素所在的行列，因此，要准确获取在im中的元素值，首先需要</span><br><span class="line">** 减去pad以获取在im中真实的行列数</span><br><span class="line">*&#x2F;</span><br><span class="line">float im2col_get_pixel(float *im, int height, int width, int channels,</span><br><span class="line">                        int row, int col, int channel, int pad)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F;减去补0长度，获取像素真实的行列数</span><br><span class="line">    row -&#x3D; pad;</span><br><span class="line">    col -&#x3D; pad;</span><br><span class="line">	&#x2F;&#x2F; 如果行列数&lt;0，或者超过height&#x2F;width，则返回0(刚好是补0的效果)</span><br><span class="line">    if (row &lt; 0 || col &lt; 0 ||</span><br><span class="line">        row &gt;&#x3D; height || col &gt;&#x3D; width) return 0;</span><br><span class="line">	&#x2F;&#x2F; im存储多通道二维图像的数据格式为: 各个通道所有的所有行并成1行，再多通道依次并成一行</span><br><span class="line">    &#x2F;&#x2F; 因此width*height*channel首先移位到所在通道的起点位置，再加上width*row移位到所在指定</span><br><span class="line">    &#x2F;&#x2F; 通道行，再加上col移位到所在列</span><br><span class="line">    return im[col + width*(row + height*channel)];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;From Berkeley Vision&#39;s Caffe!</span><br><span class="line">&#x2F;&#x2F;https:&#x2F;&#x2F;github.com&#x2F;BVLC&#x2F;caffe&#x2F;blob&#x2F;master&#x2F;LICENSE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">** 将输入图片转为便于计算的数组格式</span><br><span class="line">** data_im: 输入图像</span><br><span class="line">** height: 输入图像的高度(行)</span><br><span class="line">** width: 输入图像的宽度(列)</span><br><span class="line">** ksize: 卷积核尺寸</span><br><span class="line">** stride: 卷积核跨度</span><br><span class="line">** pad: 四周补0的长度</span><br><span class="line">** data_col: 相当于输出，为进行格式重排后的输入图像数据</span><br><span class="line">** 输出data_col的元素个数与data_im个数不相等，一般比data_im个数多，因为stride较小，各个卷积核之间有很多重叠，</span><br><span class="line">** 实际data_col中的元素个数为channels*ksize*ksize*height_col*width_col，其中channels为data_im的通道数，</span><br><span class="line">** ksize为卷积核大小，height_col和width_col如下所注。data_col的还是按行排列，只是行数为channels*ksize*ksize,</span><br><span class="line">** 列数为height_col*width_col，即一张特征图总的元素个数，每整列包含与某个位置处的卷积核计算的所有通道上的像素，</span><br><span class="line">** （比如输入图像通道数为3,卷积核尺寸为3*3，则共有27行，每列有27个元素），不同列对应卷积核在图像上的不同位置做卷积</span><br><span class="line">*&#x2F;</span><br><span class="line">void im2col_cpu(float* data_im,</span><br><span class="line">     int channels,  int height,  int width,</span><br><span class="line">     int ksize,  int stride, int pad, float* data_col)</span><br><span class="line">&#123;</span><br><span class="line">    int c,h,w;</span><br><span class="line">	&#x2F;&#x2F; 计算该层神经网络的输出图像尺寸（其实没有必要再次计算的，因为在构建卷积层时，make_convolutional_layer()函数</span><br><span class="line">    &#x2F;&#x2F; 已经调用convolutional_out_width()，convolutional_out_height()函数求取了这两个参数，</span><br><span class="line">    &#x2F;&#x2F; 此处直接使用l.out_h,l.out_w即可，函数参数只要传入该层网络指针就可了，没必要弄这么多参数）</span><br><span class="line">    int height_col &#x3D; (height + 2*pad - ksize) &#x2F; stride + 1;</span><br><span class="line">    int width_col &#x3D; (width + 2*pad - ksize) &#x2F; stride + 1;</span><br><span class="line">	&#x2F;&#x2F; 卷积核大小：ksize*ksize是一个卷积核的大小，之所以乘以通道数channels，是因为输入图像有多通道，每个卷积核在做卷积时，</span><br><span class="line">    &#x2F;&#x2F; 是同时对同一位置多通道的图像进行卷积运算，这里为了实现这一目的，将三个通道将三通道上的卷积核并在一起以便进行计算，因此卷积核</span><br><span class="line">    &#x2F;&#x2F; 实际上并不是二维的，而是三维的，比如对于3通道图像，卷积核尺寸为3*3，该卷积核将同时作用于三通道图像上，这样并起来就得</span><br><span class="line">    &#x2F;&#x2F; 到含有27个元素的卷积核，且这27个元素都是独立的需要训练的参数。所以在计算训练参数个数时，一定要注意每一个卷积核的实际</span><br><span class="line">    &#x2F;&#x2F; 训练参数需要乘以输入通道数。</span><br><span class="line">    int channels_col &#x3D; channels * ksize * ksize;</span><br><span class="line">	&#x2F;&#x2F; 外循环次数为一个卷积核的尺寸数，循环次数即为最终得到的data_col的总行数</span><br><span class="line">    for (c &#x3D; 0; c &lt; channels_col; ++c) &#123;</span><br><span class="line">		&#x2F;&#x2F; 列偏移，卷积核是一个二维矩阵，并按行存储在一维数组中，利用求余运算获取对应在卷积核中的列数，比如对于</span><br><span class="line">        &#x2F;&#x2F; 3*3的卷积核（3通道），当c&#x3D;0时，显然在第一列，当c&#x3D;5时，显然在第2列，当c&#x3D;9时，在第二通道上的卷积核的第一列，</span><br><span class="line">        &#x2F;&#x2F; 当c&#x3D;26时，在第三列（第三通道上）</span><br><span class="line">        int w_offset &#x3D; c % ksize;</span><br><span class="line">		&#x2F;&#x2F; 行偏移，卷积核是一个二维的矩阵，且是按行（卷积核所有行并成一行）存储在一维数组中的，</span><br><span class="line">        &#x2F;&#x2F; 比如对于3*3的卷积核，处理3通道的图像，那么一个卷积核具有27个元素，每9个元素对应一个通道上的卷积核（互为一样），</span><br><span class="line">        &#x2F;&#x2F; 每当c为3的倍数，就意味着卷积核换了一行，h_offset取值为0,1,2，对应3*3卷积核中的第1, 2, 3行</span><br><span class="line">        int h_offset &#x3D; (c &#x2F; ksize) % ksize;</span><br><span class="line">		&#x2F;&#x2F; 通道偏移，channels_col是多通道的卷积核并在一起的，比如对于3通道，3*3卷积核，每过9个元素就要换一通道数，</span><br><span class="line">        &#x2F;&#x2F; 当c&#x3D;0~8时，c_im&#x3D;0;c&#x3D;9~17时，c_im&#x3D;1;c&#x3D;18~26时，c_im&#x3D;2</span><br><span class="line">        int c_im &#x3D; c &#x2F; ksize &#x2F; ksize;</span><br><span class="line">		&#x2F;&#x2F; 中循环次数等于该层输出图像行数height_col，说明data_col中的每一行存储了一张特征图，这张特征图又是按行存储在data_col中的某行中</span><br><span class="line">        for (h &#x3D; 0; h &lt; height_col; ++h) &#123;</span><br><span class="line">			&#x2F;&#x2F; 内循环等于该层输出图像列数width_col，说明最终得到的data_col总有channels_col行，height_col*width_col列</span><br><span class="line">            for (w &#x3D; 0; w &lt; width_col; ++w) &#123;</span><br><span class="line">				&#x2F;&#x2F; 由上面可知，对于3*3的卷积核，h_offset取值为0,1,2,当h_offset&#x3D;0时，会提取出所有与卷积核第一行元素进行运算的像素，</span><br><span class="line">                &#x2F;&#x2F; 依次类推；加上h*stride是对卷积核进行行移位操作，比如卷积核从图像(0,0)位置开始做卷积，那么最先开始涉及(0,0)~(3,3)</span><br><span class="line">                &#x2F;&#x2F; 之间的像素值，若stride&#x3D;2，那么卷积核进行一次行移位时，下一行的卷积操作是从元素(2,0)（2为图像行号，0为列号）开始</span><br><span class="line">                int im_row &#x3D; h_offset + h * stride;</span><br><span class="line">				&#x2F;&#x2F; 对于3*3的卷积核，w_offset取值也为0,1,2，当w_offset取1时，会提取出所有与卷积核中第2列元素进行运算的像素，</span><br><span class="line">                &#x2F;&#x2F; 实际在做卷积操作时，卷积核对图像逐行扫描做卷积，加上w*stride就是为了做列移位，</span><br><span class="line">                &#x2F;&#x2F; 比如前一次卷积其实像素元素为(0,0)，若stride&#x3D;2,那么下次卷积元素起始像素位置为(0,2)（0为行号，2为列号）</span><br><span class="line">                int im_col &#x3D; w_offset + w * stride;</span><br><span class="line">				&#x2F;&#x2F; col_index为重排后图像中的像素索引，等于c * height_col * width_col + h * width_col +w（还是按行存储，所有通道再并成一行），</span><br><span class="line">                &#x2F;&#x2F; 对应第c通道，h行，w列的元素</span><br><span class="line">                int col_index &#x3D; (c * height_col + h) * width_col + w;</span><br><span class="line">				&#x2F;&#x2F; im2col_get_pixel函数获取输入图像data_im中第c_im通道，im_row,im_col的像素值并赋值给重排后的图像，</span><br><span class="line">                &#x2F;&#x2F; height和width为输入图像data_im的真实高、宽，pad为四周补0的长度（注意im_row,im_col是补0之后的行列号，</span><br><span class="line">                &#x2F;&#x2F; 不是真实输入图像中的行列号，因此需要减去pad获取真实的行列号）</span><br><span class="line">                data_col[col_index] &#x3D; im2col_get_pixel(data_im, height, width, channels,</span><br><span class="line">                        im_row, im_col, c_im, pad);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet网络的前向和反向传播介绍以及layer的详细解析</title>
    <url>/2020/02/22/AlexeyAB-DarkNet%E7%BD%91%E7%BB%9C%E7%9A%84%E5%89%8D%E5%90%91%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8Alayer%E7%9A%84%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>前面我们已经成功的获取了目标检测的网络结构(<code>cfg</code>文件的内容)，并将网络保存在了一个<code>network</code>结构体中，然后我们还分析了数据加载方式。现在数据和网络结构都有了，接下来就是开始训练/测试的过程了，这个过程主要调用的是<code>network</code>的前向传播和反向传播函数，而<code>network</code>的前向传播和反向传播又可以细分为每一个<code>layer</code>的前向传播和反向传播，今天我们来看一下网络的前向传播和反向传播以及<code>layer</code>是如何定义的。</p>
<h2 id="网络的前向传播和反向传播"><a href="#网络的前向传播和反向传播" class="headerlink" title="网络的前向传播和反向传播"></a>网络的前向传播和反向传播</h2><p>网络的前向传播函数在<code>src/network.c</code>中实现，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 前向计算网络net每一层的输出</span><br><span class="line">** state用来标记当前网络的状态，</span><br><span class="line">** 遍历net的每一层网络，从第0层到最后一层，逐层计算每层的输出</span><br><span class="line">*&#x2F;</span><br><span class="line">void forward_network(network net, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F; 网络的工作空间, 指的是所有层中占用运算空间最大的那个层的 workspace_size,</span><br><span class="line">    &#x2F;&#x2F; 因为实际上在 GPU 或 CPU 中某个时刻只有一个层在做前向或反向运算</span><br><span class="line">    state.workspace &#x3D; net.workspace;</span><br><span class="line">    int i;</span><br><span class="line">	&#x2F;&#x2F; 遍历所有层，从第一层到最后一层，逐层进行前向传播，网络共有net.n层</span><br><span class="line">    for(i &#x3D; 0; i &lt; net.n; ++i)&#123;</span><br><span class="line">		&#x2F;&#x2F; 当前正在进行第i层的处理</span><br><span class="line">        state.index &#x3D; i;</span><br><span class="line">		&#x2F;&#x2F; 获取当前层</span><br><span class="line">        layer l &#x3D; net.layers[i];</span><br><span class="line">		&#x2F;&#x2F; 如果当前层的l.delta已经动态分配了内存，则调用fill_cpu()函数将其所有元素初始化为0</span><br><span class="line">        if(l.delta &amp;&amp; state.train)&#123;</span><br><span class="line">			&#x2F;&#x2F; 第一个参数为l.delta的元素个数，第二个参数为初始化值，为0</span><br><span class="line">            scal_cpu(l.outputs * l.batch, 0, l.delta, 1);</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;double time &#x3D; get_time_point();</span><br><span class="line">		&#x2F;&#x2F; 前向传播: 完成当前层前向推理</span><br><span class="line">        l.forward(l, state);</span><br><span class="line">		&#x2F;&#x2F; 完成某一层的推理时，置网络的输入为当前层的输出（这将成为下一层网络的输入），注意此处更改的是state，而非原始的net</span><br><span class="line">        &#x2F;&#x2F;printf(&quot;%d - Predicted in %lf milli-seconds.\n&quot;, i, ((double)get_time_point() - time) &#x2F; 1000);</span><br><span class="line">        state.input &#x3D; l.output;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>网络的反向传播函数也在<code>src/network.c</code>中实现，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 反向计算网络net每一层的梯度图，并进而计算每一层的权重、偏置更新值，最后完成每一层权重与偏置更新</span><br><span class="line">** 流程: 遍历net的每一层网络，从最后一层到第一层(此处所指的第一层不是指输入层，而是与输入层直接相连的第一层隐含层)进行反向传播</span><br><span class="line">*&#x2F;</span><br><span class="line">void backward_network(network net, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">	&#x2F;&#x2F; 在进行反向传播之前先保存一下原来的net信息</span><br><span class="line">    float *original_input &#x3D; state.input;</span><br><span class="line">    float *original_delta &#x3D; state.delta;</span><br><span class="line">    state.workspace &#x3D; net.workspace;</span><br><span class="line">    for(i &#x3D; net.n-1; i &gt;&#x3D; 0; --i)&#123;</span><br><span class="line">		&#x2F;&#x2F; 标志参数，当前网络的活跃层</span><br><span class="line">        state.index &#x3D; i;</span><br><span class="line">		&#x2F;&#x2F; i &#x3D; 0时，也即已经到了网络的第1层（或者说第0层，看个人习惯了～）了，</span><br><span class="line">        &#x2F;&#x2F; 就是直接与输入层相连的第一层隐含层（注意不是输入层，我理解的输入层就是指输入的图像数据，</span><br><span class="line">        &#x2F;&#x2F; 严格来说，输入层不算一层网络，因为输入层没有训练参数，也没有激活函数），这个时候，不需要else中的赋值，1）对于第1层来说，其前面已经没有网络层了（输入层不算），</span><br><span class="line">        &#x2F;&#x2F; 因此没有必要再计算前一层的参数，故没有必要在获取上一层；2）第一层的输入就是图像输入，也即整个net最原始的输入，在开始进行反向传播之前，已经用original_*变量保存了</span><br><span class="line">        &#x2F;&#x2F; 最为原始的net，所以net.input就是第一层的输入，不需要获取上一层的输出作为当前层的输入；3）同1），第一层之前已经没有层了，</span><br><span class="line">        &#x2F;&#x2F; 也就不需要计算上一层的delta，即不需要再将net.delta链接到prev.delta，此时进入到l.backward()中后，net.delta就是NULL（可以参看darknet.h中关于delta</span><br><span class="line">        &#x2F;&#x2F; 的注释），也就不会再计算上一层的敏感度了（比如卷积神经网络中的backward_convolutional_layer()函数）</span><br><span class="line">        &#x2F;&#x2F; 这几行代码就是给net.input和net.delta赋值</span><br><span class="line">        if(i &#x3D;&#x3D; 0)&#123;</span><br><span class="line">            state.input &#x3D; original_input;</span><br><span class="line">            state.delta &#x3D; original_delta;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            layer prev &#x3D; net.layers[i-1];</span><br><span class="line">            state.input &#x3D; prev.output;</span><br><span class="line">            state.delta &#x3D; prev.delta;</span><br><span class="line">        &#125;</span><br><span class="line">        layer l &#x3D; net.layers[i];</span><br><span class="line">        if (l.stopbackward) break;</span><br><span class="line">        if (l.onlyforward) continue;</span><br><span class="line">		&#x2F;&#x2F; 反向计算第i层的敏感度图、权重及偏置更新值，并更新权重、偏置（同时会计算上一层的敏感度图，</span><br><span class="line">        &#x2F;&#x2F; 存储在net.delta中，但是还差一个环节：乘上上一层输出对加权输入的导数，也即上一层激活函数对加权输入的导数）</span><br><span class="line">        l.backward(l, state);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="layer的解释"><a href="#layer的解释" class="headerlink" title="layer的解释"></a>layer的解释</h2><p>在将具体某个层如卷积层的前向传播和反向传播之前，需要先来看一下<code>layer</code>是怎么定义的，因为网络的前向传播和反向传播实际上就是各个网络层(<code>layer</code>)的前向传播和反向传播，这部分加好注释的代码（在<code>src/darknet.h</code>中）如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;定义layer</span><br><span class="line">struct layer &#123;</span><br><span class="line">    LAYER_TYPE type; &#x2F;&#x2F; 网络层的类型，枚举类型，取值比如DROPOUT,CONVOLUTIONAL,MAXPOOL分别表示dropout层，卷积层，最大池化层，可参见LAYER_TYPE枚举类型的定义</span><br><span class="line">    ACTIVATION activation; &#x2F;&#x2F;激活函数类型，枚举类型</span><br><span class="line">    COST_TYPE cost_type; &#x2F;&#x2F;损失函数类型，枚举类型</span><br><span class="line">    void(*forward)   (struct layer, struct network_state);</span><br><span class="line">    void(*backward)  (struct layer, struct network_state);</span><br><span class="line">    void(*update)    (struct layer, int, float, float, float);</span><br><span class="line">    void(*forward_gpu)   (struct layer, struct network_state);</span><br><span class="line">    void(*backward_gpu)  (struct layer, struct network_state);</span><br><span class="line">    void(*update_gpu)    (struct layer, int, float, float, float);</span><br><span class="line">    layer *share_layer;</span><br><span class="line">    int train;</span><br><span class="line">    int avgpool;</span><br><span class="line">    int batch_normalize; &#x2F;&#x2F; 是否进行BN，如果进行BN，则值为1</span><br><span class="line">    int shortcut;</span><br><span class="line">    int batch; &#x2F;&#x2F; 一个batch中含有的图片张数，等于net.batch，详细可以参考network.h中的注释，一般在构建具体网络层时赋值（比如make_maxpool_layer()中）</span><br><span class="line">    int forced;</span><br><span class="line">    int flipped;</span><br><span class="line">    int inputs; &#x2F;&#x2F; 一张输入图片所含的元素个数（一般在各网络层构建函数中赋值，比如make_connected_layer()），第一层的值等于l.h*l.w*l.c，</span><br><span class="line">                &#x2F;&#x2F; 之后的每一层都是由上一层的输出自动推算得到的（参见parse_network_cfg()，在构建每一层后，会更新params.inputs为上一层的l.outputs）</span><br><span class="line">    int outputs; &#x2F;&#x2F; 该层对应一张输入图片的输出元素个数（一般在各网络层构建函数中赋值，比如make_connected_layer()）</span><br><span class="line">                 &#x2F;&#x2F; 对于一些网络，可由输入图片的尺寸及相关参数计算出，比如卷积层，可以通过输入尺寸以及步长、核大小计算出；</span><br><span class="line">                 &#x2F;&#x2F; 对于另一些尺寸，则需要通过网络配置文件指定，如未指定，取默认值1，比如全连接层（见parse_connected()函数）</span><br><span class="line">    int nweights;</span><br><span class="line">    int nbiases;</span><br><span class="line">    int extra;</span><br><span class="line">    int truths;  &#x2F;&#x2F; &lt; 根据region_layer.c判断，这个变量表示一张图片含有的真实值的个数，对于检测模型来说，一个真实的标签含有5个值，</span><br><span class="line">                &#x2F;&#x2F; 包括类型对应的编号以及定位矩形框用到的w,h,x,y四个参数，且在darknet中固定每张图片最大处理30个矩形框，（可查看max_boxes参数），</span><br><span class="line">                &#x2F;&#x2F; 因此，在region_layer.c的make_region_layer()函数中赋值为30*5.</span><br><span class="line">    int h, w, c;  &#x2F;&#x2F; 该层输入的图片的宽，高，通道数（一般在各网络层构建函数中赋值，比如make_connected_layer()）</span><br><span class="line">    int out_h, out_w, out_c;&#x2F;&#x2F; 该层输出图片的高、宽、通道数（一般在各网络层构建函数中赋值，比如make_connected_layer()）</span><br><span class="line">    int n; &#x2F;&#x2F; 对于卷积层，该参数表示卷积核个数，等于out_c，其值由网络配置文件指定；对于region_layer层，该参数等于配置文件中的num值</span><br><span class="line">           &#x2F;&#x2F; (该参数通过make_region_layer()函数赋值，在parser.c中调用的make_region_layer()函数)，</span><br><span class="line">           &#x2F;&#x2F; 可以在darknet&#x2F;cfg文件夹下执行命令：grep num *.cfg便可以搜索出所有设置了num参数的网络，这里面包括yolo.cfg等，其值有</span><br><span class="line">           &#x2F;&#x2F; 设定为3,5,2的，该参数就是Yolo论文中的B，也就是一个cell中预测多少个box。</span><br><span class="line">    int max_boxes; &#x2F;&#x2F; 每张图片最多含有的标签矩形框数（参看：data.c中的load_data_detection()，其输入参数boxes就是指这个参数），</span><br><span class="line">                   &#x2F;&#x2F; 什么意思呢？就是每张图片中最多打了max_boxes个标签物体，模型预测过程中，可能会预测出很多的物体，但实际上，</span><br><span class="line">                   &#x2F;&#x2F; 图片中打上标签的真正存在的物体最多就max_boxes个，预测多出来的肯定存在false positive，需要滤出与筛选，</span><br><span class="line">                   &#x2F;&#x2F; 可参看region_layer.c中forward_region_layer()函数的第二个for循环中的注释</span><br><span class="line">    int groups; &#x2F;&#x2F; 应该是控制组卷积的组数，类似于caffe的group参数</span><br><span class="line">    int group_id;</span><br><span class="line">    int size; &#x2F;&#x2F; 核尺寸（比如卷积核，池化核等）</span><br><span class="line">    int side;</span><br><span class="line">    int stride; &#x2F;&#x2F; 滑动步长，如卷积核的滑动步长</span><br><span class="line">    int stride_x;</span><br><span class="line">    int stride_y;</span><br><span class="line">    int dilation; &#x2F;&#x2F;空洞卷积参数</span><br><span class="line">    int antialiasing;</span><br><span class="line">    int maxpool_depth;</span><br><span class="line">    int out_channels; &#x2F;&#x2F; 输出通道数</span><br><span class="line">    int reverse;</span><br><span class="line">    int flatten;</span><br><span class="line">    int spatial;</span><br><span class="line">    int pad; &#x2F;&#x2F; 该层对输入数据四周的补0长度（现在发现在卷积层，最大池化层中有用到该参数），一般在构建具体网络层时赋值（比如make_maxpool_layer()中）</span><br><span class="line">    int sqrt;</span><br><span class="line">    int flip;</span><br><span class="line">    int index;</span><br><span class="line">    int scale_wh;</span><br><span class="line">    int binary;</span><br><span class="line">    int xnor;</span><br><span class="line">    int peephole;</span><br><span class="line">    int use_bin_output;</span><br><span class="line">    int keep_delta_gpu;</span><br><span class="line">    int optimized_memory;</span><br><span class="line">    int steps;</span><br><span class="line">    int state_constrain;</span><br><span class="line">    int hidden;</span><br><span class="line">    int truth;</span><br><span class="line">    float smooth;</span><br><span class="line">    float dot;</span><br><span class="line">    int deform;</span><br><span class="line">    int sway;</span><br><span class="line">    int rotate;</span><br><span class="line">    int stretch;</span><br><span class="line">    int stretch_sway;</span><br><span class="line">    float angle;</span><br><span class="line">    float jitter;</span><br><span class="line">    float saturation;</span><br><span class="line">    float exposure;</span><br><span class="line">    float shift;</span><br><span class="line">    float ratio;</span><br><span class="line">    float learning_rate_scale;</span><br><span class="line">    float clip;</span><br><span class="line">    int focal_loss;</span><br><span class="line">    float *classes_multipliers;</span><br><span class="line">    float label_smooth_eps;</span><br><span class="line">    int noloss;</span><br><span class="line">    int softmax;</span><br><span class="line">    int classes; &#x2F;&#x2F; 物体类别种数，一个训练好的网络，只能检测指定所有物体类别中的物体，比如yolo9000.cfg，设置该值为9418，</span><br><span class="line">                 &#x2F;&#x2F; 也就是该网络训练好了之后可以检测9418种物体。该参数由网络配置文件指定。目前在作者给的例子中,</span><br><span class="line">                 &#x2F;&#x2F; 有设置该值的配置文件大都是检测模型，纯识别的网络模型没有设置该值，我想是因为检测模型输出的一般会为各个类别的概率，</span><br><span class="line">                 &#x2F;&#x2F; 所以需要知道这个种类数目，而识别的话，不需要知道某个物体属于这些所有类的具体概率，因此可以不知道。</span><br><span class="line">    int coords; &#x2F;&#x2F; 这个参数一般用在检测模型中，且不是所有层都有这个参数，一般在检测模型最后一层有，比如region_layer层，该参数的含义</span><br><span class="line">                &#x2F;&#x2F; 是定位一个物体所需的参数个数，一般为4个，包括物体所在矩形框中心坐标x,y两个参数以及矩形框长宽w,h两个参数，</span><br><span class="line">                &#x2F;&#x2F; 可以在darknet&#x2F;cfg文件夹下，执行grep coords *.cfg，会搜索出所有使用该参数的模型，并可看到该值都设置为4</span><br><span class="line">    int background;</span><br><span class="line">    int rescore;</span><br><span class="line">    int objectness;</span><br><span class="line">    int does_cost;</span><br><span class="line">    int joint;</span><br><span class="line">    int noadjust;</span><br><span class="line">    int reorg;</span><br><span class="line">    int log;</span><br><span class="line">    int tanh;</span><br><span class="line">    int *mask;</span><br><span class="line">    int total;</span><br><span class="line">    float bflops;</span><br><span class="line"></span><br><span class="line">    int adam;</span><br><span class="line">    float B1;</span><br><span class="line">    float B2;</span><br><span class="line">    float eps;</span><br><span class="line"></span><br><span class="line">    int t;</span><br><span class="line"></span><br><span class="line">    float alpha;</span><br><span class="line">    float beta;</span><br><span class="line">    float kappa;</span><br><span class="line"></span><br><span class="line">    float coord_scale;</span><br><span class="line">    float object_scale;</span><br><span class="line">    float noobject_scale;</span><br><span class="line">    float mask_scale;</span><br><span class="line">    float class_scale;</span><br><span class="line">    int bias_match;</span><br><span class="line">    float random;</span><br><span class="line">    float ignore_thresh;</span><br><span class="line">    float truth_thresh;</span><br><span class="line">    float iou_thresh;</span><br><span class="line">    float thresh;</span><br><span class="line">    float focus;</span><br><span class="line">    int classfix;</span><br><span class="line">    int absolute;</span><br><span class="line">    int assisted_excitation;</span><br><span class="line"></span><br><span class="line">    int onlyforward; &#x2F;&#x2F; 标志参数，当值为1那么当前层只执行前向传播</span><br><span class="line">    int stopbackward;  &#x2F;&#x2F; 标志参数，用来强制停止反向传播过程（值为1则停止反向传播），参看network.c中的backward_network()函数</span><br><span class="line">    int dontload;</span><br><span class="line">    int dontsave;</span><br><span class="line">    int dontloadscales;</span><br><span class="line">    int numload;</span><br><span class="line"></span><br><span class="line">    float temperature; &#x2F;&#x2F; 温度参数，softmax层特有参数，在parse_softmax()函数中赋值，由网络配置文件指定，如果未指定，则使用默认值1（见parse_softmax()）</span><br><span class="line">    float probability; &#x2F;&#x2F; dropout概率，即舍弃概率，相应的1-probability为保留概率（具体的使用可参见forward_dropout_layer()），在make_dropout_layer()中赋值，</span><br><span class="line">						&#x2F;&#x2F; 其值由网络配置文件指定，如果网络配置文件未指定，则取默认值0.5（见parse_dropout()）</span><br><span class="line">    float dropblock_size_rel;</span><br><span class="line">    int dropblock_size_abs;</span><br><span class="line">    int dropblock;</span><br><span class="line">    float scale; &#x2F;&#x2F; 在dropout层中，该变量是一个比例因子，取值为保留概率的倒数（darknet实现用的是inverted dropout），用于缩放输入元素的值</span><br><span class="line">                       &#x2F;&#x2F; （在网上随便搜索关于dropout的博客，都会提到inverted dropout），在make_dropout_layer()函数中赋值</span><br><span class="line"></span><br><span class="line">    char  * cweights;</span><br><span class="line">    int   * indexes; &#x2F;&#x2F; 维度为l.out_h * l.out_w * l.out_c * l.batch，可知包含整个batch输入图片的输出，一般在构建具体网络层时动态分配内存（比如make_maxpool_layer()中）。</span><br><span class="line">                     &#x2F;&#x2F; 目前仅发现其用在在最大池化层中。该变量存储的是索引值，并与当前层所有输出元素一一对应，表示当前层每个输出元素的值是上一层输出中的哪一个元素值（存储的索引值是</span><br><span class="line">                     &#x2F;&#x2F; 在上一层所有输出元素（包含整个batch）中的索引），因为对于最大池化层，每一个输出元素的值实际是上一层输出（也即当前层输入）某个池化区域中的最大元素值，indexes就是记录</span><br><span class="line">                     &#x2F;&#x2F; 这些局部最大元素值在上一层所有输出元素中的总索引。记录这些值有什么用吗？当然有，用于反向传播过程计算上一层敏感度值，详见backward_maxpool_layer()以及forward_maxpool_layer()函数。</span><br><span class="line">    int   * input_layers; &#x2F;&#x2F;这个层有哪些输入层</span><br><span class="line">    int   * input_sizes; &#x2F;&#x2F; 输入层的尺寸</span><br><span class="line">    float **layers_output; &#x2F;&#x2F;产生的一系列输出层</span><br><span class="line">    float **layers_delta;</span><br><span class="line">    WEIGHTS_TYPE_T weights_type;</span><br><span class="line">    WEIGHTS_NORMALIZATION_T weights_normalizion;</span><br><span class="line">	&#x2F;*</span><br><span class="line">     * 这个参数用的不多，仅在region_layer.c中使用，该参数的作用是用于不同数据集间类别编号的转换，更为具体的，</span><br><span class="line">     * 是coco数据集中80类物体编号与联合数据集中9000+物体类别编号之间的转换，可以对比查看data&#x2F;coco.names与</span><br><span class="line">     * data&#x2F;9k.names以及data&#x2F;coco9k.map三个文件（旧版的darknet可能没有，新版的darknet才有coco9k.map这个文件），</span><br><span class="line">     * 可以发现，coco.names中每一个物体类别都可以在9k.names中找到,且coco.names中每个物体类别名称在9k.names</span><br><span class="line">     * 中所在的行数就是coco9k.map中的编号值（减了1,因为在程序数组中编号从0开始），也就是这个map将coco数据集中</span><br><span class="line">     * 的类别编号映射到联和数据集9k中的类别编号（这个9k数据集是一个联和多个数据集的大数集，其名称分类被层级划分，</span><br><span class="line">     * ）（注意两个文件中物体的类别名称大部分都相同，有小部分存在小差异，虽然有差异，但只是两个数据集中使用的名称有所差异而已，</span><br><span class="line">     * 对应的物体是一样的，比如在coco.names中摩托车的名称为motorbike，在联合数据集9k.names，其名称为motorcycle）.</span><br><span class="line">    *&#x2F;</span><br><span class="line">    int   * map;</span><br><span class="line">    int   * counts;</span><br><span class="line">    float ** sums;</span><br><span class="line">	&#x2F;&#x2F; 这个参数目前只发现用在dropout层，用于存储一些列的随机数，这些随机数与dropout层的输入元素一一对应，维度为l.batch*l.inputs（包含整个batch的），在make_dropout_layer()函数中用calloc动态分配内存，</span><br><span class="line">    &#x2F;&#x2F; 并在前向传播函数forward_dropout_layer()函数中逐元素赋值。里面存储的随机数满足0~1均匀分布，干什么用呢？用于决定该输入元素的去留，</span><br><span class="line">    &#x2F;&#x2F; 我们知道dropout层就完成一个事：按照一定概率舍弃输入神经元（所谓舍弃就是置该输入的值为0），rand中存储的值就是如果小于l.probability，则舍弃该输入神经元（详见：forward_dropout_layer()）。</span><br><span class="line">    &#x2F;&#x2F; 为什么要保留这些随机数呢？和最大池化层中的l.indexes类似，在反向传播函数backward_dropout_layer()中用来指示计算上一层的敏感度值，因为dropout舍弃了一些输入，</span><br><span class="line">    &#x2F;&#x2F; 这些输入（dropout层的输入，上一层的输出）对应的敏感度值可以置为0，而那些没有舍弃的输入，才有必要由当前dropout层反向传播过去。</span><br><span class="line">    float * rand;</span><br><span class="line">    float * cost;</span><br><span class="line">    float * state;</span><br><span class="line">    float * prev_state;</span><br><span class="line">    float * forgot_state;</span><br><span class="line">    float * forgot_delta;</span><br><span class="line">    float * state_delta;</span><br><span class="line">    float * combine_cpu;</span><br><span class="line">    float * combine_delta_cpu;</span><br><span class="line"></span><br><span class="line">    float *concat;</span><br><span class="line">    float *concat_delta;</span><br><span class="line"></span><br><span class="line">    float *binary_weights;</span><br><span class="line"></span><br><span class="line">    float *biases;  &#x2F;&#x2F; 当前层所有偏置，对于卷积层，维度l.n，每个卷积核有一个偏置；对于全连接层，维度等于单张输入图片对应的元素个数即outputs，一般在各网络构建函数中动态分配内存（比如make_connected_layer()</span><br><span class="line">    float *bias_updates; &#x2F;&#x2F; 当前层所有偏置更新值，对于卷积层，维度l.n，每个卷积核有一个偏置；对于全连接层，维度为outputs。所谓权重系数更新值，就是梯度下降中与步长相乘的那项，也即误差对偏置的导数，</span><br><span class="line">                          &#x2F;&#x2F; 一般在各网络构建函数中动态分配内存（比如make_connected_layer())</span><br><span class="line"></span><br><span class="line">    float *scales;</span><br><span class="line">    float *scale_updates;</span><br><span class="line"></span><br><span class="line">    float *weights; &#x2F;&#x2F;当前层所有权重系数（连接当前层和上一层的系数，但记在当前层上），对于卷积层，维度为l.n*l.c*l.size*l.size，即卷积核个数乘以卷积核尺寸再乘以输入通道数（各个通道上的权重系数独立不一样）；</span><br><span class="line">                     &#x2F;&#x2F; 对于全连接层，维度为单张图片输入与输出元素个数之积inputs*outputs，一般在各网络构建函数中动态分配内存（比如make_connected_layer()）</span><br><span class="line">    float *weight_updates;&#x2F;&#x2F; 当前层所有权重系数更新值，对于卷积层维度为l.n*l.c*l.size*l.size；对于全连接层，维度为单张图片输入与输出元素个数之积inputs*outputs，</span><br><span class="line">                            &#x2F;&#x2F; 所谓权重系数更新值，就是梯度下降中与步长相乘的那项，也即误差对权重的导数，一般在各网络构建函数中动态分配内存（比如make_connected_layer()</span><br><span class="line"></span><br><span class="line">    float scale_x_y;</span><br><span class="line">    float max_delta;</span><br><span class="line">    float uc_normalizer;</span><br><span class="line">    float iou_normalizer;</span><br><span class="line">    float cls_normalizer;</span><br><span class="line">    IOU_LOSS iou_loss;</span><br><span class="line">    NMS_KIND nms_kind;</span><br><span class="line">    float beta_nms;</span><br><span class="line">    YOLO_POINT yolo_point;</span><br><span class="line"></span><br><span class="line">    char *align_bit_weights_gpu;</span><br><span class="line">    float *mean_arr_gpu;</span><br><span class="line">    float *align_workspace_gpu;</span><br><span class="line">    float *transposed_align_workspace_gpu;</span><br><span class="line">    int align_workspace_size;</span><br><span class="line"></span><br><span class="line">    char *align_bit_weights;</span><br><span class="line">    float *mean_arr;</span><br><span class="line">    int align_bit_weights_size;</span><br><span class="line">    int lda_align;</span><br><span class="line">    int new_lda;</span><br><span class="line">    int bit_align;</span><br><span class="line"></span><br><span class="line">    float *col_image;</span><br><span class="line">    float * delta; &#x2F;&#x2F; 存储每一层的敏感度图：包含所有输出元素的敏感度值（整个batch所有图片）。所谓敏感度，即误差函数关于当前层每个加权输入的导数值，</span><br><span class="line">                   &#x2F;&#x2F; 关于敏感度图这个名称，其实就是梯度，可以参考https:&#x2F;&#x2F;www.zybuluo.com&#x2F;hanbingtao&#x2F;note&#x2F;485480。</span><br><span class="line">                   &#x2F;&#x2F; 元素个数为l.batch * l.outputs（其中l.outputs &#x3D; l.out_h * l.out_w * l.out_c），</span><br><span class="line">                   &#x2F;&#x2F; 对于卷积神经网络，在make_convolutional_layer()动态分配内存，按行存储，可视为l.batch行，l.outputs列，</span><br><span class="line">                   &#x2F;&#x2F; 即batch中每一张图片，对应l.delta中的一行，而这一行，又可以视作有l.out_c行，l.out_h*l.out_c列，</span><br><span class="line">                   &#x2F;&#x2F; 其中每小行对应一张输入图片的一张输出特征图的敏感度。一般在构建具体网络层时动态分配内存（比如make_maxpool_layer()中）。</span><br><span class="line">    float * output; &#x2F;&#x2F; 存储该层所有的输出，维度为l.out_h * l.out_w * l.out_c * l.batch，可知包含整个batch输入图片的输出，一般在构建具体网络层时动态分配内存（比如make_maxpool_layer()中）。</span><br><span class="line">                    &#x2F;&#x2F; 按行存储：每张图片按行铺排成一大行，图片间再并成一行。</span><br><span class="line">    float * activation_input;</span><br><span class="line">    int delta_pinned;</span><br><span class="line">    int output_pinned;</span><br><span class="line">    float * loss;</span><br><span class="line">    float * squared;</span><br><span class="line">    float * norms;</span><br><span class="line"></span><br><span class="line">    float * spatial_mean;</span><br><span class="line">    float * mean;</span><br><span class="line">    float * variance;</span><br><span class="line"></span><br><span class="line">    float * mean_delta;</span><br><span class="line">    float * variance_delta;</span><br><span class="line"></span><br><span class="line">    float * rolling_mean;</span><br><span class="line">    float * rolling_variance;</span><br><span class="line"></span><br><span class="line">    float * x;</span><br><span class="line">    float * x_norm;</span><br><span class="line"></span><br><span class="line">    float * m;</span><br><span class="line">    float * v;</span><br><span class="line"></span><br><span class="line">    float * bias_m;</span><br><span class="line">    float * bias_v;</span><br><span class="line">    float * scale_m;</span><br><span class="line">    float * scale_v;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    float *z_cpu;</span><br><span class="line">    float *r_cpu;</span><br><span class="line">    float *h_cpu;</span><br><span class="line">    float *stored_h_cpu;</span><br><span class="line">    float * prev_state_cpu;</span><br><span class="line"></span><br><span class="line">    float *temp_cpu;</span><br><span class="line">    float *temp2_cpu;</span><br><span class="line">    float *temp3_cpu;</span><br><span class="line"></span><br><span class="line">    float *dh_cpu;</span><br><span class="line">    float *hh_cpu;</span><br><span class="line">    float *prev_cell_cpu;</span><br><span class="line">    float *cell_cpu;</span><br><span class="line">    float *f_cpu;</span><br><span class="line">    float *i_cpu;</span><br><span class="line">    float *g_cpu;</span><br><span class="line">    float *o_cpu;</span><br><span class="line">    float *c_cpu;</span><br><span class="line">    float *stored_c_cpu;</span><br><span class="line">    float *dc_cpu;</span><br><span class="line"></span><br><span class="line">    float *binary_input;</span><br><span class="line">    uint32_t *bin_re_packed_input;</span><br><span class="line">    char *t_bit_input;</span><br><span class="line"></span><br><span class="line">    struct layer *input_layer;</span><br><span class="line">    struct layer *self_layer;</span><br><span class="line">    struct layer *output_layer;</span><br><span class="line"></span><br><span class="line">    struct layer *reset_layer;</span><br><span class="line">    struct layer *update_layer;</span><br><span class="line">    struct layer *state_layer;</span><br><span class="line"></span><br><span class="line">    struct layer *input_gate_layer;</span><br><span class="line">    struct layer *state_gate_layer;</span><br><span class="line">    struct layer *input_save_layer;</span><br><span class="line">    struct layer *state_save_layer;</span><br><span class="line">    struct layer *input_state_layer;</span><br><span class="line">    struct layer *state_state_layer;</span><br><span class="line"></span><br><span class="line">    struct layer *input_z_layer;</span><br><span class="line">    struct layer *state_z_layer;</span><br><span class="line"></span><br><span class="line">    struct layer *input_r_layer;</span><br><span class="line">    struct layer *state_r_layer;</span><br><span class="line"></span><br><span class="line">    struct layer *input_h_layer;</span><br><span class="line">    struct layer *state_h_layer;</span><br><span class="line"></span><br><span class="line">    struct layer *wz;</span><br><span class="line">    struct layer *uz;</span><br><span class="line">    struct layer *wr;</span><br><span class="line">    struct layer *ur;</span><br><span class="line">    struct layer *wh;</span><br><span class="line">    struct layer *uh;</span><br><span class="line">    struct layer *uo;</span><br><span class="line">    struct layer *wo;</span><br><span class="line">    struct layer *vo;</span><br><span class="line">    struct layer *uf;</span><br><span class="line">    struct layer *wf;</span><br><span class="line">    struct layer *vf;</span><br><span class="line">    struct layer *ui;</span><br><span class="line">    struct layer *wi;</span><br><span class="line">    struct layer *vi;</span><br><span class="line">    struct layer *ug;</span><br><span class="line">    struct layer *wg;</span><br><span class="line"> </span><br><span class="line">    tree *softmax_tree; &#x2F;&#x2F; softmax层用到的一个参数，不过这个参数似乎并不常见，很多用到softmax层的网络并没用使用这个参数，目前仅发现darknet9000.cfg中使用了该参数，如果未用到该参数，其值为NULL，如果用到了则会在parse_softmax()中赋值，</span><br><span class="line">                       &#x2F;&#x2F; 目前个人的初步猜测是利用该参数来组织标签数据，以方便访问</span><br><span class="line"></span><br><span class="line">    size_t workspace_size; &#x2F;&#x2F; net.workspace的元素个数，为所有层中最大的l.out_h*l.out_w*l.size*l.size*l.c，（在make_convolutional_layer()计算得到workspace_size的大小，在parse_network_cfg()中动态分配内存，此值对应未使用gpu时的情况</span><br><span class="line"> </span><br><span class="line">#ifdef GPU</span><br><span class="line">    int *indexes_gpu;</span><br><span class="line"></span><br><span class="line">    float *z_gpu;</span><br><span class="line">    float *r_gpu;</span><br><span class="line">    float *h_gpu;</span><br><span class="line">    float *stored_h_gpu;</span><br><span class="line"></span><br><span class="line">    float *temp_gpu;</span><br><span class="line">    float *temp2_gpu;</span><br><span class="line">    float *temp3_gpu;</span><br><span class="line"></span><br><span class="line">    float *dh_gpu;</span><br><span class="line">    float *hh_gpu;</span><br><span class="line">    float *prev_cell_gpu;</span><br><span class="line">    float *prev_state_gpu;</span><br><span class="line">    float *last_prev_state_gpu;</span><br><span class="line">    float *last_prev_cell_gpu;</span><br><span class="line">    float *cell_gpu;</span><br><span class="line">    float *f_gpu;</span><br><span class="line">    float *i_gpu;</span><br><span class="line">    float *g_gpu;</span><br><span class="line">    float *o_gpu;</span><br><span class="line">    float *c_gpu;</span><br><span class="line">    float *stored_c_gpu;</span><br><span class="line">    float *dc_gpu;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; adam</span><br><span class="line">    float *m_gpu;</span><br><span class="line">    float *v_gpu;</span><br><span class="line">    float *bias_m_gpu;</span><br><span class="line">    float *scale_m_gpu;</span><br><span class="line">    float *bias_v_gpu;</span><br><span class="line">    float *scale_v_gpu;</span><br><span class="line"></span><br><span class="line">    float * combine_gpu;</span><br><span class="line">    float * combine_delta_gpu;</span><br><span class="line"></span><br><span class="line">    float * forgot_state_gpu;</span><br><span class="line">    float * forgot_delta_gpu;</span><br><span class="line">    float * state_gpu;</span><br><span class="line">    float * state_delta_gpu;</span><br><span class="line">    float * gate_gpu;</span><br><span class="line">    float * gate_delta_gpu;</span><br><span class="line">    float * save_gpu;</span><br><span class="line">    float * save_delta_gpu;</span><br><span class="line">    float * concat_gpu;</span><br><span class="line">    float * concat_delta_gpu;</span><br><span class="line"></span><br><span class="line">    float *binary_input_gpu;</span><br><span class="line">    float *binary_weights_gpu;</span><br><span class="line">    float *bin_conv_shortcut_in_gpu;</span><br><span class="line">    float *bin_conv_shortcut_out_gpu;</span><br><span class="line"></span><br><span class="line">    float * mean_gpu;</span><br><span class="line">    float * variance_gpu;</span><br><span class="line"></span><br><span class="line">    float * rolling_mean_gpu;</span><br><span class="line">    float * rolling_variance_gpu;</span><br><span class="line"></span><br><span class="line">    float * variance_delta_gpu;</span><br><span class="line">    float * mean_delta_gpu;</span><br><span class="line"></span><br><span class="line">    float * col_image_gpu;</span><br><span class="line"></span><br><span class="line">    float * x_gpu;</span><br><span class="line">    float * x_norm_gpu;</span><br><span class="line">    float * weights_gpu;</span><br><span class="line">    float * weight_updates_gpu;</span><br><span class="line">    float * weight_deform_gpu;</span><br><span class="line">    float * weight_change_gpu;</span><br><span class="line"></span><br><span class="line">    float * weights_gpu16;</span><br><span class="line">    float * weight_updates_gpu16;</span><br><span class="line"></span><br><span class="line">    float * biases_gpu;</span><br><span class="line">    float * bias_updates_gpu;</span><br><span class="line">    float * bias_change_gpu;</span><br><span class="line"></span><br><span class="line">    float * scales_gpu;</span><br><span class="line">    float * scale_updates_gpu;</span><br><span class="line">    float * scale_change_gpu;</span><br><span class="line"></span><br><span class="line">    float * input_antialiasing_gpu;</span><br><span class="line">    float * output_gpu;</span><br><span class="line">    float * activation_input_gpu;</span><br><span class="line">    float * loss_gpu;</span><br><span class="line">    float * delta_gpu;</span><br><span class="line">    float * rand_gpu;</span><br><span class="line">    float * squared_gpu;</span><br><span class="line">    float * norms_gpu;</span><br><span class="line"></span><br><span class="line">    float *gt_gpu;</span><br><span class="line">    float *a_avg_gpu;</span><br><span class="line"></span><br><span class="line">    int *input_sizes_gpu;</span><br><span class="line">    float **layers_output_gpu;</span><br><span class="line">    float **layers_delta_gpu;</span><br><span class="line">#ifdef CUDNN</span><br><span class="line">    cudnnTensorDescriptor_t srcTensorDesc, dstTensorDesc;</span><br><span class="line">    cudnnTensorDescriptor_t srcTensorDesc16, dstTensorDesc16;</span><br><span class="line">    cudnnTensorDescriptor_t dsrcTensorDesc, ddstTensorDesc;</span><br><span class="line">    cudnnTensorDescriptor_t dsrcTensorDesc16, ddstTensorDesc16;</span><br><span class="line">    cudnnTensorDescriptor_t normTensorDesc, normDstTensorDesc, normDstTensorDescF16;</span><br><span class="line">    cudnnFilterDescriptor_t weightDesc, weightDesc16;</span><br><span class="line">    cudnnFilterDescriptor_t dweightDesc, dweightDesc16;</span><br><span class="line">    cudnnConvolutionDescriptor_t convDesc;</span><br><span class="line">    cudnnConvolutionFwdAlgo_t fw_algo, fw_algo16;</span><br><span class="line">    cudnnConvolutionBwdDataAlgo_t bd_algo, bd_algo16;</span><br><span class="line">    cudnnConvolutionBwdFilterAlgo_t bf_algo, bf_algo16;</span><br><span class="line">    cudnnPoolingDescriptor_t poolingDesc;</span><br><span class="line">#endif  &#x2F;&#x2F; CUDNN</span><br><span class="line">#endif  &#x2F;&#x2F; GPU</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet加载数据进行训练</title>
    <url>/2020/02/21/AlexeyAB-DarkNet%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>之前讲了DarkNet的底层数据结构，并且将网络配置文件进行了解析存放到了一个<code>network</code>结构体中，那么我们就要来看一下Darknet是如何加载数据进行训练的。</p>
<h2 id="加载训练数据"><a href="#加载训练数据" class="headerlink" title="加载训练数据"></a>加载训练数据</h2><p>DarkNet的数据加载函数<code>load_data()</code>在<code>src/data.c</code>中实现（<code>src/detector.c</code>函数中的<code>train_detector</code>直接调用这个函数加载数据）。<code>load_data()</code>函数调用流程如下：<code>load_data(args)-&gt;load_threads()-&gt;load_data_in_threads()-&gt;load_thread()-&gt;load_data_detection()</code>，前四个函数都是在对线程的调用进行封装。最底层的数据加载任务由<code>load_data_detection()</code>函数完成。所有的数据(图片数据和标注信息数据)加载完成之后再拼接到一个大的数组中。在DarkNet中，图片的存储形式是一个行向量，向量长度为<code>h*w*3</code>。同时图片被归一化到<code>[0, 1]</code>之间。</p>
<h2 id="load-threads-完成线程分配和数据拼接"><a href="#load-threads-完成线程分配和数据拼接" class="headerlink" title="load_threads()完成线程分配和数据拼接"></a>load_threads()完成线程分配和数据拼接</h2><p><code>load_threads</code>在<code>src/data.c</code>中实现，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; copy from https:&#x2F;&#x2F;github.com&#x2F;hgpvision&#x2F;darknet&#x2F;blob&#x2F;master&#x2F;src&#x2F;data.c#L355</span><br><span class="line">&#x2F;*</span><br><span class="line">** 开辟多个线程读入图片数据，读入数据存储至ptr.d中（主要调用load_in_thread()函数完成）</span><br><span class="line">** 输入：ptr    包含所有线程要读入图片数据的信息（读入多少张，开几个线程读入，读入图片最终的宽高，图片路径等等）</span><br><span class="line">** 返回：void*  万能指针（实际上不需要返回什么）</span><br><span class="line">** 说明：1) load_threads()是一个指针函数，只是一个返回变量为void*的普通函数，不是函数指针</span><br><span class="line">**       2) 输入ptr是一个void*指针（万能指针），使用时需要强转为具体类型的指针</span><br><span class="line">**       3) 函数中涉及四个用来存储读入数据的变量：ptr, args, out, buffers，除args外都是data*类型，所有这些变量的</span><br><span class="line">**          指针变量其实都指向同一块内存（当然函数中间有些动态变化），因此读入的数据都是互通的。</span><br><span class="line">** 流程：本函数首先会获取要读入图片的张数、要开启线程的个数，而后计算每个线程应该读入的图片张数（尽可能的均匀分配），</span><br><span class="line">**       并创建所有的线程，并行读入数据，最后合并每个线程读入的数据至一个大data中，这个data的指针变量与ptr的指针变量</span><br><span class="line">**       指向的是统一块内存，因此也就最终将数据读入到ptr.d中（所以其实没有返回值）</span><br><span class="line">*&#x2F;</span><br><span class="line">void *load_threads(void *ptr)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F;srand(time(0));</span><br><span class="line">    int i;</span><br><span class="line">	&#x2F;&#x2F; 先使用(load_args*)强转void*指针，而后取ptr所指内容赋值给args</span><br><span class="line">    &#x2F;&#x2F; 虽然args不是指针，args是深拷贝了ptr中的内容，但是要知道ptr（也就是load_args数据类型），有很多的</span><br><span class="line">    &#x2F;&#x2F; 指针变量，args深拷贝将拷贝这些指针变量到args中（这些指针变量本身对ptr来说就是内容，</span><br><span class="line">    &#x2F;&#x2F; 而args所指的值是args的内容，不是ptr的，不要混为一谈），因此，args与ptr将会共享所有指针变量所指的内容</span><br><span class="line">    load_args args &#x3D; *(load_args *)ptr;</span><br><span class="line">    if (args.threads &#x3D;&#x3D; 0) args.threads &#x3D; 1;</span><br><span class="line">	&#x2F;&#x2F; 另指针变量out&#x3D;args.d，使得out与args.d指向统一块内存，之后，args.d所指的内存块会变（反正也没什么用了，变就变吧），</span><br><span class="line">    &#x2F;&#x2F; 但out不会变，这样可以保证out与最原始的ptr指向同一块存储读入图片数据的内存块，因此最终将图片读到out中，</span><br><span class="line">    &#x2F;&#x2F; 实际就是读到了最原始的ptr中，比如train_detector()函数中定义的args.d中</span><br><span class="line">    data *out &#x3D; args.d;</span><br><span class="line">	&#x2F;&#x2F; 读入图片的总张数&#x3D; batch * subdivision * ngpus，可参见train_detector()函数中的赋值</span><br><span class="line">    int total &#x3D; args.n;</span><br><span class="line">	&#x2F;&#x2F; 释放ptr：ptr是传入的指针变量，传入的指针变量本身也是按值传递的，即传入函数之后，指针变量得到复制，函数内的形参ptr</span><br><span class="line">    &#x2F;&#x2F; 获取外部实参的值之后，二者本身没有关系，但是由于是指针变量，二者之间又存在一丝关系，那就是函数内形参与函数外实参指向</span><br><span class="line">    &#x2F;&#x2F; 同一块内存。又由于函数外实参内存是动态分配的，因此函数内的形参可以使用free()函数进行内存释放，但一般不推荐这么做，因为函数内释放内存，</span><br><span class="line">    &#x2F;&#x2F; 会影响函数外实参的使用，可能使之成为野指针，那为什么这里可以用free()释放ptr呢，不会出现问题吗？</span><br><span class="line">    &#x2F;&#x2F; 其一，因为ptr是一个结构体，是一个包含众多的指针变量的结构体，如data* d等（当然还有其他非指针变量如int h等），</span><br><span class="line">    &#x2F;&#x2F; 直接free(ptr)将会导致函数外实参无法再访问非指针变量int h等（实际经过测试，在gcc编译器下，能访问但是值被重新初始化为0），</span><br><span class="line">    &#x2F;&#x2F; 因为函数内形参和函数外实参共享一块堆内存，而这些非指针变量都是存在这块堆内存上的，内存一释放，就无法访问了；</span><br><span class="line">    &#x2F;&#x2F; 但是对于指针变量，free(ptr)将无作为（这个结论也是经过测试的，也是用的gcc编译器），不会释放或者擦写掉ptr指针变量本身的值，</span><br><span class="line">    &#x2F;&#x2F; 当然也不会影响函数外实参，更不会牵扯到这些指针变量所指的内存块，总的来说，</span><br><span class="line">    &#x2F;&#x2F; free(ptr)将使得ptr不能再访问指针变量（如int h等，实际经过测试，在gcc编译器下，能访问但是值被重新初始化为0），</span><br><span class="line">    &#x2F;&#x2F; 但其指针变量本身没有受影响，依旧可以访问；对于函数外实参，同样不能访问非指针变量，而指针变量不受影响，依旧可以访问。</span><br><span class="line">    &#x2F;&#x2F; 其二，darknet数据读取的实现一层套一层（似乎有点罗嗦，总感觉代码可以不用这么写的:)），具体调用过程如下：</span><br><span class="line">    &#x2F;&#x2F; load_data(load_args args)-&gt;load_threads(load_args* ptr)-&gt;load_data_in_thread(load_args args)-&gt;load_thread(load_args* ptr)，</span><br><span class="line">    &#x2F;&#x2F; 就在load_data()中，重新定义了ptr，并为之动态分配了内存，且深拷贝了传给load_data()函数的值args，也就是说在此之后load_data()函数中的args除了其中的指针变量指着同一块堆内存之外，</span><br><span class="line">    &#x2F;&#x2F; 二者的非指针变量再无瓜葛，不管之后经过多少个函数，对ptr的非指针变量做了什么改动，比如这里直接free(ptr)，使得非指针变量值为0,都不会影响load_data()中的args的非指针变量，也就不会影响更为顶层函数中定义的args的非指针变量的值，</span><br><span class="line">    &#x2F;&#x2F; 比如train_detector()函数中的args，train_detector()对args非指针变量赋的值都不会受影响，保持不变。综其两点，此处直接free(ptr)是安全的。</span><br><span class="line">    &#x2F;&#x2F; 说明：free(ptr)函数，确定会做的事是使得内存块可以重新分配，且不会影响指针变量ptr本身的值，也就是ptr还是指向那块地址， 虽然可以使用，但很危险，因为这块内存实际是无效的，</span><br><span class="line">    &#x2F;&#x2F;      系统已经认为这块内存是可分配的，会毫不考虑的将这块内存分给其他变量，这样，其值随时都可能会被其他变量改变，这种情况下的ptr指针就是所谓的野指针（所以经常可以看到free之后，置原指针为NULL）。</span><br><span class="line">    &#x2F;&#x2F;      而至于free(ptr)还不会做其他事情，比如会不会重新初始化这块内存为0（擦写掉），以及怎么擦写，这些操作，是不确定的，可能跟具体的编译器有关（个人猜测），</span><br><span class="line">    &#x2F;&#x2F;      经过测试，对于gcc编译器，free(ptr)之后，ptr中的非指针变量的地址不变，但其值全部擦写为0；ptr中的指针变量，丝毫不受影响，指针变量本身没有被擦写，</span><br><span class="line">    &#x2F;&#x2F;      存储的地址还是指向先前分配的内存块，所以ptr能够正常访问其指针变量所指的值。测试代码为darknet_test_struct_memory_free.c。</span><br><span class="line">    &#x2F;&#x2F;      不知道这段测试代码在VS中执行会怎样，还没经过测试，也不知道换用其他编译器（darknet的Makefile文件中，指定了编译器为gcc），darknet的编译会不会有什么问题？？</span><br><span class="line">    &#x2F;&#x2F;      关于free()，可以看看：http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;s&#x2F;blog_615ec1630102uwle.html，文章最后有一个很有意思的比喻，但意思好像就和我这里说的有点不一样了（到底是不是编译器搞得鬼呢？？）。</span><br><span class="line">    free(ptr);</span><br><span class="line">	&#x2F;&#x2F; 每一个线程都会读入一个data，定义并分配args.thread个data的内存</span><br><span class="line">    data* buffers &#x3D; (data*)xcalloc(args.threads, sizeof(data));</span><br><span class="line">    pthread_t* threads &#x3D; (pthread_t*)xcalloc(args.threads, sizeof(pthread_t));</span><br><span class="line">	&#x2F;&#x2F; 此处定义了多个线程，并为每个线程动态分配内存</span><br><span class="line">    for(i &#x3D; 0; i &lt; args.threads; ++i)&#123;</span><br><span class="line">		&#x2F;&#x2F; 此处就承应了上面的注释，args.d指针变量本身发生了改动，使得本函数的args.d与out不再指向同一块内存，</span><br><span class="line">        &#x2F;&#x2F; 改为指向buffers指向的某一段内存，因为下面的load_data_in_thread()函数统一了结口，需要输入一个load_args类型参数，</span><br><span class="line">        &#x2F;&#x2F; 实际是想把图片数据读入到buffers[i]中，只能令args.d与buffers[i]指向同一块内存</span><br><span class="line">        args.d &#x3D; buffers + i;</span><br><span class="line">		 &#x2F;&#x2F; 下面这句很有意思，因为有多个线程，所有线程读入的总图片张数为total，需要将total均匀的分到各个线程上，</span><br><span class="line">        &#x2F;&#x2F; 但很可能会遇到total不能整除的args.threads的情况，比如total &#x3D; 61, args.threads &#x3D;8,显然不能做到</span><br><span class="line">        &#x2F;&#x2F; 完全均匀的分配，但又要保证读入图片的总张数一定等于total，用下面的语句刚好在尽量均匀的情况下，</span><br><span class="line">        &#x2F;&#x2F; 保证总和为total，比如61,那么8个线程各自读入的照片张数分别为：7, 8, 7, 8, 8, 7, 8, 8</span><br><span class="line">        args.n &#x3D; (i+1) * total&#x2F;args.threads - i * total&#x2F;args.threads;</span><br><span class="line">		&#x2F;&#x2F; 开启线程，读入数据到args.d中（也就读入到buffers[i]中）</span><br><span class="line">        &#x2F;&#x2F; load_data_in_thread()函数返回所开启的线程，并存储之前已经动态分配内存用来存储所有线程的threads中，</span><br><span class="line">        &#x2F;&#x2F; 方便下面使用pthread_join()函数控制相应线程</span><br><span class="line">        threads[i] &#x3D; load_data_in_thread(args);</span><br><span class="line">    &#125;</span><br><span class="line">    for(i &#x3D; 0; i &lt; args.threads; ++i)&#123;</span><br><span class="line">		&#x2F;&#x2F; 以阻塞的方式等待线程threads[i]结束：阻塞是指阻塞启动该子线程的母线程（此处应为主线程），</span><br><span class="line">        &#x2F;&#x2F; 是母线程处于阻塞状态，一直等待所有子线程执行完（读完所有数据）才会继续执行下面的语句</span><br><span class="line">        &#x2F;&#x2F; 关于多线程的使用，进行过代码测试，测试代码对应：darknet_test_pthread_join.c</span><br><span class="line">        pthread_join(threads[i], 0);</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F; 多个线程读入所有数据之后，分别存储到buffers[0],buffers[1]...中，接着使用concat_datas()函数将buffers中的数据全部合并成一个大数组得到out</span><br><span class="line">    *out &#x3D; concat_datas(buffers, args.threads);</span><br><span class="line">	 &#x2F;&#x2F; 也就只有out的shallow敢置为0了，为什么呢？因为out是此次迭代读入的最终数据，该数据参与训练（用完）之后，当然可以深层释放了，而此前的都是中间变量，</span><br><span class="line">    &#x2F;&#x2F; 还处于读入数据阶段，万不可设置shallow&#x3D;0</span><br><span class="line">    out-&gt;shallow &#x3D; 0;</span><br><span class="line">	&#x2F;&#x2F; 释放buffers，buffers也是个中间变量，切记shallow设置为1,如果设置为0,那就连out中的数据也没了</span><br><span class="line">    for(i &#x3D; 0; i &lt; args.threads; ++i)&#123;</span><br><span class="line">        buffers[i].shallow &#x3D; 1;</span><br><span class="line">        free_data(buffers[i]);</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F; 最终直接释放buffers,threads，注意buffers是一个存储data的一维数组，上面循环中的内存释放，实际是释放每一个data的部分内存</span><br><span class="line">    &#x2F;&#x2F; （这部分内存对data而言是非主要内存，不是存储读入数据的内存块，而是存储指向这些内存块的指针变量，可以释放的）</span><br><span class="line">    free(buffers);</span><br><span class="line">    free(threads);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="load-data-in-thread-分配线程"><a href="#load-data-in-thread-分配线程" class="headerlink" title="load_data_in_thread()分配线程"></a>load_data_in_thread()分配线程</h2><p><code>load_data_in_thread()</code>函数仍然在<code>src/data.c</code>中，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 创建一个线程，读入相应图片数据（此时args.n不再是一次迭代读入的所有图片的张数，而是经过load_threads()均匀分配给每个线程的图片张数）</span><br><span class="line">** 输入：args    包含该线程要读入图片数据的信息（读入多少张，读入图片最终的宽高，图片路径等等）</span><br><span class="line">** 返回：phtread_t   线程id</span><br><span class="line">** 说明：本函数实际没有做什么，就是深拷贝了args给ptr,然后创建了一个调用load_thread()函数的线程并返回线程id</span><br><span class="line">*&#x2F;</span><br><span class="line">pthread_t load_data_in_thread(load_args args)</span><br><span class="line">&#123;</span><br><span class="line">    pthread_t thread;</span><br><span class="line">	&#x2F;&#x2F; 同样第一件事深拷贝了args给ptr</span><br><span class="line">    struct load_args* ptr &#x3D; (load_args*)xcalloc(1, sizeof(struct load_args));</span><br><span class="line">    *ptr &#x3D; args;</span><br><span class="line">	&#x2F;&#x2F; 创建一个线程，读入相应数据，绑定load_thread()函数到该线程上，第四个参数是load_thread()的输入参数，第二个参数表示线程属性，设置为0（即NULL）</span><br><span class="line">	&#x2F;&#x2F;当创建线程成功时，函数返回0，若不为0则说明创建线程失败</span><br><span class="line">    if(pthread_create(&amp;thread, 0, load_thread, ptr)) error(&quot;Thread creation failed&quot;); </span><br><span class="line">    return thread;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="load-data-detection-完成底层的数据加载任务"><a href="#load-data-detection-完成底层的数据加载任务" class="headerlink" title="load_data_detection()完成底层的数据加载任务"></a>load_data_detection()完成底层的数据加载任务</h2><p><code>load_data_detection()</code>函数也定义在<code>src/data.c</code>中，带注释的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 可以参考，看一下对图像进行jitter处理的各种效果:</span><br><span class="line">** https:&#x2F;&#x2F;github.com&#x2F;vxy10&#x2F;ImageAugmentation</span><br><span class="line">** 从所有训练图片中，随机读取n张，并对这n张图片进行数据增强，同时矫正增强后的数据标签信息。最终得到的图片的宽高为w,h（原始训练集中的图片尺寸不定），也就是网络能够处理的图片尺寸，</span><br><span class="line">** 数据增强包括：对原始图片进行宽高方向上的插值缩放（两方向上缩放系数不一定相同），下面称之为缩放抖动；随机抠取或者平移图片（位置抖动）；</span><br><span class="line">** 在hsv颜色空间增加噪声（颜色抖动）；左右水平翻转，不含旋转抖动。</span><br><span class="line">** 输入： n         一个线程读入的图片张数（详见函数内部注释）</span><br><span class="line">**       paths     所有训练图片所在路径集合，是一个二维数组，每一行对应一张图片的路径（将在其中随机取n个）</span><br><span class="line">**       m         paths的行数，也即训练图片总数</span><br><span class="line">**       w         网络能够处理的图的宽度（也就是输入图片经过一系列数据增强、变换之后最终输入到网络的图的宽度）</span><br><span class="line">**       h         网络能够处理的图的高度（也就是输入图片经过一系列数据增强、变换之后最终输入到网络的图的高度）</span><br><span class="line">**       c         用来指定训练图片的通道数（默认为3，即RGB图）</span><br><span class="line">**       boxes     每张训练图片最大处理的矩形框数（图片内可能含有更多的物体，即更多的矩形框，那么就在其中随机选择boxes个参与训练，具体执行在fill_truth_detection()函数中）</span><br><span class="line">**       classes   类别总数，本函数并未用到（fill_truth_detection函数其实并没有用这个参数）</span><br><span class="line">**       use_flip  是否使用水平翻转</span><br><span class="line">**       use_mixup 是否使用mixup数据增强</span><br><span class="line">**       jitter    这个参数为缩放抖动系数，就是图片缩放抖动的剧烈程度，越大，允许的抖动范围越大（所谓缩放抖动，就是在宽高上插值缩放图片，宽高两方向上缩放的系数不一定相同）</span><br><span class="line">**       hue       颜色（hsv颜色空间）数据增强参数：色调（取值0度到360度）偏差最大值，实际色调偏差为-hue~hue之间的随机值</span><br><span class="line">**       saturation 颜色（hsv颜色空间）数据增强参数：色彩饱和度（取值范围0~1）缩放最大值，实际为范围内的随机值</span><br><span class="line">**       exposure  颜色（hsv颜色空间）数据增强参数：明度（色彩明亮程度，0~1）缩放最大值，实际为范围内的随机值</span><br><span class="line">**       mini_batch      和目标跟踪有关，这里不关注</span><br><span class="line">**       track           和目标跟踪有关，这里不关注</span><br><span class="line">**       augment_speed   和目标跟踪有关，这里不关注</span><br><span class="line">**       letter_box 是否进行letter_box变换</span><br><span class="line">**       show_imgs</span><br><span class="line">** 返回：data类型数据，包含一个线程读入的所有图片数据（含有n张图片）</span><br><span class="line">** 说明：最后四个参数用于数据增强，主要对原图进行缩放抖动，位置抖动（平移）以及颜色抖动（颜色值增加一定噪声），抖动一定程度上可以理解成对图像增加噪声。</span><br><span class="line">**       通过对原始图像进行抖动，实现数据增强。最后三个参数具体用法参考本函数内调用的random_distort_image()函数</span><br><span class="line">** 说明2：从此函数可以看出，darknet对训练集中图片的尺寸没有要求，可以是任意尺寸的图片，因为经该函数处理（缩放&#x2F;裁剪）之后，</span><br><span class="line">**       不管是什么尺寸的照片，都会统一为网络训练使用的尺寸</span><br><span class="line">*&#x2F;</span><br><span class="line"></span><br><span class="line">data load_data_detection(int n, char **paths, int m, int w, int h, int c, int boxes, int classes, int use_flip, int use_blur, int use_mixup, float jitter,</span><br><span class="line">    float hue, float saturation, float exposure, int mini_batch, int track, int augment_speed, int letter_box, int show_imgs)</span><br><span class="line">&#123;</span><br><span class="line">    const int random_index &#x3D; random_gen();</span><br><span class="line">    c &#x3D; c ? c : 3;</span><br><span class="line">    char **random_paths;</span><br><span class="line">    char **mixup_random_paths &#x3D; NULL;</span><br><span class="line">	&#x2F;&#x2F; paths包含所有训练图片的路径，get_random_paths函数从中随机提出n条，即为此次读入的n张图片的路径</span><br><span class="line">    if(track) random_paths &#x3D; get_sequential_paths(paths, n, m, mini_batch, augment_speed);</span><br><span class="line">    else random_paths &#x3D; get_random_paths(paths, n, m);</span><br><span class="line"></span><br><span class="line">    assert(use_mixup &lt; 2);</span><br><span class="line">    int mixup &#x3D; use_mixup ? random_gen() % 2 : 0;</span><br><span class="line">    &#x2F;&#x2F;printf(&quot;\n mixup &#x3D; %d \n&quot;, mixup);</span><br><span class="line">	&#x2F;&#x2F; 如果使用mixup策略，需要再随机取出n条数据，即n张图片</span><br><span class="line">    if (mixup) &#123;</span><br><span class="line">        if (track) mixup_random_paths &#x3D; get_sequential_paths(paths, n, m, mini_batch, augment_speed);</span><br><span class="line">        else mixup_random_paths &#x3D; get_random_paths(paths, n, m);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int i;</span><br><span class="line">	&#x2F;&#x2F; 初始化为0,清空内存中之前的旧值</span><br><span class="line">    data d &#x3D; &#123; 0 &#125;;</span><br><span class="line">    d.shallow &#x3D; 0;</span><br><span class="line">	&#x2F;&#x2F; 一次读入的图片张数：d.X中每行就是一张图片的数据，因此d.X.cols等于h*w*3</span><br><span class="line">    &#x2F;&#x2F; n &#x3D; net.batch * net.subdivisions * ngpus</span><br><span class="line">    &#x2F;&#x2F; 从parse_net_option()函数可知，net.batch &#x3D; net.batch &#x2F; net.subdivision</span><br><span class="line">    &#x2F;&#x2F; net.batch * net.subdivisions就得到了在网络配置文件中设定的batch值，即网络配置文件.cfg中设置的每个batch的图片数量, 然后乘以ngpus，是考虑多个GPU实现数据并行，</span><br><span class="line">    &#x2F;&#x2F; 一次读入多个batch的数据，分配到不同GPU上进行训练。在load_threads()函数中，又将整个的n仅可能均匀的划分到每个线程上，</span><br><span class="line">    &#x2F;&#x2F; 也就是总的读入图片张数为n &#x3D; net.batch * net.subdivisions * ngpus，但这些图片不是一个线程读完的，而是分配到多个线程并行读入，</span><br><span class="line">    &#x2F;&#x2F; 因此本函数中的n实际不是总的n，而是分配到该线程上的n，比如总共要读入128张图片，共开启8个线程读数据，那么本函数中的n为16,而不是总数128</span><br><span class="line">    d.X.rows &#x3D; n;</span><br><span class="line">	&#x2F;&#x2F;d.X为一个matrix类型数据，其中d.X.vals是其具体数据，是指针的指针（即为二维数组），此处先为第一维动态分配内存</span><br><span class="line">    d.X.vals &#x3D; (float**)xcalloc(d.X.rows, sizeof(float*));</span><br><span class="line">    d.X.cols &#x3D; h*w*c;</span><br><span class="line"></span><br><span class="line">    float r1 &#x3D; 0, r2 &#x3D; 0, r3 &#x3D; 0, r4 &#x3D; 0, r_scale;</span><br><span class="line">    float dhue &#x3D; 0, dsat &#x3D; 0, dexp &#x3D; 0, flip &#x3D; 0;</span><br><span class="line">    int augmentation_calculated &#x3D; 0;</span><br><span class="line">	&#x2F;&#x2F; d.y存储了所有读入照片的标签信息，每条标签包含5条信息：类别，以及矩形框的x,y,w,h</span><br><span class="line">    &#x2F;&#x2F; boxes为一张图片最多能够处理（参与训练）的矩形框的数（如果图片中的矩形框数多于这个数，那么随机挑选boxes个，这个参数仅在parse_region以及parse_detection中出现</span><br><span class="line">    &#x2F;&#x2F; 在其他网络解析函数中并没有出现。同样，d.y是一个matrix，make_matrix会指定y的行数和列数，同时会为其第一维动态分配内存</span><br><span class="line">    d.y &#x3D; make_matrix(n, 5 * boxes);</span><br><span class="line">    int i_mixup &#x3D; 0;</span><br><span class="line">    for (i_mixup &#x3D; 0; i_mixup &lt;&#x3D; mixup; i_mixup++) &#123;</span><br><span class="line">        if (i_mixup) augmentation_calculated &#x3D; 0;</span><br><span class="line">        for (i &#x3D; 0; i &lt; n; ++i) &#123;</span><br><span class="line">			</span><br><span class="line">            float *truth &#x3D; (float*)xcalloc(5 * boxes, sizeof(float));</span><br><span class="line">            char *filename &#x3D; (i_mixup) ? mixup_random_paths[i] : random_paths[i];</span><br><span class="line">			&#x2F;&#x2F;读入原始的图片</span><br><span class="line">            image orig &#x3D; load_image(filename, 0, 0, c);</span><br><span class="line">			&#x2F;&#x2F; 原始图像长宽</span><br><span class="line">            int oh &#x3D; orig.h;</span><br><span class="line">            int ow &#x3D; orig.w;</span><br><span class="line">			&#x2F;&#x2F; 缩放抖动大小：缩放抖动系数乘以原始图宽高即得像素单位意义上的缩放抖动</span><br><span class="line">            int dw &#x3D; (ow*jitter);</span><br><span class="line">            int dh &#x3D; (oh*jitter);</span><br><span class="line"></span><br><span class="line">            if (!augmentation_calculated || !track)</span><br><span class="line">            &#123;</span><br><span class="line">                augmentation_calculated &#x3D; 1;</span><br><span class="line">                r1 &#x3D; random_float();</span><br><span class="line">                r2 &#x3D; random_float();</span><br><span class="line">                r3 &#x3D; random_float();</span><br><span class="line">                r4 &#x3D; random_float();</span><br><span class="line"></span><br><span class="line">                r_scale &#x3D; random_float();</span><br><span class="line"></span><br><span class="line">                dhue &#x3D; rand_uniform_strong(-hue, hue);</span><br><span class="line">                dsat &#x3D; rand_scale(saturation);</span><br><span class="line">                dexp &#x3D; rand_scale(exposure);</span><br><span class="line"></span><br><span class="line">                flip &#x3D; use_flip ? random_gen() % 2 : 0;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            int pleft &#x3D; rand_precalc_random(-dw, dw, r1);</span><br><span class="line">            int pright &#x3D; rand_precalc_random(-dw, dw, r2);</span><br><span class="line">            int ptop &#x3D; rand_precalc_random(-dh, dh, r3);</span><br><span class="line">            int pbot &#x3D; rand_precalc_random(-dh, dh, r4);</span><br><span class="line">			&#x2F;&#x2F; 这个系数没用到</span><br><span class="line">            float scale &#x3D; rand_precalc_random(.25, 2, r_scale); &#x2F;&#x2F; unused currently</span><br><span class="line"></span><br><span class="line">            if (letter_box)</span><br><span class="line">            &#123;</span><br><span class="line">                float img_ar &#x3D; (float)ow &#x2F; (float)oh; &#x2F;&#x2F;原始图像宽高比</span><br><span class="line">                float net_ar &#x3D; (float)w &#x2F; (float)h; &#x2F;&#x2F;输入到网络要求的图像宽高比</span><br><span class="line">                float result_ar &#x3D; img_ar &#x2F; net_ar; &#x2F;&#x2F;两者求比值来判断如何进行letter_box缩放</span><br><span class="line">                &#x2F;&#x2F;printf(&quot; ow &#x3D; %d, oh &#x3D; %d, w &#x3D; %d, h &#x3D; %d, img_ar &#x3D; %f, net_ar &#x3D; %f, result_ar &#x3D; %f \n&quot;, ow, oh, w, h, img_ar, net_ar, result_ar);</span><br><span class="line">                if (result_ar &gt; 1)  &#x2F;&#x2F; sheight - should be increased</span><br><span class="line">                &#123;</span><br><span class="line">                    float oh_tmp &#x3D; ow &#x2F; net_ar;</span><br><span class="line">                    float delta_h &#x3D; (oh_tmp - oh) &#x2F; 2;</span><br><span class="line">                    ptop &#x3D; ptop - delta_h;</span><br><span class="line">                    pbot &#x3D; pbot - delta_h;</span><br><span class="line">                    &#x2F;&#x2F;printf(&quot; result_ar &#x3D; %f, oh_tmp &#x3D; %f, delta_h &#x3D; %d, ptop &#x3D; %f, pbot &#x3D; %f \n&quot;, result_ar, oh_tmp, delta_h, ptop, pbot);</span><br><span class="line">                &#125;</span><br><span class="line">                else  &#x2F;&#x2F; swidth - should be increased</span><br><span class="line">                &#123;</span><br><span class="line">                    float ow_tmp &#x3D; oh * net_ar;</span><br><span class="line">                    float delta_w &#x3D; (ow_tmp - ow) &#x2F; 2;</span><br><span class="line">                    pleft &#x3D; pleft - delta_w;</span><br><span class="line">                    pright &#x3D; pright - delta_w;</span><br><span class="line">                    &#x2F;&#x2F;printf(&quot; result_ar &#x3D; %f, ow_tmp &#x3D; %f, delta_w &#x3D; %d, pleft &#x3D; %f, pright &#x3D; %f \n&quot;, result_ar, ow_tmp, delta_w, pleft, pright);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">			&#x2F;&#x2F; 以下步骤就是执行了letter_box变换</span><br><span class="line">            int swidth &#x3D; ow - pleft - pright;</span><br><span class="line">            int sheight &#x3D; oh - ptop - pbot;</span><br><span class="line"></span><br><span class="line">            float sx &#x3D; (float)swidth &#x2F; ow;</span><br><span class="line">            float sy &#x3D; (float)sheight &#x2F; oh;</span><br><span class="line"></span><br><span class="line">            image cropped &#x3D; crop_image(orig, pleft, ptop, swidth, sheight);</span><br><span class="line"></span><br><span class="line">            float dx &#x3D; ((float)pleft &#x2F; ow) &#x2F; sx;</span><br><span class="line">            float dy &#x3D; ((float)ptop &#x2F; oh) &#x2F; sy;</span><br><span class="line">			&#x2F;&#x2F; resize到指定大小</span><br><span class="line">            image sized &#x3D; resize_image(cropped, w, h);</span><br><span class="line">            &#x2F;&#x2F; 翻转</span><br><span class="line">			if (flip) flip_image(sized);</span><br><span class="line">			&#x2F;&#x2F;随机对图像jitter（在hsv三个通道上添加扰动），实现数据增强</span><br><span class="line">            distort_image(sized, dhue, dsat, dexp);</span><br><span class="line">            &#x2F;&#x2F;random_distort_image(sized, hue, saturation, exposure);</span><br><span class="line">			&#x2F;&#x2F; truth包含所有图像的标签信息（包括真实类别与位置</span><br><span class="line">            &#x2F;&#x2F; 因为对原始图片进行了数据增强，其中的平移抖动势必会改动每个物体的矩形框标签信息（主要是矩形框的像素坐标信息），需要根据具体的数据增强方式进行相应矫正</span><br><span class="line">            &#x2F;&#x2F; 后面的参数就是用于数据增强后的矩形框信息矫正</span><br><span class="line">            fill_truth_detection(filename, boxes, truth, classes, flip, dx, dy, 1. &#x2F; sx, 1. &#x2F; sy, w, h);</span><br><span class="line"></span><br><span class="line">            if (i_mixup) &#123;</span><br><span class="line">                image old_img &#x3D; sized;</span><br><span class="line">                old_img.data &#x3D; d.X.vals[i];</span><br><span class="line">                &#x2F;&#x2F;show_image(sized, &quot;new&quot;);</span><br><span class="line">                &#x2F;&#x2F;show_image(old_img, &quot;old&quot;);</span><br><span class="line">                &#x2F;&#x2F;wait_until_press_key_cv();</span><br><span class="line">				&#x2F;&#x2F; 做mixup，混合系数为0.5</span><br><span class="line">                blend_images(sized, 0.5, old_img, 0.5);</span><br><span class="line">				&#x2F;&#x2F; 标签也要对应改变</span><br><span class="line">                blend_truth(truth, boxes, d.y.vals[i]);</span><br><span class="line">                free_image(old_img);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            d.X.vals[i] &#x3D; sized.data;</span><br><span class="line">            memcpy(d.y.vals[i], truth, 5 * boxes * sizeof(float));</span><br><span class="line"></span><br><span class="line">            if (show_imgs)&#x2F;&#x2F; &amp;&amp; i_mixup)</span><br><span class="line">            &#123;</span><br><span class="line">                char buff[1000];</span><br><span class="line">                sprintf(buff, &quot;aug_%d_%d_%s_%d&quot;, random_index, i, basecfg(filename), random_gen());</span><br><span class="line"></span><br><span class="line">                int t;</span><br><span class="line">                for (t &#x3D; 0; t &lt; boxes; ++t) &#123;</span><br><span class="line">                    box b &#x3D; float_to_box_stride(d.y.vals[i] + t*(4 + 1), 1);</span><br><span class="line">                    if (!b.x) break;</span><br><span class="line">                    int left &#x3D; (b.x - b.w &#x2F; 2.)*sized.w;</span><br><span class="line">                    int right &#x3D; (b.x + b.w &#x2F; 2.)*sized.w;</span><br><span class="line">                    int top &#x3D; (b.y - b.h &#x2F; 2.)*sized.h;</span><br><span class="line">                    int bot &#x3D; (b.y + b.h &#x2F; 2.)*sized.h;</span><br><span class="line">                    draw_box_width(sized, left, top, right, bot, 1, 150, 100, 50); &#x2F;&#x2F; 3 channels RGB</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                save_image(sized, buff);</span><br><span class="line">                if (show_imgs &#x3D;&#x3D; 1) &#123;</span><br><span class="line">                    show_image(sized, buff);</span><br><span class="line">                    wait_until_press_key_cv();</span><br><span class="line">                &#125;</span><br><span class="line">                printf(&quot;\nYou use flag -show_imgs, so will be saved aug_...jpg images. Press Enter: \n&quot;);</span><br><span class="line">                &#x2F;&#x2F;getchar();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            free_image(orig);</span><br><span class="line">            free_image(cropped);</span><br><span class="line">            free(truth);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    free(random_paths);</span><br><span class="line">    if (mixup_random_paths) free(mixup_random_paths);</span><br><span class="line">    return d;</span><br><span class="line">&#125;</span><br><span class="line">#endif    &#x2F;&#x2F; OPENCV</span><br></pre></td></tr></table></figure>
<h2 id="load-data-args-使用方法"><a href="#load-data-args-使用方法" class="headerlink" title="load_data(args)使用方法"></a>load_data(args)使用方法</h2><p>在<code>src/detector.c</code>中的的<code>train_detector()</code>函数共有<code>3</code>次调用<code>load_data(args)</code>，第一次调用是为训练阶段做好数据准备工作，充分利用这段时间来加载数据。第二次调用是在<code>resize</code>操作中，可以看到这里只有<code>random</code>和<code>count</code>同时满足条件的情况下会做<code>resize</code>操作，也就是说<code>resize</code>加载的数据是未进行<code>resize</code>过的，因此，需要调整<code>args</code>中的图像宽高之后再重新调用<code>load_data(args)</code>加载数据。反之，不做任何处理，之前加载的数据仍然可用。第三次调用就是在数据加载完成后，将加载好的数据保存起来<code>train=buffer</code>; 然后开始下一次的加载工作。这一次的数据就会进行这一次的训练操作(调用<code>train_network</code>函数)。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet数据结构解析</title>
    <url>/2020/02/21/AlexeyAB-DarkNet%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基础数据结构"><a href="#基础数据结构" class="headerlink" title=" 基础数据结构"></a><a id="more"></a> 基础数据结构</h2><p>为了解析网络配置参数，DarkNet 中定义了三个关键的数据结构类型。<code>list</code>类型变量保存所有的网络参数, <code>section</code>类型变量保存的是网络中每一层的网络类型和参数, 其中的参数又是使用list类型来表示。<code>kvp</code>键值对类型用来保存解析后的参数变量和参数值。</p>
<ul>
<li>list类型定义在<code>src/list.h</code>中，代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 链表上的节点</span><br><span class="line">typedef struct node&#123;</span><br><span class="line">    void *val;</span><br><span class="line">    struct node *next;</span><br><span class="line">    struct node *prev;</span><br><span class="line">&#125; node;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;双向链表</span><br><span class="line">typedef struct list&#123;</span><br><span class="line">    int size; &#x2F;&#x2F;list的所有节点个数</span><br><span class="line">    node *front; &#x2F;&#x2F;list的首节点</span><br><span class="line">    node *back; &#x2F;&#x2F;list的普通节点</span><br><span class="line">&#125; list;</span><br></pre></td></tr></table></figure>
<ul>
<li>section 类型定义在<code>src/parser.c</code>文件中，代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 定义section</span><br><span class="line">typedef struct&#123;</span><br><span class="line">    char *type;</span><br><span class="line">    list *options;</span><br><span class="line">&#125;section;</span><br></pre></td></tr></table></figure>
<ul>
<li>kvp 键值对类型定义在<code>src/option_list.h</code>文件中，具体定义如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; kvp 键值对</span><br><span class="line">typedef struct&#123;</span><br><span class="line">    char *key;</span><br><span class="line">    char *val;</span><br><span class="line">    int used;</span><br><span class="line">&#125; kvp;</span><br></pre></td></tr></table></figure>
<p>在Darknet的网络配置文件(<code>.cfg</code>结尾)中，以<code>[</code>开头的行被称为一个段(<code>section</code>)。所有的网络配置参数保存在<code>list</code>类型变量中，<code>list</code>中有很多的<code>section</code>节点，每个<code>section</code>中又有一个保存层参数的小<code>list</code>，整体上出现了一种大链挂小链的结构。大链的每个节点为<code>section</code>，每个<code>section</code>中包含的参数保存在小链中，小链的节点值的数据类型为kvp键值对，这里有个图片可以解释这种结构。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/648.webp" alt></p>
<p>我们来大概解释下该参数网，首先创建一个<code>list</code>，取名<code>sections</code>，记录一共有多少个<code>section</code>（一个<code>section</code>存储了某一网络层所需参数）；然后创建一个<code>node</code>，该<code>node</code>的<code>void</code>类型的指针指向一个新创建的<code>section</code>；该<code>section</code>的<code>char</code>类型指针指向<code>.cfg</code>文件中的某一行（<code>line</code>），然后将该<code>section</code>的<code>list</code>指针指向一个新创建的<code>node</code>，该<code>node</code>的<code>void</code>指针指向一个<code>kvp</code>结构体，<code>kvp</code>结构体中的<code>key</code>就是<code>.cfg</code>文件中的关键字（如：<code>batch，subdivisions</code>等），<code>val</code>就是对应的值；如此循环就形成了上述的参数网络图。</p>
<h2 id="解析并保存网络参数到链表中"><a href="#解析并保存网络参数到链表中" class="headerlink" title="解析并保存网络参数到链表中"></a>解析并保存网络参数到链表中</h2><p>读取配置文件由<code>src/parser.c</code>中的<code>read_cfg()</code>函数实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * 读取神经网络结构配置文件（.cfg文件）中的配置数据， 将每个神经网络层参数读取到每个</span><br><span class="line"> * section 结构体 (每个 section 是 sections 的一个节点) 中， 而后全部插入到</span><br><span class="line"> * list 结构体 sections 中并返回</span><br><span class="line"> *</span><br><span class="line"> * \param: filename    C 风格字符数组， 神经网络结构配置文件路径</span><br><span class="line"> *</span><br><span class="line"> * \return: list 结构体指针，包含从神经网络结构配置文件中读入的所有神经网络层的参数</span><br><span class="line"> * 每个 section 的所在行的开头是 ‘[’ , ‘\0’ , ‘#’ 和 ‘;’ 符号开头的行为无效行, 除此</span><br><span class="line"> *之外的行为 section 对应的参数行. 每一行都是一个等式, 类似键值对的形式.</span><br><span class="line"></span><br><span class="line"> *可以看到, 如果某一行开头是符号 ‘[’ , 说明读到了一个新的 section: current, 然后第1508行</span><br><span class="line"> *list_insert(options, current);&#96; 将该新的 section 保存起来.</span><br><span class="line"></span><br><span class="line"> *在读取到下一个开头符号为 ‘[’ 的行之前的所有行都是该 section 的参数, 在第 1518 行</span><br><span class="line"> *read_option(line, current-&gt;options) 将读取到的参数保存在 current 变量的 options 中.</span><br><span class="line"> *注意, 这里保存在 options 节点中的数据为 kvp 键值对类型.</span><br><span class="line"></span><br><span class="line"> *当然对于 kvp 类型的参数, 需要先将每一行中对应的键和值(用 ‘&#x3D;’ 分割) 分离出来, 然后再</span><br><span class="line"> *构造一个 kvp 类型的变量作为节点元素的数据.</span><br><span class="line"> *&#x2F;</span><br><span class="line">list *read_cfg(char *filename)</span><br><span class="line">&#123;</span><br><span class="line">    FILE *file &#x3D; fopen(filename, &quot;r&quot;);</span><br><span class="line">	&#x2F;&#x2F;一个section表示配置文件中的一个字段，也就是网络结构中的一层</span><br><span class="line">    &#x2F;&#x2F;因此，一个section将读取并存储某一层的参数以及该层的type</span><br><span class="line">    if(file &#x3D;&#x3D; 0) file_error(filename);</span><br><span class="line">    char *line;</span><br><span class="line">    int nu &#x3D; 0; &#x2F;&#x2F;当前读取行号</span><br><span class="line">    list *sections &#x3D; make_list(); &#x2F;&#x2F;sections包含所有的神经网络层参数</span><br><span class="line">    section *current &#x3D; 0;&#x2F;&#x2F;当前读取到某一层</span><br><span class="line">    while((line&#x3D;fgetl(file)) !&#x3D; 0)&#123;</span><br><span class="line">        ++ nu;</span><br><span class="line">        strip(line); &#x2F;&#x2F;去除读入行中含有的空格符</span><br><span class="line">        switch(line[0])&#123;</span><br><span class="line">			 &#x2F;&#x2F; 以 &#39;[&#39; 开头的行是一个新的 section , 其内容是层的 type</span><br><span class="line">            &#x2F;&#x2F; 比如 [net], [maxpool], [convolutional] ...</span><br><span class="line">            case &#39;[&#39;:</span><br><span class="line">                current &#x3D; (section*)xmalloc(sizeof(section));</span><br><span class="line">                list_insert(sections, current);</span><br><span class="line">                current-&gt;options &#x3D; make_list();</span><br><span class="line">                current-&gt;type &#x3D; line;</span><br><span class="line">                break;</span><br><span class="line">            case &#39;\0&#39;: &#x2F;&#x2F;空行</span><br><span class="line">            case &#39;#&#39;: &#x2F;&#x2F;注释</span><br><span class="line">            case &#39;;&#39;: &#x2F;&#x2F;空行</span><br><span class="line">                free(line); &#x2F;&#x2F; 对于上述三种情况直接释放内存即可</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">			    &#x2F;&#x2F; 剩下的才真正是网络结构的数据，调用 read_option() 函数读取</span><br><span class="line">                &#x2F;&#x2F; 返回 0 说明文件中的数据格式有问题，将会提示错误</span><br><span class="line">                if(!read_option(line, current-&gt;options))&#123;</span><br><span class="line">                    fprintf(stderr, &quot;Config file error line %d, could parse: %s\n&quot;, nu, line);</span><br><span class="line">                    free(line);</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F;关闭文件</span><br><span class="line">    fclose(file);</span><br><span class="line">    return sections;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="链表的插入操作"><a href="#链表的插入操作" class="headerlink" title="链表的插入操作"></a>链表的插入操作</h2><p>保存<code>section</code>和每个参数组成的键值对时使用的是<code>list_insert()</code>函数, 前面提到了参数保存的结构其实是大链(节点为<code>section</code>)上边挂着很多小链(每个<code>section</code>节点的各个参数)。<code>list_insert()</code>函数实现了链表插入操作，该函数定义在<code>src/list.c</code>文件中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * 简介: 将 val 指针插入 list 结构体 l 中，这里相当于是用 C 实现了 C++ 中的</span><br><span class="line"> *         list 的元素插入功能</span><br><span class="line"> *</span><br><span class="line"> * 参数: l    链表指针</span><br><span class="line"> *         val  链表节点的元素值</span><br><span class="line"> *</span><br><span class="line"> * 流程：list 中保存的是 node 指针. 因此，需要用 node 结构体将 val 包裹起来后才可以</span><br><span class="line"> *       插入 list 指针 l 中</span><br><span class="line"> *</span><br><span class="line"> * 注意: 此函数类似 C++ 的 insert() 插入方式；</span><br><span class="line"> *      而 opion_insert() 函数类似 C++ map 的按值插入方式，比如 map[key]&#x3D; value</span><br><span class="line"> *</span><br><span class="line"> *      两个函数操作对象都是 list 变量， 只是操作方式略有不同。</span><br><span class="line">*&#x2F;</span><br><span class="line">void list_insert(list *l, void *val)</span><br><span class="line">&#123;</span><br><span class="line">    node* newnode &#x3D; (node*)xmalloc(sizeof(node));</span><br><span class="line">    newnode-&gt;val &#x3D; val;</span><br><span class="line">    newnode-&gt;next &#x3D; 0;</span><br><span class="line">    &#x2F;&#x2F; 如果 list 的 back 成员为空(初始化为 0), 说明 l 到目前为止，还没有存入数据</span><br><span class="line">    &#x2F;&#x2F; 另外, 令 l 的 front 为 new （此后 front 将不会再变，除非删除）</span><br><span class="line">    if(!l-&gt;back)&#123;</span><br><span class="line">        l-&gt;front &#x3D; newnode;</span><br><span class="line">        newnode-&gt;prev &#x3D; 0;</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">        l-&gt;back-&gt;next &#x3D; newnode;</span><br><span class="line">        newnode-&gt;prev &#x3D; l-&gt;back;</span><br><span class="line">    &#125;</span><br><span class="line">    l-&gt;back &#x3D; newnode;</span><br><span class="line">    ++l-&gt;size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到, 插入的数据都会被重新包装在一个新的<code>node</code> : 变量<code>new</code>中，然后再将这个节点插入到链表中。网络结构解析到链表中后还不能直接使用, 因为想使用任意一个参数都不得不每次去遍历整个链表, 这样就会导致程序效率变低, 所以最好的办法是将其保存到一个结构体变量中, 使用的时候按照成员进行访问。复杂度从$O(n)-&gt;O(1)$。</p>
<h2 id="将链表中的网络结构保存到network结构体"><a href="#将链表中的网络结构保存到network结构体" class="headerlink" title="将链表中的网络结构保存到network结构体"></a>将链表中的网络结构保存到network结构体</h2><ul>
<li>首先来看看<code>network</code>结构体的定义，在<code>include/darknet.h</code>中：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 定义network结构体</span><br><span class="line">typedef struct network &#123;</span><br><span class="line">    int n; &#x2F;&#x2F;网络的层数，调用make_network(int n)时赋值</span><br><span class="line">    int batch; &#x2F;&#x2F;一批训练中的图片参数，和subdivsions参数相关</span><br><span class="line">    uint64_t *seen; &#x2F;&#x2F;目前已经读入的图片张数(网络已经处理的图片张数)</span><br><span class="line">    int *t;</span><br><span class="line">    float epoch; &#x2F;&#x2F;到目前为止训练了整个数据集的次数</span><br><span class="line">    int subdivisions;</span><br><span class="line">    layer *layers; &#x2F;&#x2F;存储网络中的所有层</span><br><span class="line">    float *output;</span><br><span class="line">    learning_rate_policy policy; &#x2F;&#x2F; 学习率下降策略</span><br><span class="line">    int benchmark_layers;</span><br><span class="line">    &#x2F;&#x2F; 梯度下降法相关参数</span><br><span class="line">    float learning_rate; &#x2F;&#x2F;学习率</span><br><span class="line">    float learning_rate_min; &#x2F;&#x2F;学习率最小值</span><br><span class="line">    float learning_rate_max;  &#x2F;&#x2F;学习率最大值</span><br><span class="line">    int batches_per_cycle; &#x2F;&#x2F;</span><br><span class="line">    int batches_cycle_mult;</span><br><span class="line">    float momentum;</span><br><span class="line">    float decay;</span><br><span class="line">    float gamma;</span><br><span class="line">    float scale;</span><br><span class="line">    float power;</span><br><span class="line">    int time_steps;</span><br><span class="line">    int step;</span><br><span class="line">    int max_batches;</span><br><span class="line">    int num_boxes;</span><br><span class="line">    int train_images_num;</span><br><span class="line">    float *seq_scales;</span><br><span class="line">    float *scales;</span><br><span class="line">    int   *steps;</span><br><span class="line">    int num_steps;</span><br><span class="line">    int burn_in;</span><br><span class="line">    int cudnn_half;</span><br><span class="line">    &#x2F;&#x2F; ADAM优化方法相关策略</span><br><span class="line">    int adam;</span><br><span class="line">    float B1;</span><br><span class="line">    float B2;</span><br><span class="line">    float eps;</span><br><span class="line"></span><br><span class="line">    int inputs;</span><br><span class="line">    int outputs;</span><br><span class="line">    int truths;</span><br><span class="line">    int notruth;</span><br><span class="line">    int h, w, c;</span><br><span class="line">    int max_crop;</span><br><span class="line">    int min_crop;</span><br><span class="line">    float max_ratio;</span><br><span class="line">    float min_ratio;</span><br><span class="line">    int center;</span><br><span class="line">    int flip; &#x2F;&#x2F; horizontal flip 50% probability augmentaiont for classifier training (default &#x3D; 1)</span><br><span class="line">    int blur;</span><br><span class="line">    int mixup;</span><br><span class="line">    float label_smooth_eps;</span><br><span class="line">    int resize_step;</span><br><span class="line">    int letter_box;</span><br><span class="line">    float angle;</span><br><span class="line">    float aspect;</span><br><span class="line">    float exposure;</span><br><span class="line">    float saturation;</span><br><span class="line">    float hue;</span><br><span class="line">    int random;</span><br><span class="line">    int track;</span><br><span class="line">    int augment_speed;</span><br><span class="line">    int sequential_subdivisions;</span><br><span class="line">    int init_sequential_subdivisions;</span><br><span class="line">    int current_subdivision;</span><br><span class="line">    int try_fix_nan;</span><br><span class="line">    &#x2F;&#x2F;darknet 为每个 GPU 维护一个相同的 network, 每个 network 以 gpu_index 区分</span><br><span class="line">    int gpu_index;</span><br><span class="line">    tree *hierarchy;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F;中间变量，用来暂存某层网络的输入（包含一个 batch 的输入，比如某层网络完成前向，</span><br><span class="line">    &#x2F;&#x2F;将其输出赋给该变量，作为下一层的输入，可以参看 network.c 中的forward_network()</span><br><span class="line">    float *input;</span><br><span class="line">	&#x2F;&#x2F; 中间变量，与上面的 input 对应，用来暂存 input 数据对应的标签数据（真实数据）</span><br><span class="line">    float *truth;</span><br><span class="line">	 &#x2F;&#x2F; 中间变量，用来暂存某层网络的敏感度图（反向传播处理当前层时，用来存储上一层的敏</span><br><span class="line">    &#x2F;&#x2F;感度图，因为当前层会计算部分上一层的敏感度图，可以参看 network.c 中的 backward_network() 函数）</span><br><span class="line">    float *delta;</span><br><span class="line">	&#x2F;&#x2F; 网络的工作空间, 指的是所有层中占用运算空间最大的那个层的 workspace_size,</span><br><span class="line">    &#x2F;&#x2F; 因为实际上在 GPU 或 CPU 中某个时刻只有一个层在做前向或反向运算</span><br><span class="line">    float *workspace;</span><br><span class="line">	&#x2F;&#x2F; 网络是否处于训练阶段的标志参数，如果是则值为1. 这个参数一般用于训练与测试阶段有不</span><br><span class="line">    &#x2F;&#x2F; 同操作的情况，比如 dropout 层，在训练阶段才需要进行 forward_dropout_layer()</span><br><span class="line">    &#x2F;&#x2F; 函数， 测试阶段则不需要进入到该函数</span><br><span class="line">    int train;</span><br><span class="line">	&#x2F;&#x2F; 标志参数，当前网络的活跃层</span><br><span class="line">    int index;</span><br><span class="line">	&#x2F;&#x2F;每一层的损失，只有[yolo]层有值</span><br><span class="line">    float *cost;</span><br><span class="line">    float clip;</span><br><span class="line"></span><br><span class="line">#ifdef GPU</span><br><span class="line">    &#x2F;&#x2F;float *input_gpu;</span><br><span class="line">    &#x2F;&#x2F;float *truth_gpu;</span><br><span class="line">    float *delta_gpu;</span><br><span class="line">    float *output_gpu;</span><br><span class="line"></span><br><span class="line">    float *input_state_gpu;</span><br><span class="line">    float *input_pinned_cpu;</span><br><span class="line">    int input_pinned_cpu_flag;</span><br><span class="line"></span><br><span class="line">    float **input_gpu;</span><br><span class="line">    float **truth_gpu;</span><br><span class="line">    float **input16_gpu;</span><br><span class="line">    float **output16_gpu;</span><br><span class="line">    size_t *max_input16_size;</span><br><span class="line">    size_t *max_output16_size;</span><br><span class="line">    int wait_stream;</span><br><span class="line"></span><br><span class="line">    float *global_delta_gpu;</span><br><span class="line">    float *state_delta_gpu;</span><br><span class="line">    size_t max_delta_gpu_size;</span><br><span class="line">#endif</span><br><span class="line">    int optimized_memory;</span><br><span class="line">    size_t workspace_size_limit;</span><br><span class="line">&#125; network;</span><br></pre></td></tr></table></figure>
<ul>
<li>为<code>network</code>结构体分配内存空间，函数定义在<code>src/network.c</code>文件中，代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;为network结构体分配内存空间</span><br><span class="line">network make_network(int n)</span><br><span class="line">&#123;</span><br><span class="line">    network net &#x3D; &#123;0&#125;;</span><br><span class="line">    net.n &#x3D; n;</span><br><span class="line">    net.layers &#x3D; (layer*)xcalloc(net.n, sizeof(layer));</span><br><span class="line">    net.seen &#x3D; (uint64_t*)xcalloc(1, sizeof(uint64_t));</span><br><span class="line">#ifdef GPU</span><br><span class="line">    net.input_gpu &#x3D; (float**)xcalloc(1, sizeof(float*));</span><br><span class="line">    net.truth_gpu &#x3D; (float**)xcalloc(1, sizeof(float*));</span><br><span class="line"></span><br><span class="line">    net.input16_gpu &#x3D; (float**)xcalloc(1, sizeof(float*));</span><br><span class="line">    net.output16_gpu &#x3D; (float**)xcalloc(1, sizeof(float*));</span><br><span class="line">    net.max_input16_size &#x3D; (size_t*)xcalloc(1, sizeof(size_t));</span><br><span class="line">    net.max_output16_size &#x3D; (size_t*)xcalloc(1, sizeof(size_t));</span><br><span class="line">#endif</span><br><span class="line">    return net;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>src/parser.c</code>中的<code>parse_network_cfg()</code>函数中，从<code>net</code>变量开始，依次为其中的指针变量分配内存。由于第一个段<code>[net]</code>中存放的是和网络并不直接相关的配置参数, 因此网络层的数目为<code>sections-&gt;size - 1</code>，即：<code>network *net = make_network(sections-&gt;size - 1);</code></p>
<ul>
<li><p>将链表中的网络参数解析后保存到<code>network</code>结构体，配置文件的第一个段一定是<code>[net]</code>段，该段的参数解析由<code>parse_net_options()</code>函数完成，函数定义在<code>src/parser.c</code>中。之后的各段都是网络中的层。比如完成特定特征提取的卷积层，用来降低训练误差的<code>shortcur</code>层和防止过拟合的<code>dropout</code>层等。这些层都有特定的解析函数：比如<code>parse_convolutional()</code>, <code>parse_shortcut()</code>和<code>parse_dropout()</code>。每个解析函数返回一个填充好的层<code>l</code>，将这些层全部添加到<code>network</code>结构体的<code>layers</code>数组中。即是：<code>net-&gt;layers[count] = l</code>;另外需要注意的是这行代码：<code>if (l.workspace_size &gt; workspace_size) workspace_size = l.workspace_size</code>;，其中<code>workspace</code>代表网络的工作空间，指的是所有层中占用运算空间最大那个层的<code>workspace</code>。因为在CPU或GPU中某个时刻只有一个层在做前向或反向传播。输出层只能在网络搭建完毕之后才可以确定，输入层需要考虑<code>batch_size</code>的因素，<code>truth</code>是输入标签，同样需要考虑<code>batch_size</code>的因素。具体层的参数解析后面专门写一篇推文来帮助理解。</p>
</li>
<li><p>到这里，网络的宏观解析结束。<code>parse_network_cfg()</code>(<code>src/parser.c</code>中)函数返回解析好的<code>network</code>类型的指针变量。</p>
</li>
</ul>
<h2 id="为啥需要中间数据结构缓存？"><a href="#为啥需要中间数据结构缓存？" class="headerlink" title="为啥需要中间数据结构缓存？"></a>为啥需要中间数据结构缓存？</h2><p>这里可能有个疑问，为什么不将配置文件读取并解析到<code>network</code>结构体变量中, 而要使用一个中间数据结构来缓存读取到的文件呢？因为，如果不使用中间数据结构来缓存. 将读取和解析流程串行进行的话, 如果配置文件较为复杂,  就会长时间使文件处于打开状态。如果此时用户更改了配置文件中的一些条目,  就会导致读取和解析过程出现问题。分开两步进行可以先快速读取文件信息到内存中组织好的结构中, 这时就可以关闭文件.  然后再慢慢的解析参数。这种机制类似于操作系统中断的底半部机制, 先处理重要的中断信号, 然后在系统负荷较小时再处理中断信号中携带的任务。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB DarkNet框架总览</title>
    <url>/2020/02/21/AlexeyAB-DarkNet%E6%A1%86%E6%9E%B6%E6%80%BB%E8%A7%88/</url>
    <content><![CDATA[<h2 id="Darknet框架分析主线"><a href="#Darknet框架分析主线" class="headerlink" title=" Darknet框架分析主线"></a><a id="more"></a> Darknet框架分析主线</h2><h3 id="分析主线的确定"><a href="#分析主线的确定" class="headerlink" title="分析主线的确定"></a>分析主线的确定</h3><p>Darknet相比当前训练的C/C++主流框架（如Caffe）来讲，具有编译速度快，依赖少，易部署等众多优点，我们先定位到<code>src/darknet.c</code>里面的<code>main</code>函数，这是这个框架实现分类，定位，回归，分割等功能的初始入口。这一节的核心代码如下，注意一下就是<code>run_yolo</code>只提供了<code>yolo</code>目标检测算法的原始实现。而<code>run_detector</code>函数提供了AlexeyAB添加了各种新特性的目标检测算法，所以之后我们会从这个函数跟进去来解析Darknet框架。Darknet提供的其他功能如<code>run_super</code>（高分辨率重建），<code>run_classifier</code>（图像分类），<code>run_char_rnn</code>（RNN文本识别）有兴趣可以自己去读（这个框架用来做目标检测比较好，其他算法建议还是去其它框架实现吧），本系列只讲目标检测。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;average&quot;))&#123;</span><br><span class="line">       average(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;yolo&quot;))&#123;</span><br><span class="line">       run_yolo(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;voxel&quot;))&#123;</span><br><span class="line">       run_voxel(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;super&quot;))&#123;</span><br><span class="line">       run_super(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;detector&quot;))&#123;</span><br><span class="line">       run_detector(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;detect&quot;))&#123;</span><br><span class="line">       float thresh &#x3D; find_float_arg(argc, argv, &quot;-thresh&quot;, .24);</span><br><span class="line">	int ext_output &#x3D; find_arg(argc, argv, &quot;-ext_output&quot;);</span><br><span class="line">       char *filename &#x3D; (argc &gt; 4) ? argv[4]: 0;</span><br><span class="line">       test_detector(&quot;cfg&#x2F;coco.data&quot;, argv[2], argv[3], filename, thresh, 0.5, 0, ext_output, 0, NULL, 0, 0);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;cifar&quot;))&#123;</span><br><span class="line">       run_cifar(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;go&quot;))&#123;</span><br><span class="line">       run_go(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;rnn&quot;))&#123;</span><br><span class="line">       run_char_rnn(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;vid&quot;))&#123;</span><br><span class="line">       run_vid_rnn(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;coco&quot;))&#123;</span><br><span class="line">       run_coco(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;classify&quot;))&#123;</span><br><span class="line">       predict_classifier(&quot;cfg&#x2F;imagenet1k.data&quot;, argv[2], argv[3], argv[4], 5);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;classifier&quot;))&#123;</span><br><span class="line">       run_classifier(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;art&quot;))&#123;</span><br><span class="line">       run_art(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;tag&quot;))&#123;</span><br><span class="line">       run_tag(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;compare&quot;))&#123;</span><br><span class="line">       run_compare(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;dice&quot;))&#123;</span><br><span class="line">       run_dice(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;writing&quot;))&#123;</span><br><span class="line">       run_writing(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;3d&quot;))&#123;</span><br><span class="line">       composite_3d(argv[2], argv[3], argv[4], (argc &gt; 5) ? atof(argv[5]) : 0);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;test&quot;))&#123;</span><br><span class="line">       test_resize(argv[2]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;captcha&quot;))&#123;</span><br><span class="line">       run_captcha(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;nightmare&quot;))&#123;</span><br><span class="line">       run_nightmare(argc, argv);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;rgbgr&quot;))&#123;</span><br><span class="line">       rgbgr_net(argv[2], argv[3], argv[4]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;reset&quot;))&#123;</span><br><span class="line">       reset_normalize_net(argv[2], argv[3], argv[4]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;denormalize&quot;))&#123;</span><br><span class="line">       denormalize_net(argv[2], argv[3], argv[4]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;statistics&quot;))&#123;</span><br><span class="line">       statistics_net(argv[2], argv[3]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;normalize&quot;))&#123;</span><br><span class="line">       normalize_net(argv[2], argv[3], argv[4]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;rescale&quot;))&#123;</span><br><span class="line">       rescale_net(argv[2], argv[3], argv[4]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;ops&quot;))&#123;</span><br><span class="line">       operations(argv[2]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;speed&quot;))&#123;</span><br><span class="line">       speed(argv[2], (argc &gt; 3 &amp;&amp; argv[3]) ? atoi(argv[3]) : 0);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;oneoff&quot;))&#123;</span><br><span class="line">       oneoff(argv[2], argv[3], argv[4]);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;partial&quot;))&#123;</span><br><span class="line">       partial(argv[2], argv[3], argv[4], atoi(argv[5]));</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;visualize&quot;))&#123;</span><br><span class="line">       visualize(argv[2], (argc &gt; 3) ? argv[3] : 0);</span><br><span class="line">   &#125; else if (0 &#x3D;&#x3D; strcmp(argv[1], &quot;imtest&quot;))&#123;</span><br><span class="line">       test_resize(argv[2]);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">       fprintf(stderr, &quot;Not an option: %s\n&quot;, argv[1]);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h3 id="跟进run-detector"><a href="#跟进run-detector" class="headerlink" title="跟进run_detector"></a>跟进run_detector</h3><p><code>run_detector</code>函数在<code>src/detector.c</code>里面，这个函数首先有很多超参数可以设置，然后我们可以看到这个函数包含了训练验证，测试，计算Anchors，demo展示，计算map值和recall值等功能。由于训练，测试，验证阶段差不多，我们跟进去一个看看就好，至于后面那几个功能是AlexeyAB添加的，之后再逐一解释。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void run_detector(int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">    int dont_show &#x3D; find_arg(argc, argv, &quot;-dont_show&quot;);&#x2F;&#x2F;展示图像界面</span><br><span class="line">    int benchmark &#x3D; find_arg(argc, argv, &quot;-benchmark&quot;);&#x2F;&#x2F;评估模型的表现</span><br><span class="line">    int benchmark_layers &#x3D; find_arg(argc, argv, &quot;-benchmark_layers&quot;);</span><br><span class="line">    &#x2F;&#x2F;if (benchmark_layers) benchmark &#x3D; 1;</span><br><span class="line">    if (benchmark) dont_show &#x3D; 1;</span><br><span class="line">    int show &#x3D; find_arg(argc, argv, &quot;-show&quot;);</span><br><span class="line">    int letter_box &#x3D; find_arg(argc, argv, &quot;-letter_box&quot;);&#x2F;&#x2F;是否对图像做letter-box变换</span><br><span class="line">    int calc_map &#x3D; find_arg(argc, argv, &quot;-map&quot;);&#x2F;&#x2F;是否计算map值</span><br><span class="line">    int map_points &#x3D; find_int_arg(argc, argv, &quot;-points&quot;, 0);</span><br><span class="line">    check_mistakes &#x3D; find_arg(argc, argv, &quot;-check_mistakes&quot;);&#x2F;&#x2F;检查数据是否有误</span><br><span class="line">    int show_imgs &#x3D; find_arg(argc, argv, &quot;-show_imgs&quot;);&#x2F;&#x2F;显示图片</span><br><span class="line">    int mjpeg_port &#x3D; find_int_arg(argc, argv, &quot;-mjpeg_port&quot;, -1);</span><br><span class="line">    int json_port &#x3D; find_int_arg(argc, argv, &quot;-json_port&quot;, -1);</span><br><span class="line">    char *http_post_host &#x3D; find_char_arg(argc, argv, &quot;-http_post_host&quot;, 0);</span><br><span class="line">    int time_limit_sec &#x3D; find_int_arg(argc, argv, &quot;-time_limit_sec&quot;, 0);</span><br><span class="line">    char *out_filename &#x3D; find_char_arg(argc, argv, &quot;-out_filename&quot;, 0);</span><br><span class="line">    char *outfile &#x3D; find_char_arg(argc, argv, &quot;-out&quot;, 0);</span><br><span class="line">    char *prefix &#x3D; find_char_arg(argc, argv, &quot;-prefix&quot;, 0);&#x2F;&#x2F;模型保存的前缀</span><br><span class="line">    float thresh &#x3D; find_float_arg(argc, argv, &quot;-thresh&quot;, .25);    &#x2F;&#x2F; 置信度</span><br><span class="line">    float iou_thresh &#x3D; find_float_arg(argc, argv, &quot;-iou_thresh&quot;, .5);    &#x2F;&#x2F; 0.5 for mAP</span><br><span class="line">    float hier_thresh &#x3D; find_float_arg(argc, argv, &quot;-hier&quot;, .5);</span><br><span class="line">    int cam_index &#x3D; find_int_arg(argc, argv, &quot;-c&quot;, 0);&#x2F;&#x2F;摄像头编号</span><br><span class="line">    int frame_skip &#x3D; find_int_arg(argc, argv, &quot;-s&quot;, 0);&#x2F;&#x2F;跳帧检测间隔</span><br><span class="line">    int num_of_clusters &#x3D; find_int_arg(argc, argv, &quot;-num_of_clusters&quot;, 5);</span><br><span class="line">    int width &#x3D; find_int_arg(argc, argv, &quot;-width&quot;, -1);&#x2F;&#x2F; 输入网络的图像宽度</span><br><span class="line">    int height &#x3D; find_int_arg(argc, argv, &quot;-height&quot;, -1);&#x2F;&#x2F; 输入网络的图像高度</span><br><span class="line">    &#x2F;&#x2F; extended output in test mode (output of rect bound coords)</span><br><span class="line">    &#x2F;&#x2F; and for recall mode (extended output table-like format with results for best_class fit)</span><br><span class="line">    int ext_output &#x3D; find_arg(argc, argv, &quot;-ext_output&quot;);</span><br><span class="line">    int save_labels &#x3D; find_arg(argc, argv, &quot;-save_labels&quot;);</span><br><span class="line">    if (argc &lt; 4) &#123;</span><br><span class="line">        fprintf(stderr, &quot;usage: %s %s [train&#x2F;test&#x2F;valid&#x2F;demo&#x2F;map] [data] [cfg] [weights (optional)]\n&quot;, argv[0], argv[1]);</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    char *gpu_list &#x3D; find_char_arg(argc, argv, &quot;-gpus&quot;, 0);&#x2F;&#x2F; 多个gpu训练</span><br><span class="line">    int *gpus &#x3D; 0;</span><br><span class="line">    int gpu &#x3D; 0;</span><br><span class="line">    int ngpus &#x3D; 0;</span><br><span class="line">    if (gpu_list) &#123;</span><br><span class="line">        printf(&quot;%s\n&quot;, gpu_list);</span><br><span class="line">        int len &#x3D; (int)strlen(gpu_list);</span><br><span class="line">        ngpus &#x3D; 1;</span><br><span class="line">        int i;</span><br><span class="line">        for (i &#x3D; 0; i &lt; len; ++i) &#123;</span><br><span class="line">            if (gpu_list[i] &#x3D;&#x3D; &#39;,&#39;) ++ngpus;</span><br><span class="line">        &#125;</span><br><span class="line">        gpus &#x3D; (int*)xcalloc(ngpus, sizeof(int));</span><br><span class="line">        for (i &#x3D; 0; i &lt; ngpus; ++i) &#123;</span><br><span class="line">            gpus[i] &#x3D; atoi(gpu_list);</span><br><span class="line">            gpu_list &#x3D; strchr(gpu_list, &#39;,&#39;) + 1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        gpu &#x3D; gpu_index;</span><br><span class="line">        gpus &#x3D; &amp;gpu;</span><br><span class="line">        ngpus &#x3D; 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int clear &#x3D; find_arg(argc, argv, &quot;-clear&quot;);</span><br><span class="line"></span><br><span class="line">    char *datacfg &#x3D; argv[3];&#x2F;&#x2F;存储训练集，验证集，以及类别对应名字等信息的cfg文件</span><br><span class="line">    char *cfg &#x3D; argv[4];&#x2F;&#x2F;要训练的网络cfg文件</span><br><span class="line">    char *weights &#x3D; (argc &gt; 5) ? argv[5] : 0;&#x2F;&#x2F;是否有预训练模型</span><br><span class="line">    if (weights)</span><br><span class="line">        if (strlen(weights) &gt; 0)</span><br><span class="line">            if (weights[strlen(weights) - 1] &#x3D;&#x3D; 0x0d) weights[strlen(weights) - 1] &#x3D; 0;</span><br><span class="line">    char *filename &#x3D; (argc &gt; 6) ? argv[6] : 0;</span><br><span class="line">    if (0 &#x3D;&#x3D; strcmp(argv[2], &quot;test&quot;)) test_detector(datacfg, cfg, weights, filename, thresh, hier_thresh, dont_show, ext_output, save_labels, outfile, letter_box, benchmark_layers);&#x2F;&#x2F;执行目标检测模型测试</span><br><span class="line">    else if (0 &#x3D;&#x3D; strcmp(argv[2], &quot;train&quot;)) train_detector(datacfg, cfg, weights, gpus, ngpus, clear, dont_show, calc_map, mjpeg_port, show_imgs, benchmark_layers);&#x2F;&#x2F;目标检测模型训练</span><br><span class="line">    else if (0 &#x3D;&#x3D; strcmp(argv[2], &quot;valid&quot;)) validate_detector(datacfg, cfg, weights, outfile);&#x2F;&#x2F;目标检测模型验证</span><br><span class="line">    else if (0 &#x3D;&#x3D; strcmp(argv[2], &quot;recall&quot;)) validate_detector_recall(datacfg, cfg, weights);&#x2F;&#x2F;&#x2F;计算验证集的召回率</span><br><span class="line">    else if (0 &#x3D;&#x3D; strcmp(argv[2], &quot;map&quot;)) validate_detector_map(datacfg, cfg, weights, thresh, iou_thresh, map_points, letter_box, NULL);&#x2F;&#x2F;计算验证集的map值</span><br><span class="line">    else if (0 &#x3D;&#x3D; strcmp(argv[2], &quot;calc_anchors&quot;)) calc_anchors(datacfg, num_of_clusters, width, height, show);&#x2F;&#x2F;计算验证集的anchors</span><br><span class="line">    else if (0 &#x3D;&#x3D; strcmp(argv[2], &quot;demo&quot;)) &#123;&#x2F;&#x2F;demo展示</span><br><span class="line">        list *options &#x3D; read_data_cfg(datacfg);</span><br><span class="line">        int classes &#x3D; option_find_int(options, &quot;classes&quot;, 20);</span><br><span class="line">        char *name_list &#x3D; option_find_str(options, &quot;names&quot;, &quot;data&#x2F;names.list&quot;);</span><br><span class="line">        char **names &#x3D; get_labels(name_list);</span><br><span class="line">        if (filename)</span><br><span class="line">            if (strlen(filename) &gt; 0)</span><br><span class="line">                if (filename[strlen(filename) - 1] &#x3D;&#x3D; 0x0d) filename[strlen(filename) - 1] &#x3D; 0;</span><br><span class="line">        demo(cfg, weights, thresh, hier_thresh, cam_index, filename, names, classes, frame_skip, prefix, out_filename,</span><br><span class="line">            mjpeg_port, json_port, dont_show, ext_output, letter_box, time_limit_sec, http_post_host, benchmark, benchmark_layers);</span><br><span class="line"></span><br><span class="line">        free_list_contents_kvp(options);</span><br><span class="line">        free_list(options);</span><br><span class="line">    &#125;</span><br><span class="line">    else printf(&quot; There isn&#39;t such command: %s&quot;, argv[2]);</span><br><span class="line"></span><br><span class="line">    if (gpus &amp;&amp; gpu_list &amp;&amp; ngpus &gt; 1) free(gpus);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="跟进train-detector"><a href="#跟进train-detector" class="headerlink" title="跟进train_detector"></a>跟进train_detector</h3><p>由于训练，验证和测试阶段代码几乎是差不多的，只不过训练多了一个反向传播的过程。所以我们主要分析一下训练过程，训练过程是一个比较复杂的过程，不过宏观上大致分为解析网络配置文件，加载训练样本图像和标签，开启训练，结束训练保存模型这样一个过程，部分代码如下（我省略了很多代码，因为这一节是框架总览，后面会详细解释的）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void train_detector(char *datacfg, char *cfgfile, char *weightfile, int *gpus, int ngpus, int clear, int dont_show, int calc_map, int mjpeg_port, int show_imgs, int benchmark_layers)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; 从options找出训练图片路径信息，如果没找到，默认使用&quot;data&#x2F;train.list&quot;路径下的图片信息（train.list含有标准的信息格式：&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;），</span><br><span class="line">    &#x2F;&#x2F; 该文件可以由darknet提供的scripts&#x2F;voc_label.py根据自行在网上下载的voc数据集生成，所以说是默认路径，其实也需要使用者自行调整，也可以任意命名，不一定要为train.list，</span><br><span class="line">    &#x2F;&#x2F; 甚至可以不用voc_label.py生成，可以自己不厌其烦的制作一个（当然规模应该是很小的，不然太累了。。。）</span><br><span class="line">    &#x2F;&#x2F; 读入后，train_images将含有训练图片中所有图片的标签以及定位信息</span><br><span class="line">    list *options &#x3D; read_data_cfg(datacfg);</span><br><span class="line">    char *train_images &#x3D; option_find_str(options, &quot;train&quot;, &quot;data&#x2F;train.txt&quot;);</span><br><span class="line">    char *valid_images &#x3D; option_find_str(options, &quot;valid&quot;, train_images);</span><br><span class="line">    char *backup_directory &#x3D; option_find_str(options, &quot;backup&quot;, &quot;&#x2F;backup&#x2F;&quot;);</span><br><span class="line"></span><br><span class="line">    network net_map;</span><br><span class="line">    &#x2F;&#x2F;如果要计算map</span><br><span class="line">    if (calc_map) &#123;</span><br><span class="line">        FILE* valid_file &#x3D; fopen(valid_images, &quot;r&quot;);</span><br><span class="line">        if (!valid_file) &#123;</span><br><span class="line">            printf(&quot;\n Error: There is no %s file for mAP calculation!\n Don&#39;t use -map flag.\n Or set valid&#x3D;%s in your %s file. \n&quot;, valid_images, train_images, datacfg);</span><br><span class="line">            getchar();</span><br><span class="line">            exit(-1);</span><br><span class="line">        &#125;</span><br><span class="line">        else fclose(valid_file);</span><br><span class="line"></span><br><span class="line">        cuda_set_device(gpus[0]);</span><br><span class="line">        printf(&quot; Prepare additional network for mAP calculation...\n&quot;);</span><br><span class="line">        net_map &#x3D; parse_network_cfg_custom(cfgfile, 1, 1);</span><br><span class="line">        &#x2F;&#x2F;分类数</span><br><span class="line">        const int net_classes &#x3D; net_map.layers[net_map.n - 1].classes;</span><br><span class="line"></span><br><span class="line">        int k;  &#x2F;&#x2F; free memory unnecessary arrays</span><br><span class="line">        for (k &#x3D; 0; k &lt; net_map.n - 1; ++k) free_layer_custom(net_map.layers[k], 1);</span><br><span class="line"></span><br><span class="line">        char *name_list &#x3D; option_find_str(options, &quot;names&quot;, &quot;data&#x2F;names.list&quot;);</span><br><span class="line">        int names_size &#x3D; 0;</span><br><span class="line">        &#x2F;&#x2F;获取类别对应的名字</span><br><span class="line">        char **names &#x3D; get_labels_custom(name_list, &amp;names_size);</span><br><span class="line">        if (net_classes !&#x3D; names_size) &#123;</span><br><span class="line">            printf(&quot; Error: in the file %s number of names %d that isn&#39;t equal to classes&#x3D;%d in the file %s \n&quot;,</span><br><span class="line">                name_list, names_size, net_classes, cfgfile);</span><br><span class="line">            if (net_classes &gt; names_size) getchar();</span><br><span class="line">        &#125;</span><br><span class="line">        free_ptrs((void**)names, net_map.layers[net_map.n - 1].classes);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    srand(time(0));</span><br><span class="line">     &#x2F;&#x2F; 提取配置文件名称中的主要信息，用于输出打印（并无实质作用），比如提取cfg&#x2F;yolo.cfg中的yolo，用于下面的输出打印</span><br><span class="line">    char *base &#x3D; basecfg(cfgfile);</span><br><span class="line">    printf(&quot;%s\n&quot;, base);</span><br><span class="line">    float avg_loss &#x3D; -1;</span><br><span class="line">    &#x2F;&#x2F; 构建网络：用多少块GPU，就会构建多少个相同的网络（不使用GPU时，ngpus&#x3D;1）</span><br><span class="line">    network* nets &#x3D; (network*)xcalloc(ngpus, sizeof(network));</span><br><span class="line">	</span><br><span class="line">	&#x2F;&#x2F;设定随机数种子</span><br><span class="line">    srand(time(0));</span><br><span class="line">    int seed &#x3D; rand();</span><br><span class="line">    int i;</span><br><span class="line">      &#x2F;&#x2F; for循环次数为ngpus，使用多少块GPU，就循环多少次（不使用GPU时，ngpus&#x3D;1，也会循环一次）</span><br><span class="line">    &#x2F;&#x2F; 这里每一次循环都会构建一个相同的神经网络，如果提供了初始训练参数，也会为每个网络导入相同的初始训练参数</span><br><span class="line">    for (i &#x3D; 0; i &lt; ngpus; ++i) &#123;</span><br><span class="line">        srand(seed);</span><br><span class="line">#ifdef GPU</span><br><span class="line">        cuda_set_device(gpus[i]);</span><br><span class="line">#endif</span><br><span class="line">		&#x2F;&#x2F;解析网络配置文件</span><br><span class="line">        nets[i] &#x3D; parse_network_cfg(cfgfile);</span><br><span class="line">        &#x2F;&#x2F;测试某一个网络层的相关指标如运行时间</span><br><span class="line">        nets[i].benchmark_layers &#x3D; benchmark_layers;</span><br><span class="line">        &#x2F;&#x2F;如果有预训练模型则加载</span><br><span class="line">        if (weightfile) &#123;</span><br><span class="line">            load_weights(&amp;nets[i], weightfile);</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;</span><br><span class="line">        if (clear) *nets[i].seen &#x3D; 0;</span><br><span class="line">        nets[i].learning_rate *&#x3D; ngpus;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="解析配置文件"><a href="#解析配置文件" class="headerlink" title="解析配置文件"></a>解析配置文件</h3><p>截图部分<code>yolov3.cfg</code>网络配置文件如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/647.webp" alt></p>
<p>可以看到配置参数大概分为2类：</p>
<ul>
<li>与训练相关的项，以 [net] 行开头的段. 其中包含的参数有: <code>batch_size, width,height,channel,momentum,decay,angle,saturation,  exposure,hue,learning_rate,burn_in,max_batches,policy,steps,scales</code>。</li>
<li>不同类型的层的配置参数. 如<code>[convolutional], [short_cut], [yolo], [route], [upsample]</code>层等。</li>
</ul>
<p>在src/parse.c中我们会看到一行代码，<code>net-&gt;batch /= net-&gt;subdivisions;</code>，也就是说<code>batch_size</code> 在 darknet 内部又被均分为 <code>net-&gt;subdivisions</code>份, 成为更小的<code>batch_size</code>。 但是这些小的 <code>batch_size</code> 最终又被汇总, 因此 darknet 中的<code>batch_size = net-&gt;batch / net-&gt;subdivisions * net-&gt;subdivisions</code>。此外，和这个参数相关的计算训练图片数目的时候是这样，<code>int imgs = net-&gt;batch * net-&gt;subdivisions * ngpus;</code>，这样可以保证<code>imgs</code>可以被<code>subdivisions</code>整除，因此，通常将这个参数设为8的倍数。从这里也可以看出每个gpu或者cpu都会训练<code>batch</code>个样本。</p>
<p>我们知道了参数是什么样子，那么darknet是如何保存这些参数的呢？这就要看下基本数据结构了。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexeyAB版Darknet使用教程</title>
    <url>/2020/02/20/AlexeyAB%E7%89%88Darknet%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>自从Joseph Redmon提出了yolov3后，其darknet仓库已经获得了16k的star，足以说明darknet的流行。该作者最新一次更新也是一年前了，没有继续维护。不过自来自俄国的大神AlexeyAB在不断地更新darknet, 不仅添加了darknet在window下的适配，而且实现了多种SOTA目标检测算法。AlexeyAB也在库中提供了一份详细的建议，从编译、配置、涉及网络到测量指标等，一应俱全。通过阅读和理解AlexeyAB的建议，可以为我们带来很多启发。本文是来自翻译AlexeyAB的darknet中的README。</p>
<p>下图是CSPNet中统计的目前的State of the Art的目标检测模型。其中csresnext50-panet-spp-optimal模型是CSPNet中提出来的，结合AlexeyAB版本的Darknet就可以实现。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/640.jpg" alt></p>
<h2 id="1-依赖"><a href="#1-依赖" class="headerlink" title="1. 依赖"></a>1. 依赖</h2><h3 id="1-1-环境要求"><a href="#1-1-环境要求" class="headerlink" title="1.1 环境要求"></a>1.1 环境要求</h3><ul>
<li>window系统或者linux系统。</li>
<li>CMake版本高于3.8。</li>
<li>CUDA 10.0，cuDNN&gt;=7.0。</li>
<li>OpenCV版本高于2.4。</li>
<li>Linux下需要GCC 或者Clang, Window下需要Visual Studio 15、17或19版。</li>
</ul>
<h3 id="1-2-数据集获取"><a href="#1-2-数据集获取" class="headerlink" title="1.2 数据集获取"></a>1.2 数据集获取</h3><ol>
<li>MS COCO数据集: 使用<code>./scripits/get_coco_dataset.sh</code>来获取数据集。</li>
<li>OpenImages数据集: 使用<code>./scripits/get_openimages_dataset.py</code>获取数据集,并按照规定的格式重排训练集。</li>
<li>Pascal VOC数据集: 使用<code>./scripits/voc_label.py</code>对数据集标注进行处理。</li>
<li>ILSVRC2012数据集(ImageNet Classification): 使用<code>./scripits/get_imagenet_train.sh</code>获取数据集，运行<code>./scripits/imagenet_label.sh</code>用于验证集。</li>
<li>German/Belgium/Russian/LISA/MASTIF 交通标志数据集。</li>
<li>其他数据集，请访问<code>https://github.com/AlexeyAB/darknet/tree/master/scripts#datasets</code></li>
</ol>
<p>结果示意：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/641.webp" alt></p>
<p>其他测试结果可以访问:<code>https://www.youtube.com/user/pjreddie/videos</code></p>
<h2 id="2-相比原作者Darknet的改进"><a href="#2-相比原作者Darknet的改进" class="headerlink" title="2. 相比原作者Darknet的改进"></a>2. 相比原作者Darknet的改进</h2><ul>
<li>添加了对windows下运行darknet的支持。</li>
<li>添加了SOTA模型： CSPNet, PRN, EfficientNet。</li>
<li>在官方Darknet的基础上添加了新的层：[conv_lstm], [scale_channels] SE/ASFF/BiFPN, [local_avgpool], [sam],  [Gaussian_yolo], [reorg3d] (修复 [reorg]), 修复 [batchnorm]。</li>
<li>可以使用<code>[conv_lstm]</code>层或者<code>[crnn]</code>层来实现针对视频的目标检测。</li>
<li>添加了多种数据增强策略: <code>[net] mixup=1 cutmix=1 mosaic=1 blur=1</code>。</li>
<li>添加了多种激活函数: SWISH, MISH, NORM_CHAN, NORM\CHAN_SOFTMAX。</li>
<li>增加了使用CPU-RAM提高GPU处理训练的能力，以增加<code>mini_batch_size</code>和准确性。</li>
<li>提升了二值网络，让其在CPU和GPU上的训练和测试速度变为原来的2-4倍。</li>
<li>通过将Convolutional层和Batch-Norm层合并成一个层，提升了约7%速度。</li>
<li>如果在Makefile中使用CUDNN_HALF参数，可以让网络在TeslaV100，GeForce RTX等型号的GPU上的检测速度提升两倍。</li>
<li>针对视频的检测进行了优化，对高清视频检测速度可以提升1.2倍，对4k的视频检测速度可以提升2倍。</li>
<li>数据增强部分使用Opencv SSE/AVX指令优化了原来朴素实现的数据增强，数据增强速度提升为原来的3.5倍。</li>
<li>在CPU上使用AVX指令来提高了检测速度，yolov3提高了约85%。</li>
<li>在网络多尺度训练（<code>random=1</code>）的时候优化了内存分配。</li>
<li>优化了检测时的GPU初始化策略，在bacth=1的时候执行初始化而不是当batch=1的时候重新初始化。</li>
<li>添加了计算mAP,F1,IoU, Precision-Recall等指标的方法，只需要运行<code>darknet detector map</code>命令即可。</li>
<li>支持在训练的过程中画loss曲线和准确率曲线，只需要添加<code>-map</code>标志即可。</li>
<li>提供了<code>-json_port</code>,<code>-mjpeg_port</code>选项，支持作为json和mjpeg 服务器来在线获取的结果。可以使用你的编写的软件或者web浏览器与<strong>json和mjpeg服务器</strong>连接。</li>
<li>添加了Anchor的计算功能，可以根据数据集来聚类得到合适的Anchor。</li>
<li>添加了一些目标检测和目标跟踪的示例：<code>https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</code></li>
<li>在使用错误的cfg文件或者数据集的时候，添加了运行时的建议和警告。</li>
<li>其它一些代码修复。</li>
</ul>
<h2 id="3-命令行使用"><a href="#3-命令行使用" class="headerlink" title="3. 命令行使用"></a>3. 命令行使用</h2><p>Linux中使用./darknet，window下使用darknet.exe.</p>
<p>Linux中命令格式类似<code>./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights</code></p>
<p>Linux中的可执行文件在根目录下，Window下则在<code>\build\darknet\x64</code>文件夹中。以是不同情况下应该使用的命令：</p>
<ul>
<li>Yolo v3 COCO - <strong>图片测试</strong>: <code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25</code></li>
<li><strong>输出坐标</strong> of objects: <code>darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg</code></li>
<li>Yolo v3 COCO - <strong>视频测试</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4</code></li>
<li><strong>网络摄像头</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0</code></li>
<li><strong>网络视频摄像头</strong> - Smart WebCam: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg</code></li>
<li>Yolo v3 - <strong>保存视频结果为res.avi</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi</code></li>
<li>Yolo v3 <strong>Tiny版本</strong> COCO - video: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4</code></li>
<li><strong>JSON and MJPEG 服务器</strong> ：创建JSON和MJPEG服务器，允许软件或Web浏览器进行与服务器之间进行多个连接 。假设两者需要的端口为<code>ip-address:8070</code> 和 <code>8090</code>: <code>./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights  test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output</code></li>
<li>Yolo v3 <strong>Tiny</strong> <strong>on GPU</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4</code></li>
<li>另一个可进行图片测试的命令 Yolo v3 COCO - <strong>图片测试</strong>: <code>darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25</code></li>
<li>在 <strong>Amazon EC2</strong>上训练, 如果想要看mAP和Loss曲线，运行以下命令: <code>http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090</code>  (<strong>Darknet 必须使用OpenCV进行编译才能使用该功能</strong>): <code>./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code></li>
<li>186 MB Yolo9000 - <strong>图片分类</strong>: <code>darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights</code></li>
<li><strong>处理一系列图片，并保存结果为json文件</strong>：<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output  -dont_show -out result.json &lt; data/train.txt</code></li>
<li><strong>处理一系列图片，并保存结果为txt文件</strong>:<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output &lt; data/train.txt &gt; result.txt</code></li>
<li><strong>伪标注：</strong> 处理一个list的图片 <code>data/new_train.txt</code> ，可以让结果保存为Yolo训练所需的格式，标注文件为 <code>.txt</code> 。通过这种方法可以迅速增加训练数据量。具体命令为:<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25  -dont_show -save_labels &lt; data/new_train.txt</code></li>
<li><strong>如何计算anchor</strong>(通过聚类得到): <code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code></li>
<li><strong>计算mAP@IoU=50</strong>: <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li>
<li><strong>计算mAP@IoU=75</strong>: <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75</code></li>
</ul>
<p><strong>利用Video-Camera和Mjepg-Stream在Android智能设备中运行YOLOv3</strong></p>
<ol>
<li><p>下载 mjpeg-stream APP: IP Webcam / Smart WebCam:</p>
</li>
<li><ul>
<li>Smart WebCam - 从此处下载: <code>https://play.google.com/store/apps/details?id=com.acontech.android.SmartWebCam2</code></li>
<li>IP Webcam下载地址: <code>https://play.google.com/store/apps/details?id=com.pas.webcam</code></li>
</ul>
</li>
<li><p>将你的手机与电脑通过WIFI或者USB相连。</p>
</li>
<li><p>开启手机中的Smart WebCam APP。</p>
</li>
<li><p>将以下IP地址替换,在Smart WebCam APP中显示，并运行以下命令：</p>
</li>
</ol>
<p>Yolo v3 COCO-model: <code>darknet.exe detector demo data/coco.data yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0</code></p>
<h2 id="4-Linux下如何编译Darknet"><a href="#4-Linux下如何编译Darknet" class="headerlink" title="4. Linux下如何编译Darknet"></a>4. Linux下如何编译Darknet</h2><h3 id="4-1-使用CMake编译Darknet"><a href="#4-1-使用CMake编译Darknet" class="headerlink" title="4.1 使用CMake编译Darknet"></a>4.1 使用CMake编译Darknet</h3><p>CMakeList.txt是一个尝试发现所有安装过的、可选的依赖项(比如CUDA，cuDNN, ZED)的配置文件，然后使用这些依赖项进行编译。它将创建一个共享库文件，这样就可以使用Darknet进行代码开发。</p>
<p>在克隆了项目库以后按照以下命令进行执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir build-release</span><br><span class="line">cd build-release</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<h3 id="4-2-使用make编译Darknet"><a href="#4-2-使用make编译Darknet" class="headerlink" title="4.2 使用make编译Darknet"></a>4.2 使用make编译Darknet</h3><p>在克隆了项目库以后，直接运行<code>make</code>命令，需要注意的是Makefile中有一些可选参数：</p>
<ul>
<li>GPU=1代表编译完成后将可以使用CUDA来进行GPU加速(CUDA应该在<code>/usr/local/cuda</code>中)。</li>
<li>CUDNN=1代表通过cuDNN v5-v7进行编译，这样将可以加速使用GPU训练过程(cuDNN应该在<code>/usr/local/cudnn</code>中)。</li>
<li>CUDNN_HALF=1代表在编译的过程中是否添加Tensor Cores, 编译完成后将可以将目标检测速度提升为原来的3倍，训练网络的速度提高为原来的2倍。</li>
<li>OPENCV=1代表编译的过程中加入OpenCV, 目前支持的OpenCV的版本有4.x/3.x/2.4.x， 编译结束后将允许Darknet对网络摄像头的视频流或者视频文件进行目标检测。</li>
<li>DEBUG=1 代表是否开启YOLO的debug模式。</li>
<li>OPENMP=1代表编译过程将引入openmp,编译结束后将代表可以使用多核CPU对yolo进行加速。</li>
<li>LIBSO=1 代表编译库darknet.so。</li>
<li>ZED_CAMERA=1 构建具有ZED-3D相机支持的库(应安装ZED SDK)，然后运行。</li>
</ul>
<h2 id="5-如何在Window下编译Darknet"><a href="#5-如何在Window下编译Darknet" class="headerlink" title="5. 如何在Window下编译Darknet"></a>5. 如何在Window下编译Darknet</h2><h3 id="5-1-使用CMake-GUI进行编译"><a href="#5-1-使用CMake-GUI进行编译" class="headerlink" title="5.1 使用CMake-GUI进行编译"></a>5.1 使用CMake-GUI进行编译</h3><p>建议使用以下方法来完成Window下Darknet的编译，需要环境有：Visual Studio 15/17/19, CUDA&gt;10.0, cuDNN&gt;7.0, OpenCV&gt;2.4</p>
<p>使用CMake-GUI编译流程：</p>
<ol>
<li>Configure.</li>
<li>Optional platform for generator (Set: x64) .</li>
<li>Finish.</li>
<li>Generate.</li>
<li>Open Project.</li>
<li>Set: x64 &amp; Release.</li>
<li>Build.</li>
<li>Build solution.</li>
</ol>
<h3 id="5-2-使用vcpkg进行编译"><a href="#5-2-使用vcpkg进行编译" class="headerlink" title="5.2 使用vcpkg进行编译"></a>5.2 使用vcpkg进行编译</h3><p>如果你已经满足Visual Studio 15/17/19 、CUDA&gt;10.0、 cuDNN&gt;7.0、OpenCV&gt;2.4的条件, 那么推荐使用通过CMake-GUI的方式进行编译。</p>
<p>否则按照以下步骤进行编译:</p>
<ul>
<li>安装或更新Visual Studio到17+,确保已经对其进行全面修补。</li>
<li>安装CUDA和cuDNN。</li>
<li>安装Git和CMake, 并将它们加入环境变量中。</li>
<li>安装vcpkg然后尝试安装一个测试库来确认安装是正确的，比如：<code>vcpkg install opengl</code>。</li>
<li>定义一个环境变量<code>VCPKG_ROOT</code>, 指向vcpkg的安装路径。</li>
<li>定义另一个环境变量<code>VCPKG_DEFAULT_TRIPLET</code>将其指向x64-windows。</li>
<li>打开Powershell然后运行以下命令：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PS \&gt;                  cd $env:VCPKG_ROOT</span><br><span class="line">PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] </span><br><span class="line">#replace with opencv[cuda,ffmpeg] in case you want to use cuda-accelerated openCV</span><br></pre></td></tr></table></figure>
<ul>
<li>打开Powershell, 切换到darknet文件夹，然后运行<code>.\build.ps1</code>进行编译。如果要使用Visual Studio，将在Build后找到CMake为您创建的两个自定义解决方案，一个在<code>build_win_debug</code>中，另一个在<code>build_win_release</code>中，其中包含适用于系统的所有配置标志。</li>
</ul>
<h3 id="5-3-使用legacy-way进行编译"><a href="#5-3-使用legacy-way进行编译" class="headerlink" title="5.3 使用legacy way进行编译"></a>5.3 使用legacy way进行编译</h3><ul>
<li><p>如果你有CUDA10.0、cuDNN 7.4 和OpenCV 3.x , 那么打开<code>build\darknet\darknet.sln</code>, 设置x64和Release 然后运行Build， 进行darknet的编译，将cuDNN加入环境变量中。</p>
<ul>
<li>在<code>C:\opencv_3.0\opencv\build\x64\vc14\bin</code>找到<code>opencv_world320.dll</code>和<code>opencv_ffmpeg320_64.dll</code>, 然后将其复制到<code>darknet.exe</code>同级目录中。</li>
<li>在<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</code>中检查是否含有bin和include文件夹。如果没有这两个文件夹，那就将他们从CUDA安装的地方复制到这个地方。</li>
<li>安装cuDNN 7.4.1 来匹配CUDA 10.0, 将cuDNN添加到环境变量<code>CUDNN</code>。将<code>cudnn64_7.dll</code>复制到<code>\build\darknet\x64</code>中。</li>
</ul>
</li>
<li><p>如果你是用的是其他版本的CUDA（不是CUDA 10.0）, 那么使用Notepad打开<code>build\darknet\darknet.vxcproj</code>, 将其中的CUDA 10.0替换为你的CUDA的版本。然后打开<code>\darknet.sln</code>, 然后右击工程，点击属性properties, 选择CUDA C/C++, 然后选择Device , 然后移除<code>compute_75,sm_75</code>。之后从第一步从头开始执行。</p>
</li>
<li><p>如果你没有GPU但是有OpenCV3.0， 那么打开<code>build\darknet\darknet_no_gpu.sln</code>, 设置x64和Release， 然后运行build -&gt; build darknet_no_gpu。</p>
</li>
<li><p>如果你只安装了OpenCV 2.4.14，那你应该修改<code>\darknet.sln</code>中的路径。</p>
</li>
<li><ul>
<li>(右键点击工程) -&gt; properties -&gt; C/C++ -&gt; General -&gt; Additional Include Directories: <code>C:\opencv_2.4.13\opencv\build\include</code></li>
<li>(右键点击工程)-&gt; properties -&gt; Linker -&gt; General -&gt; Additional Library Directories: <code>C:\opencv_2.4.13\opencv\build\x64\vc14\lib</code></li>
</ul>
</li>
<li><p>如果你的GPU有Tensor Cores(Nvidia Titan V/ Tesla V100/ DGX-2等型号)， 可以提升目标检测模型测试速度为原来的3倍，训练速度变为原来的2倍。<code>\darknet.sln</code> -&gt; (右键点击工程) -&gt; properties -&gt; C/C++ -&gt; Preprocessor -&gt; Preprocessor Definitions, and add here: <code>CUDNN_HALF;</code></p>
<p><strong>注意</strong>：CUDA 必须在Visual Studio安装后再安装。</p>
</li>
</ul>
<h2 id="6-如何训练"><a href="#6-如何训练" class="headerlink" title="6. 如何训练"></a>6. 如何训练</h2><h3 id="6-1-Pascal-VOC-dataset"><a href="#6-1-Pascal-VOC-dataset" class="headerlink" title="6.1 Pascal VOC dataset"></a>6.1 Pascal VOC dataset</h3><ol>
<li><p>下载预训练模型 (154 MB): <code>http://pjreddie.com/media/files/darknet53.conv.74</code> 将其放在 <code>build\darknet\x64</code>文件夹中。</p>
</li>
<li><p>下载pascal voc数据集并解压到 <code>build\darknet\x64\data\voc</code> 放在 <code>build\darknet\x64\data\voc\VOCdevkit\</code>文件夹中:</p>
<p>2.1 下载 <code>voc_label.py</code> 到 <code>build\darknet\x64\data\voc</code>，地址为: <code>http://pjreddie.com/media/files/voc_label.py。</code></p>
</li>
<li><ul>
<li><code>http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar</code>。</li>
<li><code>http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar</code>。</li>
<li><code>http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar</code>。</li>
</ul>
</li>
<li><p>下载并安装python: <code>https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe</code></p>
</li>
<li><p>运行命令: <code>python build\darknet\x64\data\voc\voc_label.py</code> (来生成文件: 2007_test.txt, 2007_train.txt, 2007_val.txt, 2012_train.txt, 2012_val.txt)。</p>
</li>
<li><p>运行命令: <code>type 2007_train.txt 2007_val.txt 2012_*.txt &gt; train.txt</code>。</p>
</li>
<li><p>在 <code>yolov3-voc.cfg</code>文件中设置 <code>batch=64</code> 和<code>subdivisions=8</code>。</p>
</li>
<li><p>使用 <code>train_voc.cmd</code> 或者使用以下命令开始训练:</p>
<p><code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code>。</p>
</li>
</ol>
<p>(<strong>Note:</strong> 如果想要停止loss显示，添加 <code>-dont_show</code>标志. 如果使用CPU运行, 用<code>darknet_no_gpu.exe</code> 代替 <code>darknet.exe</code>。)</p>
<p>如果想要改数据集路径的话，请修改 <code>build\darknet\cfg\voc.data</code>文件。</p>
<p><strong>Note:</strong> 在训练中如果你看到avg为nan，那证明训练出错。但是如果在其他部分出现nan，这属于正常现象，训练过程是正常的。</p>
<h3 id="6-2-如何使用多GPU训练？"><a href="#6-2-如何使用多GPU训练？" class="headerlink" title="6.2 如何使用多GPU训练？"></a>6.2 如何使用多GPU训练？</h3><ol>
<li>首先在一个GPU中训练大概1000个轮次: <code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code>。</li>
<li>然后停下来基于这个保存的模型 <code>/backup/yolov3-voc_1000.weights</code> 使用多GPU (最多4个GPU): <code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg /backup/yolov3-voc_1000.weights -gpus 0,1,2,3</code>。</li>
</ol>
<p>在多GPU训练的时候，<code>learning rate</code>需要进行修改，比如单<code>gpu使用0.001</code>，那么多gpu应该使用0.001/GPUS。然后<code>cfg</code>文件中的<code>burn_in</code>参数和<code>max_batches</code>参数要设置为原来的GPUS倍。</p>
<h3 id="6-3-训练自定义数据集-重点关注"><a href="#6-3-训练自定义数据集-重点关注" class="headerlink" title="6.3 训练自定义数据集(重点关注)"></a>6.3 训练自定义数据集(重点关注)</h3><p>训练较早提出的Yolo系列算法如<code>yolov2-voc.cfg</code>, <code>yolov2-tiny-voc.cfg</code>, <code>yolo-voc.cfg</code>, <code>yolo-voc.2.0.cfg</code>，请看<code>https://github.com/AlexeyAB/darknet/tree/47c7af1cea5bbdedf1184963355e6418cb8b1b4f#how-to-train-pascal-voc-data</code>。</p>
<p>Training Yolo v3:</p>
<ol>
<li>创建与 <code>yolov3.cfg</code>内容相同的 <code>yolo-obj.cfg</code> 或者直接复制然后重命名为<code>yolo-obj.cfg</code> 然后</li>
</ol>
<ul>
<li><p>设置<code>cfg</code>文件中 <code>batch=64</code>。</p>
</li>
<li><p>设置<code>cfg</code>文件中 <code>subdivisions=16</code>。</p>
</li>
<li><p>设置<code>cfg</code>文件中<code>max_batches</code>参数 (一般可以设置为<code>classes*2000</code> 但是不要低于 <code>4000</code>), 比如 如果你有三个类，那么设置<code>max_batches=6000</code>。</p>
</li>
<li><p>设置<code>steps</code>参数，一般为80%和90%的<code>max_batches</code>。比如 <code>steps=4800,5400</code></p>
</li>
<li><p>设置网络输入长宽必须能够整除32，比如 <code>width=416 height=416</code> `</p>
</li>
<li><p>修改yolo层中的 <code>classes=80</code> 改为你的类别的个数，比如<code>classes=3</code>:</p>
</li>
<li><p>修改yolo层前一个卷积层convolutional输出通道数。修改的<code>filter</code>个数有一定要求，按照公式<code>filters=(classes+5)×3</code>来设置。这里的<code>5</code>代表<code>x, y, w, h, conf</code>, 这里的<code>3</code>代表分配<code>3</code>个anchor。</p>
</li>
<li><p>如果使用 <code>[Gaussian_yolo]</code> (Gaussian_yolov3_BDD.cfg)，<code>filters</code>计算方式不太一样，按照 <code>filters=(classes + 9)x3</code>进行计算。</p>
</li>
<li><p>通常来讲，filters的个数计算依赖于类别个数，坐标以及<code>mask</code>的个数（<code>cfg</code>中的<code>mask</code>参数也就是<code>anchors</code>的个数）。</p>
<p>举个例子,对于两个目标,你的 <code>yolo-obj.cfg</code> 和<code>yolov3.cfg</code> 不同的地方应该在每个<code>[yolo]/[region]</code>层的下面几行:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">filters&#x3D;21</span><br><span class="line"></span><br><span class="line">[region]</span><br><span class="line">classes&#x3D;2</span><br></pre></td></tr></table></figure>
<ol>
<li>在<code>build\darknet\x64\data\</code>创建文件 <code>obj.names</code> , 每行一个类别的名称。</li>
<li>在<code>build\darknet\x64\data\</code> 创建<code>obj.data</code>, 具体内容如下:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classes&#x3D; 2 # 你的类别的个数</span><br><span class="line">train  &#x3D; data&#x2F;train.txt # 存储用于训练的图片位置</span><br><span class="line">valid  &#x3D; data&#x2F;test.txt # 存储用于测试的图片的位置</span><br><span class="line">names &#x3D; data&#x2F;obj.names # 每行一个类别的名称</span><br><span class="line">backup &#x3D; backup&#x2F;</span><br></pre></td></tr></table></figure>
<ol>
<li>将你的图片放在 <code>build\darknet\x64\data\obj\</code>文件夹下。</li>
<li>你应该标注你的数据集中的每一张图片，使用<code>Yolo_mark</code>这个可视化GUI软件来标注出目标框并且产生标注文件。地址： <code>https://github.com/AlexeyAB/Yolo_mark</code>。</li>
</ol>
<p>软件将会为每一个图像创建一个<code>txt</code>文件，并将其放在同一个文件夹中，命名与原图片的名称相同，唯一不同的就是后缀是txt。txt标注文件中每一个目标独占一行，按照<code>&lt;object-class&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code>的格式排布。</p>
<p>具体参数解释：</p>
<ul>
<li><p><code>&lt;object-class&gt;</code>- 是从 <code>0</code> 到 <code>(classes-1)</code>的整数，代表具体的类别。</p>
</li>
<li><p><code>&lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> -  是归一化到<code>(0.0 to 1.0]</code>之间的浮点数，都是相对于图片整体的宽和高的一个相对值。</p>
</li>
<li><p>比如: <code>&lt;x&gt; = &lt;absolute_x&gt; / &lt;image_width&gt;</code> 或者 <code>&lt;height&gt; = &lt;absolute_height&gt; / &lt;image_height&gt;</code></p>
</li>
<li><p>需要注意的是: <code>&lt;x_center&gt; &lt;y_center&gt;</code> - 是标注框的中心点，而不是左上角。请注意格式。</p>
<p>举个例子，img1.txt中内容如下，代表有两个类别的三个目标：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 0.716797 0.395833 0.216406 0.147222</span><br><span class="line">0 0.687109 0.379167 0.255469 0.158333</span><br><span class="line">1 0.420312 0.395833 0.140625 0.166667</span><br></pre></td></tr></table></figure>
<ol>
<li>在<code>build\darknet\x64\data\</code>文件夹中创建train.txt文件，每行包含的是训练集图片的内容。其路径是相对于 <code>darknet.exe</code>的路径或者绝对路径：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data&#x2F;obj&#x2F;img1.jpg</span><br><span class="line">data&#x2F;obj&#x2F;img2.jpg</span><br><span class="line">data&#x2F;obj&#x2F;img3.jpg</span><br></pre></td></tr></table></figure>
<ol>
<li><p>下载预训练权重，并将其放在 <code>build\darknet\x64</code>文件夹中。</p>
<ul>
<li>对于<code>csresnext50-panet-spp.cfg</code> (133 MB)：请查看原工程。</li>
<li>对于<code>yolov3.cfg, yolov3-spp.cfg</code> (154 MB)：请查看原工程。</li>
<li>对于<code>yolov3-tiny-prn.cfg , yolov3-tiny.cfg</code> (6 MB)：请查看原工程。</li>
<li>对于<code>enet-coco.cfg (EfficientNetB0-Yolov3)</code>：请查看原工程。</li>
</ul>
</li>
<li><p>使用以下命令行开始训练: <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74</code>。</p>
<p>对于linux用户使用以下命令开始训练: <code>./darknet detector train data/obj.data yolo-obj.cfg darknet53.conv.74</code> (使用<code>./darknet</code> 而不是 <code>darknet.exe</code>)。</p>
<p>如果想训练的过程中同步显示mAP（每四个epoch进行一次更新），运行命令: <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map</code>。</p>
<ul>
<li>权重文件 <code>yolo-obj_last.weights</code> 将会保存在 <code>build\darknet\x64\backup\</code> 文件夹中，每100个迭代保存一次。</li>
<li>权重文件<code>yolo-obj_xxxx.weights</code> 将会保存在 <code>build\darknet\x64\backup\</code> 文件夹中，每1000个迭代保存一次。</li>
<li>如果不想在训练的过程中同步展示loss曲线，请执行以下命令 <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show</code>。</li>
<li>如果想在训练过程中查看mAP和Loss曲线，可以使用以下命令：<code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code> ，然后在浏览器中打开 URL <code>http://ip-address:8090</code> 。</li>
</ul>
</li>
<li><p>训练结束以后，将会在文件夹<code>build\darknet\x64\backup\</code>中得到权重文件 <code>yolo-obj_final.weights</code> 。</p>
</li>
</ol>
<ul>
<li>在100次迭代以后，你可以停下来，然后从这个点加载模型继续训练。比如说, 你在2000次迭代以后停止训练，如果你之后想要恢复训练，只需要运行命令： <code>darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights</code>，而不需要重头开始训练。</li>
</ul>
<p><strong>注意</strong>：</p>
<ol>
<li>如果在训练的过程中，发现<code>avg</code>指标变为<code>nan</code>，那证明训练过程有误，可能是数据标注越界导致的问题。但是其他指标有<code>nan</code>是正常的。</li>
<li>修改<code>width</code>,<code>height</code>的时候必须要保证两者都能够被32整除。</li>
<li>训练结束后，可以使用以下命令来进行测试：<code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></li>
<li>如果出现<code>Ouf of memery</code>问题，那说明显卡的显存不够，你可以通过设置<code>subdivisions</code>参数，将其从原来的<code>16</code>提高为<code>32</code>或者<code>64</code>，这样就能降低使用的显存，保证程序正常运行。</li>
</ol>
<h3 id="6-4-训练tiny-yolo"><a href="#6-4-训练tiny-yolo" class="headerlink" title="6.4 训练tiny-yolo"></a>6.4 训练tiny-yolo</h3><p>训练tiny yolo与以上的训练过程并无明显区别，除了以下几点：</p>
<ul>
<li>下载tiny yolo的预训练权重：<code>https://pjreddie.com/media/files/yolov3-tiny.weights</code></li>
<li>使用以下命令行来获取预训练权重: <code>darknet.exe partial cfg/yolov3-tiny.cfg yolov3-tiny.weights yolov3-tiny.conv.15 15</code>， 这里的15代表前15个层，也就是backbone所在的层。</li>
<li>使用的配置文件应该是 <code>cfg/yolov3-tiny_obj.cfg</code> 而不是 <code>yolov3.cfg</code></li>
<li>使用以下命令开始训练: <code>darknet.exe detector train data/obj.data yolov3-tiny-obj.cfg yolov3-tiny.conv.15</code></li>
</ul>
<p>如果想使用其他backbone进行训练比如 DenseNet201-Yolo或者ResNet50-Yolo, 你可以在以下链接中找到:<code>https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd</code></p>
<p>如果你采用的是自己设计的backbone,那就无法进行迁移学习，backbone可以直接进行参数随机初始化。</p>
<h3 id="6-5-什么时候停止训练"><a href="#6-5-什么时候停止训练" class="headerlink" title="6.5 什么时候停止训练"></a>6.5 什么时候停止训练</h3><p>建议为每个类分配至少2000次迭代，但是整体迭代次数不应少于4000次。如果想要更加精准地定义什么时候该停止训练，需要使用以下方法：</p>
<ol>
<li>训练过程中，你将会看到日志中有很多错误的度量指标，你需要在avg指标不再下降的时候停止训练，如下图所示:</li>
</ol>
<blockquote>
<p>Region Avg IOU: 0.798363, Class: 0.893232, Obj: 0.700808, No Obj: 0.004567, Avg Recall: 1.000000,  count: 8 Region Avg IOU: 0.800677, Class: 0.892181, Obj: 0.701590, No Obj: 0.004574, Avg Recall: 1.000000,  count: 8</p>
<p><strong>9002</strong>: 0.211667, <strong>0.60730 avg</strong>, 0.001000 rate, 3.868000 seconds, 576128 images Loaded: 0.000000 seconds</p>
</blockquote>
<ul>
<li><p><strong>9002</strong> - 代表当前的迭代次数。</p>
</li>
<li><p><strong>0.60730 avg</strong> - average loss (error) - <strong>这个指标是平均loss, 其越低越好。</strong></p>
<p>在这个指标不再下降的时候就可以停止训练了。最终的值大概分布在0.05-3.0之间，小而简单的模型通常最终loss比较小，大而复杂的loss可能会比较大。</p>
</li>
</ul>
<p>训练完成后，你就可以从 <code>darknet\build\darknet\x64\backup</code> 文件夹中取出比较靠后的几个<code>weights</code>文件，并对他们进行测试，选择最好的权重文件。</p>
<p>举个例子，你在<code>9000</code>次迭代后停止训练，但最好的权重可能是<code>7000,8000,9000</code>次的值。这种情况的出现是由于<strong>过拟合</strong>导致的。<strong>过拟合</strong>是由于过度学习训练集的分布，而降低了模型在测试集的泛化能力。</p>
<p><strong>Early Stopping Point</strong>示意图:</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/642.webp" alt></p>
<p>为了得到在early stopping point处的权重：</p>
<p>2.1 首先，你的obj.data文件中应该含有valid=valid.txt一项，用于测试在验证集的准确率。如果你没有验证集图片，那就直接复制train.txt重命名为valid.txt。</p>
<p>2.2 假如你选择在<code>9000</code>次迭代后停止，那可以通过以下命令测试<code>7000,8000,9000</code>三个模型的相关指标。选择最高<code>mAP</code>或者最高<code>IoU</code>的模型最为最终模型。</p>
<ul>
<li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li>
<li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_8000.weights</code></li>
<li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_9000.weights</code></li>
</ul>
<p>或者你可以选择使用<code>-map</code>标志符来直接实时测试mAP值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">darknet.exe detector train data&#x2F;obj.data yolo-obj.cfg darknet53.conv.74 -map</span><br></pre></td></tr></table></figure>
<p>然后你就能得到loss曲线和mAP曲线，mAP每4个epoch对验证集进行一次测试，并将结果显示在图中。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/643.webp" alt></p>
<p>指标解释</p>
<ul>
<li><p><strong>IoU</strong> (intersect over union) - 平均交并比</p>
</li>
<li><p><strong>mAP</strong> (mean average precision) - 每个类的平均精度。</p>
</li>
</ul>
<p><strong>mAP</strong> 是Pascal VOC竞赛的默认指标，与MS COCO竞赛中的AP50指标是一致的。</p>
<p>Precision和Recall参数在Pascal VOC竞赛中略微不同，但 <strong>IoU 的意义都是相同的</strong>。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/644.jpg" alt></p>
<h3 id="6-6-如何在pascal-voc2007数据集上计算mAP指标"><a href="#6-6-如何在pascal-voc2007数据集上计算mAP指标" class="headerlink" title="6.6 如何在pascal voc2007数据集上计算mAP指标"></a>6.6 如何在pascal voc2007数据集上计算mAP指标</h3><ol>
<li>在VOC2007中计算mAP：</li>
</ol>
<ul>
<li>下载VOC数据集，安装python并且下载<code>`2007_test.txt</code>文件，具体可以参考链接：<code>https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data</code></li>
<li>下载文件 <code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/scripts/voc_label_difficult.py</code> 到 <code>build\darknet\x64\data\</code> 文件夹，然后运行 <code>voc_label_difficult.py</code> 从而得到 <code>difficult_2007_test.txt</code>。</li>
<li>将下面voc.data文件中的第四行#删除</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classes&#x3D; 20</span><br><span class="line">train  &#x3D; data&#x2F;train_voc.txt</span><br><span class="line">valid  &#x3D; data&#x2F;2007_test.txt</span><br><span class="line">#difficult &#x3D; data&#x2F;difficult_2007_test.txt</span><br><span class="line">names &#x3D; data&#x2F;voc.names</span><br><span class="line">backup &#x3D; backup&#x2F;</span><br></pre></td></tr></table></figure>
<p>然后就有两个方法来计算得到mAP:</p>
<ol>
<li>使用Darknet + Python: 运行 <code>build/darknet/x64/calc_mAP_voc_py.cmd</code> ，你将得到 <code>yolo-voc.cfg</code> 模型的mAP值, mAP = 75.9%</li>
<li>直接使用命令: 运行文件 <code>build/darknet/x64/calc_mAP.cmd</code> -你将得到 <code>yolo-voc.cfg</code> 模型, 得到mAP = 75.8%</li>
</ol>
<p>YOLOv3的论文：<code>https://arxiv.org/pdf/1612.08242v1.pdf</code>指出对于416x416的YOLOv2，Pascal Voc上的mAP值是76.8%。我们得到的值较低，可能是由于模型在进行检测时的代码略有不同。</p>
<p>如果你想为<code>tiny-yolo-voc</code>计算mAP值，将脚本中<code>tiny-yolo-voc.cfg</code>取消注释，将<code>yolo-voc.cfg</code>注释掉。</p>
<p>如果你是用的是python 2.x 而不是python 3.x, 而且你选择使用Darknet+Python的方式来计算mAP, 那你应该使用 <code>reval_voc.py</code> 和 <code>voc_eval.py</code> 而不是使用 <code>reval_voc_py3.py</code> 和 <code>voc_eval_py3.py</code> 。以上脚本来自以下目录：<code>https://github.com/AlexeyAB/darknet/tree/master/scripts</code>。</p>
<p>目标检测的例子：<code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/645.webp" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/646.webp" alt></p>
<h2 id="7-如何提升目标检测性能？"><a href="#7-如何提升目标检测性能？" class="headerlink" title="7. 如何提升目标检测性能？"></a>7. 如何提升目标检测性能？</h2><ol>
<li><p>训练之前：</p>
<ul>
<li><code>train_network_width * train_obj_width / train_image_width ~= detection_network_width *  detection_obj_width / detection_image_width</code></li>
<li><p><code>train_network_height * train_obj_height / train_image_height ~= detection_network_height *  detection_obj_height / detection_image_height</code></p>
</li>
<li><p>完整模型（5个yolo层）：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg</code>。</p>
</li>
<li><p>Tiny模型（3个yolo层）：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny_3l.cfg</code>。</p>
</li>
<li><p>带空间金字塔池化的完整模型（3个yolo层）：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-spp.cfg</code>。</p>
</li>
<li><p>在<code>cfg</code>文件中将<code>random</code>设为1：这将在Yolo中使用多尺度训练，会提升检测模型准确率。</p>
</li>
<li><p>在<code>cfg</code>文件中把输入分辨率增大(<code>height=608</code>, <code>width=608</code>或者其他任意32的倍数)：这将提升检测模型准确率。</p>
</li>
<li><p>检查你要检测的每个目标在数据集中是否被标记，数据集中任何目标都不应该没有标签。在大多数训练出问题的情况中基本都是有错误的标签（通过使用某些转换脚本，使用第三方工具标注来获得标签），可以通过<code>https://github.com/AlexeyAB/Yolo_mark</code>来检查你的数据集是否全部标注正确。</p>
</li>
<li><p>我的损失函数很高并且mAP很低，训练出错了吗？在训练命令末端使用<code>-show_imgs</code> 标志来运行训练，你是否能看到有正确的边界预测框的目标（在窗口或者<code>aug_...jpg</code>）？如果没有，训练是发生错误了。</p>
</li>
<li><p>对于你要检测的每个目标，训练数据集中必须至少有一个相似的目标，它们具有大致相同的形状，物体侧面姿态，相对大小，旋转角度，倾斜度，照明度等。理想的是，你的训练数据集包括具有不同比例，旋转角度，照明度，物体侧面姿态和处于不同背景的目标图像，你最好拥有2000张不同的图像，并且至少训练<code>2000×classes</code>轮次。</p>
</li>
<li><p>希望你的训练数据集图片包含你不想检测的未标注的目标，也即是无边界框的负样本图片(空的<code>.txt</code>文件)，并且负样本图片的数量和带有目标的正样本图片数量最好一样多。</p>
</li>
<li><p>标注目标的最佳方法是：仅仅标记目标的可见部分或者标记目标的可见和重叠部分，或标记比整个目标多一点(有一点间隙)?根据你希望如何检测目标来进行标记。</p>
</li>
<li><p>为了对图片中包含大量目标的数据进行训练，添加<code>max=200</code>或者更高的值在你<code>cfg</code>文件中<code>yolo</code>层或者<code>region</code>层的最后一行（YOLOv3可以检测到的目标全局最大数量为<code>0,0615234375*(width*height)</code>其中<code>width</code>和<code>height</code>是在<code>cfg</code>文件中的<code>[net]</code>部分指定的）。</p>
</li>
<li><p>对于小目标的训练（把图像resize到416x416大小后小于16x16的目标）：设置<code>layers = -1, 11</code>而不是<code>layers=-1, 36</code>；设置<code>stride=4</code>而不是<code>stride=2</code>。</p>
</li>
<li><p>对于既有大目标又有小目标的训练使用下面的模型：</p>
</li>
<li><p>如果你要训练模型将左右目标分为单独的类别（左/右手，左/右交通标志），那就禁用翻转的数据扩充方式，即在数据输入部分添加<code>flip=0</code>。</p>
</li>
<li><p>一般规则：你的训练数据集应包括一组您想要检测的相对大小的目标，如下：</p>
<p>即，对于测试集中的每个目标，训练集中必须至少有一个同类目标和它具有大约相同的尺寸：</p>
<p><code>object width in percent from Training dataset</code> ~= <code>object width in percent from Test dataset</code></p>
<p>也就是说，如果训练集中仅存在占图像比例80%-90%的目标，则训练后的目标检测网络将无法检测到占图像比例为1-10%的目标。</p>
</li>
<li><p>为了加快训练速度（同时会降低检测精度）使用微调而不是迁移学习，在[net]下面设置<code>stopbackward=1</code>。然后执行下面的命令：<code>./darknet partial cfg/yolov3.cfg yolov3.weights yolov3.conv.81 81</code>这将会创建<code>yolov3.conv.81</code>文件，然后使用<code>yolov3.conv.81</code>文件进行训练而不是<code>darknet53.conv.74</code>。</p>
</li>
<li><p>在观察目标的时候，从不同的方向、不同的照明情况、不同尺度、不同的转角和倾斜角度来看，对神经网络来说，它们都是不同的目标。因此，要检测的目标越多，应使用越复杂的网络模型。</p>
</li>
<li><p>为了让检测框更准，你可以在每个<code>yolo</code>层添加下面三个参数<code>ignore_thresh = .9 iou_normalizer=0.5 iou_loss=giou</code>，这回提高map@0.9，但会降低map@0.5。</p>
</li>
<li><p>当你是神经网络方面的专家时，可以重新计算相对于<code>width</code>和<code>height</code>的<code>anchors</code>：<code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code>然后在3个<code>[yolo]</code>层放置这9个<code>anchors</code>。但是你需要修改每个<code>[yolo]</code>层的<code>masks</code>参数，让第一个<code>[yolo]</code>层的<code>anchors</code>尺寸大于60x60，第二个<code>[yolo]</code>层的<code>anchors</code>尺寸大于30x30，剩下就是第三个<code>[yolo]</code>层的<code>mask</code>。宁外，你需要修改每一个<code>[yolo]</code>层前面的<code>filters=(classes + 5)x</code>。如果很多计算的<code>anchors</code>都找不到合适的层，那还是使用Yolo的默认<code>anchors</code>吧。</p>
</li>
</ul>
</li>
<li><p>训练之后：</p>
<ul>
<li>没有必要重新训练模型，直接使用用<code>416x416</code>分辨率训练出来的<code>.weights</code>模型文件。</li>
<li>但是要获得更高的准确率，你应该使用<code>608x608</code>或者<code>832x832</code>来训练，注意如果<code>Out of memory</code>发生了，你应该在<code>.cfg</code>文件中增加<code>subdivisions=16，32，64</code>。</li>
<li>通过在<code>.cfg</code>文件中设置（<code>height=608</code> and <code>width=608</code>）或者（<code>height=832</code> and <code>width=832</code>）或者任何32的倍数，这会提升准确率并使得对小目标的检测更加容易。</li>
</ul>
</li>
</ol>
<h2 id="8-如何标注以及创建标注文件"><a href="#8-如何标注以及创建标注文件" class="headerlink" title="8. 如何标注以及创建标注文件"></a>8. 如何标注以及创建标注文件</h2><p>下面的工程提供了用于标记目标边界框并为YOLO v2&amp;v3 生成标注文件的带图像界面软件，地址为：<code>https://github.com/AlexeyAB/Yolo_mark</code>。</p>
<p>例如对于只有两类目标的数据集标注后有以下文件<code>train.txt</code>,<code>obj.names</code>,<code>obj.data</code>,<code>yolo-obj.cfg</code>,<code>air 1-6.txt</code>,<code>bird 1-4.txt</code>，接着配合<code>train_obj.cmd</code>就可以使用YOLO v2和YOLO v3来训练这个数据集了。</p>
<p>下面提供了5重常见的目标标注工具：</p>
<ul>
<li>C++实现的：<code>https://github.com/AlexeyAB/Yolo_mark</code></li>
<li>Python实现的：<code>https://github.com/tzutalin/labelImg</code></li>
<li>Python实现的：<code>https://github.com/Cartucho/OpenLabeling</code></li>
<li>C++实现的：<code>https://www.ccoderun.ca/darkmark/</code></li>
<li>JavaScript实现的：<code>https://github.com/opencv/cvat</code></li>
</ul>
<h2 id="9-使用YOLO9000"><a href="#9-使用YOLO9000" class="headerlink" title="9. 使用YOLO9000"></a>9. 使用YOLO9000</h2><p>同时检测和分类9000个目标：<code>darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights data/dog.jpg</code></p>
<ul>
<li><p><code>yolo9000.weights</code>：186Mb的YOLO9000模型需要4G GPU显存，训练好的模型下载地址：<code>http://pjreddie.com/media/files/yolo9000.weights</code>。</p>
</li>
<li><p><code>yolo9000.cfg</code>：YOLO9000的c网络结构文件，同时这里也有<code>9k.tree</code>和<code>coco9k.map</code>文件的路径。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tree&#x3D;data&#x2F;9k.tree</span><br><span class="line">map &#x3D; data&#x2F;coco9k.map</span><br></pre></td></tr></table></figure>
<ul>
<li><code>9k.tree</code>：9418个类别的单词数，每一行的形式为<code>`，如果</code>parent_id==-1<code>那么这个标签没有父类别，地址为：</code><a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.tree`。" target="_blank" rel="noopener">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.tree`。</a></li>
<li><code>coco9k.map</code>：将MSCOCO的80个目标类别映射到<code>9k.tree</code>的文件，地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/coco9k.map</code>。</li>
</ul>
</li>
<li><p><code>combine9k.data</code>：数据文件，分别是<code>9k.labels</code>。<code>9k.names</code>，<code>inet9k.map</code>的路径（修改<code>combine9k.train.list</code>文件的路径为你自己的）。地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/combine9k.data</code>。</p>
</li>
<li><p><code>9k.labels</code>：9418类目标的标签。地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.labels</code>。</p>
</li>
<li><p><code>9k.names</code>：9418类目标的名字。地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.names</code>。</p>
</li>
<li><p><code>inet9k.map</code>：将ImageNet的200个目标类别映射到<code>9k.tree</code>的文件，地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/inet9k.map</code>。</p>
</li>
</ul>
<h2 id="10-如何将YOLO作为DLL和SO库进行使用？"><a href="#10-如何将YOLO作为DLL和SO库进行使用？" class="headerlink" title="10. 如何将YOLO作为DLL和SO库进行使用？"></a>10. 如何将YOLO作为DLL和SO库进行使用？</h2><ul>
<li><p>在Linux上。</p>
<ul>
<li>使用<code>build.sh</code> 或者</li>
<li>使用<code>cmake</code>编译<code>darknet</code> 或者</li>
<li>将<code>Makefile</code>重的<code>LIBSO=0</code>改为<code>LIBSO=1</code>，然后执行<code>make</code>编译<code>darknet</code></li>
</ul>
</li>
<li><p>在Windows上。</p>
<ul>
<li>使用<code>build.ps1</code> 或者</li>
<li>使用<code>cmake</code>编译<code>darknet</code> 或者</li>
<li>使用<code>build\darknet\yolo_cpp_dll.sln</code>或<code>build\darknet\yolo_cpp_dll_no_gpu.sln</code>解决方法编译<code>darknet</code></li>
</ul>
</li>
<li><p>这里有两个API：</p>
<ul>
<li>使用C++ API的C++例子：<code>https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</code></li>
<li>使用C API的Python例子：<br><code>https://github.com/AlexeyAB/darknet/blob/master/darknet.py</code><br><code>https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py</code></li>
<li>C API：<code>https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h</code></li>
<li>C++ API：<code>https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp</code></li>
</ul>
</li>
</ul>
<h2 id="11-附录"><a href="#11-附录" class="headerlink" title="11. 附录"></a>11. 附录</h2><ol>
<li><p>为了将Yolo编译成C++的DLL文件<code>yolo_cpp_dll.dll</code>：打开<code>build\darknet\yolo_cpp_dll.sln</code>解决方案，编译选项选<strong>X64</strong>和<strong>Release</strong>，然后执行Build-&gt;Build yolo_cpp_dll就，编译的一些前置条件为：</p>
<ul>
<li>安装<strong>CUDA 10.0</strong>。</li>
<li>为了使用cuDNN执行以下步骤：点击工程属性-&gt;properties-&gt;C++-&gt;Preprocessor-&gt;Preprocessor Definitions，然后在开头添加一行<code>CUDNN</code>。</li>
</ul>
</li>
<li><p>在自己的C++工程中将Yolo当成DLL文件使用：打开<code>build\darknet\yolo_console_dll.sln</code>解决方案，编译选项选<strong>X64</strong>和<strong>Release</strong>，然后执行Build-&gt;Build yolo_console_dll：</p>
<p><code>yolo_cpp_dll.dll</code>-API：<code>https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp</code></p>
<ul>
<li>你可以利用Windows资源管理器运行<code>build\darknet\x64\yolo_console_dll.exe</code>可执行程序并<strong>使用下面的命令</strong>:  <code>yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4</code></li>
<li>启动控制台应用程序并输入图像文件名后，你将看到每个目标的信息：<code> </code></li>
<li>如果要使用OpenCV-GUI你应该将<code>yolo_console_dll.cpp</code>中的<code>//#define OPENCV</code>取消注释。</li>
<li>你可以看到视频检测例子的源代码，地址为yolo_console_dll.cpp的第75行。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">struct bbox_t &#123;</span><br><span class="line">    unsigned int x, y, w, h;    &#x2F;&#x2F; (x,y) - top-left corner, (w, h) - width &amp; height of bounded box</span><br><span class="line">    float prob;                    &#x2F;&#x2F; confidence - probability that the object was found correctly</span><br><span class="line">    unsigned int obj_id;        &#x2F;&#x2F; class of object - from range [0, classes-1]</span><br><span class="line">    unsigned int track_id;        &#x2F;&#x2F; tracking id for video (0 - untracked, 1 - inf - tracked object)</span><br><span class="line">    unsigned int frames_counter;&#x2F;&#x2F; counter of frames on which the object was detected</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">class Detector &#123;</span><br><span class="line">public:</span><br><span class="line">        Detector(std::string cfg_filename, std::string weight_filename, int gpu_id &#x3D; 0);</span><br><span class="line">        ~Detector();</span><br><span class="line"></span><br><span class="line">        std::vector&lt;bbox_t&gt; detect(std::string image_filename, float thresh &#x3D; 0.2, bool use_mean &#x3D; false);</span><br><span class="line">        std::vector&lt;bbox_t&gt; detect(image_t img, float thresh &#x3D; 0.2, bool use_mean &#x3D; false);</span><br><span class="line">        static image_t load_image(std::string image_filename);</span><br><span class="line">        static void free_image(image_t m);</span><br><span class="line"></span><br><span class="line">#ifdef OPENCV</span><br><span class="line">        std::vector&lt;bbox_t&gt; detect(cv::Mat mat, float thresh &#x3D; 0.2, bool use_mean &#x3D; false);</span><br><span class="line">	std::shared_ptr&lt;image_t&gt; mat_to_image_resize(cv::Mat mat) const;</span><br><span class="line">#endif</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>ECCV 2018 Convolutional Block Attention Module</title>
    <url>/2020/02/19/ECCV-2018-Convolutional-Block-Attention-Module/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>目前cv领域借鉴了nlp领域的attention机制以后生产出了很多有用的基于attention机制的论文，attention机制也是在2019年论文中非常火。这篇cbam虽然是在2018年提出的，但是其影响力比较深远，在很多领域都用到了该模块，所以一起来看一下这个模块有什么独到之处，并学着实现它。</p>
<h2 id="什么是注意力机制？"><a href="#什么是注意力机制？" class="headerlink" title="什么是注意力机制？"></a>什么是注意力机制？</h2><p>注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。</p>
<p>通俗来讲：注意力机制就是希望网络能够自动学出来图片或者文字序列中的需要注意的地方。比如人眼在看一幅画的时候，不会将注意力平等地分配给画中的所有像素，而是将更多注意力分配给人们关注的地方。</p>
<p>从实现的角度来讲：注意力机制通过神经网络的操作生成一个掩码mask,，mask上的值一个打分，评价当前需要关注的点的评分。</p>
<p>注意力机制可以分为：</p>
<ul>
<li>通道注意力机制：对通道生成掩码mask，进行打分，代表是SENet, Channel Attention Module</li>
<li>空间注意力机制：对空间进行掩码的生成，进行打分，代表是Spatial Attention Module</li>
<li>混合域注意力机制：同时对通道注意力和空间注意力进行评价打分，代表的有BAM, CBAM</li>
</ul>
<h2 id="怎么实现CBAM？-pytorch为例"><a href="#怎么实现CBAM？-pytorch为例" class="headerlink" title="怎么实现CBAM？(pytorch为例)"></a><strong>怎么实现CBAM？(pytorch为例)</strong></h2><p>CBAM全称是Convolutional Block Attention Module, 是在ECCV2018上发表的注意力机制代表作之一，<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf" target="_blank" rel="noopener">论文下载地址</a>。</p>
<p>在该论文中，作者研究了网络架构中的注意力，注意力不仅要告诉我们重点关注哪里，还要提高关注点的表示。目标是通过使用注意机制来增加表现力，关注重要特征并抑制不必要的特征。为了强调空间和通道这两个维度上的有意义特征，作者依次应用<strong>通道和空间注意模块</strong>，来分别在通道和空间维度上学习关注什么、在哪里关注。此外，通过了解要强调或抑制的信息也有助于网络内的信息流动。</p>
<p>主要网络架构也很简单，一个是通道注意力模块，另一个是空间注意力模块，CBAM就是先后集成了通道注意力模块和空间注意力模块。</p>
<h3 id="2-1-通道注意力机制"><a href="#2-1-通道注意力机制" class="headerlink" title="2.1 通道注意力机制"></a>2.1 通道注意力机制</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/640.webp" alt></p>
<p>通道注意力机制按照上图进行实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class ChannelAttention(nn.Module):</span><br><span class="line">    def __init__(self, in_planes, rotio&#x3D;16):</span><br><span class="line">        super(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool &#x3D; nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.max_pool &#x3D; nn.AdaptiveMaxPool2d(1)</span><br><span class="line"></span><br><span class="line">        self.sharedMLP &#x3D; nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_planes, in_planes &#x2F;&#x2F; ratio, 1, bias&#x3D;False), nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_planes &#x2F;&#x2F; rotio, in_planes, 1, bias&#x3D;False))</span><br><span class="line">        self.sigmoid &#x3D; nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avgout &#x3D; self.sharedMLP(self.avg_pool(x))</span><br><span class="line">        maxout &#x3D; self.sharedMLP(self.max_pool(x))</span><br><span class="line">        return self.sigmoid(avgout + maxout)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-空间注意力机制"><a href="#2-2-空间注意力机制" class="headerlink" title="2.2 空间注意力机制"></a>2.2 空间注意力机制</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/641.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class SpatialAttention(nn.Module):</span><br><span class="line">    def __init__(self, kernel_size&#x3D;7):</span><br><span class="line">        super(SpatialAttention, self).__init__()</span><br><span class="line">        assert kernel_size in (3,7), &quot;kernel size must be 3 or 7&quot;</span><br><span class="line">        padding &#x3D; 3 if kernel_size &#x3D;&#x3D; 7 else 1</span><br><span class="line"></span><br><span class="line">        self.conv &#x3D; nn.Conv2d(2,1,kernel_size, padding&#x3D;padding, bias&#x3D;False)</span><br><span class="line">        self.sigmoid &#x3D; nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avgout &#x3D; torch.mean(x, dim&#x3D;1, keepdim&#x3D;True)</span><br><span class="line">        maxout, _ &#x3D; torch.max(x, dim&#x3D;1, keepdim&#x3D;True)</span><br><span class="line">        x &#x3D; torch.cat([avgout, maxout], dim&#x3D;1)</span><br><span class="line">        x &#x3D; self.conv(x)</span><br><span class="line">        return self.sigmoid(x)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-Convolutional-bottleneck-attention-module"><a href="#2-3-Convolutional-bottleneck-attention-module" class="headerlink" title="2.3 Convolutional bottleneck attention module"></a>2.3 Convolutional bottleneck attention module</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/642.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class BasicBlock(nn.Module):</span><br><span class="line">    expansion &#x3D; 1</span><br><span class="line">    def __init__(self, inplanes, planes, stride&#x3D;1, downsample&#x3D;None):</span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 &#x3D; conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 &#x3D; nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu &#x3D; nn.ReLU(inplace&#x3D;True)</span><br><span class="line">        self.conv2 &#x3D; conv3x3(planes, planes)</span><br><span class="line">        self.bn2 &#x3D; nn.BatchNorm2d(planes)</span><br><span class="line">        self.ca &#x3D; ChannelAttention(planes)</span><br><span class="line">        self.sa &#x3D; SpatialAttention()</span><br><span class="line">        self.downsample &#x3D; downsample</span><br><span class="line">        self.stride &#x3D; stride</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        residual &#x3D; x</span><br><span class="line">        out &#x3D; self.conv1(x)</span><br><span class="line">        out &#x3D; self.bn1(out)</span><br><span class="line">        out &#x3D; self.relu(out)</span><br><span class="line">        out &#x3D; self.conv2(out)</span><br><span class="line">        out &#x3D; self.bn2(out)</span><br><span class="line">        out &#x3D; self.ca(out) * out  # 广播机制</span><br><span class="line">        out &#x3D; self.sa(out) * out  # 广播机制</span><br><span class="line">        if self.downsample is not None:</span><br><span class="line">            residual &#x3D; self.downsample(x)</span><br><span class="line">        out +&#x3D; residual</span><br><span class="line">        out &#x3D; self.relu(out)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>
<p>为何要先使用通道注意力机制然后再使用空间注意力机制？使用顺序使用这两个模块还是并行的使用两个模块？其实是作者已经做过了相关实验，并且证明了先试用通道然后再使用空间注意力机制这样的组合效果比较好，这也是CBAM的通用组合模式。</p>
<h2 id="在什么情况下可以使用？"><a href="#在什么情况下可以使用？" class="headerlink" title="在什么情况下可以使用？"></a>在什么情况下可以使用？</h2><p>提出CBAM的作者主要对分类网络和目标检测网络进行了实验,证明了CBAM模块确实是有效的。</p>
<p>以ResNet为例，论文中提供了改造的示意图，如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/643.webp" alt></p>
<p>也就是在ResNet中的每个block之间添加了CBAM模块，训练数据来自benchmark ImageNet-1K。检测使用的是Faster  R-CNN， Backbone选择的ResNet34，ResNet50，WideResNet18，ResNet50等，还跟SE等进行了对比。</p>
<p><strong>消融实验</strong>：消融实验一般是控制变量，最能看出模型变好起作用的部分在那里。分为三个部分：</p>
<ul>
<li><p>如何更有效地计算channel attention?</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/644.webp" alt>可以看出来，使用avgpool和maxpool可以更好的降低错误率，大概有1-2%的提升，这个组合就是dual pooling，能提供更加精细的信息，有利于提升模型的表现。</p>
</li>
<li><p>如何更有效地计算spatial attention?</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/645.webp" alt></p>
<p>这里的空间注意力机制参数也是有avg，max组成，另外还有一个卷积的参数kernel_size(k)，通过以上实验，可以看出，当前使用通道的平均和通道的最大化，并且设置kernel size=7是最好的。</p>
</li>
<li><p>如何组织这两个部分？</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/646.webp" alt></p>
<p>可以看出，这里与SENet中的SE模块也进行了比较，这里使用CBAM也是超出了SE的表现。除此以外，还进行了顺序和并行的测试，发现，先channel attention然后spatial attention效果最好，所以也是最终的CBAM模块的组成。</p>
<p>在MSCOCO数据及使用了ResNet50，ResNet101为backbone，Faster RCNN为检测器的模型进行目标检测，如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/647.webp" alt></p>
<p>在VOC2007数据集中采用了StairNet进行了测试，如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CBAM/648.webp" alt></p>
<p>貌似没有找到目标检测部分的代码，CBAM的作用在于对信息进行精细化分配和处理，所以猜测是在backbone的分类器之前添加的CBAM模块。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>Attention机制</tag>
      </tags>
  </entry>
  <entry>
    <title>最简单最易实现的SE模块</title>
    <url>/2020/02/19/%E6%9C%80%E7%AE%80%E5%8D%95%E6%9C%80%E6%98%93%E5%AE%9E%E7%8E%B0%E7%9A%84SE%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<h2 id="Squeeze-and-Excitation-Networks"><a href="#Squeeze-and-Excitation-Networks" class="headerlink" title=" Squeeze-and-Excitation Networks"></a><a id="more"></a> Squeeze-and-Excitation Networks</h2><p>SENet是Squeeze-and-Excitation  Networks的简称，拿到了ImageNet2017分类比赛冠军，其效果得到了认可，其提出的SE模块思想简单，易于实现，并且很容易可以加载到现有的网络模型框架中。SENet主要是学习了channel之间的相关性，筛选出了针对通道的注意力，稍微增加了一点计算量，但是效果比较好。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/se/640.webp" alt></p>
<p>通过上图可以理解他的实现过程，通过对卷积的到的feature  map进行处理，得到一个和通道数一样的一维向量作为每个通道的评价分数，然后将修改的分数分别施加到对应的通道上，得到其结果，就在原有的基础上只添加了一个模块，下边我们用pytorch实现这个很简单的模块。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/se/641.webp" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class SELayer(nn.Module):</span><br><span class="line">    def __init__(self, channel, reduction&#x3D;16):</span><br><span class="line">        super(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool &#x3D; nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.fc &#x3D; nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel &#x2F;&#x2F; reduction, bias&#x3D;False),</span><br><span class="line">            nn.ReLU(inplace&#x3D;True),</span><br><span class="line">            nn.Linear(channel &#x2F;&#x2F; reduction, channel, bias&#x3D;False),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        b, c, _, _ &#x3D; x.size()</span><br><span class="line">        y &#x3D; self.avg_pool(x).view(b, c)</span><br><span class="line">        y &#x3D; self.fc(y).view(b, c, 1, 1)</span><br><span class="line">        return x * y.expand_as(x)</span><br></pre></td></tr></table></figure>
<p>虽然核心就是以上的内容，不过不能简单地结束，我们需要看一下以下几个点：</p>
<ul>
<li><p>作为一个重要的attention机制的文章，这篇文章如何描述attention，related work如何组织？</p>
<p>attention机制当时已经有一定的研究和发展，也是集中于序列学习，image captioning, understanding in  images这些工作，也已经有很多出色的工作是探索了attention机制。senet这篇文章主要探索了通过对通道间关系进行建模来提升模型的表达能力。related work 主要从更深的网络架构，架构搜索，注意力机制三个角度进行了梳理，确实非常全面。</p>
</li>
<li><p>如何解释SE模块？</p>
<p><strong>Sequeeze</strong>：对$C \times H \times W$进行global average pooling，得到 $1 \times 1 \times C$大小的特征图，这个特征图可以理解为具有全局感受野。</p>
<p><strong>Excitation</strong>：使用一个全连接神经网络，对Sequeeze之后的结果做一个非线性变换。</p>
<p><strong>特征重标定</strong>：使用Excitation 得到的结果作为权重，乘到输入特征上。</p>
</li>
<li><p>SE模块如何加到分类网络，效果如何？</p>
<p>分类网络现在一般都是成一个block一个block，se模块就可以加到一个block结束的位置，进行一个信息refine。这里用了一些STOA的分类模型如：resnet50，resnext50，bn-inception等网络。通过添加SE模块，能使模型提升0.5-1.5%,效果还可以，增加的计算量也可以忽略不计。在轻量级网络MobileNet，ShuffleNet上也进行了实验，可以提升的点更多一点大概在1.5-2%。</p>
</li>
<li><p>SE模块如何加到目标检测网络，效果如何？</p>
<p>主要还是将SE模块添加到backbone部分，优化学习到的内容。目标检测数据集使用的是benchmark MSCOCO, 使用的Faster  R-CNN作为目标检测器，使用backbone从ResNet50替换为SE-ResNet50以后带了了两个点的AP提升，确实有效果。</p>
</li>
<li><p>这篇文章的实验部分是如何设置的？</p>
<p>这篇文章中也进行了消融实验，来证明SE模块的有效性，也说明了设置reduction=16的原因。</p>
<ul>
<li>squeeze方式：仅仅比较了max和avg，发现avg要好一点。</li>
<li>excitation方式：使用了ReLU，Tanh，Sigmoid，发现Sigmoid好。</li>
<li>stage: resnet50有不同的阶段，通过实验发现，将se施加到所有的阶段效果最好。</li>
<li>集成策略：将se放在残差单元的前部，后部还是平行于残差单元，最终发现，放到前部比较好。</li>
</ul>
</li>
<li><p>如何查看每个通道学到的attention信息并证明其有效性？</p>
<p>作者选取了ImageNet中的四个类别进行了一个实验，测试backbone最后一个SE层的内容，如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/se/642.webp" alt></p>
<p>可以看出这两个类激活出来的内容有一定的差距，起到了一定的作用。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>Attention机制</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows 如何通过VNC从远程控制Ubuntu</title>
    <url>/2020/02/18/Windows%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87VNC%E4%BB%8E%E8%BF%9C%E7%A8%8B%E6%8E%A7%E5%88%B6Ubuntu/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title=" 简介"></a><a id="more"></a> 简介</h2><p>VNC 工具是提供你一个可以远程图像化桌面的方式。其实 VNC 是一种软件的统称。只要你的 Linux 架设好了一个服务器 (Server) 的 VNC，客户端比如你的 Mac，手机，只要安装任何一种 VNC 客户端软件就能链接上服务器端的电脑。</p>
<h2 id="Ubuntu服务端安装"><a href="#Ubuntu服务端安装" class="headerlink" title="Ubuntu服务端安装"></a>Ubuntu服务端安装</h2><p>打开 Terminal，输入:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install x11vnc</span><br></pre></td></tr></table></figure><br>确认你的 Linux 用户密码，就能安装这个最常用的 x11vnc 软件。这个软件的使用，设置非常简单。安装好后，最好给你的 x11vnc 设置一个用于连接的密码，输入<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x11vnc -storepasswd</span><br></pre></td></tr></table></figure><br>会出现以下画面，设置你的密码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter VNC password:</span><br><span class="line">Verify password:</span><br><span class="line">Write password to &#x2F;home&#x2F;morvan&#x2F;.vnc&#x2F;passwd?  [y]&#x2F;n y</span><br><span class="line">Password written to: &#x2F;home&#x2F;morvan&#x2F;.vnc&#x2F;passwd</span><br></pre></td></tr></table></figure><br>开启服务:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x11vnc -auth guess -once -loop -noxdamage -repeat -rfbauth &#x2F;home&#x2F;USERNAME&#x2F;.vnc&#x2F;passwd -rfbport 5900 -shared</span><br></pre></td></tr></table></figure></p>
<p><strong>注意：/home/USERNAME/.vnc/passwd 中的USERNAME需要换成你自己的用户名。</strong></p>
<p>设为开机启动：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;x11vnc.service</span><br></pre></td></tr></table></figure><br>在打开的页面中插入以下代码，保存<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Start x11vnc at startup.</span><br><span class="line">After&#x3D;multi-user.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;x11vnc -auth guess -once -loop -noxdamage -repeat -rfbauth &#x2F;home&#x2F;USERNAME&#x2F;.vnc&#x2F;passwd -rfbport 5900 -shared</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><br>接着执行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl enable x11vnc.service</span><br></pre></td></tr></table></figure><br>好了，此时服务端配置完毕。</p>
<h2 id="Windows客户端安装"><a href="#Windows客户端安装" class="headerlink" title="Windows客户端安装"></a>Windows客户端安装</h2><p>我这里用的是RealVNC的VNC Viewer，<a href="https://www.realvnc.com/en/connect/download/viewer/" target="_blank" rel="noopener">下载地址</a></p>
<p>打开客户端，输入你要连接的服务端的IP</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VNC/Snipaste_2020-02-18_18-11-36.jpg" alt></p>
<p>接着输入你所设置的密码即可连接，效果如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VNC/Snipaste_2020-02-18_18-12-44.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>VNC</tag>
      </tags>
  </entry>
  <entry>
    <title>在YOLOv3模型中添加Attention机制</title>
    <url>/2020/02/18/%E5%9C%A8YOLOv3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%B7%BB%E5%8A%A0Attention%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="1-规定格式"><a href="#1-规定格式" class="headerlink" title=" 1. 规定格式"></a><a id="more"></a> 1. 规定格式</h2><p>正如<code>[convolutional]</code>,<code>[maxpool]</code>,<code>[net]</code>,<code>[route]</code>等层在cfg中的定义一样，我们再添加全新的模块的时候，要规定一下cfg的格式。做出以下规定：</p>
<p>在SE模块中，有一个参数为<code>reduction</code>,这个参数默认是16，所以在这个模块中的详细参数我们按照以下内容进行设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[se]</span><br><span class="line">reduction&#x3D;16</span><br></pre></td></tr></table></figure>
<p>在CBAM模块中，空间注意力机制和通道注意力机制中一共存在两个参数：<code>ratio</code>和<code>kernel_size</code>, 所以这样规定CBAM在cfg文件中的格式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[cbam]</span><br><span class="line">ratio&#x3D;16</span><br><span class="line">kernelsize&#x3D;7</span><br></pre></td></tr></table></figure>
<h2 id="2-修改解析部分"><a href="#2-修改解析部分" class="headerlink" title="2. 修改解析部分"></a>2. 修改解析部分</h2><p>由于添加的这些参数都是自定义的，所以需要修改解析cfg文件的函数，之前讲过，需要修改<code>parse_config.py</code>中的部分内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def parse_model_cfg(path):</span><br><span class="line">    # path参数为: cfg&#x2F;yolov3-tiny.cfg</span><br><span class="line">    if not path.endswith(&#39;.cfg&#39;):</span><br><span class="line">        path +&#x3D; &#39;.cfg&#39;</span><br><span class="line">    if not os.path.exists(path) and \</span><br><span class="line">    	   os.path.exists(&#39;cfg&#39; + os.sep + path):</span><br><span class="line">        path &#x3D; &#39;cfg&#39; + os.sep + path</span><br><span class="line"></span><br><span class="line">    with open(path, &#39;r&#39;) as f:</span><br><span class="line">        lines &#x3D; f.read().split(&#39;\n&#39;)</span><br><span class="line"></span><br><span class="line">    # 去除以#开头的，属于注释部分的内容</span><br><span class="line">    lines &#x3D; [x for x in lines if x and not x.startswith(&#39;#&#39;)]</span><br><span class="line">    lines &#x3D; [x.rstrip().lstrip() for x in lines]</span><br><span class="line">    mdefs &#x3D; []  # 模块的定义</span><br><span class="line">    for line in lines:</span><br><span class="line">        if line.startswith(&#39;[&#39;):  # 标志着一个模块的开始</span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            eg:</span><br><span class="line">            [shortcut]</span><br><span class="line">            from&#x3D;-3</span><br><span class="line">            activation&#x3D;linear</span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            mdefs.append(&#123;&#125;)</span><br><span class="line">            mdefs[-1][&#39;type&#39;] &#x3D; line[1:-1].rstrip()</span><br><span class="line">            if mdefs[-1][&#39;type&#39;] &#x3D;&#x3D; &#39;convolutional&#39;:</span><br><span class="line">                mdefs[-1][&#39;batch_normalize&#39;] &#x3D; 0 </span><br><span class="line">        else:</span><br><span class="line">            key, val &#x3D; line.split(&quot;&#x3D;&quot;)</span><br><span class="line">            key &#x3D; key.rstrip()</span><br><span class="line"></span><br><span class="line">            if &#39;anchors&#39; in key:</span><br><span class="line">                mdefs[-1][key] &#x3D; np.array([float(x) for x in val.split(&#39;,&#39;)]).reshape((-1, 2))</span><br><span class="line">            else:</span><br><span class="line">                mdefs[-1][key] &#x3D; val.strip()</span><br><span class="line"></span><br><span class="line">    # Check all fields are supported</span><br><span class="line">    supported &#x3D; [&#39;type&#39;, &#39;batch_normalize&#39;, &#39;filters&#39;, &#39;size&#39;,\</span><br><span class="line">                 &#39;stride&#39;, &#39;pad&#39;, &#39;activation&#39;, &#39;layers&#39;, \</span><br><span class="line">                 &#39;groups&#39;,&#39;from&#39;, &#39;mask&#39;, &#39;anchors&#39;, \</span><br><span class="line">                 &#39;classes&#39;, &#39;num&#39;, &#39;jitter&#39;, &#39;ignore_thresh&#39;,\</span><br><span class="line">                 &#39;truth_thresh&#39;, &#39;random&#39;,\</span><br><span class="line">                 &#39;stride_x&#39;, &#39;stride_y&#39;]</span><br><span class="line"></span><br><span class="line">    f &#x3D; []  # fields</span><br><span class="line">    for x in mdefs[1:]:</span><br><span class="line">        [f.append(k) for k in x if k not in f]</span><br><span class="line">    u &#x3D; [x for x in f if x not in supported]  # unsupported fields</span><br><span class="line">    assert not any(u), &quot;Unsupported fields %s in %s. See https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov3&#x2F;issues&#x2F;631&quot; % (u, path)</span><br><span class="line"></span><br><span class="line">    return mdefs</span><br></pre></td></tr></table></figure>
<p>以上内容中，需要改的是supported中的字段，将我们的内容添加进去：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">supported &#x3D; [&#39;type&#39;, &#39;batch_normalize&#39;, &#39;filters&#39;, &#39;size&#39;,\</span><br><span class="line">            &#39;stride&#39;, &#39;pad&#39;, &#39;activation&#39;, &#39;layers&#39;, \</span><br><span class="line">            &#39;groups&#39;,&#39;from&#39;, &#39;mask&#39;, &#39;anchors&#39;, \</span><br><span class="line">            &#39;classes&#39;, &#39;num&#39;, &#39;jitter&#39;, &#39;ignore_thresh&#39;,\</span><br><span class="line">            &#39;truth_thresh&#39;, &#39;random&#39;,\</span><br><span class="line">            &#39;stride_x&#39;, &#39;stride_y&#39;,\</span><br><span class="line">            &#39;ratio&#39;, &#39;reduction&#39;, &#39;kernelsize&#39;]</span><br></pre></td></tr></table></figure>
<h2 id="3-实现SE和CBAM"><a href="#3-实现SE和CBAM" class="headerlink" title="3. 实现SE和CBAM"></a>3. 实现SE和CBAM</h2><p><strong>SE</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class SELayer(nn.Module):</span><br><span class="line">    def __init__(self, channel, reduction&#x3D;16):</span><br><span class="line">        super(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool &#x3D; nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.fc &#x3D; nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel &#x2F;&#x2F; reduction, bias&#x3D;False),</span><br><span class="line">            nn.ReLU(inplace&#x3D;True),</span><br><span class="line">            nn.Linear(channel &#x2F;&#x2F; reduction, channel, bias&#x3D;False),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        b, c, _, _ &#x3D; x.size()</span><br><span class="line">        y &#x3D; self.avg_pool(x).view(b, c)</span><br><span class="line">        y &#x3D; self.fc(y).view(b, c, 1, 1)</span><br><span class="line">        return x * y.expand_as(x)</span><br></pre></td></tr></table></figure>
<p><strong>CBAM</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class SpatialAttention(nn.Module):</span><br><span class="line">    def __init__(self, kernel_size&#x3D;7):</span><br><span class="line">        super(SpatialAttention, self).__init__()</span><br><span class="line">        assert kernel_size in (3,7), &quot;kernel size must be 3 or 7&quot;</span><br><span class="line">        padding &#x3D; 3 if kernel_size &#x3D;&#x3D; 7 else 1</span><br><span class="line"></span><br><span class="line">        self.conv &#x3D; nn.Conv2d(2,1,kernel_size, padding&#x3D;padding, bias&#x3D;False)</span><br><span class="line">        self.sigmoid &#x3D; nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avgout &#x3D; torch.mean(x, dim&#x3D;1, keepdim&#x3D;True)</span><br><span class="line">        maxout, _ &#x3D; torch.max(x, dim&#x3D;1, keepdim&#x3D;True)</span><br><span class="line">        x &#x3D; torch.cat([avgout, maxout], dim&#x3D;1)</span><br><span class="line">        x &#x3D; self.conv(x)</span><br><span class="line">        return self.sigmoid(x)</span><br><span class="line">    </span><br><span class="line">class ChannelAttention(nn.Module):</span><br><span class="line">    def __init__(self, in_planes, rotio&#x3D;16):</span><br><span class="line">        super(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool &#x3D; nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.max_pool &#x3D; nn.AdaptiveMaxPool2d(1)</span><br><span class="line"></span><br><span class="line">        self.sharedMLP &#x3D; nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_planes, in_planes &#x2F;&#x2F; ratio, 1, bias&#x3D;False), nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_planes &#x2F;&#x2F; rotio, in_planes, 1, bias&#x3D;False))</span><br><span class="line">        self.sigmoid &#x3D; nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avgout &#x3D; self.sharedMLP(self.avg_pool(x))</span><br><span class="line">        maxout &#x3D; self.sharedMLP(self.max_pool(x))</span><br><span class="line">        return self.sigmoid(avgout + maxout)</span><br></pre></td></tr></table></figure>
<p>以上就是两个模块的代码，添加到<code>models.py</code>文件中。</p>
<h2 id="4-设计cfg文件"><a href="#4-设计cfg文件" class="headerlink" title="4. 设计cfg文件"></a>4. 设计cfg文件</h2><p>这里以<code>yolov3-tiny.cfg</code>为baseline，然后添加注意力机制模块。</p>
<p>CBAM与SE类似，所以以SE为例，添加到backbone之后的部分，进行信息重构(refinement)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[net]</span><br><span class="line"># Testing</span><br><span class="line">batch&#x3D;1</span><br><span class="line">subdivisions&#x3D;1</span><br><span class="line"># Training</span><br><span class="line"># batch&#x3D;64</span><br><span class="line"># subdivisions&#x3D;2</span><br><span class="line">width&#x3D;416</span><br><span class="line">height&#x3D;416</span><br><span class="line">channels&#x3D;3</span><br><span class="line">momentum&#x3D;0.9</span><br><span class="line">decay&#x3D;0.0005</span><br><span class="line">angle&#x3D;0</span><br><span class="line">saturation &#x3D; 1.5</span><br><span class="line">exposure &#x3D; 1.5</span><br><span class="line">hue&#x3D;.1</span><br><span class="line"></span><br><span class="line">learning_rate&#x3D;0.001</span><br><span class="line">burn_in&#x3D;1000</span><br><span class="line">max_batches &#x3D; 500200</span><br><span class="line">policy&#x3D;steps</span><br><span class="line">steps&#x3D;400000,450000</span><br><span class="line">scales&#x3D;.1,.1</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;16</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;32</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;64</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;1</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[se]</span><br><span class="line">reduction&#x3D;16</span><br><span class="line"></span><br><span class="line"># 在backbone结束的地方添加se模块</span><br><span class="line">#####backbone######</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;18</span><br><span class="line">activation&#x3D;linear</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[yolo]</span><br><span class="line">mask &#x3D; 3,4,5</span><br><span class="line">anchors &#x3D; 10,14,  23,27,  37,58,  81,82,  135,169,  344,319</span><br><span class="line">classes&#x3D;1</span><br><span class="line">num&#x3D;6</span><br><span class="line">jitter&#x3D;.3</span><br><span class="line">ignore_thresh &#x3D; .7</span><br><span class="line">truth_thresh &#x3D; 1</span><br><span class="line">random&#x3D;1</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers &#x3D; -4</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[upsample]</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers &#x3D; -1, 8</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;18</span><br><span class="line">activation&#x3D;linear</span><br><span class="line"></span><br><span class="line">[yolo]</span><br><span class="line">mask &#x3D; 0,1,2</span><br><span class="line">anchors &#x3D; 10,14,  23,27,  37,58,  81,82,  135,169,  344,319</span><br><span class="line">classes&#x3D;1</span><br><span class="line">num&#x3D;6</span><br><span class="line">jitter&#x3D;.3</span><br><span class="line">ignore_thresh &#x3D; .7</span><br><span class="line">truth_thresh &#x3D; 1</span><br><span class="line">random&#x3D;1</span><br></pre></td></tr></table></figure>
<h2 id="5-模型构建"><a href="#5-模型构建" class="headerlink" title="5. 模型构建"></a>5. 模型构建</h2><p>以上都是准备工作，以SE为例，我们修改<code>model.py</code>文件中的模型加载部分，并修改forward函数部分的代码，让其正常发挥作用：</p>
<p>在<code>model.py</code>中的<code>create_modules</code>函数中进行添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">elif mdef[&#39;type&#39;] &#x3D;&#x3D; &#39;se&#39;:</span><br><span class="line">    modules.add_module(</span><br><span class="line">        &#39;se_module&#39;,</span><br><span class="line">        SELayer(output_filters[-1], reduction&#x3D;int(mdef[&#39;reduction&#39;])))</span><br></pre></td></tr></table></figure>
<p>然后修改Darknet中的forward部分的函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def forward(self, x, var&#x3D;None):</span><br><span class="line">    img_size &#x3D; x.shape[-2:]</span><br><span class="line">    layer_outputs &#x3D; []</span><br><span class="line">    output &#x3D; []</span><br><span class="line"></span><br><span class="line">    for i, (mdef,</span><br><span class="line">            module) in enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">        mtype &#x3D; mdef[&#39;type&#39;]</span><br><span class="line">        if mtype in [&#39;convolutional&#39;, &#39;upsample&#39;, &#39;maxpool&#39;]:</span><br><span class="line">            x &#x3D; module(x)</span><br><span class="line">        elif mtype &#x3D;&#x3D; &#39;route&#39;:</span><br><span class="line">            layers &#x3D; [int(x) for x in mdef[&#39;layers&#39;].split(&#39;,&#39;)]</span><br><span class="line">            if len(layers) &#x3D;&#x3D; 1:</span><br><span class="line">                x &#x3D; layer_outputs[layers[0]]</span><br><span class="line">            else:</span><br><span class="line">                try:</span><br><span class="line">                    x &#x3D; torch.cat([layer_outputs[i] for i in layers], 1)</span><br><span class="line">                except:  # apply stride 2 for darknet reorg layer</span><br><span class="line">                    layer_outputs[layers[1]] &#x3D; F.interpolate(</span><br><span class="line">                        layer_outputs[layers[1]], scale_factor&#x3D;[0.5, 0.5])</span><br><span class="line">                    x &#x3D; torch.cat([layer_outputs[i] for i in layers], 1)</span><br><span class="line"></span><br><span class="line">        elif mtype &#x3D;&#x3D; &#39;shortcut&#39;:</span><br><span class="line">            x &#x3D; x + layer_outputs[int(mdef[&#39;from&#39;])]</span><br><span class="line">        elif mtype &#x3D;&#x3D; &#39;yolo&#39;:</span><br><span class="line">            output.append(module(x, img_size))</span><br><span class="line">        layer_outputs.append(x if i in self.routs else [])</span><br></pre></td></tr></table></figure>
<p>在forward中加入SE模块，其实很简单。SE模块与卷积层，上采样，最大池化层地位是一样的，不需要更多操作，只需要将以上部分代码进行修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i, (mdef,</span><br><span class="line">           module) in enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">       mtype &#x3D; mdef[&#39;type&#39;]</span><br><span class="line">       if mtype in [&#39;convolutional&#39;, &#39;upsample&#39;, &#39;maxpool&#39;, &#39;se&#39;]:</span><br><span class="line">           x &#x3D; module(x)</span><br></pre></td></tr></table></figure>
<p>CBAM的整体过程类似，可以自己尝试一下，顺便熟悉一下YOLOv3的整体流程。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3模型构建中的YOLOLayer</title>
    <url>/2020/02/18/YOLOv3%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E4%B8%AD%E7%9A%84YOLOLayer/</url>
    <content><![CDATA[<h2 id="1-Grid创建"><a href="#1-Grid创建" class="headerlink" title=" 1. Grid创建"></a><a id="more"></a> 1. Grid创建</h2><p>YOLOv3是一个单阶段的目标检测器，将目标划分为不同的grid，每个grid分配3个anchor作为先验框来进行匹配。首先读一下代码中关于grid创建的部分。</p>
<p>首先了解一下pytorch中的API：<code>torch.mershgrid</code></p>
<p>举一个简单的例子就比较清楚了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; a &#x3D; torch.arange(3)</span><br><span class="line">&gt;&gt;&gt; b &#x3D; torch.arange(5)</span><br><span class="line">&gt;&gt;&gt; x,y &#x3D; torch.meshgrid(a,b)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([0, 1, 2])</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">tensor([0, 1, 2, 3, 4])</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[0, 0, 0, 0, 0],</span><br><span class="line">        [1, 1, 1, 1, 1],</span><br><span class="line">        [2, 2, 2, 2, 2]])</span><br><span class="line">&gt;&gt;&gt; y</span><br><span class="line">tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [0, 1, 2, 3, 4],</span><br><span class="line">        [0, 1, 2, 3, 4]])</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>单纯看输入输出，可能不是很明白，列举一个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; for i in range(3):</span><br><span class="line">...     for j in range(4):</span><br><span class="line">...         print(&quot;(&quot;, x[i,j], &quot;,&quot; ,y[i,j],&quot;)&quot;)</span><br><span class="line">...</span><br><span class="line">( tensor(0) , tensor(0) )</span><br><span class="line">( tensor(0) , tensor(1) )</span><br><span class="line">( tensor(0) , tensor(2) )</span><br><span class="line">( tensor(0) , tensor(3) )</span><br><span class="line">( tensor(1) , tensor(0) )</span><br><span class="line">( tensor(1) , tensor(1) )</span><br><span class="line">( tensor(1) , tensor(2) )</span><br><span class="line">( tensor(1) , tensor(3) )</span><br><span class="line">( tensor(2) , tensor(0) )</span><br><span class="line">( tensor(2) , tensor(1) )</span><br><span class="line">( tensor(2) , tensor(2) )</span><br><span class="line">( tensor(2) , tensor(3) )</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.stack((x,y),2)</span><br><span class="line">tensor([[[0, 0],</span><br><span class="line">         [0, 1],</span><br><span class="line">         [0, 2],</span><br><span class="line">         [0, 3],</span><br><span class="line">         [0, 4]],</span><br><span class="line"></span><br><span class="line">        [[1, 0],</span><br><span class="line">         [1, 1],</span><br><span class="line">         [1, 2],</span><br><span class="line">         [1, 3],</span><br><span class="line">         [1, 4]],</span><br><span class="line"></span><br><span class="line">        [[2, 0],</span><br><span class="line">         [2, 1],</span><br><span class="line">         [2, 2],</span><br><span class="line">         [2, 3],</span><br><span class="line">         [2, 4]]])</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>现在就比较清楚了，划分了3×4的网格，通过遍历得到的x和y就能遍历全部格子。</p>
<p>下面是yolov3中提供的代码(需要注意的是这是针对某一层YOLOLayer，而不是所有的YOLOLayer：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def create_grids(self,</span><br><span class="line">                 img_size&#x3D;416,</span><br><span class="line">                 ng&#x3D;(13, 13),</span><br><span class="line">                 device&#x3D;&#39;cpu&#39;,</span><br><span class="line">                 type&#x3D;torch.float32):</span><br><span class="line">    nx, ny &#x3D; ng  # 网格尺寸</span><br><span class="line">    self.img_size &#x3D; max(img_size)</span><br><span class="line">    #下采样倍数为32</span><br><span class="line">    self.stride &#x3D; self.img_size &#x2F; max(ng)</span><br><span class="line"></span><br><span class="line">    # 划分网格，构建相对左上角的偏移量</span><br><span class="line">    yv, xv &#x3D; torch.meshgrid([torch.arange(ny), torch.arange(nx)])</span><br><span class="line">    # 通过以上例子很容易理解</span><br><span class="line">    self.grid_xy &#x3D; torch.stack((xv, yv), 2).to(device).type(type).view(</span><br><span class="line">        (1, 1, ny, nx, 2))</span><br><span class="line"></span><br><span class="line">    # 处理anchor，将其除以下采样倍数</span><br><span class="line">    self.anchor_vec &#x3D; self.anchors.to(device) &#x2F; self.stride</span><br><span class="line">    self.anchor_wh &#x3D; self.anchor_vec.view(1, self.na, 1, 1,</span><br><span class="line">                                          2).to(device).type(type)</span><br><span class="line">    self.ng &#x3D; torch.Tensor(ng).to(device)</span><br><span class="line">    self.nx &#x3D; nx</span><br><span class="line">    self.ny &#x3D; ny</span><br></pre></td></tr></table></figure>
<h2 id="2-YOLOLayer"><a href="#2-YOLOLayer" class="headerlink" title="2. YOLOLayer"></a>2. YOLOLayer</h2><p>在之前的文章中讲过，YOLO层前一层卷积层的filter个数具有特殊的要求，计算方法为：</p>
<p>如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/653.webp" alt></p>
<p><strong>训练过程：</strong></p>
<p>YOLOLayer的作用就是对上一个卷积层得到的张量进行处理，具体可以看training过程涉及的代码(暂时不关心ONNX部分的代码)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class YOLOLayer(nn.Module):</span><br><span class="line">    def __init__(self, anchors, nc, img_size, yolo_index, arc):</span><br><span class="line">        super(YOLOLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.anchors &#x3D; torch.Tensor(anchors)</span><br><span class="line">        self.na &#x3D; len(anchors)  # 该YOLOLayer分配给每个grid的anchor的个数</span><br><span class="line">        self.nc &#x3D; nc  # 类别个数</span><br><span class="line">        self.no &#x3D; nc + 5  # 每个格子对应输出的维度 class + 5 中5代表x,y,w,h,conf</span><br><span class="line">        self.nx &#x3D; 0  # 初始化x方向上的格子数量</span><br><span class="line">        self.ny &#x3D; 0  # 初始化y方向上的格子数量</span><br><span class="line">        self.arc &#x3D; arc</span><br><span class="line"></span><br><span class="line">        if ONNX_EXPORT:  # grids must be computed in __init__</span><br><span class="line">            stride &#x3D; [32, 16, 8][yolo_index]  # stride of this layer</span><br><span class="line">            nx &#x3D; int(img_size[1] &#x2F; stride)  # number x grid points</span><br><span class="line">            ny &#x3D; int(img_size[0] &#x2F; stride)  # number y grid points</span><br><span class="line">            create_grids(self, img_size, (nx, ny))</span><br><span class="line"></span><br><span class="line">    def forward(self, p, img_size, var&#x3D;None):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        onnx代表开放式神经网络交换</span><br><span class="line">        pytorch中的模型都可以导出或转换为标准ONNX格式</span><br><span class="line">        在模型采用ONNX格式后，即可在各种平台和设备上运行</span><br><span class="line">        在这里ONNX代表规范化的推理过程</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        if ONNX_EXPORT:</span><br><span class="line">            bs &#x3D; 1  # batch size</span><br><span class="line">        else:</span><br><span class="line">            bs, _, ny, nx &#x3D; p.shape  # bs, 255, 13, 13</span><br><span class="line">            if (self.nx, self.ny) !&#x3D; (nx, ny):</span><br><span class="line">                create_grids(self, img_size, (nx, ny), p.device, p.dtype)</span><br><span class="line"></span><br><span class="line">        # p.view(bs, 255, 13, 13) -- &gt; (bs, 3, 13, 13, 85)</span><br><span class="line">        # (bs, anchors, grid, grid, classes + xywh)</span><br><span class="line">        p &#x3D; p.view(bs, self.na, self.no, self.ny,</span><br><span class="line">                   self.nx).permute(0, 1, 3, 4, 2).contiguous()</span><br><span class="line"></span><br><span class="line">        if self.training:</span><br><span class="line">            return p</span><br></pre></td></tr></table></figure>
<p>在理解以上代码的时候，需要理解每一个通道所代表的意义，原先的P是由上一层卷积得到的feature map, 形状为(以80个类别、输入416、下采样32倍为例)：【batch size, anchor×(80+5), 13,  13】，在训练的过程中，将feature map通过张量操作转化的形状为：【batch size, anchor, 13, 13, 85】。</p>
<p><strong>测试过程：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># p的形状目前为：【bs, anchor_num, gridx,gridy,xywhc+class】</span><br><span class="line">else:  # 测试推理过程</span><br><span class="line">   # s &#x3D; 1.5  # scale_xy  (pxy &#x3D; pxy * s - (s - 1) &#x2F; 2)</span><br><span class="line">   io &#x3D; p.clone()  # 测试过程输出就是io</span><br><span class="line">   io[..., :2] &#x3D; torch.sigmoid(io[..., :2]) + self.grid_xy  # xy</span><br><span class="line">   # grid_xy是左上角再加上偏移量io[...:2]代表xy偏移</span><br><span class="line">   io[..., 2:4] &#x3D; torch.exp(</span><br><span class="line">       io[..., 2:4]) * self.anchor_wh  # wh yolo method</span><br><span class="line">   # io[..., 2:4] &#x3D; ((torch.sigmoid(io[..., 2:4]) * 2) ** 3) * self.anchor_wh</span><br><span class="line">   # wh power method</span><br><span class="line">   io[..., :4] *&#x3D; self.stride</span><br><span class="line"></span><br><span class="line">   if &#39;default&#39; in self.arc:  # seperate obj and cls</span><br><span class="line">       torch.sigmoid_(io[..., 4])</span><br><span class="line">   elif &#39;BCE&#39; in self.arc:  # unified BCE (80 classes)</span><br><span class="line">       torch.sigmoid_(io[..., 5:])</span><br><span class="line">       io[..., 4] &#x3D; 1</span><br><span class="line">   elif &#39;CE&#39; in self.arc:  # unified CE (1 background + 80 classes)</span><br><span class="line">       io[..., 4:] &#x3D; F.softmax(io[..., 4:], dim&#x3D;4)</span><br><span class="line">       io[..., 4] &#x3D; 1</span><br><span class="line"></span><br><span class="line">   if self.nc &#x3D;&#x3D; 1:</span><br><span class="line">       io[..., 5] &#x3D; 1</span><br><span class="line">       # single-class model https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov3&#x2F;issues&#x2F;235</span><br><span class="line"></span><br><span class="line">   # reshape from [1, 3, 13, 13, 85] to [1, 507, 85]</span><br><span class="line">   return io.view(bs, -1, self.no), p</span><br></pre></td></tr></table></figure>
<p>理解以上内容是需要对应以下公式：</p>
<script type="math/tex; mode=display">
\begin{aligned} b_{x}=& \sigma\left(t_{x}\right)+c_{x} \\ b_{y}=& \sigma\left(t_{y}\right)+c_{y} \\ b_{w}=& p_{w} e^{t_{x}} \\ b_{h}=& p_{h} e^{t_{h}} \end{aligned}</script><p><strong>xy部分:</strong></p>
<script type="math/tex; mode=display">
b_{x}=\sigma\left(t_{x}\right)+c_{x}\\
b_{y}=\sigma\left(t_{y}\right)+c_{y}</script><p>$c_{x}, c_{y}$代表的是格子的左上角坐标；$t_{x}, t_{y}$代表的是网络预测的结果； $\sigma$代表sigmoid激活函数。对应代码理解：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">io[..., :2] &#x3D; torch.sigmoid(io[..., :2]) + self.grid_xy  # xy</span><br><span class="line"># grid_xy是左上角再加上偏移量io[...:2]代表xy偏移</span><br></pre></td></tr></table></figure>
<p><strong>wh部分:</strong></p>
<script type="math/tex; mode=display">
b_{w}=p_{w} e^{t_{x}}\\
b_{h}=p_{h} e^{t_{h}}</script><p>$p_{w}, p_{h}$代表的是anchor先验框在feature map上对应的大小。$t_{w}, t_{h}$代表的是网络学习得到的缩放系数。对应代码理解：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># wh yolo method</span><br><span class="line">io[..., 2:4] &#x3D; torch.exp(io[..., 2:4]) * self.anchor_wh</span><br></pre></td></tr></table></figure>
<p><strong>class部分：</strong></p>
<p>在类别部分，提供了几种方法，根据arc参数来进行不同模式的选择。以CE（crossEntropy）为例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#io：(bs, anchors, grid, grid, xywh+classes)</span><br><span class="line">io[..., 4:] &#x3D; F.softmax(io[..., 4:], dim&#x3D;4)# 使用softmax</span><br><span class="line">io[..., 4] &#x3D; 1</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3网络模型的构建</title>
    <url>/2020/02/18/YOLOv3%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%84%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="1-cfg文件"><a href="#1-cfg文件" class="headerlink" title=" 1. cfg文件"></a><a id="more"></a> 1. cfg文件</h2><p>在YOLOv3中，修改网络结构很容易，只需要修改cfg文件即可。目前，cfg文件支持convolutional, maxpool, unsample, route, shortcut, yolo这几个层。</p>
<p>而且作者也提供了多个cfg文件来进行网络构建，比如：yolov3.cfg、yolov3-tiny.cfg、yolov3-spp.cfg、csresnext50-panet-spp.cfg文件（提供的yolov3-spp-pan-scale.cfg文件，在代码级别还没有提供支持）。</p>
<p>如果想要添加自定义的模块也很方便，比如说注意力机制模块、空洞卷积等，都可以简单地得到添加或者修改。</p>
<p>为了更加方便的理解cfg文件网络是如何构建的，在这里推荐一个Github上的网络结构可视化软件：<code>Netron</code>，下图是可视化yolov3-tiny的结果：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/648.jpg" alt></p>
<h2 id="2-网络模型构建"><a href="#2-网络模型构建" class="headerlink" title="2. 网络模型构建"></a>2. 网络模型构建</h2><p>从<code>train.py</code>文件入手，其中涉及的网络构建的代码为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Initialize model</span><br><span class="line">model &#x3D; Darknet(cfg, arc&#x3D;opt.arc).to(device)</span><br></pre></td></tr></table></figure>
<p>然后沿着Darknet实现进行讲解：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Darknet(nn.Module):</span><br><span class="line">    # YOLOv3 object detection model</span><br><span class="line">    def __init__(self, cfg, img_size&#x3D;(416, 416), arc&#x3D;&#39;default&#39;):</span><br><span class="line">        super(Darknet, self).__init__()</span><br><span class="line">        self.module_defs &#x3D; parse_model_cfg(cfg)</span><br><span class="line">        self.module_list, self.routs &#x3D; create_modules(self.module_defs, img_size, arc)</span><br><span class="line">        self.yolo_layers &#x3D; get_yolo_layers(self)</span><br><span class="line"></span><br><span class="line">        # Darknet Header</span><br><span class="line">        self.version &#x3D; np.array([0, 2, 5], dtype&#x3D;np.int32)</span><br><span class="line">        # (int32) version info: major, minor, revision</span><br><span class="line">        self.seen &#x3D; np.array([0], dtype&#x3D;np.int64)</span><br><span class="line">        # (int64) number of images seen during training</span><br></pre></td></tr></table></figure>
<p>以上文件中，比较关键的就是成员函变量<code>module_defs</code>、<code>module_list</code>、<code>routs</code>、<code>yolo_layers</code>四个成员函数，先对这几个参数的意义进行解释：</p>
<h3 id="2-1-module-defs"><a href="#2-1-module-defs" class="headerlink" title="2.1 module_defs"></a>2.1 module_defs</h3><p>调用了<code>parse_model_cfg</code>函数，得到了<code>module_defs</code>对象。实际上该函数是通过解析cfg文件，得到一个list，list中包含多个字典，每个字典保存的内容就是一个模块内容，比如说：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;2</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br></pre></td></tr></table></figure>
<p>函数代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def parse_model_cfg(path):</span><br><span class="line">    # path参数为: cfg&#x2F;yolov3-tiny.cfg</span><br><span class="line">    if not path.endswith(&#39;.cfg&#39;):</span><br><span class="line">        path +&#x3D; &#39;.cfg&#39;</span><br><span class="line">    if not os.path.exists(path) and os.path.exists(&#39;cfg&#39; + os.sep + path):</span><br><span class="line">        path &#x3D; &#39;cfg&#39; + os.sep + path</span><br><span class="line"></span><br><span class="line">    with open(path, &#39;r&#39;) as f:</span><br><span class="line">        lines &#x3D; f.read().split(&#39;\n&#39;)</span><br><span class="line"></span><br><span class="line">    # 去除以#开头的，属于注释部分的内容</span><br><span class="line">    lines &#x3D; [x for x in lines if x and not x.startswith(&#39;#&#39;)]</span><br><span class="line">    lines &#x3D; [x.rstrip().lstrip() for x in lines]</span><br><span class="line">    mdefs &#x3D; []  # 模块的定义</span><br><span class="line">    for line in lines:</span><br><span class="line">        if line.startswith(&#39;[&#39;):  # 标志着一个模块的开始</span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            比如:</span><br><span class="line">            [shortcut]</span><br><span class="line">            from&#x3D;-3</span><br><span class="line">            activation&#x3D;linear</span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            mdefs.append(&#123;&#125;)</span><br><span class="line">            mdefs[-1][&#39;type&#39;] &#x3D; line[1:-1].rstrip()</span><br><span class="line">            if mdefs[-1][&#39;type&#39;] &#x3D;&#x3D; &#39;convolutional&#39;:</span><br><span class="line">                mdefs[-1][&#39;batch_normalize&#39;] &#x3D; 0  </span><br><span class="line">                # pre-populate with zeros (may be overwritten later)</span><br><span class="line">        else:</span><br><span class="line">            # 将键和键值放入字典</span><br><span class="line">            key, val &#x3D; line.split(&quot;&#x3D;&quot;)</span><br><span class="line">            key &#x3D; key.rstrip()</span><br><span class="line"></span><br><span class="line">            if &#39;anchors&#39; in key:</span><br><span class="line">                mdefs[-1][key] &#x3D; np.array([float(x) for x in val.split(&#39;,&#39;)]).reshape((-1, 2))  # np anchors</span><br><span class="line">            else:</span><br><span class="line">                mdefs[-1][key] &#x3D; val.strip()</span><br><span class="line"></span><br><span class="line">    # 支持的参数类型</span><br><span class="line">    supported &#x3D; [&#39;type&#39;, &#39;batch_normalize&#39;, &#39;filters&#39;, &#39;size&#39;,\</span><br><span class="line">                 &#39;stride&#39;, &#39;pad&#39;, &#39;activation&#39;, &#39;layers&#39;, &#39;groups&#39;,\</span><br><span class="line">                 &#39;from&#39;, &#39;mask&#39;, &#39;anchors&#39;, &#39;classes&#39;, &#39;num&#39;, &#39;jitter&#39;, \</span><br><span class="line">                 &#39;ignore_thresh&#39;, &#39;truth_thresh&#39;, &#39;random&#39;,\</span><br><span class="line">                 &#39;stride_x&#39;, &#39;stride_y&#39;]</span><br><span class="line"></span><br><span class="line">    # 判断所有参数中是否有不符合要求的key</span><br><span class="line">    f &#x3D; []</span><br><span class="line">    for x in mdefs[1:]:</span><br><span class="line">        [f.append(k) for k in x if k not in f]</span><br><span class="line">    u &#x3D; [x for x in f if x not in supported]  # unsupported fields</span><br><span class="line">    assert not any(u), &quot;Unsupported fields %s in %s. See https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov3&#x2F;issues&#x2F;631&quot; % (u, path)</span><br><span class="line"></span><br><span class="line">    return mdefs</span><br></pre></td></tr></table></figure>
<p>返回的内容通过debug模式进行查看：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/649.webp" alt></p>
<p>其中需要关注的就是anchor的组织：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/650.webp" alt></p>
<p>可以看出，anchor是按照每两个一对进行组织的，与我们的理解一致。</p>
<h3 id="2-2-module-list-amp-routs"><a href="#2-2-module-list-amp-routs" class="headerlink" title="2.2 module_list&amp;routs"></a>2.2 module_list&amp;routs</h3><p>这个部分是本文的核心，也是理解模型构建的关键。</p>
<p>在pytorch中，构建模型常见的有通过Sequential或者ModuleList进行构建。</p>
<p><strong>通过Sequential构建</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model&#x3D;nn.Sequential()</span><br><span class="line">model.add_module(&#39;conv&#39;,nn.Conv2d(3,3,3))</span><br><span class="line">model.add_module(&#39;batchnorm&#39;,nn.BatchNorm2d(3))</span><br><span class="line">model.add_module(&#39;activation_layer&#39;,nn.ReLU())</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model&#x3D;nn.Sequential(</span><br><span class="line">    nn.Conv2d(3,3,3),</span><br><span class="line">    nn.BatchNorm2d(3),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections import OrderedDict</span><br><span class="line">model&#x3D;nn.Sequential(OrderedDict([</span><br><span class="line">    (&#39;conv&#39;,nn.Conv2d(3,3,3)),</span><br><span class="line">    (&#39;batchnorm&#39;,nn.BatchNorm2d(3)),</span><br><span class="line">    (&#39;activation_layer&#39;,nn.ReLU())</span><br><span class="line">]))</span><br></pre></td></tr></table></figure>
<p>通过sequential构建的模块内部<strong>实现了forward函数</strong>，可以直接传入参数，进行调用。</p>
<p><strong>通过ModuleList构建</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model&#x3D;nn.ModuleList([nn.Linear(3,4),</span><br><span class="line">						 nn.ReLU(),</span><br><span class="line">						 nn.Linear(4,2)])</span><br></pre></td></tr></table></figure>
<p>ModuleList类似list，内部<strong>没有实现forward函数</strong>，使用的时候需要构建forward函数,构建自己模型常用ModuleList函数建立子模型,建立forward函数实现前向传播。</p>
<p>在YOLOv3中，灵活地结合了两种使用方式，通过解析以上得到的module_defs，进行构建一个ModuleList，然后再通过构建forward函数进行前向传播即可。</p>
<p>具体代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def create_modules(module_defs, img_size, arc):</span><br><span class="line">    # 通过module_defs进行构建模型</span><br><span class="line">    hyperparams &#x3D; module_defs.pop(0)</span><br><span class="line">    output_filters &#x3D; [int(hyperparams[&#39;channels&#39;])]</span><br><span class="line">    module_list &#x3D; nn.ModuleList()</span><br><span class="line">    routs &#x3D; []  # 存储了所有的层，在route、shortcut会使用到。</span><br><span class="line">    yolo_index &#x3D; -1</span><br><span class="line"></span><br><span class="line">    for i, mdef in enumerate(module_defs):</span><br><span class="line">        modules &#x3D; nn.Sequential()</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        通过type字样不同的类型，来进行模型构建</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        if mdef[&#39;type&#39;] &#x3D;&#x3D; &#39;convolutional&#39;:</span><br><span class="line">            bn &#x3D; int(mdef[&#39;batch_normalize&#39;])</span><br><span class="line">            filters &#x3D; int(mdef[&#39;filters&#39;])</span><br><span class="line">            size &#x3D; int(mdef[&#39;size&#39;])</span><br><span class="line">            stride &#x3D; int(mdef[&#39;stride&#39;]) if &#39;stride&#39; in mdef else (int(</span><br><span class="line">                mdef[&#39;stride_y&#39;]), int(mdef[&#39;stride_x&#39;]))</span><br><span class="line">            pad &#x3D; (size - 1) &#x2F;&#x2F; 2 if int(mdef[&#39;pad&#39;]) else 0</span><br><span class="line">            modules.add_module(</span><br><span class="line">                &#39;Conv2d&#39;,</span><br><span class="line">                nn.Conv2d(</span><br><span class="line">                    in_channels&#x3D;output_filters[-1],</span><br><span class="line">                    out_channels&#x3D;filters,</span><br><span class="line">                    kernel_size&#x3D;size,</span><br><span class="line">                    stride&#x3D;stride,</span><br><span class="line">                    padding&#x3D;pad,</span><br><span class="line">                    groups&#x3D;int(mdef[&#39;groups&#39;]) if &#39;groups&#39; in mdef else 1,</span><br><span class="line">                    bias&#x3D;not bn))</span><br><span class="line">            if bn:</span><br><span class="line">                modules.add_module(&#39;BatchNorm2d&#39;,</span><br><span class="line">                                   nn.BatchNorm2d(filters, momentum&#x3D;0.1))</span><br><span class="line">            if mdef[&#39;activation&#39;] &#x3D;&#x3D; &#39;leaky&#39;:  # TODO: activation study https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov3&#x2F;issues&#x2F;441</span><br><span class="line">                modules.add_module(&#39;activation&#39;, nn.LeakyReLU(0.1,</span><br><span class="line">                                                              inplace&#x3D;True))</span><br><span class="line">            elif mdef[&#39;activation&#39;] &#x3D;&#x3D; &#39;swish&#39;:</span><br><span class="line">                modules.add_module(&#39;activation&#39;, Swish())</span><br><span class="line">            # 在此处可以添加新的激活函数</span><br><span class="line"></span><br><span class="line">        elif mdef[&#39;type&#39;] &#x3D;&#x3D; &#39;maxpool&#39;:</span><br><span class="line">            # 最大池化操作</span><br><span class="line">            size &#x3D; int(mdef[&#39;size&#39;])</span><br><span class="line">            stride &#x3D; int(mdef[&#39;stride&#39;])</span><br><span class="line">            maxpool &#x3D; nn.MaxPool2d(kernel_size&#x3D;size,</span><br><span class="line">                                   stride&#x3D;stride,</span><br><span class="line">                                   padding&#x3D;int((size - 1) &#x2F;&#x2F; 2))</span><br><span class="line">            if size &#x3D;&#x3D; 2 and stride &#x3D;&#x3D; 1:  # yolov3-tiny</span><br><span class="line">                modules.add_module(&#39;ZeroPad2d&#39;, nn.ZeroPad2d((0, 1, 0, 1)))</span><br><span class="line">                modules.add_module(&#39;MaxPool2d&#39;, maxpool)</span><br><span class="line">            else:</span><br><span class="line">                modules &#x3D; maxpool</span><br><span class="line"></span><br><span class="line">        elif mdef[&#39;type&#39;] &#x3D;&#x3D; &#39;upsample&#39;:</span><br><span class="line">            # 通过近邻插值完成上采样</span><br><span class="line">            modules &#x3D; nn.Upsample(scale_factor&#x3D;int(mdef[&#39;stride&#39;]),</span><br><span class="line">                                  mode&#x3D;&#39;nearest&#39;)</span><br><span class="line"></span><br><span class="line">        elif mdef[&#39;type&#39;] &#x3D;&#x3D; &#39;route&#39;:</span><br><span class="line">            # nn.Sequential() placeholder for &#39;route&#39; layer</span><br><span class="line">            layers &#x3D; [int(x) for x in mdef[&#39;layers&#39;].split(&#39;,&#39;)]</span><br><span class="line">            filters &#x3D; sum(</span><br><span class="line">                [output_filters[i + 1 if i &gt; 0 else i] for i in layers])</span><br><span class="line">            # extend表示添加一系列对象</span><br><span class="line">            routs.extend([l if l &gt; 0 else l + i for l in layers])</span><br><span class="line"></span><br><span class="line">        elif mdef[&#39;type&#39;] &#x3D;&#x3D; &#39;shortcut&#39;:</span><br><span class="line">            # nn.Sequential() placeholder for &#39;shortcut&#39; layer</span><br><span class="line">            filters &#x3D; output_filters[int(mdef[&#39;from&#39;])]</span><br><span class="line">            layer &#x3D; int(mdef[&#39;from&#39;])</span><br><span class="line">            routs.extend([i + layer if layer &lt; 0 else layer])</span><br><span class="line"></span><br><span class="line">        elif mdef[&#39;type&#39;] &#x3D;&#x3D; &#39;yolo&#39;:</span><br><span class="line">            yolo_index +&#x3D; 1</span><br><span class="line">            mask &#x3D; [int(x) for x in mdef[&#39;mask&#39;].split(&#39;,&#39;)]  # anchor mask</span><br><span class="line">            modules &#x3D; YOLOLayer(</span><br><span class="line">                anchors&#x3D;mdef[&#39;anchors&#39;][mask],  # anchor list</span><br><span class="line">                nc&#x3D;int(mdef[&#39;classes&#39;]),  # number of classes</span><br><span class="line">                img_size&#x3D;img_size,  # (416, 416)</span><br><span class="line">                yolo_index&#x3D;yolo_index,  # 0, 1 or 2</span><br><span class="line">                arc&#x3D;arc)  # yolo architecture</span><br><span class="line"></span><br><span class="line">            # 这是在focal loss文章中提到的为卷积层添加bias</span><br><span class="line">            # 主要用于解决样本不平衡问题</span><br><span class="line">            # (论文地址 https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1708.02002.pdf section 3.3)</span><br><span class="line">            # 具体讲解见下方</span><br><span class="line">            try:</span><br><span class="line">                if arc &#x3D;&#x3D; &#39;defaultpw&#39; or arc &#x3D;&#x3D; &#39;Fdefaultpw&#39;:</span><br><span class="line">                    # default with positive weights</span><br><span class="line">                    b &#x3D; [-5.0, -5.0]  # obj, cls</span><br><span class="line">                elif arc &#x3D;&#x3D; &#39;default&#39;:</span><br><span class="line">                    # default no pw (40 cls, 80 obj)</span><br><span class="line">                    b &#x3D; [-5.0, -5.0]</span><br><span class="line">                elif arc &#x3D;&#x3D; &#39;uBCE&#39;:</span><br><span class="line">                    # unified BCE (80 classes)</span><br><span class="line">                    b &#x3D; [0, -9.0]</span><br><span class="line">                elif arc &#x3D;&#x3D; &#39;uCE&#39;:</span><br><span class="line">                    # unified CE (1 background + 80 classes)</span><br><span class="line">                    b &#x3D; [10, -0.1]</span><br><span class="line">                elif arc &#x3D;&#x3D; &#39;Fdefault&#39;:</span><br><span class="line">                    # Focal default no pw (28 cls, 21 obj, no pw)</span><br><span class="line">                    b &#x3D; [-2.1, -1.8]</span><br><span class="line">                elif arc &#x3D;&#x3D; &#39;uFBCE&#39; or arc &#x3D;&#x3D; &#39;uFBCEpw&#39;:</span><br><span class="line">                    # unified FocalBCE (5120 obj, 80 classes)</span><br><span class="line">                    b &#x3D; [0, -6.5]</span><br><span class="line">                elif arc &#x3D;&#x3D; &#39;uFCE&#39;:</span><br><span class="line">                    # unified FocalCE (64 cls, 1 background + 80 classes)</span><br><span class="line">                    b &#x3D; [7.7, -1.1]</span><br><span class="line"></span><br><span class="line">                bias &#x3D; module_list[-1][0].bias.view(len(mask), -1)</span><br><span class="line">                # 255 to 3x85</span><br><span class="line">                bias[:, 4] +&#x3D; b[0] - bias[:, 4].mean()  # obj</span><br><span class="line">                bias[:, 5:] +&#x3D; b[1] - bias[:, 5:].mean()  # cls</span><br><span class="line">                </span><br><span class="line">                # 将新的偏移量赋值回模型中</span><br><span class="line">                module_list[-1][0].bias &#x3D; torch.nn.Parameter(bias.view(-1))</span><br><span class="line"></span><br><span class="line">            except:</span><br><span class="line">                print(&#39;WARNING: smart bias initialization failure.&#39;)</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line">            print(&#39;Warning: Unrecognized Layer Type: &#39; + mdef[&#39;type&#39;])</span><br><span class="line"></span><br><span class="line">        # 将module内容保存在module_list中。</span><br><span class="line">        module_list.append(modules)</span><br><span class="line">        # 保存所有的filter个数</span><br><span class="line">        output_filters.append(filters)</span><br><span class="line"></span><br><span class="line">    return module_list, routs</span><br></pre></td></tr></table></figure>
<p><strong>bias部分讲解</strong></p>
<p>其中在YOLO Layer部分涉及到一个初始化的trick，来自Focal Loss中关于模型初始化的讨论，具体内容请阅读论文，<code>https://arxiv.org/pdf/1708.02002.pdf</code> 的第3.3节。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/651.webp" alt></p>
<p>这里涉及到一个非常insight的点，在第一篇中介绍了，YOLO层前一个卷积的filter个数计算公式如下：</p>
<script type="math/tex; mode=display">
filter=(c l a s s+5) \times 3</script><p>5代表x,y,w,h, score，score代表该格子中是否存在目标，3代表这个格子中会分配3个anchor进行匹配。在YOLOLayer中的forward函数中，有以下代码，需要通过sigmoid激活函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if &#39;default&#39; in self.arc:  # seperate obj and cls</span><br><span class="line">	torch.sigmoid_(io[..., 4])</span><br><span class="line">elif &#39;BCE&#39; in self.arc:  # unified BCE (80 classes)</span><br><span class="line">	torch.sigmoid_(io[..., 5:])</span><br><span class="line">	io[..., 4] &#x3D; 1</span><br><span class="line">elif &#39;CE&#39; in self.arc:  # unified CE (1 background + 80 classes)</span><br><span class="line">	io[..., 4:] &#x3D; F.softmax(io[..., 4:], dim&#x3D;4)</span><br><span class="line">	io[..., 4] &#x3D; 1</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/652.png" alt></p>
<p>可以观察到，Sigmoid梯度是有限的，在<code>[-5,5]</code>之间。</p>
<p>而pytorch中的卷积层默认的初始化是以0为中心点的正态分布，这样进行的初始化会导致很多gird中大约一半得到了激活，在计算loss的时候就会计算上所有的激活的点对应的坐标信息，这样计算loss就会变得很大。</p>
<p>根据这个现象，作者选择在YOLOLayer的前一个卷积层添加bias，来避免这种情况，实际操作就是在原有的bias上减去5，这样通过卷积得到的数值就不会被激活，可以防止在初始阶段的第一个batch中就进行过拟合。通过以上操作，能够让所有的神经元在前几个batch中输出空的检测。</p>
<p>经过作者的实验，通过使用bias的trick，可以提升mAP、F1、P、R等指标，还能让训练过程更加平滑。</p>
<h3 id="2-3-yolo-layers"><a href="#2-3-yolo-layers" class="headerlink" title="2.3 yolo_layers"></a>2.3 yolo_layers</h3><p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_yolo_layers(model):</span><br><span class="line">    return [i for i, x in enumerate(model.module_defs) if x[&#39;type&#39;] &#x3D;&#x3D; &#39;yolo&#39;]</span><br><span class="line">    # [82, 94, 106] for yolov3</span><br></pre></td></tr></table></figure>
<p>yolo layer的获取是通过解析module_defs这个存储cfg文件中的信息的变量得到的。以yolov3.cfg为例，最终返回的是yolo层在整个module的序号。比如：第83,94,106个层是YOLO层。</p>
<h2 id="3-forward函数"><a href="#3-forward函数" class="headerlink" title="3. forward函数"></a>3. forward函数</h2><p>在YOLO中，如果能理解前向传播的过程，那整个网络的构建也就很清楚明了了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def forward(self, x, var&#x3D;None):</span><br><span class="line">    img_size &#x3D; x.shape[-2:]</span><br><span class="line">    layer_outputs &#x3D; []</span><br><span class="line">    output &#x3D; []</span><br><span class="line"></span><br><span class="line">    for i, (mdef,</span><br><span class="line">            module) in enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">        mtype &#x3D; mdef[&#39;type&#39;]</span><br><span class="line">        if mtype in [&#39;convolutional&#39;, &#39;upsample&#39;, &#39;maxpool&#39;]:</span><br><span class="line">            # 卷积层，上采样，池化层只需要经过即可</span><br><span class="line">            x &#x3D; module(x)</span><br><span class="line">        elif mtype &#x3D;&#x3D; &#39;route&#39;:</span><br><span class="line">            # route操作就是将几个层的内容拼接起来，具体可以看cfg文件解析</span><br><span class="line">            layers &#x3D; [int(x) for x in mdef[&#39;layers&#39;].split(&#39;,&#39;)]</span><br><span class="line">            if len(layers) &#x3D;&#x3D; 1:</span><br><span class="line">                x &#x3D; layer_outputs[layers[0]]</span><br><span class="line">            else:</span><br><span class="line">                try:</span><br><span class="line">                    x &#x3D; torch.cat([layer_outputs[i] for i in layers], 1)</span><br><span class="line">                except:</span><br><span class="line">                    # apply stride 2 for darknet reorg layer</span><br><span class="line">                    layer_outputs[layers[1]] &#x3D; F.interpolate(</span><br><span class="line">                        layer_outputs[layers[1]], scale_factor&#x3D;[0.5, 0.5])</span><br><span class="line">                    x &#x3D; torch.cat([layer_outputs[i] for i in layers], 1)</span><br><span class="line"></span><br><span class="line">        elif mtype &#x3D;&#x3D; &#39;shortcut&#39;:</span><br><span class="line">            x &#x3D; x + layer_outputs[int(mdef[&#39;from&#39;])]</span><br><span class="line">        elif mtype &#x3D;&#x3D; &#39;yolo&#39;:</span><br><span class="line">            output.append(module(x, img_size))</span><br><span class="line">        #记录route对应的层</span><br><span class="line">        layer_outputs.append(x if i in self.routs else [])</span><br><span class="line"></span><br><span class="line">    if self.training:</span><br><span class="line">        # 如果训练，直接输出YOLO要求的Tensor</span><br><span class="line">        # 3*(class+5)</span><br><span class="line">        return output</span><br><span class="line">    </span><br><span class="line">    elif ONNX_EXPORT:# 这个是对应的onnx导出的内容</span><br><span class="line">        x &#x3D; [torch.cat(x, 0) for x in zip(*output)]</span><br><span class="line">        return x[0], torch.cat(x[1:3], 1)  # scores, boxes: 3780x80, 3780x4</span><br><span class="line">    else:</span><br><span class="line">        # 对应测试阶段</span><br><span class="line">        io, p &#x3D; list(zip(*output))  # inference output, training output</span><br><span class="line">        return torch.cat(io, 1), p</span><br></pre></td></tr></table></figure>
<p>forward的过程也比较简单，通过得到的module_defs和module_list变量，通过for循环将整个module_list中的内容进行一遍串联，需要得到的最终结果是YOLO层的输出。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>使用frp服务实现对内网机器的远程连接</title>
    <url>/2020/02/16/%E4%BD%BF%E7%94%A8frp%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0%E5%AF%B9%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8%E7%9A%84%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<h2 id="为何要用"><a href="#为何要用" class="headerlink" title=" 为何要用"></a><a id="more"></a> 为何要用</h2><p>我们并不是每天都会接触到实验室内网环境。当不在学校时，如何访问内网的资源成了一个头疼的问题。本文旨在提出一种内网穿透解决方案，在外网环境下优雅的访问到内网的任何端口。即使身离学校也可以方便的修改内网模型，访问内网计算资源。 </p>
<h2 id="特殊需要"><a href="#特殊需要" class="headerlink" title="特殊需要"></a>特殊需要</h2><ul>
<li><p>一台公网机（有公网ip的vps， 比如阿里云服务器）</p>
</li>
<li><p>frp 端口转发软件 :<a href="https://github.com/fatedier/frp/releases" target="_blank" rel="noopener">下载地址</a></p>
</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>windows不用说，这里讲Ubuntu的安装</p>
<p>选择相应版本，右键复制链接</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/frp/Snipaste_2020-02-16_15-19-58.jpg" alt></p>
<p>打开终端，输入<code>wget 复制的链接地址</code>即可开始下载</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/frp/Snipaste_2020-02-16_15-25-08.jpg" alt></p>
<p>下载完成后，找到安装包，解压即可，解压命令为<code>tar -zxvf 包名</code></p>
<p><strong>公网机、内网机都需要安装，且frp版本要一致。</strong></p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="公网机配置"><a href="#公网机配置" class="headerlink" title="公网机配置"></a>公网机配置</h3><p>修改 <strong>frps.ini</strong> 文件，这里使用了最简化的配置：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># frps.ini</span></span><br><span class="line">[common]</span><br><span class="line">bind_port = 7000 <span class="comment">#这个端口代表内网机连到公网所需端口，默认7000，可自己定义</span></span><br></pre></td></tr></table></figure>
<p>Windows 启动 frps：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;frps.exe -c .&#x2F;frps.ini</span><br></pre></td></tr></table></figure>
<p>Ubuntu 启动 frps:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;frps -c .&#x2F;frps.ini</span><br></pre></td></tr></table></figure>
<p>Ubuntu 在后台开启运行 frp 服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup .&#x2F;frps -c frps.ini &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<h3 id="内网机器配置"><a href="#内网机器配置" class="headerlink" title="内网机器配置"></a>内网机器配置</h3><p>修改 <strong>frpc.ini</strong> 文件，假设 <strong>frps</strong> 所在服务器的公网 IP 为 x.x.x.x；</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># frpc.ini</span><br><span class="line">[common]</span><br><span class="line">server_addr &#x3D; x.x.x.x #这里可以是公网机的域名，也可以是ip</span><br><span class="line">server_port &#x3D; 7000 #刚刚公网机配置的与内网机交换的端口</span><br><span class="line"></span><br><span class="line">[ssh]</span><br><span class="line">type &#x3D; tcp</span><br><span class="line">local_ip &#x3D; 127.0.0.1</span><br><span class="line">local_port &#x3D; 22 #本地端口号，本地需要转发的端口 默认22 可自己设定</span><br><span class="line">remote_port &#x3D; 6000  #远程端口号，将端口映射到公网相应的端口 可自己设定</span><br></pre></td></tr></table></figure>
<p>Windows 启动 frpc：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;frpc.exe -c .&#x2F;frpc.ini</span><br></pre></td></tr></table></figure>
<p>Ubuntu 启动 frpc：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;frpc -c .&#x2F;frpc.ini</span><br></pre></td></tr></table></figure>
<p>此时，你就可以通过你的外网 IP + 远程端口号来实现对内网相关服务的访问了。</p>
<h2 id="设置开机启动"><a href="#设置开机启动" class="headerlink" title="设置开机启动"></a>设置开机启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;frpc.service</span><br></pre></td></tr></table></figure>
<p>按如下修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;frpc daemon</span><br><span class="line">After&#x3D;syslog.target  network.target</span><br><span class="line">Wants&#x3D;network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;sbin&#x2F;frp&#x2F;frpc -c &#x2F;etc&#x2F;frp&#x2F;frpc.ini</span><br><span class="line">Restart&#x3D; always</span><br><span class="line">RestartSec&#x3D;1min</span><br><span class="line">ExecStop&#x3D;&#x2F;usr&#x2F;bin&#x2F;killall frpc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure>
<p>使用<code>sudo systemctl enable frpc.service</code>启用服务</p>
<p>服务器端（公网机器）同理</p>
<h2 id="白嫖简化步骤"><a href="#白嫖简化步骤" class="headerlink" title="白嫖简化步骤"></a>白嫖简化步骤</h2><p>考虑到很多人负担不起公网机的价格，这里提供发布免费公网域名的网站<br><a href="http://www.frps.top/" target="_blank" rel="noopener">frp免费公共服务器列表</a></p>
<p>由于域名供应商已经配置好了公网机端，所以我们只需要配置我们需要连接的内网机即可</p>
<p>首先找到可用的服务器，比如</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/frp/Snipaste_2020-02-16_15-47-16.jpg" alt></p>
<p>上面显示它所用的frp版本为0.14.1，所以我们也必须安装<strong>相应的frp版本</strong>在内网机上，否则由于软件版本不匹配，连不上。</p>
<p>按照之前的教程，根据所提供的信息，修改<strong>frpc.ini</strong>，如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/frp/TIM%E5%9B%BE%E7%89%8720200216154853.jpg" alt></p>
<p>这里注意0.17.0版本之前用的是<code>privilege_token = xxxx</code>，而之后的版本用的是<code>token = xxxx</code>，<strong>远程端口号必须设置在服务商所提供的范围内</strong>，如图中TCP/UDP端口为1000-65535，则再此区间任取端口填入就行。</p>
<p>设置完，你便可以通过相应的SSH软件，连接内网机器，主机名为公网域名，端口号为你设置的远程端口</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/frp/TIM%E5%9B%BE%E7%89%8720200216155354.png" alt></p>
<p>有的服务商还提供了二级或三级域名，如图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/frp/Snipaste_2020-02-16_15-58-29.jpg" alt></p>
<p>则 frpc.ini 配置如下格式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[common]</span><br><span class="line">#server_addr一定要写域名形式，不要直接写IP地址</span><br><span class="line">server_addr &#x3D; frp1.chuantou.org</span><br><span class="line">server_port &#x3D; 7000</span><br><span class="line">privilege_token &#x3D; www.xxorg.com</span><br><span class="line"># 标注你的代理名字，随便选择一个跟别人不一样即可</span><br><span class="line">user &#x3D; myname</span><br><span class="line"></span><br><span class="line">[xxorg] # 可以自己取</span><br><span class="line">type &#x3D; http</span><br><span class="line">local_ip &#x3D; 127.0.0.1</span><br><span class="line">local_port &#x3D; 80 # 按照图中显示为80</span><br><span class="line"># 自己取一个可用的子域名，你的访问地址将会是http:&#x2F;&#x2F;xxorg.frp1.chuantou.org</span><br><span class="line">subdomain &#x3D; xxorg</span><br><span class="line"></span><br><span class="line">[tcp3389] # 可以自己取</span><br><span class="line">type &#x3D; tcp</span><br><span class="line">local_ip &#x3D; 127.0.0.1</span><br><span class="line">local_port &#x3D; 3389 # 自己设</span><br><span class="line">remote_port &#x3D; 53389 #自己设，图中范围为50000-60000</span><br></pre></td></tr></table></figure>
<p>设置完，你便可以通过相应的SSH软件，连接内网机器</p>
<p>如：主机名为<code>http://xxorg.frp1.chuantou.org</code>，端口号为53389</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>frp</tag>
      </tags>
  </entry>
  <entry>
    <title>如何配置SwitchyOmega插件</title>
    <url>/2020/02/16/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AESwitchyOmega%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title=" 简介"></a><a id="more"></a> 简介</h2><p>SwitchyOmega是 Chrome 和 Firefox 浏览器上的代理扩展程序,可以轻松快捷的管理和切换多个代理设置。接管浏览器代理方式，可瞬间切换代理和本地连接方式，配合socks5（等其他代理工具）可实现只代理部分国内无法访问的网站。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>进入插件选项界面</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_13-39-31.jpg" alt></p>
<p>选择新建情景模式—代理服务器，名称自取</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_13-41-23.jpg" alt></p>
<p>找到软件所提供的Socks端口号，这里以clash为例</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_14-51-02.jpg" alt></p>
<p>SSR、Netch等同理</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_14-52-44.jpg" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_14-53-32.jpg" alt></p>
<p>按以下格式配置：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_14-49-43.jpg" alt></p>
<p>接着新建情景模式—自动切换模式，名字自取</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_14-55-29.jpg" alt></p>
<p>规则列表格式为：AutoProxy</p>
<p>规则列表网址为：<code>https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</code></p>
<p>然后点击立即更新情景模式，则会自动加载PAC列表</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_14-57-22.jpg" alt></p>
<p>切换规则按如下配置：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_15-01-34.jpg" alt></p>
<p>意思是，选择auto模式时，插件为根据PAC规则判断是否要走代理</p>
<p>现在软件在后台启动着，不需要开启代理，都可以通过浏览器的此插件进行切换，PAC模式选auto，全局选代理服务器，不代理选直连</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/switchyomega/Snipaste_2020-02-16_15-06-10.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>如何给热点翻墙</title>
    <url>/2020/02/16/%E5%A6%82%E4%BD%95%E7%BB%99%E7%83%AD%E7%82%B9%E7%BF%BB%E5%A2%99/</url>
    <content><![CDATA[<h2 id="Netch简介"><a href="#Netch简介" class="headerlink" title=" Netch简介"></a><a id="more"></a> Netch简介</h2><p>Netch是一款开源的网络游戏工具，支持Socks5、55R、V2等协议，UDP NAT  FullCone及指定进程加速（不需要麻烦的IP规则）。功能上和SSTAP差不多，不过听说加速效果比后者要更好，甚至堪比一些付费的加速器，当然前提需要你的线路给力，不然加速就没意义了。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/windows-netch-1.jpg" alt></p>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>下载 Netch 客户端，解压后以管理员身份运行 Netch.exe。若系统提示需要安装 .NET Framework，请<a href="https://www.microsoft.com/net/download/dotnet-framework-runtime" target="_blank" rel="noopener">点此</a>访问微软官网下载安装。</p>
<p><strong>Github地址：</strong><a href="https://github.com/netchx/Netch" target="_blank" rel="noopener">https://github.com/netchx/Netch</a></p>
<p><strong>下载地址：</strong><a href="https://github.com/netchx/Netch/releases" target="_blank" rel="noopener">https://github.com/netchx/Netch/releases</a></p>
<p>打开程序后，选中 “订阅” &gt; “管理订阅链接”</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/windows-netch-2.jpg" alt></p>
<p>粘贴服务商提供的订阅链接到左下角的链接，备注随便填写，点击添加，然后点击保存</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/windows-netch-3.jpg" alt></p>
<p>也可以从剪切板添加节点</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2019-06-24_10-48-31.png" alt></p>
<p>选择<code>Bypass LAN and China #绕过局域网和大陆</code>，点击启动，即可</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2020-02-16_12-45-58.jpg" alt></p>
<p>模式中也可以选择相应的游戏进行加速，也支持给Xshell和Xftp连接国外服务器进行加速</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/windows-netch-7.jpg" alt></p>
<p>如果需要加速的游戏不在列表里面，那么就选中 “模式” &gt; “创建进程模式”</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/windows-netch-4.jpg" alt></p>
<p>点击 “扫描” 选取你游戏安装目录后选择确定即可添加，它将自动加载其中的exe文件，点击保存即可选择应用此模式</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/windows-netch-6.jpg" alt>                            </p>
<p>若需开启热点，给其他设备也翻墙，则需要Tap虚拟网卡的支持，自行查看自己电脑是否装有</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2020-02-16_12-58-05.jpg" alt></p>
<p>若未安装则去<a href="https://build.openvpn.net/downloads/releases/tap-windows-9.21.2.exe" target="_blank" rel="noopener">TAP-Windows</a>下载安装，Clash、OpenVPN等软件中也提供了安装</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2020-02-16_12-57-35.jpg" alt></p>
<p>确认Tap网卡ip为自动获取</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2020-02-16_13-21-22.jpg" alt></p>
<p>选择<code>Bypass LAN and China (TUN/TAP)</code>，点击启动</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2020-02-16_12-52-59.jpg" alt></p>
<p>启动热点，会发现在网络适配器中多出一个本地连接</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2020-02-16_13-26-48.jpg" alt></p>
<p>右键Tap网卡，属性—共享，选择Internet连接共享，添加热点所对应的本地连接</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/Snipaste_2020-02-16_13-28-32.jpg" alt></p>
<p>此时，热点即可成功翻墙</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/netch/TIM%E5%9B%BE%E7%89%8720200216133203.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3中的参数进化</title>
    <url>/2020/02/15/YOLOv3%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E8%BF%9B%E5%8C%96/</url>
    <content><![CDATA[<p>YOLOv3代码中也提供了参数进化(搜索)，可以为对应的数据集进化一套合适的超参数。本文建档分析一下有关这部分的操作方法以及其参数的具体进化方法。</p>
<h2 id="1-超参数"><a href="#1-超参数" class="headerlink" title=" 1. 超参数"></a><a id="more"></a> 1. 超参数</h2><p>YOLOv3中的超参数在train.py中提供，其中包含了一些数据增强参数设置，具体内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hyp &#x3D; &#123;&#39;giou&#39;: 3.54,  # giou loss gain</span><br><span class="line">       &#39;cls&#39;: 37.4,  # cls loss gain</span><br><span class="line">       &#39;cls_pw&#39;: 1.0,  # cls BCELoss positive_weight</span><br><span class="line">       &#39;obj&#39;: 49.5,  # obj loss gain (*&#x3D;img_size&#x2F;320 if img_size !&#x3D; 320)</span><br><span class="line">       &#39;obj_pw&#39;: 1.0,  # obj BCELoss positive_weight</span><br><span class="line">       &#39;iou_t&#39;: 0.225,  # iou training threshold</span><br><span class="line">       &#39;lr0&#39;: 0.00579,  # initial learning rate (SGD&#x3D;1E-3, Adam&#x3D;9E-5)</span><br><span class="line">       &#39;lrf&#39;: -4.,  # final LambdaLR learning rate &#x3D; lr0 * (10 ** lrf)</span><br><span class="line">       &#39;momentum&#39;: 0.937,  # SGD momentum</span><br><span class="line">       &#39;weight_decay&#39;: 0.000484,  # optimizer weight decay</span><br><span class="line">       &#39;fl_gamma&#39;: 0.5,  # focal loss gamma</span><br><span class="line">       &#39;hsv_h&#39;: 0.0138,  # image HSV-Hue augmentation (fraction)</span><br><span class="line">       &#39;hsv_s&#39;: 0.678,  # image HSV-Saturation augmentation (fraction)</span><br><span class="line">       &#39;hsv_v&#39;: 0.36,  # image HSV-Value augmentation (fraction)</span><br><span class="line">       &#39;degrees&#39;: 1.98,  # image rotation (+&#x2F;- deg)</span><br><span class="line">       &#39;translate&#39;: 0.05,  # image translation (+&#x2F;- fraction)</span><br><span class="line">       &#39;scale&#39;: 0.05,  # image scale (+&#x2F;- gain)</span><br><span class="line">       &#39;shear&#39;: 0.641&#125;  # image shear (+&#x2F;- deg)</span><br></pre></td></tr></table></figure>
<h2 id="2-使用方法"><a href="#2-使用方法" class="headerlink" title="2. 使用方法"></a>2. 使用方法</h2><p>在训练的时候，train.py提供了一个可选参数<code>--evolve</code>, 这个参数决定了是否进行超参数搜索与进化（默认是不开启超参数搜索的）。</p>
<p>具体使用方法也很简单：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python train.py --data data&#x2F;voc.data</span><br><span class="line">				--cfg cfg&#x2F;yolov3-tiny.cfg</span><br><span class="line">				--img-size 416 </span><br><span class="line">				--epochs 273 </span><br><span class="line">				--evolve</span><br></pre></td></tr></table></figure>
<p>实际使用的时候，需要进行修改，train.py中的约444行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for _ in range(1):  # generations to evolve</span><br></pre></td></tr></table></figure>
<p>将其中的1修改为你想设置的迭代数，比如200代，如果不设置，结果将会如下图所示，实际上就是只有一代。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/646.webp" alt></p>
<h2 id="3-原理"><a href="#3-原理" class="headerlink" title="3. 原理"></a>3. 原理</h2><p>整个过程比较简单，对于进化过程中的新一代，都选了了适应性最高的前一代（在前几代中）进行突变。以上所有的参数将有约20%的 1-sigma的正态分布几率同时突变。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; 0.2 # sigma</span><br></pre></td></tr></table></figure>
<p>整个进化过程需要搞清楚两个点：</p>
<ol>
<li>如何评判其中一代的好坏？</li>
<li>下一代如何根据上一代进行进化？</li>
</ol>
<p><strong>第一个问题：</strong>判断好坏的标准。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def fitness(x):</span><br><span class="line">    w &#x3D; [0.0, 0.0, 0.8, 0.2]</span><br><span class="line">    # weights for [P, R, mAP, F1]@0.5</span><br><span class="line">    return (x[:, :4] * w).sum(1)</span><br></pre></td></tr></table></figure>
<p>YOLOv3进化部分是通过以上的适应度函数判断的，适应度越高，代表这一代的性能越好。而在适应度中，是通过Precision,Recall ,mAP,F1这四个指标作为适应度的评价标准。</p>
<p>其中的w是设置的加权，如果更关心mAP的值，可以提高mAP的权重；如果更关心F1,则设置更高的权重在对应的F1上。这里分配mAP权重为0.8、F1权重为0.2。</p>
<p><strong>第二个问题：</strong>如何进行进化？</p>
<p>进化过程中有<strong>两个重要的参数</strong>:</p>
<p>第一个参数为<strong>parent</strong>, 可选值为<code>single</code>或者<code>weighted</code>，这个参数的作用是：决定如何选择上一代。如果选择single，代表只选择上一代中最好的那个。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if parent &#x3D;&#x3D; &#39;single&#39; or len(x) &#x3D;&#x3D; 1:</span><br><span class="line">	x &#x3D; x[fitness(x).argmax()]</span><br></pre></td></tr></table></figure>
<p>如果选择weighted，代表选择得分的前10个加权平均的结果作为下一代，具体操作如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">elif parent &#x3D;&#x3D; &#39;weighted&#39;:  # weighted combination</span><br><span class="line">    n &#x3D; min(10, len(x))  # number to merge</span><br><span class="line">    x &#x3D; x[np.argsort(-fitness(x))][:n]  # top n mutations</span><br><span class="line">    w &#x3D; fitness(x) - fitness(x).min()  # weights</span><br><span class="line">    x &#x3D; (x * w.reshape(n, 1)).sum(0) &#x2F; w.sum()  # new parent</span><br></pre></td></tr></table></figure>
<p>第二个参数为<strong>method</strong>，可选值为<code>1,2,3</code>, 分别代表使用三种模式来进化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Mutate</span><br><span class="line">method &#x3D; 2</span><br><span class="line">s &#x3D; 0.2  # 20% sigma</span><br><span class="line">np.random.seed(int(time.time()))</span><br><span class="line">g &#x3D; np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, \</span><br><span class="line">              1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains</span><br><span class="line"># 这里的g类似加权</span><br><span class="line">ng &#x3D; len(g)</span><br><span class="line">if method &#x3D;&#x3D; 1:</span><br><span class="line">    v &#x3D; (np.random.randn(ng) *</span><br><span class="line">         np.random.random() * g * s + 1) ** 2.0</span><br><span class="line">elif method &#x3D;&#x3D; 2:</span><br><span class="line">    v &#x3D; (np.random.randn(ng) *</span><br><span class="line">         np.random.random(ng) * g * s + 1) ** 2.0</span><br><span class="line">elif method &#x3D;&#x3D; 3:</span><br><span class="line">    v &#x3D; np.ones(ng)</span><br><span class="line">    while all(v &#x3D;&#x3D; 1):</span><br><span class="line">        # 为了防止重复，直到有变化才停下来</span><br><span class="line">         r &#x3D; (np.random.random(ng) &lt; 0.1) * np.random.randn(ng)</span><br><span class="line">         # 10% 的突变几率</span><br><span class="line">         v &#x3D; (g * s * r + 1) ** 2.0</span><br><span class="line"></span><br><span class="line">for i, k in enumerate(hyp.keys()):</span><br><span class="line">    hyp[k] &#x3D; x[i + 7] * v[i]</span><br><span class="line">    # 进行突变</span><br></pre></td></tr></table></figure>
<p>另外，为了防止突变过程，导致参数出现明显不合理的范围，需要用一个范围进行框定，将超出范围的内容剪切掉。具体方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Clip to limits</span><br><span class="line">keys &#x3D; [&#39;lr0&#39;, &#39;iou_t&#39;, &#39;momentum&#39;,</span><br><span class="line">        &#39;weight_decay&#39;, &#39;hsv_s&#39;,</span><br><span class="line">        &#39;hsv_v&#39;, &#39;translate&#39;,</span><br><span class="line">        &#39;scale&#39;, &#39;fl_gamma&#39;]</span><br><span class="line">limits &#x3D; [(1e-5, 1e-2), (0.00, 0.70),</span><br><span class="line">          (0.60, 0.98), (0, 0.001),</span><br><span class="line">          (0, .9), (0, .9), (0, .9),</span><br><span class="line">          (0, .9), (0, 3)]</span><br><span class="line"></span><br><span class="line">for k, v in zip(keys, limits):</span><br><span class="line">    hyp[k] &#x3D; np.clip(hyp[k], v[0], v[1])</span><br></pre></td></tr></table></figure>
<p>最终训练的超参数搜索的结果可视化：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/647.webp" alt></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3的数据加载机制和增强方法</title>
    <url>/2020/02/15/YOLOv3%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="1-标注格式"><a href="#1-标注格式" class="headerlink" title=" 1. 标注格式"></a><a id="more"></a> 1. 标注格式</h2><p><code>voc_label.py</code>，其作用是将xml文件转成txt文件格式，具体文件如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># class id, x, y, w, h</span><br><span class="line">0 0.8604166666666666 0.5403899721448469 0.058333333333333334 0.055710306406685235</span><br></pre></td></tr></table></figure>
<p>其中的x,y 的意义是归一化以后的框的中心坐标，w,h是归一化后的框的宽和高。</p>
<p>具体的归一化方式为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def convert(size, box):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    size是图片的长和宽</span><br><span class="line">    box是xmin,xmax,ymin,ymax坐标值</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    dw &#x3D; 1. &#x2F; (size[0])</span><br><span class="line">    dh &#x3D; 1. &#x2F; (size[1])</span><br><span class="line">    # 得到长和宽的缩放比</span><br><span class="line">    x &#x3D; (box[0] + box[1])&#x2F;2.0  </span><br><span class="line">    y &#x3D; (box[2] + box[3])&#x2F;2.0  </span><br><span class="line">    w &#x3D; box[1] - box[0]</span><br><span class="line">    h &#x3D; box[3] - box[2]</span><br><span class="line">    # 分别计算中心点坐标，框的宽和高</span><br><span class="line">    x &#x3D; x * dw</span><br><span class="line">    w &#x3D; w * dw</span><br><span class="line">    y &#x3D; y * dh</span><br><span class="line">    h &#x3D; h * dh</span><br><span class="line">    # 按照图片长和宽进行归一化</span><br><span class="line">    return (x,y,w,h)</span><br></pre></td></tr></table></figure>
<p>可以看出，归一化都是相对于图片的宽和高进行归一化的。</p>
<h2 id="2-调用"><a href="#2-调用" class="headerlink" title="2. 调用"></a>2. 调用</h2><p>下边是train.py文件中的有关数据的调用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Dataset</span><br><span class="line">dataset &#x3D; LoadImagesAndLabels(train_path, img_size, batch_size,</span><br><span class="line">                              augment&#x3D;True,</span><br><span class="line">                              hyp&#x3D;hyp,  # augmentation hyperparameters</span><br><span class="line">                              rect&#x3D;opt.rect,  # rectangular training</span><br><span class="line">                              cache_labels&#x3D;True,</span><br><span class="line">                              cache_images&#x3D;opt.cache_images)</span><br><span class="line"></span><br><span class="line">batch_size &#x3D; min(batch_size, len(dataset))</span><br><span class="line"></span><br><span class="line"># 使用多少个线程加载数据集</span><br><span class="line">nw &#x3D; min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 1])</span><br><span class="line"></span><br><span class="line">dataloader &#x3D; DataLoader(dataset,</span><br><span class="line">                        batch_size&#x3D;batch_size,</span><br><span class="line">                        num_workers&#x3D;nw,</span><br><span class="line">                        shuffle&#x3D;not opt.rect,</span><br><span class="line">                        # Shuffle&#x3D;True</span><br><span class="line">                        # unless rectangular training is used</span><br><span class="line">                        pin_memory&#x3D;True,</span><br><span class="line">                        collate_fn&#x3D;dataset.collate_fn)</span><br></pre></td></tr></table></figure>
<p>在pytorch中，数据集加载主要是重构datasets类，然后再使用dataloader中加载dataset，就构建好了数据部分。</p>
<p>下面是一个简单的使用模板：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line"># 根据自己的数据集格式进行重构</span><br><span class="line">class MyDataset(Dataset):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        #下载数据、初始化数据，都可以在这里完成</span><br><span class="line">        xy &#x3D; np.loadtxt(&#39;label.txt&#39;, delimiter&#x3D;&#39;,&#39;, dtype&#x3D;np.float32)</span><br><span class="line">        # 使用numpy读取数据</span><br><span class="line">        self.x_data &#x3D; torch.from_numpy(xy[:, 0:-1])</span><br><span class="line">        self.y_data &#x3D; torch.from_numpy(xy[:, [-1]])</span><br><span class="line">        self.len &#x3D; xy.shape[0]</span><br><span class="line">    </span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        # dataloader中使用该方法，通过index进行访问</span><br><span class="line">        return self.x_data[index], self.y_data[index]</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        # 查询数据集中数量，可以通过len(mydataset)得到</span><br><span class="line">        return self.len</span><br><span class="line"></span><br><span class="line"># 实例化这个类，然后我们就得到了Dataset类型的数据，记下来就将这个类传给DataLoader，就可以了。</span><br><span class="line">myDataset &#x3D; MyDataset()</span><br><span class="line"></span><br><span class="line"># 构建dataloader</span><br><span class="line">train_loader &#x3D; DataLoader(dataset&#x3D;myDataset,</span><br><span class="line">                          batch_size&#x3D;32,</span><br><span class="line">                          shuffle&#x3D;True)</span><br><span class="line"></span><br><span class="line">for epoch in range(2):</span><br><span class="line">    for i, data in enumerate(train_loader):</span><br><span class="line">        # 将数据从 train_loader 中读出来,一次读取的样本数是32个</span><br><span class="line">        inputs, labels &#x3D; data</span><br><span class="line">        # 将这些数据转换成Variable类型</span><br><span class="line">        inputs, labels &#x3D; Variable(inputs), Variable(labels)</span><br><span class="line">		# 模型训练...</span><br></pre></td></tr></table></figure>
<p>通过以上模板就能大致了解pytorch中的数据加载机制，下面开始介绍YOLOv3中的数据加载。</p>
<h2 id="3-YOLOv3中的数据加载"><a href="#3-YOLOv3中的数据加载" class="headerlink" title="3. YOLOv3中的数据加载"></a>3. YOLOv3中的数据加载</h2><p>下面解析的是LoadImagesAndLabels类中的几个主要的函数：</p>
<h3 id="3-1-init函数"><a href="#3-1-init函数" class="headerlink" title="3.1 init函数"></a>3.1 init函数</h3><p>init函数中包含了大部分需要处理的数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class LoadImagesAndLabels(Dataset):  # for training&#x2F;testing</span><br><span class="line">    def __init__(self,</span><br><span class="line">                 path,</span><br><span class="line">                 img_size&#x3D;416,</span><br><span class="line">                 batch_size&#x3D;16,</span><br><span class="line">                 augment&#x3D;False,</span><br><span class="line">                 hyp&#x3D;None,</span><br><span class="line">                 rect&#x3D;False,</span><br><span class="line">                 image_weights&#x3D;False,</span><br><span class="line">                 cache_labels&#x3D;False,</span><br><span class="line">                 cache_images&#x3D;False):</span><br><span class="line">        path &#x3D; str(Path(path))  # os-agnostic</span><br><span class="line">        assert os.path.isfile(path), &#39;File not found %s. See %s&#39; % (path,</span><br><span class="line">                                                                    help_url)</span><br><span class="line">        with open(path, &#39;r&#39;) as f:</span><br><span class="line">            self.img_files &#x3D; [</span><br><span class="line">                x.replace(&#39;&#x2F;&#39;, os.sep)</span><br><span class="line">                for x in f.read().splitlines()  # os-agnostic</span><br><span class="line">                if os.path.splitext(x)[-1].lower() in img_formats</span><br><span class="line">            ]</span><br><span class="line">        # img_files是一个list，保存的是图片的路径</span><br><span class="line"></span><br><span class="line">        n &#x3D; len(self.img_files)</span><br><span class="line">        assert n &gt; 0, &#39;No images found in %s. See %s&#39; % (path, help_url)</span><br><span class="line">        bi &#x3D; np.floor(np.arange(n) &#x2F; batch_size).astype(np.int)  # batch index</span><br><span class="line">        # 如果n&#x3D;10, batch&#x3D;2, bi&#x3D;[0,0,1,1,2,2,3,3,4,4]</span><br><span class="line">        nb &#x3D; bi[-1] + 1  # 最多有多少个batch</span><br><span class="line"></span><br><span class="line">        self.n &#x3D; n</span><br><span class="line">        self.batch &#x3D; bi  # 图片的batch索引，代表第几个batch的图片</span><br><span class="line">        self.img_size &#x3D; img_size</span><br><span class="line">        self.augment &#x3D; augment</span><br><span class="line">        self.hyp &#x3D; hyp</span><br><span class="line">        self.image_weights &#x3D; image_weights # 是否选择根据权重进行采样</span><br><span class="line">        self.rect &#x3D; False if image_weights else rect</span><br><span class="line">        # 如果选择根据权重进行采样，将无法使用矩形训练：</span><br><span class="line">        # 具体内容见下文</span><br><span class="line"></span><br><span class="line">        # 标签文件是通过images替换为labels, .jpg替换为.txt得到的。</span><br><span class="line">        self.label_files &#x3D; [</span><br><span class="line">            x.replace(&#39;images&#39;,</span><br><span class="line">                      &#39;labels&#39;).replace(os.path.splitext(x)[-1], &#39;.txt&#39;)</span><br><span class="line">            for x in self.img_files</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        # 矩形训练具体内容见下文解析</span><br><span class="line">        if self.rect:</span><br><span class="line">            # 获取图片的长和宽 (wh)</span><br><span class="line">            sp &#x3D; path.replace(&#39;.txt&#39;, &#39;.shapes&#39;)</span><br><span class="line">            # 字符串替换</span><br><span class="line">            # shapefile path</span><br><span class="line">            try:</span><br><span class="line">                with open(sp, &#39;r&#39;) as f:  # 读取shape文件</span><br><span class="line">                    s &#x3D; [x.split() for x in f.read().splitlines()]</span><br><span class="line">                    assert len(s) &#x3D;&#x3D; n, &#39;Shapefile out of sync&#39;</span><br><span class="line">            except:</span><br><span class="line">                s &#x3D; [</span><br><span class="line">                    exif_size(Image.open(f))</span><br><span class="line">                    for f in tqdm(self.img_files, desc&#x3D;&#39;Reading image shapes&#39;)</span><br><span class="line">                ]</span><br><span class="line">                np.savetxt(sp, s, fmt&#x3D;&#39;%g&#39;)  # overwrites existing (if any)</span><br><span class="line"></span><br><span class="line">            # 根据长宽比进行排序</span><br><span class="line">            s &#x3D; np.array(s, dtype&#x3D;np.float64)</span><br><span class="line">            ar &#x3D; s[:, 1] &#x2F; s[:, 0]  # aspect ratio</span><br><span class="line">            i &#x3D; ar.argsort()</span><br><span class="line"></span><br><span class="line">            # 根据顺序重排顺序</span><br><span class="line">            self.img_files &#x3D; [self.img_files[i] for i in i]</span><br><span class="line">            self.label_files &#x3D; [self.label_files[i] for i in i]</span><br><span class="line">            self.shapes &#x3D; s[i]  # wh</span><br><span class="line">            ar &#x3D; ar[i]</span><br><span class="line"></span><br><span class="line">            # 设置训练的图片形状</span><br><span class="line">            shapes &#x3D; [[1, 1]] * nb</span><br><span class="line">            for i in range(nb):</span><br><span class="line">                ari &#x3D; ar[bi &#x3D;&#x3D; i]</span><br><span class="line">                mini, maxi &#x3D; ari.min(), ari.max()</span><br><span class="line">                if maxi &lt; 1:</span><br><span class="line">                    shapes[i] &#x3D; [maxi, 1]</span><br><span class="line">                elif mini &gt; 1:</span><br><span class="line">                    shapes[i] &#x3D; [1, 1 &#x2F; mini]</span><br><span class="line"></span><br><span class="line">            self.batch_shapes &#x3D; np.ceil(</span><br><span class="line">                np.array(shapes) * img_size &#x2F; 32.).astype(np.int) * 32</span><br><span class="line"></span><br><span class="line">        # 预载标签</span><br><span class="line">        # weighted CE 训练时需要这个步骤</span><br><span class="line">        # 否则无法按照权重进行采样</span><br><span class="line">        self.imgs &#x3D; [None] * n</span><br><span class="line">        self.labels &#x3D; [None] * n</span><br><span class="line">        if cache_labels or image_weights:  # cache labels for faster training</span><br><span class="line">            self.labels &#x3D; [np.zeros((0, 5))] * n</span><br><span class="line">            extract_bounding_boxes &#x3D; False</span><br><span class="line">            create_datasubset &#x3D; False</span><br><span class="line">            pbar &#x3D; tqdm(self.label_files, desc&#x3D;&#39;Caching labels&#39;)</span><br><span class="line">            nm, nf, ne, ns, nd &#x3D; 0, 0, 0, 0, 0  # number missing, found, empty, datasubset, duplicate</span><br><span class="line">            for i, file in enumerate(pbar):</span><br><span class="line">                try:</span><br><span class="line">                    # 读取每个文件内容</span><br><span class="line">                    with open(file, &#39;r&#39;) as f:</span><br><span class="line">                        l &#x3D; np.array(</span><br><span class="line">                            [x.split() for x in f.read().splitlines()],</span><br><span class="line">                            dtype&#x3D;np.float32)</span><br><span class="line">                except:</span><br><span class="line">                    nm +&#x3D; 1  # print(&#39;missing labels for image %s&#39; % self.img_files[i])  # file missing</span><br><span class="line">                    continue</span><br><span class="line"></span><br><span class="line">                if l.shape[0]:</span><br><span class="line">                    # 判断文件内容是否符合要求</span><br><span class="line">                    # 所有的值需要&gt;0, &lt;1, 一共5列</span><br><span class="line">                    assert l.shape[1] &#x3D;&#x3D; 5, &#39;&gt; 5 label columns: %s&#39; % file</span><br><span class="line">                    assert (l &gt;&#x3D; 0).all(), &#39;negative labels: %s&#39; % file</span><br><span class="line">                    assert (l[:, 1:] &lt;&#x3D; 1).all(</span><br><span class="line">                    ), &#39;non-normalized or out of bounds coordinate labels: %s&#39; % file</span><br><span class="line">                    if np.unique(</span><br><span class="line">                            l, axis&#x3D;0).shape[0] &lt; l.shape[0]:  # duplicate rows</span><br><span class="line">                        nd +&#x3D; 1  # print(&#39;WARNING: duplicate rows in %s&#39; % self.label_files[i])  # duplicate rows</span><br><span class="line"></span><br><span class="line">                    self.labels[i] &#x3D; l</span><br><span class="line">                    nf +&#x3D; 1  # file found</span><br><span class="line"></span><br><span class="line">                    # 创建一个小型的数据集进行试验</span><br><span class="line">                    if create_datasubset and ns &lt; 1E4:</span><br><span class="line">                        if ns &#x3D;&#x3D; 0:</span><br><span class="line">                            create_folder(path&#x3D;&#39;.&#x2F;datasubset&#39;)</span><br><span class="line">                            os.makedirs(&#39;.&#x2F;datasubset&#x2F;images&#39;)</span><br><span class="line">                        exclude_classes &#x3D; 43</span><br><span class="line">                        if exclude_classes not in l[:, 0]:</span><br><span class="line">                            ns +&#x3D; 1</span><br><span class="line">                            # shutil.copy(src&#x3D;self.img_files[i], dst&#x3D;&#39;.&#x2F;datasubset&#x2F;images&#x2F;&#39;)  # copy image</span><br><span class="line">                            with open(&#39;.&#x2F;datasubset&#x2F;images.txt&#39;, &#39;a&#39;) as f:</span><br><span class="line">                                f.write(self.img_files[i] + &#39;\n&#39;)</span><br><span class="line"></span><br><span class="line">                    # 为两阶段分类器提取目标检测的检测框</span><br><span class="line">                    # 默认开关是关掉的，不是很理解</span><br><span class="line">                    if extract_bounding_boxes:</span><br><span class="line">                        p &#x3D; Path(self.img_files[i])</span><br><span class="line">                        img &#x3D; cv2.imread(str(p))</span><br><span class="line">                        h, w &#x3D; img.shape[:2]</span><br><span class="line">                        for j, x in enumerate(l):</span><br><span class="line">                            f &#x3D; &#39;%s%sclassifier%s%g_%g_%s&#39; % (p.parent.parent,</span><br><span class="line">                                                              os.sep, os.sep,</span><br><span class="line">                                                              x[0], j, p.name)</span><br><span class="line">                            if not os.path.exists(Path(f).parent):</span><br><span class="line">                                os.makedirs(Path(f).parent)</span><br><span class="line">                                # make new output folder</span><br><span class="line"></span><br><span class="line">                            b &#x3D; x[1:] * np.array([w, h, w, h])  # box</span><br><span class="line">                            b[2:] &#x3D; b[2:].max()  # rectangle to square</span><br><span class="line">                            b[2:] &#x3D; b[2:] * 1.3 + 30  # pad</span><br><span class="line"></span><br><span class="line">                            b &#x3D; xywh2xyxy(b.reshape(-1,4)).ravel().astype(np.int)</span><br><span class="line"></span><br><span class="line">                            b[[0,2]] &#x3D; np.clip(b[[0, 2]], 0,w)  # clip boxes outside of image</span><br><span class="line">                            b[[1, 3]] &#x3D; np.clip(b[[1, 3]], 0, h)</span><br><span class="line">                            assert cv2.imwrite(f, img[b[1]:b[3], b[0]:b[2]]), &#39;Failure extracting classifier boxes&#39;</span><br><span class="line">                else:</span><br><span class="line">                    ne +&#x3D; 1</span><br><span class="line"></span><br><span class="line">                pbar.desc &#x3D; &#39;Caching labels (%g found, %g missing, %g empty, %g duplicate, for %g images)&#39; </span><br><span class="line">                % (nf, nm, ne, nd, n) # 统计发现，丢失，空，重复标签的数量。</span><br><span class="line">            assert nf &gt; 0, &#39;No labels found. See %s&#39; % help_url</span><br><span class="line"></span><br><span class="line">        # 将图片加载到内存中，可以加速训练</span><br><span class="line">        # 警告：如果在数据比较多的情况下可能会超出RAM</span><br><span class="line">        if cache_images:  # if training</span><br><span class="line">            gb &#x3D; 0  # 计算缓存到内存中的图片占用的空间GB为单位</span><br><span class="line">            pbar &#x3D; tqdm(range(len(self.img_files)), desc&#x3D;&#39;Caching images&#39;)</span><br><span class="line">            self.img_hw0, self.img_hw &#x3D; [None] * n, [None] * n</span><br><span class="line">            for i in pbar:  # max 10k images</span><br><span class="line">                self.imgs[i], self.img_hw0[i], self.img_hw[i] &#x3D; load_image(</span><br><span class="line">                    self, i)  # img, hw_original, hw_resized</span><br><span class="line">                gb +&#x3D; self.imgs[i].nbytes</span><br><span class="line">                pbar.desc &#x3D; &#39;Caching images (%.1fGB)&#39; % (gb &#x2F; 1E9)</span><br><span class="line"></span><br><span class="line">        # 删除损坏的文件</span><br><span class="line">        # 根据需要进行手动开关</span><br><span class="line">        detect_corrupted_images &#x3D; False</span><br><span class="line">        if detect_corrupted_images:</span><br><span class="line">            from skimage import io  # conda install -c conda-forge scikit-image</span><br><span class="line">            for file in tqdm(self.img_files,</span><br><span class="line">                             desc&#x3D;&#39;Detecting corrupted images&#39;):</span><br><span class="line">                try:</span><br><span class="line">                    _ &#x3D; io.imread(file)</span><br><span class="line">                except:</span><br><span class="line">                    print(&#39;Corrupted image detected: %s&#39; % file)</span><br></pre></td></tr></table></figure>
<p><strong>Rectangular inference（矩形推理）</strong></p>
<ol>
<li>矩形推理是在detect.py，也就是测试过程中的实现，可以减少推理时间。YOLOv3中是下采样32倍，长宽也必须是32的倍数，所以在进入模型前，数据需要处理到416×416大小，这个过程称为仿射变换，如果用opencv实现可以用以下代码：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 来自 https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;93822508</span><br><span class="line">def cv2_letterbox_image(image, expected_size):</span><br><span class="line">    ih, iw &#x3D; image.shape[0:2]</span><br><span class="line">    ew, eh &#x3D; expected_size</span><br><span class="line">    scale &#x3D; min(eh &#x2F; ih, ew &#x2F; iw)</span><br><span class="line">    nh &#x3D; int(ih * scale)</span><br><span class="line">    nw &#x3D; int(iw * scale)</span><br><span class="line">    image &#x3D; cv2.resize(image, (nw, nh), interpolation&#x3D;cv2.INTER_CUBIC)</span><br><span class="line">    top &#x3D; (eh - nh) &#x2F;&#x2F; 2</span><br><span class="line">    bottom &#x3D; eh - nh - top</span><br><span class="line">    left &#x3D; (ew - nw) &#x2F;&#x2F; 2</span><br><span class="line">    right &#x3D; ew - nw - left</span><br><span class="line">    new_img &#x3D; cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)</span><br><span class="line">    return new_img</span><br></pre></td></tr></table></figure>
<p>比如下图是一个h&gt;w，一个是w&gt;h的图片经过仿射变换后resize到416×416的示例：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/643.webp" alt></p>
<p>以上就是正方形推理，但是可以看出以上通过补充得到的结果会存在很多冗余信息，而Rectangular Training思路就是想要去掉这些冗余的部分。</p>
<p>具体过程为：求得较长边缩放到416的比例，然后对图片w:h按这个比例缩放，使得较长边达到416,再对较短边进行尽量少的填充使得较短边满足32的倍数。</p>
<p>示例如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/644.webp" alt></p>
<p><strong>Rectangular Training（矩形训练）</strong></p>
<p>很自然的，训练的过程也可以用到这个想法，减少冗余。不过训练的时候情况比较复杂，由于在训练过程中是一个batch的图片，而每个batch图片是有可能长宽比不同的，这就是与测试最大的区别。具体是实现是取这个batch中最大的场合宽，然后将整个batch中填充到max width和max  height,这样操作对小一些的图片来说也是比较浪费。这里的yolov3的实现主要就是优化了一下如何将比例相近的图片放在一个batch，这样显然填充的就更少一些了。作者在issue中提到，在coco数据集中使用这个策略进行训练，能够快1/3。</p>
<p>而如果选择开启矩形训练，必须要关闭dataloader中的shuffle参数，防止对数据的顺序进行调整。同时如果选择image_weights, 根据图片进行采样，也无法与矩阵训练同时使用。</p>
<h3 id="3-2-getitem函数"><a href="#3-2-getitem函数" class="headerlink" title="3.2 getitem函数"></a>3.2 getitem函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def __getitem__(self, index):</span><br><span class="line">    # 新的下角标</span><br><span class="line">    if self.image_weights:</span><br><span class="line">        index &#x3D; self.indices[index]</span><br><span class="line"></span><br><span class="line">    img_path &#x3D; self.img_files[index]</span><br><span class="line">    label_path &#x3D; self.label_files[index]</span><br><span class="line"></span><br><span class="line">    hyp &#x3D; self.hyp</span><br><span class="line">    mosaic &#x3D; True and self.augment</span><br><span class="line">    # 如果开启组合变化、数据增强</span><br><span class="line">    # 加载四张图片，作为一个拼图，具体看下文解析。</span><br><span class="line">    if mosaic:</span><br><span class="line">        # 加载拼图内容</span><br><span class="line">        img, labels &#x3D; load_mosaic(self, index)</span><br><span class="line">        shapes &#x3D; None</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line">        # 加载图片</span><br><span class="line">        img, (h0, w0), (h, w) &#x3D; load_image(self, index)</span><br><span class="line"></span><br><span class="line">        # 仿射变换</span><br><span class="line">        shape &#x3D; self.batch_shapes[self.batch[</span><br><span class="line">            index]] if self.rect else self.img_size</span><br><span class="line">        img, ratio, pad &#x3D; letterbox(img,</span><br><span class="line">                                    shape,</span><br><span class="line">                                    auto&#x3D;False,</span><br><span class="line">                                    scaleup&#x3D;self.augment)</span><br><span class="line">        shapes &#x3D; (h0, w0), (</span><br><span class="line">            (h &#x2F; h0, w &#x2F; w0), pad)</span><br><span class="line"></span><br><span class="line">        # 加载标注文件</span><br><span class="line">        labels &#x3D; []</span><br><span class="line">        if os.path.isfile(label_path):</span><br><span class="line">            x &#x3D; self.labels[index]</span><br><span class="line">            if x is None:  # 如果标签没有加载，读取label_path内容</span><br><span class="line">                with open(label_path, &#39;r&#39;) as f:</span><br><span class="line">                    x &#x3D; np.array(</span><br><span class="line">                        [x.split() for x in f.read().splitlines()],</span><br><span class="line">                        dtype&#x3D;np.float32)</span><br><span class="line"></span><br><span class="line">            if x.size &gt; 0:</span><br><span class="line">                # 将归一化后的xywh转化为左上角、右下角的表达形式</span><br><span class="line">                labels &#x3D; x.copy()</span><br><span class="line">                labels[:, 1] &#x3D; ratio[0] * w * (</span><br><span class="line">                    x[:, 1] - x[:, 3] &#x2F; 2) + pad[0]  # pad width</span><br><span class="line">                labels[:, 2] &#x3D; ratio[1] * h * (</span><br><span class="line">                    x[:, 2] - x[:, 4] &#x2F; 2) + pad[1]  # pad height</span><br><span class="line">                labels[:, 3] &#x3D; ratio[0] * w * (x[:, 1] +</span><br><span class="line">                                               x[:, 3] &#x2F; 2) + pad[0]</span><br><span class="line">                labels[:, 4] &#x3D; ratio[1] * h * (x[:, 2] +</span><br><span class="line">                                               x[:, 4] &#x2F; 2) + pad[1]</span><br><span class="line"></span><br><span class="line">    if self.augment:</span><br><span class="line">        # 图片空间的数据增强</span><br><span class="line">        if not mosaic:</span><br><span class="line">            # 如果没有使用组合的方法，那么对图片进行随机放射</span><br><span class="line">            img, labels &#x3D; random_affine(img,</span><br><span class="line">                                        labels,</span><br><span class="line">                                        degrees&#x3D;hyp[&#39;degrees&#39;],</span><br><span class="line">                                        translate&#x3D;hyp[&#39;translate&#39;],</span><br><span class="line">                                        scale&#x3D;hyp[&#39;scale&#39;],</span><br><span class="line">                                        shear&#x3D;hyp[&#39;shear&#39;])</span><br><span class="line"></span><br><span class="line">        # 增强hsv空间</span><br><span class="line">        augment_hsv(img,</span><br><span class="line">                    hgain&#x3D;hyp[&#39;hsv_h&#39;],</span><br><span class="line">                    sgain&#x3D;hyp[&#39;hsv_s&#39;],</span><br><span class="line">                    vgain&#x3D;hyp[&#39;hsv_v&#39;])</span><br><span class="line"></span><br><span class="line">    nL &#x3D; len(labels)  # 标注文件个数</span><br><span class="line"></span><br><span class="line">    if nL:</span><br><span class="line">        # 将 xyxy 格式转化为 xywh 格式</span><br><span class="line">        labels[:, 1:5] &#x3D; xyxy2xywh(labels[:, 1:5])</span><br><span class="line"></span><br><span class="line">        # 归一化到0-1之间</span><br><span class="line">        labels[:, [2, 4]] &#x2F;&#x3D; img.shape[0]  # height</span><br><span class="line">        labels[:, [1, 3]] &#x2F;&#x3D; img.shape[1]  # width</span><br><span class="line"></span><br><span class="line">    if self.augment:</span><br><span class="line">        # 随机左右翻转</span><br><span class="line">        lr_flip &#x3D; True</span><br><span class="line">        if lr_flip and random.random() &lt; 0.5:</span><br><span class="line">            img &#x3D; np.fliplr(img)</span><br><span class="line">            if nL:</span><br><span class="line">                labels[:, 1] &#x3D; 1 - labels[:, 1]</span><br><span class="line"></span><br><span class="line">        # 随机上下翻转</span><br><span class="line">        ud_flip &#x3D; False</span><br><span class="line">        if ud_flip and random.random() &lt; 0.5:</span><br><span class="line">            img &#x3D; np.flipud(img)</span><br><span class="line">            if nL:</span><br><span class="line">                labels[:, 2] &#x3D; 1 - labels[:, 2]</span><br><span class="line"></span><br><span class="line">    labels_out &#x3D; torch.zeros((nL, 6))</span><br><span class="line">    if nL:</span><br><span class="line">        labels_out[:, 1:] &#x3D; torch.from_numpy(labels)</span><br><span class="line"></span><br><span class="line">    # 图像维度转换</span><br><span class="line">    img &#x3D; img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416</span><br><span class="line">    img &#x3D; np.ascontiguousarray(img)</span><br><span class="line"></span><br><span class="line">    return torch.from_numpy(img), labels_out, img_path, shapes</span><br></pre></td></tr></table></figure>
<p>下图是开启了组合和旋转以后的增强效果</p>
<p>这里理解组合就是将四张图片，以不同的比例，合成为一张图片。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/645.webp" alt></p>
<h3 id="3-3-collate-fn函数"><a href="#3-3-collate-fn函数" class="headerlink" title="3.3 collate_fn函数"></a>3.3 collate_fn函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@staticmethod</span><br><span class="line">def collate_fn(batch):</span><br><span class="line">    img, label, path, shapes &#x3D; zip(*batch)  # transposed</span><br><span class="line">    for i, l in enumerate(label):</span><br><span class="line">        l[:, 0] &#x3D; i  # add target image index for build_targets()</span><br><span class="line">    return torch.stack(img, 0), torch.cat(label, 0), path, shapes</span><br></pre></td></tr></table></figure>
<p>还有最后一点内容，是关于pytorch的数据读取机制，在pytorch的dataloader中是会对通过getitem方法得到的结果（batch）进行包装，而这个包装可能与我们想要的有所不同。默认的方法可以看以下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def default_collate(batch):</span><br><span class="line">    r&quot;&quot;&quot;Puts each data field into a tensor with outer dimension batch size&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    elem_type &#x3D; type(batch[0])</span><br><span class="line">    if isinstance(batch[0], torch.Tensor):</span><br><span class="line">        out &#x3D; None</span><br><span class="line">        if _use_shared_memory:</span><br><span class="line">            # If we&#39;re in a background process, concatenate directly into a</span><br><span class="line">            # shared memory tensor to avoid an extra copy</span><br><span class="line">            numel &#x3D; sum([x.numel() for x in batch])</span><br><span class="line">            storage &#x3D; batch[0].storage()._new_shared(numel)</span><br><span class="line">            out &#x3D; batch[0].new(storage)</span><br><span class="line">        return torch.stack(batch, 0, out&#x3D;out)</span><br><span class="line">    elif elem_type.__module__ &#x3D;&#x3D; &#39;numpy&#39; and elem_type.__name__ !&#x3D; &#39;str_&#39; \</span><br><span class="line">            and elem_type.__name__ !&#x3D; &#39;string_&#39;:</span><br><span class="line">        elem &#x3D; batch[0]</span><br><span class="line">        if elem_type.__name__ &#x3D;&#x3D; &#39;ndarray&#39;:</span><br><span class="line">            # array of string classes and object</span><br><span class="line">            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:</span><br><span class="line">                raise TypeError(error_msg_fmt.format(elem.dtype))</span><br><span class="line"></span><br><span class="line">            return default_collate([torch.from_numpy(b) for b in batch])</span><br><span class="line">        if elem.shape &#x3D;&#x3D; ():  # scalars</span><br><span class="line">            py_type &#x3D; float if elem.dtype.name.startswith(&#39;float&#39;) else int</span><br><span class="line">            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))</span><br><span class="line">    elif isinstance(batch[0], float):</span><br><span class="line">        return torch.tensor(batch, dtype&#x3D;torch.float64)</span><br><span class="line">    elif isinstance(batch[0], int_classes):</span><br><span class="line">        return torch.tensor(batch)</span><br><span class="line">    elif isinstance(batch[0], string_classes):</span><br><span class="line">        return batch</span><br><span class="line">    elif isinstance(batch[0], container_abcs.Mapping):</span><br><span class="line">        return &#123;key: default_collate([d[key] for d in batch]) for key in batch[0]&#125;</span><br><span class="line">    elif isinstance(batch[0], tuple) and hasattr(batch[0], &#39;_fields&#39;):  # namedtuple</span><br><span class="line">        return type(batch[0])(*(default_collate(samples) for samples in zip(*batch)))</span><br><span class="line">    elif isinstance(batch[0], container_abcs.Sequence):</span><br><span class="line">        transposed &#x3D; zip(*batch)</span><br><span class="line">        return [default_collate(samples) for samples in transposed]</span><br><span class="line"></span><br><span class="line">    raise TypeError((error_msg_fmt.format(type(batch[0]))))</span><br></pre></td></tr></table></figure>
<p>会根据你的数据类型进行相应的处理，但是这往往不是我们需要的，所以需要修改<code>collate_fn</code>,具体内容请看代码，比较简单，就不多赘述。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch版YOLOv3中的代码配置和数据集构建</title>
    <url>/2020/02/15/Pytorch%E7%89%88YOLOv3%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E9%85%8D%E7%BD%AE%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA/</url>
    <content><![CDATA[<h3 id="1-环境搭建"><a href="#1-环境搭建" class="headerlink" title=" 1. 环境搭建"></a><a id="more"></a> 1. 环境搭建</h3><ol>
<li>将github库download下来。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov3.git</span><br></pre></td></tr></table></figure>
<ol>
<li>建议在linux环境下使用anaconda进行搭建</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n yolov3 python&#x3D;3.7</span><br></pre></td></tr></table></figure>
<ol>
<li>安装需要的软件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>环境要求：</p>
<ul>
<li>python &gt;= 3.7</li>
<li>pytorch &gt;= 1.1</li>
<li>numpy</li>
<li>tqdm</li>
<li>opencv-python</li>
</ul>
<p>其中只需要注意pytorch的安装：</p>
<p>到<a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a>中根据操作系统，python版本，cuda版本等选择命令即可。</p>
<h3 id="2-数据集构建"><a href="#2-数据集构建" class="headerlink" title="2. 数据集构建"></a>2. 数据集构建</h3><h4 id="1-xml文件生成需要Labelimg软件"><a href="#1-xml文件生成需要Labelimg软件" class="headerlink" title="1. xml文件生成需要Labelimg软件"></a>1. xml文件生成需要Labelimg软件</h4><p>在Windows下使用LabelImg软件进行标注：</p>
<ul>
<li>使用快捷键：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Ctrl + u  加载目录中的所有图像，鼠标点击Open dir同功能</span><br><span class="line">Ctrl + r  更改默认注释目标目录(xml文件保存的地址)</span><br><span class="line">Ctrl + s  保存</span><br><span class="line">Ctrl + d  复制当前标签和矩形框</span><br><span class="line">space     将当前图像标记为已验证</span><br><span class="line">w         创建一个矩形框</span><br><span class="line">d         下一张图片</span><br><span class="line">a         上一张图片</span><br><span class="line">del       删除选定的矩形框</span><br><span class="line">Ctrl++    放大</span><br><span class="line">Ctrl--    缩小</span><br><span class="line">↑→↓←        键盘箭头移动选定的矩形框</span><br></pre></td></tr></table></figure>
<h4 id="2-VOC2007-数据集格式"><a href="#2-VOC2007-数据集格式" class="headerlink" title="2. VOC2007 数据集格式"></a>2. VOC2007 数据集格式</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-data</span><br><span class="line">    - VOCdevkit2007</span><br><span class="line">        - VOC2007</span><br><span class="line">            - Annotations (标签XML文件，用对应的图片处理工具人工生成的)</span><br><span class="line">            - ImageSets (生成的方法是用sh或者MATLAB语言生成)</span><br><span class="line">                - Main</span><br><span class="line">                    - test.txt</span><br><span class="line">                    - train.txt</span><br><span class="line">                    - trainval.txt</span><br><span class="line">                    - val.txt</span><br><span class="line">            - JPEGImages(原始文件)</span><br><span class="line">            - labels (xml文件对应的txt文件)</span><br></pre></td></tr></table></figure>
<p>通过以上软件主要构造好JPEGImages和Annotations文件夹中内容,Main文件夹中的txt文件可以通过以下python脚本生成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import random</span><br><span class="line">  </span><br><span class="line">trainval_percent &#x3D; 0.9</span><br><span class="line">train_percent &#x3D; 1</span><br><span class="line">xmlfilepath &#x3D; &#39;Annotations&#39;</span><br><span class="line">txtsavepath &#x3D; &#39;ImageSets\Main&#39;</span><br><span class="line">total_xml &#x3D; os.listdir(xmlfilepath)</span><br><span class="line">  </span><br><span class="line">num&#x3D;len(total_xml)</span><br><span class="line">list&#x3D;range(num)</span><br><span class="line">tv&#x3D;int(num*trainval_percent)</span><br><span class="line">tr&#x3D;int(tv*train_percent)</span><br><span class="line">trainval&#x3D; random.sample(list,tv)</span><br><span class="line">train&#x3D;random.sample(trainval,tr)</span><br><span class="line">  </span><br><span class="line">ftrainval &#x3D; open(&#39;ImageSets&#x2F;Main&#x2F;trainval.txt&#39;, &#39;w&#39;)</span><br><span class="line">ftest &#x3D; open(&#39;ImageSets&#x2F;Main&#x2F;test.txt&#39;, &#39;w&#39;)</span><br><span class="line">ftrain &#x3D; open(&#39;ImageSets&#x2F;Main&#x2F;train.txt&#39;, &#39;w&#39;)</span><br><span class="line">fval &#x3D; open(&#39;ImageSets&#x2F;Main&#x2F;val.txt&#39;, &#39;w&#39;)</span><br><span class="line">  </span><br><span class="line">for i  in list:</span><br><span class="line">    name&#x3D;total_xml[i][:-4]+&#39;\n&#39;</span><br><span class="line">    if i in trainval:</span><br><span class="line">        ftrainval.write(name)</span><br><span class="line">        if i in train:</span><br><span class="line">            ftrain.write(name)</span><br><span class="line">        else:</span><br><span class="line">            fval.write(name)</span><br><span class="line">    else:</span><br><span class="line">        ftest.write(name)</span><br><span class="line">  </span><br><span class="line">ftrainval.close()</span><br><span class="line">ftrain.close()</span><br><span class="line">fval.close()</span><br><span class="line">ftest.close()</span><br></pre></td></tr></table></figure>
<p>接下来生成labels文件夹中的txt文件，voc_label.py文件具体内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on Tue Oct  2 11:42:13 2018</span><br><span class="line">将本文件放到VOC2007目录下，然后就可以直接运行</span><br><span class="line">需要修改的地方：</span><br><span class="line">1. sets中替换为自己的数据集</span><br><span class="line">2. classes中替换为自己的类别</span><br><span class="line">3. 将本文件放到VOC2007目录下</span><br><span class="line">4. 直接开始运行</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import xml.etree.ElementTree as ET</span><br><span class="line">import pickle</span><br><span class="line">import os</span><br><span class="line">from os import listdir, getcwd</span><br><span class="line">from os.path import join</span><br><span class="line">sets&#x3D;[(&#39;2007&#39;, &#39;train&#39;), (&#39;2007&#39;, &#39;val&#39;), (&#39;2007&#39;, &#39;test&#39;)]  #替换为自己的数据集</span><br><span class="line">classes &#x3D; [&quot;person&quot;]     #修改为自己的类别</span><br><span class="line"></span><br><span class="line">#进行归一化</span><br><span class="line">def convert(size, box):</span><br><span class="line">    dw &#x3D; 1.&#x2F;(size[0])</span><br><span class="line">    dh &#x3D; 1.&#x2F;(size[1])</span><br><span class="line">    x &#x3D; (box[0] + box[1])&#x2F;2.0 - 1</span><br><span class="line">    y &#x3D; (box[2] + box[3])&#x2F;2.0 - 1</span><br><span class="line">    w &#x3D; box[1] - box[0]</span><br><span class="line">    h &#x3D; box[3] - box[2]</span><br><span class="line">    x &#x3D; x*dw</span><br><span class="line">    w &#x3D; w*dw</span><br><span class="line">    y &#x3D; y*dh</span><br><span class="line">    h &#x3D; h*dh</span><br><span class="line">    return (x,y,w,h)</span><br><span class="line"></span><br><span class="line">def convert_annotation(year, image_id):</span><br><span class="line">    in_file &#x3D; open(&#39;VOC%s&#x2F;Annotations&#x2F;%s.xml&#39;%(year, image_id))  #将数据集放于当前目录下</span><br><span class="line">    out_file &#x3D; open(&#39;VOC%s&#x2F;labels&#x2F;%s.txt&#39;%(year, image_id), &#39;w&#39;)</span><br><span class="line">    tree&#x3D;ET.parse(in_file)</span><br><span class="line">    root &#x3D; tree.getroot()</span><br><span class="line">    size &#x3D; root.find(&#39;size&#39;)</span><br><span class="line">    w &#x3D; int(size.find(&#39;width&#39;).text)</span><br><span class="line">    h &#x3D; int(size.find(&#39;height&#39;).text)</span><br><span class="line">    for obj in root.iter(&#39;object&#39;):</span><br><span class="line">        difficult &#x3D; obj.find(&#39;difficult&#39;).text</span><br><span class="line">        cls &#x3D; obj.find(&#39;name&#39;).text</span><br><span class="line">        if cls not in classes or int(difficult)&#x3D;&#x3D;1:</span><br><span class="line">            continue</span><br><span class="line">        cls_id &#x3D; classes.index(cls)</span><br><span class="line">        xmlbox &#x3D; obj.find(&#39;bndbox&#39;)</span><br><span class="line">        b &#x3D; (float(xmlbox.find(&#39;xmin&#39;).text), float(xmlbox.find(&#39;xmax&#39;).text), float(xmlbox.find(&#39;ymin&#39;).text), float(xmlbox.find(&#39;ymax&#39;).text))</span><br><span class="line">        bb &#x3D; convert((w,h), b)</span><br><span class="line">        out_file.write(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]) + &#39;\n&#39;)</span><br><span class="line">wd &#x3D; getcwd()</span><br><span class="line">for year, image_set in sets:</span><br><span class="line">    if not os.path.exists(&#39;VOC%s&#x2F;labels&#x2F;&#39;%(year)):</span><br><span class="line">        os.makedirs(&#39;VOC%s&#x2F;labels&#x2F;&#39;%(year))</span><br><span class="line">    image_ids &#x3D; open(&#39;VOC%s&#x2F;ImageSets&#x2F;Main&#x2F;%s.txt&#39;%(year, image_set)).read().strip().split()</span><br><span class="line">    list_file &#x3D; open(&#39;%s_%s.txt&#39;%(year, image_set), &#39;w&#39;)</span><br><span class="line">    for image_id in image_ids:</span><br><span class="line">        list_file.write(&#39;VOC%s&#x2F;JPEGImages&#x2F;%s.jpg\n&#39;%(year, image_id))</span><br><span class="line">        convert_annotation(year, image_id)</span><br><span class="line">    list_file.close()</span><br></pre></td></tr></table></figure>
<p>到底为止，VOC格式数据集构造完毕，但是还需要继续构造符合darknet格式的数据集(coco)。</p>
<p>需要说明的是：如果打算使用coco评价标准，需要构造coco中json格式，如果要求不高，只需要VOC格式即可，使用作者写的mAP计算程序即可。</p>
<h4 id="3-创建-names-file"><a href="#3-创建-names-file" class="headerlink" title="3. 创建*.names file,"></a>3. 创建*.names file,</h4><p>其中保存的是你的所有的类别，每行一个类别，如data/coco.names：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">person</span><br></pre></td></tr></table></figure>
<h4 id="4-更新data-coco-data-其中保存的是很多配置信息"><a href="#4-更新data-coco-data-其中保存的是很多配置信息" class="headerlink" title="4. 更新data/coco.data,其中保存的是很多配置信息"></a>4. 更新data/coco.data,其中保存的是很多配置信息</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classes &#x3D; 1 # 改成你的数据集的类别个数</span><br><span class="line">train &#x3D; .&#x2F;data&#x2F;2007_train.txt # 通过voc_label.py文件生成的txt文件</span><br><span class="line">valid &#x3D; .&#x2F;data&#x2F;2007_test.txt # 通过voc_label.py文件生成的txt文件</span><br><span class="line">names &#x3D; data&#x2F;coco.names # 记录类别</span><br><span class="line">backup &#x3D; backup&#x2F; # 在本库中没有用到</span><br><span class="line">eval &#x3D; coco # 选择map计算方式</span><br></pre></td></tr></table></figure>
<h4 id="5-更新cfg文件，修改类别相关信息"><a href="#5-更新cfg文件，修改类别相关信息" class="headerlink" title="5. 更新cfg文件，修改类别相关信息"></a>5. 更新cfg文件，修改类别相关信息</h4><p>打开cfg文件夹下的yolov3.cfg文件，大体而言，cfg文件记录的是整个网络的结构，是核心部分。</p>
<p>只需要更改每个[yolo]层前边卷积层的filter个数即可：</p>
<blockquote>
<p>每一个[region/yolo]层前的最后一个卷积层中的 filters=预测框的个数(mask对应的个数，比如mask=0,1,2,  代表使用了anchors中的前三对，这里预测框个数就应该是3*(classes+5)  ,5的意义是5个坐标（论文中的tx,ty,tw,th,po），3的意义就是用了3个anchor。</p>
</blockquote>
<p>举个例子：假如我有三个类，n = 3, 那么filter  = 3 × (n+5) = 24</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;255 # 改为 24</span><br><span class="line">activation&#x3D;linear</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[yolo]</span><br><span class="line">mask &#x3D; 6,7,8</span><br><span class="line">anchors &#x3D; 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326</span><br><span class="line">classes&#x3D;80 # 改为 3</span><br><span class="line">num&#x3D;9</span><br><span class="line">jitter&#x3D;.3</span><br><span class="line">ignore_thresh &#x3D; .7</span><br><span class="line">truth_thresh &#x3D; 1</span><br><span class="line">random&#x3D;1</span><br></pre></td></tr></table></figure>
<h4 id="6-数据集格式说明"><a href="#6-数据集格式说明" class="headerlink" title="6. 数据集格式说明"></a>6. 数据集格式说明</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- yolov3</span><br><span class="line">    - data</span><br><span class="line">      - 2007_train.txt</span><br><span class="line">      - 2007_test.txt</span><br><span class="line">      - coco.names</span><br><span class="line">      - coco.data</span><br><span class="line">      - annotations(json files)</span><br><span class="line">      - images(将2007_train.txt中的图片放到train2014文件夹中，test同理)</span><br><span class="line">        - train2014</span><br><span class="line">          - 0001.jpg</span><br><span class="line">          - 0002.jpg</span><br><span class="line">        - val2014</span><br><span class="line">          - 0003.jpg</span><br><span class="line">          - 0004.jpg</span><br><span class="line">      - labels（voc_labels.py生成的内容需要重新组织一下）</span><br><span class="line">        - train2014</span><br><span class="line">          - 0001.txt</span><br><span class="line">          - 0002.txt</span><br><span class="line">        - val2014</span><br><span class="line">          - 0003.txt</span><br><span class="line">          - 0004.txt</span><br><span class="line">      - samples(存放待测试图片)</span><br></pre></td></tr></table></figure>
<p>2007_train.txt内容示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;home&#x2F;dpj&#x2F;yolov3-master&#x2F;data&#x2F;images&#x2F;val2014&#x2F;Cow_1192.jpg</span><br><span class="line">&#x2F;home&#x2F;dpj&#x2F;yolov3-master&#x2F;data&#x2F;images&#x2F;val2014&#x2F;Cow_1196.jpg</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>
<p>注意images和labels文件架构一致性，因为txt是通过简单的替换得到的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">images -&gt; labels</span><br><span class="line">.jpg -&gt; .txt</span><br></pre></td></tr></table></figure>
<p>具体内容可以在datasets.py文件中找到详细的替换。</p>
<h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3. 训练模型"></a>3. 训练模型</h3><p>预训练模型：</p>
<ul>
<li><p>Darknet <code>*.weights</code> format: <a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov3.weights</a></p>
</li>
<li><p>PyTorch <code>*.pt</code> format: </p>
<p><a href="https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0" target="_blank" rel="noopener">https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0</a></p>
</li>
</ul>
<p>开始训练：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python train.py --data data&#x2F;coco.data --cfg cfg&#x2F;yolov3.cfg</span><br></pre></td></tr></table></figure>
<p>如果日志正常输出那证明可以运行了</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/640.webp" alt></p>
<p>如果中断了，可以恢复训练</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python train.py --data data&#x2F;coco.data --cfg cfg&#x2F;yolov3.cfg --resume</span><br></pre></td></tr></table></figure>
<h3 id="4-测试模型"><a href="#4-测试模型" class="headerlink" title="4. 测试模型"></a>4. 测试模型</h3><p>将待测试图片放到data/samples中，然后运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python detect.py --cfg cfg&#x2F;yolov3.cfg --weights weights&#x2F;best.pt</span><br></pre></td></tr></table></figure>
<p>目前该文件中也可以放入视频进行视频目标检测。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/641.webp" alt></p>
<ul>
<li>Image: <code>--source file.jpg</code></li>
<li>Video: <code>--source file.mp4</code></li>
<li>Directory: <code>--source dir/</code></li>
<li>Webcam: <code>--source 0</code></li>
<li>RTSP stream: <code>--source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa</code></li>
<li>HTTP stream: <code>--source http://wmccpinetop.axiscam.net/mjpg/video.mjpg</code></li>
</ul>
<h3 id="5-评估模型"><a href="#5-评估模型" class="headerlink" title="5. 评估模型"></a>5. 评估模型</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python test.py --weights weights&#x2F;best.pt</span><br></pre></td></tr></table></figure>
<p>如果使用cocoAPI使用以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ python3 test.py --img-size 608 --iou-thr 0.6 --weights ultralytics68.pt --cfg yolov3-spp.cfg</span><br><span class="line"></span><br><span class="line">Namespace(batch_size&#x3D;32, cfg&#x3D;&#39;yolov3-spp.cfg&#39;, conf_thres&#x3D;0.001, data&#x3D;&#39;data&#x2F;coco2014.data&#39;, device&#x3D;&#39;&#39;, img_size&#x3D;608, iou_thres&#x3D;0.6, save_json&#x3D;True, task&#x3D;&#39;test&#39;, weights&#x3D;&#39;ultralytics68.pt&#39;)</span><br><span class="line">Using CUDA device0 _CudaDeviceProperties(name&#x3D;&#39;Tesla V100-SXM2-16GB&#39;, total_memory&#x3D;16130MB)</span><br><span class="line">               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 157&#x2F;157 [03:30&lt;00:00,  1.16it&#x2F;s]</span><br><span class="line">                 all     5e+03  3.51e+04    0.0353     0.891     0.606    0.0673</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.409</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.615</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.437</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; 0.242</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; 0.448</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.519</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.337</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.557</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.612</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; 0.438</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; 0.658</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.746</span><br></pre></td></tr></table></figure>
<p><strong>mAP计算</strong></p>
<ul>
<li>mAP@0.5 run at <code>--iou-thr 0.5</code>, mAP@0.5…0.95 run at <code>--iou-thr 0.7</code></li>
</ul>
<h3 id="6-可视化"><a href="#6-可视化" class="headerlink" title="6. 可视化"></a>6. 可视化</h3><p>可以使用<code>python -c from utils import utils;utils.plot_results()</code></p>
<p>创建drawLog.py</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plot_results():</span><br><span class="line">    # Plot YOLO training results file &#39;results.txt&#39;</span><br><span class="line">    import glob</span><br><span class="line">    import numpy as np</span><br><span class="line">    import matplotlib.pyplot as plt</span><br><span class="line">    #import os; os.system(&#39;rm -rf results.txt &amp;&amp; wget https:&#x2F;&#x2F;storage.googleapis.com&#x2F;ultralytics&#x2F;results_v1_0.txt&#39;)</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize&#x3D;(16, 8))</span><br><span class="line">    s &#x3D; [&#39;X&#39;, &#39;Y&#39;, &#39;Width&#39;, &#39;Height&#39;, &#39;Objectness&#39;, &#39;Classification&#39;, &#39;Total Loss&#39;, &#39;Precision&#39;, &#39;Recall&#39;, &#39;mAP&#39;]</span><br><span class="line">    files &#x3D; sorted(glob.glob(&#39;results.txt&#39;))</span><br><span class="line">    for f in files:</span><br><span class="line">        results &#x3D; np.loadtxt(f, usecols&#x3D;[2, 3, 4, 5, 6, 7, 8, 17, 18, 16]).T  # column 16 is mAP</span><br><span class="line">        n &#x3D; results.shape[1]</span><br><span class="line">        for i in range(10):</span><br><span class="line">            plt.subplot(2, 5, i + 1)</span><br><span class="line">            plt.plot(range(1, n), results[i, 1:], marker&#x3D;&#39;.&#39;, label&#x3D;f)</span><br><span class="line">            plt.title(s[i])</span><br><span class="line">            if i &#x3D;&#x3D; 0:</span><br><span class="line">                plt.legend()</span><br><span class="line">    plt.savefig(&#39;.&#x2F;plot.png&#39;)</span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    plot_results()</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pyyolov3/642.webp" alt></p>
<p><strong>7. 数据集配套代码</strong></p>
<p>如果你看到这里了，恭喜你，你可以避开以上略显复杂的数据处理。这里提供了一套代码，集成了以上脚本，只需要你有jpg图片和对应的xml文件，就可以直接生成符合要求的数据集，然后按照要求修改一些代码即可。</p>
<p>代码地址：<a href="https://github.com/pprp/voc2007_for_yolo_torch" target="_blank" rel="noopener">https://github.com/pprp/voc2007_for_yolo_torch</a></p>
<p>请按照readme中进行处理就可以得到数据集。</p>
<blockquote>
<p>后记：这套代码一直由一个外国的团队进行维护，也添加了很多新的trick。目前已获得了3.3k个star，1k  fork。不仅如此，其团队会经常回复issue，目前也有接近1k的issue。只要处理过一遍数据，就会了解到这个库的亮点，非常容易配置，不需要进行编译等操作，易用性极强。再加上提供的配套数据处理代码，在短短10多分钟就可以配置好。(✪ω✪)</p>
</blockquote>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>使用VS Code + SSH进行远程开发</title>
    <url>/2020/02/14/%E4%BD%BF%E7%94%A8VS-Code-+-SSH%E8%BF%9B%E8%A1%8C%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<h2 id="为什么需要远程开发"><a href="#为什么需要远程开发" class="headerlink" title=" 为什么需要远程开发"></a><a id="more"></a> 为什么需要远程开发</h2><p>在进行Linux开发的时候，为了方便，通常在Windows上使用代码编辑器编辑代码，交叉编译工具在Linux虚拟机或者服务器上，在开发期间需要不停的进行如下的循环操作：</p>
<ul>
<li>编辑好代码，使用<strong>基于SSH的ftp</strong>将文件上传到服务器</li>
<li><strong>使用SSH远程终端</strong>，在服务器上编译出可执行文件</li>
<li>编译完成后使用<strong>基于SSH的ftp</strong>将文件传回到本地</li>
</ul>
<p>这些操作都是基于SSH的，但是需要终端软件，文件传输软件， 并且不停地切换操作，过程很麻烦。</p>
<p>如果<strong>本地的编辑器可以直接通过SSH打开远程服务器的目录，操作文件，执行命令</strong>，这就称之为远程开发，使用远程开发可以大大方便我们的开发过程。</p>
<h2 id="Visual-Studio-Code-Remote-SSH扩展"><a href="#Visual-Studio-Code-Remote-SSH扩展" class="headerlink" title="Visual Studio Code Remote - SSH扩展"></a>Visual Studio Code Remote - SSH扩展</h2><p>Remote Development extension pack是VS Code在今年5月份发布的扩展，该扩展包括三个扩展：</p>
<ul>
<li>Remote - SSH</li>
<li>Remote - Containers</li>
<li>Remote - WSL</li>
</ul>
<p>这三个扩展分别支持将远程计算机，容器，或Windows子系统Linux（WSL）用作功能齐全的后台开发环境，本地的VS Code只是一个前端的界面，在本文中我们主要讲述如何使用SSH扩展，如图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190522113131209.png" alt></p>
<h2 id="确保在命令行可以使用ssh命令"><a href="#确保在命令行可以使用ssh命令" class="headerlink" title="确保在命令行可以使用ssh命令"></a>确保在命令行可以使用ssh命令</h2><ul>
<li>如果使用的系统是<code>Windows10</code>，系统中已经自带了<code>SSH</code>，<strong>不能再使用Git的ssh</strong>：如图：</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190630104856278.png" alt></p>
<ul>
<li>如果使用的系统是<code>Windows7</code>，<strong>不能安装OpenSSH</strong>，只能使用Git中的ssh命令，将Git安装目录中的<code>usr\bin</code>文件夹添加到系统环境变量中，该目录下包含ssh命令的可执行程序；</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190522115459170.png" alt></p>
<h2 id="安装SSH扩展"><a href="#安装SSH扩展" class="headerlink" title="安装SSH扩展"></a>安装SSH扩展</h2><p>在VS  Code扩展市场搜索<code>remote</code>，选择<code>Remote-SSH</code>，点击安装：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190522114027910.png" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190522114100203.png" alt></p>
<h2 id="远程主机安装SSH服务器"><a href="#远程主机安装SSH服务器" class="headerlink" title="远程主机安装SSH服务器"></a>远程主机安装SSH服务器</h2><p>特别注意：<strong>SSH扩展只能连接64位的Linux操作系统。</strong><br>在远程Linux主机上安装ssh服务器：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure></p>
<h2 id="设置SSH扩展显示登录终端"><a href="#设置SSH扩展显示登录终端" class="headerlink" title="设置SSH扩展显示登录终端"></a>设置SSH扩展显示登录终端</h2><p>打开命令面板<code>ctrl+shift+p</code>，输入<code>ssh</code>，选择设置：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523120834360.png" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523120925461.png" alt></p>
<h2 id="启动SSH连接远程主机"><a href="#启动SSH连接远程主机" class="headerlink" title="启动SSH连接远程主机"></a>启动SSH连接远程主机</h2><p>SSH启动的方式有两种：</p>
<ul>
<li>使用<code>Ctrl+Shift+P</code>打开命令面板，输入<code>ssh</code>，选择<code>Connect to Host</code>：</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190522120252523.png" alt></p>
<ul>
<li><p>直接点击左下角的ssh图标：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190522120345995.png" alt></p>
</li>
</ul>
<p>启动之后输入远程主机的<code>用户名@ip地址</code>，按回车进行连接：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523121056234.png" alt></p>
<p>会显示出SSH登录终端，输入用户的密码即可：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523115727683.png" alt></p>
<p>首次登录后，VS Code会自动弹出一个新的窗口用于远程工作，并且会自动在远程主机上安装VS Code server：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523115814332.png" alt></p>
<p>登录成功后如图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523122154883.png" alt></p>
<h2 id="打开远程目录作为工作区"><a href="#打开远程目录作为工作区" class="headerlink" title="打开远程目录作为工作区"></a>打开远程目录作为工作区</h2><p>点击文件视图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523122256594.png" alt></p>
<p>然后选择要打开的目录：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523122341344.png" alt></p>
<p>打开成功如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523122740634.png" alt></p>
<h2 id="使用远程终端"><a href="#使用远程终端" class="headerlink" title="使用远程终端"></a>使用远程终端</h2><p><strong>直接点击新建终端即可打开Bash</strong>：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523122927502.png" alt></p>
<h2 id="安装扩展"><a href="#安装扩展" class="headerlink" title="安装扩展"></a>安装扩展</h2><p>注意，在远程开发的时候扩展分为本地扩展和远程扩展：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190523123334817.png" alt></p>
<h2 id="记住常用主机"><a href="#记住常用主机" class="headerlink" title="记住常用主机"></a>记住常用主机</h2><p>如图，打开配置文件：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190524091244209.png" alt></p>
<p>选择 一个配置文件：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/2019052409132074.png" alt></p>
<p>按如下格式填写内容，保存：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190524091455578.png" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/vscode%20ssh/20190524091601192.png" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3的cfg文件解析</title>
    <url>/2020/02/14/YOLOv3%E7%9A%84cfg%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h2 id="1-Net层"><a href="#1-Net层" class="headerlink" title=" 1. Net层"></a><a id="more"></a> 1. Net层</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[net]</span><br><span class="line">#Testing</span><br><span class="line">#batch&#x3D;1</span><br><span class="line">#subdivisions&#x3D;1</span><br><span class="line">#在测试的时候，设置batch&#x3D;1,subdivisions&#x3D;1</span><br><span class="line">#Training</span><br><span class="line">batch&#x3D;16</span><br><span class="line">subdivisions&#x3D;4</span><br><span class="line">#这里的batch与普遍意义上的batch不是一致的。</span><br><span class="line">#训练的过程中将一次性加载16张图片进内存，然后分4次完成前向传播，每次4张。</span><br><span class="line">#经过16张图片的前向传播以后，进行一次反向传播。</span><br><span class="line">width&#x3D;416</span><br><span class="line">height&#x3D;416</span><br><span class="line">channels&#x3D;3</span><br><span class="line">#设置图片进入网络的宽、高和通道个数。</span><br><span class="line">#由于YOLOv3的下采样一般是32倍，所以宽高必须能被32整除。</span><br><span class="line">#多尺度训练选择为32的倍数最小320*320，最大608*608。</span><br><span class="line">#长和宽越大，对小目标越好，但是占用显存也会高，需要权衡。</span><br><span class="line">momentum&#x3D;0.9</span><br><span class="line">#动量参数影响着梯度下降到最优值的速度。</span><br><span class="line">decay&#x3D;0.0005</span><br><span class="line">#权重衰减正则项，防止过拟合。</span><br><span class="line">angle&#x3D;0</span><br><span class="line">#数据增强，设置旋转角度。</span><br><span class="line">saturation &#x3D; 1.5</span><br><span class="line">#饱和度</span><br><span class="line">exposure &#x3D; 1.5</span><br><span class="line">#曝光量</span><br><span class="line">hue&#x3D;.1</span><br><span class="line">#色调</span><br><span class="line"></span><br><span class="line">learning_rate&#x3D;0.001</span><br><span class="line">#学习率:刚开始训练时可以将学习率设置的高一点，而一定轮数之后，将其减小。</span><br><span class="line">#在训练过程中，一般根据训练轮数设置动态变化的学习率。</span><br><span class="line">burn_in&#x3D;1000</span><br><span class="line">#在迭代次数小于burn_in时，其学习率的更新有一种方式，大于burn in时，才采用policy的更新方式</span><br><span class="line">max_batches &#x3D; 500200</span><br><span class="line">#最大batch</span><br><span class="line">policy&#x3D;steps</span><br><span class="line">#学习率调整的策略，有以下policy：</span><br><span class="line">#constant, steps, exp, poly, step, sig, RANDOM，constant等方式</span><br><span class="line">#调整学习率的policy，</span><br><span class="line">#有如下policy：constant, steps, exp, poly, step, sig, RANDOM。</span><br><span class="line">#steps#比较好理解，按照steps来改变学习率。</span><br><span class="line"></span><br><span class="line">steps&#x3D;400000,450000</span><br><span class="line">scales&#x3D;.1,.1</span><br><span class="line">#在达到40000、45000的时候将学习率乘以对应的scale</span><br><span class="line">#即steps和scale是设置学习率的变化，迭代到40000次时，学习率衰减10倍，45000次迭代时，学习率又会在前一个学习率的基础上衰减10倍</span><br></pre></td></tr></table></figure>
<h2 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2. 卷积层"></a>2. 卷积层</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1    		</span><br><span class="line">#是否做BN操作</span><br><span class="line">filters&#x3D;32                  </span><br><span class="line">#输出特征图的数量</span><br><span class="line">size&#x3D;3               		</span><br><span class="line">#卷积核的尺寸</span><br><span class="line">stride&#x3D;1                	</span><br><span class="line">#做卷积运算的步长</span><br><span class="line">pad&#x3D;1               		</span><br><span class="line">#卷积时是否进行补零padding；padding的个数与卷积核尺寸有关，为size&#x2F;2向下取整，如3&#x2F;2&#x3D;1</span><br><span class="line">activation&#x3D;leaky		</span><br><span class="line">#激活函数的类型：logistic，loggy，relu，</span><br><span class="line">#elu，relie，plse，hardtan，lhtan，</span><br><span class="line">#linear，ramp，leaky，tanh，stair</span><br><span class="line"># alexeyAB版添加了mish, swish, nrom_chan等新的激活函数</span><br></pre></td></tr></table></figure>
<p>feature map计算公式：</p>
<script type="math/tex; mode=display">
OutFeature =\frac{\text {InFeature}+2 \times \text { padding }-\text {size}}{\text {stride}}+1</script><h2 id="3-下采样"><a href="#3-下采样" class="headerlink" title="3. 下采样"></a>3. 下采样</h2><p>可以通过调整卷积层参数进行下采样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;2</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br></pre></td></tr></table></figure>
<p>可以通过带入以上公式，可以得到OutFeature是InFeature的一半。</p>
<p>也可以使用maxpooling进行下采样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br></pre></td></tr></table></figure>
<h2 id="4-上采样"><a href="#4-上采样" class="headerlink" title="4. 上采样"></a>4. 上采样</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[upsample]</span><br><span class="line">stride&#x3D;2</span><br></pre></td></tr></table></figure>
<p>上采样是通过线性插值实现的。</p>
<h2 id="5-Shortcut和Route层"><a href="#5-Shortcut和Route层" class="headerlink" title="5. Shortcut和Route层"></a>5. Shortcut和Route层</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[shortcut]</span><br><span class="line">from&#x3D;-3</span><br><span class="line">activation&#x3D;linear</span><br><span class="line">#shortcut操作是类似ResNet的跨层连接，参数from是−3，</span><br><span class="line">#意思是shortcut的输出是当前层与先前的倒数第三层相加而得到。</span><br><span class="line"># 通俗来讲就是add操作</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers &#x3D; -1, 36</span><br><span class="line"># 当属性有两个值，就是将上一层和第36层进行concate</span><br><span class="line">#即沿深度的维度连接，这也要求feature map大小是一致的。</span><br><span class="line">[route]</span><br><span class="line">layers &#x3D; -4</span><br><span class="line">#当属性只有一个值时，它会输出由该值索引的网络层的特征图。</span><br><span class="line">#本例子中就是提取从当前倒数第四个层输出</span><br></pre></td></tr></table></figure>
<h2 id="6-YOLO层"><a href="#6-YOLO层" class="headerlink" title="6. YOLO层"></a>6. YOLO层</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;18</span><br><span class="line">#每一个[region&#x2F;yolo]层前的最后一个卷积层中的</span><br><span class="line">#filters&#x3D;num(yolo层个数)*(classes+5) ,5的意义是5个坐标，</span><br><span class="line">#代表论文中的tx,ty,tw,th,po</span><br><span class="line">#这里类别个数为1，（1+5）*3&#x3D;18</span><br><span class="line">activation&#x3D;linear</span><br><span class="line"></span><br><span class="line">[yolo]	</span><br><span class="line">mask &#x3D; 6,7,8 				</span><br><span class="line">#训练框mask的值是0,1,2，			</span><br><span class="line">#这意味着使用第一，第二和第三个anchor</span><br><span class="line">anchors &#x3D; 10,13,  16,30,  33,23,  30,61,  62,45,\</span><br><span class="line">		  59,119,  116,90,  156,198,  373,326</span><br><span class="line"># 总共有三个检测层，共计9个anchor</span><br><span class="line"># 这里的anchor是由kmeans聚类算法得到的。</span><br><span class="line">classes&#x3D;1 </span><br><span class="line">#类别个数</span><br><span class="line">num&#x3D;9     			</span><br><span class="line">#每个grid预测的BoundingBox num&#x2F;yolo层个数</span><br><span class="line">jitter&#x3D;.3    		</span><br><span class="line">#利用数据抖动产生更多数据，</span><br><span class="line">#属于TTA（Test Time Augmentation）</span><br><span class="line">ignore_thresh &#x3D; .5</span><br><span class="line"># ignore_thresh 指的是参与计算的IOU阈值大小。</span><br><span class="line">#当预测的检测框与ground true的IOU大于ignore_thresh的时候，</span><br><span class="line">#参与loss的计算，否则，检测框的不参与损失计算。</span><br><span class="line">#目的是控制参与loss计算的检测框的规模，当ignore_thresh过于大，</span><br><span class="line">#接近于1的时候，那么参与检测框回归loss的个数就会比较少，同时也容易造成过拟合；</span><br><span class="line">#而如果ignore_thresh设置的过于小，那么参与计算的会数量规模就会很大。</span><br><span class="line">#同时也容易在进行检测框回归的时候造成欠拟合。</span><br><span class="line">#ignore_thresh 一般选取0.5-0.7之间的一个值</span><br><span class="line"># 小尺度（13*13）用的是0.7，</span><br><span class="line"># 大尺度（26*26）用的是0.5。</span><br></pre></td></tr></table></figure>
<h2 id="7-模块总结"><a href="#7-模块总结" class="headerlink" title="7. 模块总结"></a>7. 模块总结</h2><p>Darket-53结构如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-e65c08c61bfaa7c7.png" alt></p>
<p>它是由重复的类似于ResNet的模块组成的，其下采样是通过卷积来完成的。通过对cfg文件的观察，提出了以下总结：</p>
<p><strong>不改变feature大小的模块：</strong></p>
<ol>
<li>残差模块：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[shortcut]</span><br><span class="line">from&#x3D;-3</span><br><span class="line">activation&#x3D;linear</span><br></pre></td></tr></table></figure>
<ol>
<li>1×1卷积：可以降低计算量</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br></pre></td></tr></table></figure>
<ol>
<li>普通3×3卷积：可以对filter个数进行调整</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br></pre></td></tr></table></figure>
<p><strong>改变feature map大小</strong></p>
<ol>
<li>feature map减半：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;2</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br></pre></td></tr></table></figure>
<ol>
<li>feature map加倍:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>特征融合操作</strong></p>
<ol>
<li>使用Route层获取指定的层（13×13）。</li>
<li>添加卷积层进行学习但不改变feature map大小。</li>
<li>进行上采样（26×26）。</li>
<li>从backbone中找到对应feature map大小的层进行Route或者Shortcut（26×26）。</li>
<li>融合完成。</li>
</ol>
<blockquote>
<p>后记：以上是使用darknet过程中收集和总结的一些经验，掌握以上内容并读懂yolov3论文后，就可以着手运行代码了。目前使用与darknet一致的cfg文件解析的有一些，比如原版Darknet，AlexeyAB版本的Darknet，还有一个pytorch版本的yolov3。AlexeyAB版本的添加了很多新特性，比如 [conv_lstm], [scale_channels] SE/ASFF/BiFPN, [local_avgpool], [sam],  [Gaussian_yolo], [reorg3d] (fixed [reorg]),  [batchnorm]等等。而pytorch版本的yolov3可以很方便的添加我们需要的功能。之后我们将会对这个版本进行改进，添加孔洞卷积、SE、CBAM、SK等模块。</p>
</blockquote>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测算法之常见评价指标(mAP)的详细计算方法及代码解析</title>
    <url>/2020/02/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E4%B9%8B%E5%B8%B8%E8%A7%81%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87(mAP)%E7%9A%84%E8%AF%A6%E7%BB%86%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>定义一些评价标准：</p>
<ul>
<li><p>准确率(Acc)：准确率(Acc)的计算公式为$A c c=\frac{T P+T N}{N}$，即预测正确的样本比例，$N$代表测试的样本数。在检测任务中没有预测正确的负样本的概念，所以Acc自然用不到了。</p>
</li>
<li><p>查准率(Precision)：查准率是针对某一个具体类别而言的，公式为：Precision $=\frac{T P}{T P+F P}=\frac{T P}{N}$，其中$N$代表所有检测到的某个具体类的目标框个数。</p>
</li>
<li><p>召回率(Recall)：召回率仍然是针对某一个具体类别而言的，公式为：Recall$=\frac{T P}{T P+F N}$，即预测正确的目标框和所有Ground Truth框的比值。</p>
</li>
<li><p>F1 Score：定位查准率和召回率的调和平均，公式如下：</p>
<script type="math/tex; mode=display">
F_{1}=2 \times \frac{\text {Precision} * \text {Recall}}{\text {Precision}+\text {Recall}}=\frac{2 T P}{2 T P+F N+F P}</script></li>
<li><p>IOU：先为计算mAP值做一个铺垫，即IOU阈值是如何影响Precision和Recall值的？比如在PASCAL  VOC竞赛中采用的IoU阈值为0.5，而COCO竞赛中在计算mAP较复杂，其计算了一系列IoU阈值（0.05至0.95）下的mAP当成最后的mAP值。</p>
</li>
<li><p>mAP：全称为Average Precision，AP值是Precision-Recall曲线下方的面积。那么问题来了，目标检测中PR曲线怎么来的？可以在这篇论文找到答案，截图如下：</p>
</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/map%20code/640.webp" alt></p>
<p>我来解释一下，要得到Precision-Recall曲线(以下简称PR)曲线，首先要对检测模型的预测结果按照目标置信度降序排列。然后给定一个<code>rank</code>值，Recall和Precision仅在置信度高于该<code>rank</code>值的预测结果中计算，改变<code>rank</code>值会相应的改变Recall值和Precision值。这里选择了11个不同的<code>rank</code>值，也就得到了11组Precision和Recall值，然后AP值即定义为在这11个Recall下Precision值的平均值，其可以表征整个PR曲线下方的面积。即：</p>
<script type="math/tex; mode=display">
A P=\frac{1}{11} \sum_{r \in\{0,0.1, \ldots, 1\}} p_{i n t e r p}(r)</script><p>还有另外一种插值的计算方法，即对于某个Recall值<code>r</code>，Precision取所有Recall值大于<code>r</code>中的最大值，这样保证了PR曲线是单调递减的，避免曲线出现摇摆。另外需要注意的一点是在2010年后计算AP值时是取了所有的数据点，而不仅仅只是11个Recall值。我们在计算出AP之后，对所有类别求平均之后就是mAP值了，也是当前目标检测用的最多的评判标准。</p>
<ul>
<li>AP50，AP60，AP70等等代表什么意思？代表IOU阈值分别取0.5，0.6，0.7等对应的AP值。</li>
</ul>
<h1 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h1><p>下面解析一下Faster-RCNN中对VOC数据集计算每个类别AP值的代码，mAP就是所有类的AP值平均值。代码来自py-faster-rcnn项目。代码解析如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># --------------------------------------------------------</span><br><span class="line"># Fast&#x2F;er R-CNN</span><br><span class="line"># Licensed under The MIT License [see LICENSE for details]</span><br><span class="line"># Written by Bharath Hariharan</span><br><span class="line"># --------------------------------------------------------</span><br><span class="line"></span><br><span class="line">import xml.etree.ElementTree as ET #读取xml文件</span><br><span class="line">import os</span><br><span class="line">import cPickle #序列化存储模块</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def parse_rec(filename):</span><br><span class="line">    &quot;&quot;&quot; Parse a PASCAL VOC xml file &quot;&quot;&quot;</span><br><span class="line">    tree &#x3D; ET.parse(filename)</span><br><span class="line">    objects &#x3D; []</span><br><span class="line">    # 解析xml文件，将GT框信息放入一个列表</span><br><span class="line">    for obj in tree.findall(&#39;object&#39;):</span><br><span class="line">        obj_struct &#x3D; &#123;&#125;</span><br><span class="line">        obj_struct[&#39;name&#39;] &#x3D; obj.find(&#39;name&#39;).text</span><br><span class="line">        obj_struct[&#39;pose&#39;] &#x3D; obj.find(&#39;pose&#39;).text</span><br><span class="line">        obj_struct[&#39;truncated&#39;] &#x3D; int(obj.find(&#39;truncated&#39;).text)</span><br><span class="line">        obj_struct[&#39;difficult&#39;] &#x3D; int(obj.find(&#39;difficult&#39;).text)</span><br><span class="line">        bbox &#x3D; obj.find(&#39;bndbox&#39;)</span><br><span class="line">        obj_struct[&#39;bbox&#39;] &#x3D; [int(bbox.find(&#39;xmin&#39;).text),</span><br><span class="line">                              int(bbox.find(&#39;ymin&#39;).text),</span><br><span class="line">                              int(bbox.find(&#39;xmax&#39;).text),</span><br><span class="line">                              int(bbox.find(&#39;ymax&#39;).text)]</span><br><span class="line">        objects.append(obj_struct)</span><br><span class="line"></span><br><span class="line">    return objects</span><br><span class="line"></span><br><span class="line"># 单个计算AP的函数，输入参数为精确率和召回率，原理见上面</span><br><span class="line">def voc_ap(rec, prec, use_07_metric&#x3D;False):</span><br><span class="line">    &quot;&quot;&quot; ap &#x3D; voc_ap(rec, prec, [use_07_metric])</span><br><span class="line">    Compute VOC AP given precision and recall.</span><br><span class="line">    If use_07_metric is true, uses the</span><br><span class="line">    VOC 07 11 point method (default:False).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 如果使用2007年的计算AP的方式(插值的方式)</span><br><span class="line">    if use_07_metric:</span><br><span class="line">        # 11 point metric</span><br><span class="line">        ap &#x3D; 0.</span><br><span class="line">        for t in np.arange(0., 1.1, 0.1):</span><br><span class="line">            if np.sum(rec &gt;&#x3D; t) &#x3D;&#x3D; 0:</span><br><span class="line">                p &#x3D; 0</span><br><span class="line">            else:</span><br><span class="line">                p &#x3D; np.max(prec[rec &gt;&#x3D; t])</span><br><span class="line">            ap &#x3D; ap + p &#x2F; 11.</span><br><span class="line">    else:</span><br><span class="line">       # 使用2010年后的计算AP值的方式</span><br><span class="line">        # 这里是新增一个(0,0)，方便计算</span><br><span class="line">        mrec &#x3D; np.concatenate(([0.], rec, [1.]))</span><br><span class="line">        mpre &#x3D; np.concatenate(([0.], prec, [0.]))</span><br><span class="line"></span><br><span class="line">        # compute the precision envelope</span><br><span class="line">        for i in range(mpre.size - 1, 0, -1):</span><br><span class="line">            mpre[i - 1] &#x3D; np.maximum(mpre[i - 1], mpre[i])</span><br><span class="line"></span><br><span class="line">        # to calculate area under PR curve, look for points</span><br><span class="line">        # where X axis (recall) changes value</span><br><span class="line">        i &#x3D; np.where(mrec[1:] !&#x3D; mrec[:-1])[0]</span><br><span class="line"></span><br><span class="line">        # and sum (\Delta recall) * prec</span><br><span class="line">        ap &#x3D; np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])</span><br><span class="line">    return ap</span><br><span class="line"></span><br><span class="line"># 主函数</span><br><span class="line">def voc_eval(detpath,</span><br><span class="line">             annopath,</span><br><span class="line">             imagesetfile,</span><br><span class="line">             classname,</span><br><span class="line">             cachedir,</span><br><span class="line">             ovthresh&#x3D;0.5,</span><br><span class="line">             use_07_metric&#x3D;False):</span><br><span class="line">    &quot;&quot;&quot;rec, prec, ap &#x3D; voc_eval(detpath,</span><br><span class="line">                                annopath,</span><br><span class="line">                                imagesetfile,</span><br><span class="line">                                classname,</span><br><span class="line">                                [ovthresh],</span><br><span class="line">                                [use_07_metric])</span><br><span class="line">    Top level function that does the PASCAL VOC evaluation.</span><br><span class="line">    detpath: 产生的txt文件，里面是一张图片的各个检测框结果。</span><br><span class="line">    annopath: xml 文件与对应的图像相呼应。</span><br><span class="line">    imagesetfile: 一个txt文件，里面是每个图片的地址，每行一个地址。</span><br><span class="line">    classname: 种类的名字，即类别。</span><br><span class="line">    cachedir: 缓存标注的目录。</span><br><span class="line">    [ovthresh]: IOU阈值，默认为0.5，即mAP50。</span><br><span class="line">    [use_07_metric]: 是否使用2007的计算AP的方法，默认为Fasle</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # assumes detections are in detpath.format(classname)</span><br><span class="line">    # assumes annotations are in annopath.format(imagename)</span><br><span class="line">    # assumes imagesetfile is a text file with each line an image name</span><br><span class="line">    # cachedir caches the annotations in a pickle file</span><br><span class="line"></span><br><span class="line">    # 首先加载Ground Truth标注信息。</span><br><span class="line">    if not os.path.isdir(cachedir):</span><br><span class="line">        os.mkdir(cachedir)</span><br><span class="line">    # 即将新建文件的路径</span><br><span class="line">    cachefile &#x3D; os.path.join(cachedir, &#39;annots.pkl&#39;)</span><br><span class="line">    # 读取文本里的所有图片路径</span><br><span class="line">    with open(imagesetfile, &#39;r&#39;) as f:</span><br><span class="line">        lines &#x3D; f.readlines()</span><br><span class="line">    # 获取文件名，strip用来去除头尾字符、空白符(包括\n、\r、\t、&#39; &#39;，即：换行、回车、制表符、空格)</span><br><span class="line">    imagenames &#x3D; [x.strip() for x in lines]</span><br><span class="line">    #如果cachefile文件不存在，则写入</span><br><span class="line">    if not os.path.isfile(cachefile):</span><br><span class="line">        # load annots</span><br><span class="line">        recs &#x3D; &#123;&#125;</span><br><span class="line">        for i, imagename in enumerate(imagenames):</span><br><span class="line">            #annopath.format(imagename): label的xml文件所在的路径</span><br><span class="line">            recs[imagename] &#x3D; parse_rec(annopath.format(imagename))</span><br><span class="line">            if i % 100 &#x3D;&#x3D; 0:</span><br><span class="line">                print &#39;Reading annotation for &#123;:d&#125;&#x2F;&#123;:d&#125;&#39;.format(</span><br><span class="line">                    i + 1, len(imagenames))</span><br><span class="line">        # save</span><br><span class="line">        print &#39;Saving cached annotations to &#123;:s&#125;&#39;.format(cachefile)</span><br><span class="line">        with open(cachefile, &#39;w&#39;) as f:</span><br><span class="line">            #写入cPickle文件里面。写入的是一个字典，左侧为xml文件名，右侧为文件里面个各个参数。</span><br><span class="line">            cPickle.dump(recs, f)</span><br><span class="line">    else:</span><br><span class="line">        # load</span><br><span class="line">        with open(cachefile, &#39;r&#39;) as f:</span><br><span class="line">            recs &#x3D; cPickle.load(f)</span><br><span class="line"></span><br><span class="line">    # 对每张图片的xml获取函数指定类的bbox等</span><br><span class="line">    class_recs &#x3D; &#123;&#125;# 保存的是 Ground Truth的数据</span><br><span class="line">    npos &#x3D; 0</span><br><span class="line">    for imagename in imagenames:</span><br><span class="line">        # 获取Ground Truth每个文件中某种类别的物体</span><br><span class="line">        R &#x3D; [obj for obj in recs[imagename] if obj[&#39;name&#39;] &#x3D;&#x3D; classname]</span><br><span class="line">        bbox &#x3D; np.array([x[&#39;bbox&#39;] for x in R])</span><br><span class="line">        #  different基本都为0&#x2F;False</span><br><span class="line">        difficult &#x3D; np.array([x[&#39;difficult&#39;] for x in R]).astype(np.bool)</span><br><span class="line">        det &#x3D; [False] * len(R)</span><br><span class="line">        npos &#x3D; npos + sum(~difficult) #自增，~difficult取反,统计样本个数</span><br><span class="line">        # # 记录Ground Truth的内容</span><br><span class="line">        class_recs[imagename] &#x3D; &#123;&#39;bbox&#39;: bbox,</span><br><span class="line">                                 &#39;difficult&#39;: difficult,</span><br><span class="line">                                 &#39;det&#39;: det&#125;</span><br><span class="line"></span><br><span class="line">    # read dets 读取某类别预测输出</span><br><span class="line">    detfile &#x3D; detpath.format(classname)</span><br><span class="line">    with open(detfile, &#39;r&#39;) as f:</span><br><span class="line">        lines &#x3D; f.readlines()</span><br><span class="line"></span><br><span class="line">    splitlines &#x3D; [x.strip().split(&#39; &#39;) for x in lines]</span><br><span class="line">    image_ids &#x3D; [x[0] for x in splitlines] # 图片ID</span><br><span class="line">    confidence &#x3D; np.array([float(x[1]) for x in splitlines]) # IOU值</span><br><span class="line">    BB &#x3D; np.array([[float(z) for z in x[2:]] for x in splitlines]) # bounding box数值</span><br><span class="line"></span><br><span class="line">    # 对confidence的index根据值大小进行降序排列。</span><br><span class="line">    sorted_ind &#x3D; np.argsort(-confidence)</span><br><span class="line">    sorted_scores &#x3D; np.sort(-confidence)</span><br><span class="line">    #重排bbox，由大概率到小概率。</span><br><span class="line">    BB &#x3D; BB[sorted_ind, :]</span><br><span class="line">    # 图片重排，由大概率到小概率。</span><br><span class="line">    image_ids &#x3D; [image_ids[x] for x in sorted_ind]</span><br><span class="line"></span><br><span class="line">    # go down dets and mark TPs and FPs</span><br><span class="line">    nd &#x3D; len(image_ids)</span><br><span class="line">    tp &#x3D; np.zeros(nd)</span><br><span class="line">    fp &#x3D; np.zeros(nd)</span><br><span class="line">    for d in range(nd):</span><br><span class="line">        R &#x3D; class_recs[image_ids[d]]</span><br><span class="line">        bb &#x3D; BB[d, :].astype(float)</span><br><span class="line">        ovmax &#x3D; -np.inf</span><br><span class="line">        BBGT &#x3D; R[&#39;bbox&#39;].astype(float)</span><br><span class="line"></span><br><span class="line">        if BBGT.size &gt; 0:</span><br><span class="line">            # compute overlaps</span><br><span class="line">            # intersection</span><br><span class="line">            ixmin &#x3D; np.maximum(BBGT[:, 0], bb[0])</span><br><span class="line">            iymin &#x3D; np.maximum(BBGT[:, 1], bb[1])</span><br><span class="line">            ixmax &#x3D; np.minimum(BBGT[:, 2], bb[2])</span><br><span class="line">            iymax &#x3D; np.minimum(BBGT[:, 3], bb[3])</span><br><span class="line">            iw &#x3D; np.maximum(ixmax - ixmin + 1., 0.)</span><br><span class="line">            ih &#x3D; np.maximum(iymax - iymin + 1., 0.)</span><br><span class="line">            inters &#x3D; iw * ih</span><br><span class="line"></span><br><span class="line">            # union</span><br><span class="line">            uni &#x3D; ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +</span><br><span class="line">                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *</span><br><span class="line">                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)</span><br><span class="line"></span><br><span class="line">            overlaps &#x3D; inters &#x2F; uni</span><br><span class="line">            ovmax &#x3D; np.max(overlaps)</span><br><span class="line">            jmax &#x3D; np.argmax(overlaps)</span><br><span class="line"></span><br><span class="line">        if ovmax &gt; ovthresh:</span><br><span class="line">            if not R[&#39;difficult&#39;][jmax]:</span><br><span class="line">                if not R[&#39;det&#39;][jmax]:</span><br><span class="line">                    tp[d] &#x3D; 1.</span><br><span class="line">                    R[&#39;det&#39;][jmax] &#x3D; 1</span><br><span class="line">                else:</span><br><span class="line">                    fp[d] &#x3D; 1.</span><br><span class="line">        else:</span><br><span class="line">            fp[d] &#x3D; 1.</span><br><span class="line"></span><br><span class="line">    # compute precision recall</span><br><span class="line">    fp &#x3D; np.cumsum(fp)</span><br><span class="line">    tp &#x3D; np.cumsum(tp)</span><br><span class="line">    rec &#x3D; tp &#x2F; float(npos)</span><br><span class="line">    # avoid divide by zero in case the first detection matches a difficult</span><br><span class="line">    # ground truth</span><br><span class="line">    prec &#x3D; tp &#x2F; np.maximum(tp + fp, np.finfo(np.float64).eps)</span><br><span class="line">    ap &#x3D; voc_ap(rec, prec, use_07_metric)</span><br><span class="line"></span><br><span class="line">	return rec, prec, ap</span><br></pre></td></tr></table></figure>
<p>这个脚本可以直接调用来计算mAP值，具体例子可以看这个：<a href="https://blog.csdn.net/amusi1994/article/details/81564504" target="_blank" rel="noopener">在Darknet中调用上面的脚本来计算mAP值</a></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测算法的评价标准和常见数据集</title>
    <url>/2020/02/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%9A%84%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86%E5%92%8C%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<h2 id="评价指标"><a href="#评价指标" class="headerlink" title=" 评价指标"></a><a id="more"></a> 评价指标</h2><h3 id="1-准确率-Accuracy"><a href="#1-准确率-Accuracy" class="headerlink" title="1.准确率(Accuracy)"></a>1.准确率(Accuracy)</h3><p>检测时分对的样本数除以所有的样本数。准确率一般被用来评估检测模型的全局准确程度，包含的信息有限，不能完全评价一个模型性能。</p>
<h3 id="2-混淆矩阵-Confusion-Matrix"><a href="#2-混淆矩阵-Confusion-Matrix" class="headerlink" title="2.混淆矩阵(Confusion Matrix)"></a>2.混淆矩阵(Confusion Matrix)</h3><p>混淆矩阵是以模型预测的类别数量统计信息为横轴，真实标签的数量统计信息为纵轴画出的矩阵。对角线代表了模型预测和数据标签一致的数目，所以准确率也可以用<strong>混淆矩阵对角线之和除以测试集图片数量</strong>来计算。对角线上的数字越大越好，在混淆矩阵可视化结果中颜色越深，代表模型在该类的预测结果更好。其他地方自然是预测错误的地方，自然值越小，颜色越浅说明模型预测的更好。</p>
<h3 id="3-精确率-Precision-和召回率-Recall-和PR曲线"><a href="#3-精确率-Precision-和召回率-Recall-和PR曲线" class="headerlink" title="3.精确率(Precision)和召回率(Recall)和PR曲线"></a>3.精确率(Precision)和召回率(Recall)和PR曲线</h3><p>一个经典例子是存在一个测试集合，测试集合只有大雁和飞机两种图片组成，假设你的分类系统最终的目的是：能取出测试集中所有飞机的图片，而不是大雁的图片。然后就可以定义：</p>
<ul>
<li>True positives: 简称为TP，即正样本被正确识别为正样本，飞机的图片被正确的识别成了飞机。</li>
<li>True negatives: 简称为TN，即负样本被正确识别为负样本，大雁的图片没有被识别出来，系统正确地认为它们是大雁。</li>
<li>False Positives: 简称为FP，即负样本被错误识别为正样本，大雁的图片被错误地识别成了飞机。</li>
<li>False negatives: 简称为FN，即正样本被错误识别为负样本，飞机的图片没有被识别出来，系统错误地认为它们是大雁。</li>
</ul>
<p><strong>精确率</strong>就是在识别出来的图片中，True positives所占的比率。也就是本假设中，所有被识别出来的飞机中，真正的飞机所占的比例，公式如下：Precision $=\frac{T P}{T P+F P}=\frac{T P}{N}$ ，其中N代表测试集样本数。</p>
<p><strong>召回率</strong>是测试集中所有正样本样例中，被正确识别为正样本的比例。也就是本假设中，被正确识别出来的飞机个数与测试集中所有真实飞机的个数的比值，公式如下： Recall$=\frac{T P}{T P+F N}$</p>
<p>所谓<strong>PR曲线</strong>就是改变识别阈值，使得系统依次能够识别前K张图片，阈值的变化同时会导致Precision与Recall值发生变化，从而得到曲线。曲线图大概如下，这里有3条PR曲线，周志华机器学习的解释如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/metrics%20and%20databases/640.webp" alt></p>
<h3 id="4-平均精度-Average-Precision，AP-和mAP"><a href="#4-平均精度-Average-Precision，AP-和mAP" class="headerlink" title="4.平均精度(Average-Precision，AP)和mAP"></a>4.平均精度(Average-Precision，AP)和mAP</h3><p><strong>AP</strong>就是Precision-recall 曲线下面的面积，通常来说一个越好的分类器，AP值越高。<strong>mAP</strong>是多个类别AP的平均值。这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间，越大越好。该指标是目标检测算法中最重要的一个。</p>
<h3 id="5-ROC曲线"><a href="#5-ROC曲线" class="headerlink" title="5.ROC曲线"></a>5.ROC曲线</h3><p>如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/metrics%20and%20databases/641.webp" alt></p>
<p>ROC的横轴是假正率(False positive rate， FPR)，FPR = FP / [ FP + TN]  ，代表所有负样本中错误预测为正样本的概率，假警报率。ROC的纵轴是真正率(True positive rate， TPR)，TPR  = TP / [ TP + FN]  ，代表所有正样本中预测正确的概率，命中率。ROC曲线的对角线坐标对应于随即猜测，而坐标点(0,1)也即是左上角坐标对应理想模型。曲线越接近左上角代表检测模型的效果越好。</p>
<p>即：从 (0, 0) 到 (1,1) 的对角线将ROC空间划分为左上／右下两个区域，在这条线的以上的点代表了一个好的分类结果（胜过随机分类），而在这条线以下的点代表了差的分类结果（劣于随机分类）。</p>
<p>完美的预测是一个在左上角的点，在ROC空间座标 (0,1)点，X=0 代表着没有伪阳性，Y=1  代表着没有伪阴性（所有的阳性都是真阳性）；也就是说，不管分类器输出结果是阳性或阴性，都是100%正确。一个随机的预测会得到位于从 (0, 0) 到 (1, 1) 对角线（也叫无识别率线）上的一个点；最直观的随机预测的例子就是抛硬币。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/metrics%20and%20databases/v2-fc750883be72300fa2373d1c5b81ee9e_hd.jpg" alt></p>
<p>那么ROC曲线是怎么绘制的呢？有如下几个步骤：</p>
<ul>
<li>根据每个测试样本属于正样本的概率值从大到小排序。</li>
<li>从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。</li>
<li>每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。当我们将threshold设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当threshold取值越多，ROC曲线越平滑。</li>
</ul>
<h3 id="6-AUC-Area-Uner-Curve"><a href="#6-AUC-Area-Uner-Curve" class="headerlink" title="6.AUC(Area Uner Curve)"></a>6.AUC(Area Uner Curve)</h3><p>即为ROC曲线下的面积。AUC越接近于1，分类器性能越好。AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。AUC的计算公式如下：</p>
<script type="math/tex; mode=display">
A U C=\frac{\sum p r e d_{p o s}>p r e d_{n e g}}{p o s i t i v e N u m * n e g a t i v e N  u m}</script><p>即：分别随机从样本集中抽取一个正负样本，正样本的预测值大于负样本的概率。</p>
<h3 id="PR曲线和ROC曲线选用时机"><a href="#PR曲线和ROC曲线选用时机" class="headerlink" title="PR曲线和ROC曲线选用时机"></a>PR曲线和ROC曲线选用时机</h3><p>目标检测中用的最多的是MAP值，但我们最好再了解一下PR曲线和ROC曲线的应用场景，在不同的数据集中选择合适的评价标准更好的判断我们的模型是否训好了。</p>
<h3 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h3><p>从PR的计算公式可以看出，PR曲线聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。</p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>当测试集中的正负样本的分布发生变化时，ROC曲线可以保持不变。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。但是在关心正例的预测准确性的场景，ROC曲线就不能更好的反应模型的性能了，因为ROC曲线的横轴采用FPR，根据FPR公式 ，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。</p>
<p>因此，PR曲线和ROC曲线的选用时机可以总结如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/metrics%20and%20databases/642.webp" alt></p>
<p>从目标检测任务来讲，一般关心MAP值即可。</p>
<h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>刚才介绍了目标检测算法的常见评价标准，这里再介绍一下目标检测常用的数据集。以下介绍来自于github工程整理：DeepLearning-500-questions</p>
<h3 id="PASCAL-VOC数据集"><a href="#PASCAL-VOC数据集" class="headerlink" title="PASCAL VOC数据集"></a>PASCAL VOC数据集</h3><p>VOC数据集是目标检测经常用的一个数据集，自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的train/val 2007和16k的train/val 2012作为训练集，test 2007作为测试集，用10k的train/val 2007+test  2007和16k的train/val 2012作为训练集，test2012作为测试集，分别汇报结果。</p>
<h3 id="MSCOCO数据集"><a href="#MSCOCO数据集" class="headerlink" title="MSCOCO数据集"></a>MSCOCO数据集</h3><p>COCO数据集是微软团队发布的一个可以用来图像recognition+segmentation+captioning的数据集，该数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic  images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic  images则更适合浅语义的图像分类等任务。COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p>
<h3 id="Google-Open-Image数据集"><a href="#Google-Open-Image数据集" class="headerlink" title="Google Open Image数据集"></a>Google Open Image数据集</h3><p>pen Image是谷歌团队发布的数据集。最新发布的Open Images  V4包含190万图像、600个种类，1540万个bounding-box标注，是当前最大的带物体位置标注信息的数据集。这些边界框大部分都是由专业注释人员手动绘制的，确保了它们的准确性和一致性。另外，这些图像是非常多样化的，并且通常包含有多个对象的复杂场景（平均每个图像 8 个）。</p>
<h3 id="ImageNet数据集"><a href="#ImageNet数据集" class="headerlink" title="ImageNet数据集"></a>ImageNet数据集</h3><p>ImageNet是一个计算机视觉系统识别项目，  是目前世界上图像识别最大的数据库。ImageNet是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。Imagenet数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。</p>
<h3 id="DOTA数据集"><a href="#DOTA数据集" class="headerlink" title="DOTA数据集"></a>DOTA数据集</h3><p>DOTA是遥感航空图像检测的常用数据集，包含2806张航空图像，尺寸大约为4k x 4k，包含15个类别共计188282个实例，其中14个主类，small vehicle 和 large  vehicle都是vehicle的子类。其标注方式为四点确定的任意形状和方向的四边形。航空图像区别于传统数据集，有其自己的特点，如：尺度变化性更大；密集的小物体检测；检测目标的不确定性。数据划分为1/6验证集，1/3测试集，1/2训练集。目前发布了训练集和验证集，图像尺寸从800 x 800到4000 x 4000不等。</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu clash配置</title>
    <url>/2020/02/05/Ubuntu-clash%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="1"><a href="#1" class="headerlink" title=" 1."></a><a id="more"></a> 1.</h2><p>执行 <code>cd &amp;&amp; mkdir clash</code>在用户目录下创建 clash 文件夹，用于存放解压后的文件及配置文件,将下载的压缩文件解压至此，得到一个可执行文件“ clash-linux-amd64 ”。。</p>
<p>一般个人的64位电脑下载 clash-linux-amd64.tar.gz 即可。</p>
<p><a href="https://github.com/Dreamacro/clash/releases" target="_blank" rel="noopener">下载客户端</a><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/clash/windows-cfw-1.png" alt></p>
<h2 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h2><p>在终端中执行该文件<code>sudo ./clash-linux-amd64</code>，提示缺少config.yml配置文件，并自动生成/home/当前用户ID/.config/clash文件夹，其中包含两个文件 config.yml 和 Country.mmdb ，若看不到这文件夹，则按<code>ctrl+H</code>显示隐藏文件。其中 Country.mmdb由于墙的原因下载较慢，这里提供<a href="https://quqi.gblhgk.com/s/2796382/PtRJqrmJaeM9Nh17" target="_blank" rel="noopener">网盘下载</a>，直接拷贝进/home/当前用户ID/.config/clash中。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp Country.mmdb的路径 ~&#x2F;home&#x2F;当前用户ID&#x2F;.config&#x2F;clash</span><br></pre></td></tr></table></figure></p>
<h2 id="3"><a href="#3" class="headerlink" title="3."></a>3.</h2><p>编辑/home/当前用户ID/.config/clash下的 config.yml配置文件，内容为自己的服务器及规则等信息(有些商家会提供相应的yml文件，下载后将内容copy至该文件)，保存更改后复制该文件至先前创建的Clash文件夹(by:这两个文件夹不要弄混，一个是手动建立的，一个是自动创建的，都需要.yml文件)。</p>
<p>在终端 cd 到 Clash 二进制文件所在的目录，执行下载 Clash 配置文件，得到商家提供的服务器及规则等信息<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/clash/linux-clash-2.jpg" alt></p>
<p>将其在/home/当前用户ID/.config/clash下复制一份<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp ~&#x2F;home&#x2F;当前用户ID&#x2F;clash&#x2F;config.yml ~&#x2F;home&#x2F;当前用户ID&#x2F;.config&#x2F;clash</span><br></pre></td></tr></table></figure></p>
<h2 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h2><p>重启终端，执行 <code>sudo ./clash-linux-amd64</code>以加载配置文件</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/clash/linux-clash-3.jpg" alt></p>
<h2 id="5"><a href="#5" class="headerlink" title="5."></a>5.</h2><p>访问 <a href="http://clash.razord.top/" target="_blank" rel="noopener">Clash Dashboard </a> 可以进行切换节点、测延迟等操作。</p>
<p>Host: <code>127.0.0.1</code>，端口: <code>9090</code><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/clash/linux-clash-4.jpg" alt></p>
<h2 id="6"><a href="#6" class="headerlink" title="6."></a>6.</h2><p>以 Ubuntu 19.04 为例，打开系统设置，选择网络，点击网络代理右边的 ⚙ 按钮，选择手动，填写 HTTP 和 HTTPS 代理为 <code>127.0.0.1:7890</code>，填写 Socks 主机为 <code>127.0.0.1:7891</code>，即可启用系统代理。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/clash/linux-clash-5.jpg" alt></p>
<p>若需关掉代理，则<code>ctrl+c</code>停止终端，并在系统代理设置中重新改为无即可。</p>
<h2 id="7"><a href="#7" class="headerlink" title="7."></a>7.</h2><p>若需固定到启动器，则选择合适的图片文件，用作图标，放置于创建的clash文件夹下</p>
<p>使用<code>sudo gedit /usr/share/applications/Clash.desktop</code>在 /usr/share/applications/ 中创建一个文件 Clash.desktop ，步骤及内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以下各项根据自己的情况填写</span><br><span class="line">[Desktop Entry]</span><br><span class="line"> Version&#x3D;0.17.0</span><br><span class="line"> Name&#x3D;Clash</span><br><span class="line"> Comment&#x3D;A rule-based tunnel in Go</span><br><span class="line"> Exec&#x3D;&#x2F;full&#x2F;path&#x2F;to&#x2F;clash-linux</span><br><span class="line"> Icon&#x3D;&#x2F;full&#x2F;path&#x2F;to&#x2F;clash-logo.png</span><br><span class="line"> Terminal&#x3D;false</span><br><span class="line"> Type&#x3D;Application</span><br><span class="line"> Categories&#x3D;Network</span><br></pre></td></tr></table></figure></p>
<p>关键词说明</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Desktop Entry] 文件头</span><br><span class="line">Version    版本</span><br><span class="line">Name    应用名称</span><br><span class="line">Name[xx]    不同语言的应用名称</span><br><span class="line">Comment 注释</span><br><span class="line">Exec    执行文件路径</span><br><span class="line">Icon    图标路径</span><br><span class="line">Terminal    是否使用终端</span><br><span class="line">Type    启动器类型</span><br><span class="line">Categories  应用的类型（内容相关）</span><br></pre></td></tr></table></figure>
<p>上述操作完成后，即可在启动器中看到该应用图标，对其右键单击，选择固定到任务栏，方便以后打开 。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/clash/Snipaste_2020-02-05_23-07-03.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>华硕路由器刷固件</title>
    <url>/2020/02/03/%E5%8D%8E%E7%A1%95%E8%B7%AF%E7%94%B1%E5%99%A8%E5%88%B7%E5%9B%BA%E4%BB%B6/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title=" 介绍"></a><a id="more"></a> 介绍</h2><p>现在市面上的路由器基本上使用的都是官方的固件，这是因为路由器的设备厂商是为了设备的稳定性着想而出发的，还有就是所谓的刷梅林固件也就是刷第三方的固件，最大的作用可以解封一些功能。对于梅林固件来说，这有一半的性质是属于官方的路由器固件，但是还有一半是可以用户自己来进行设置的，其实梅林固件就是一个华硕路由器的自定义固件，是华硕官方的闭源驱动，因此稳定性要好很多，不支持超频，很少出现死机、卡顿的情况。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/185417q0zxbdexeze1x1nb.jpg" alt></p>
<p>其最具特色的便是多了软件中心，在里面安装各种各样的插件可以实现各种功能，比如翻墙，去广告，离线下载，双拨等。</p>
<p>并不是所有的路由器都能刷梅林固件，非华硕的路由器需要与华硕某个版本的硬件一致才能刷，以网件和华硕自身的路由器居多。</p>
<p>我手头的路由器为华硕AC86U，华硕刷梅林固件相对简单，风险低，若其他品牌的路由器请去查找相应教程，此教程只适用于华硕品牌。</p>
<h2 id="固件下载"><a href="#固件下载" class="headerlink" title="固件下载"></a>固件下载</h2><p><a href="https://firmware.koolshare.cn/" target="_blank" rel="noopener">Koolshare</a>是路由器刷机的论坛，可以去里面下载固件。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/Snipaste_2020-02-03_16-18-35.jpg" alt></p>
<p>常用到的为红色框出来的这四个，分别是华硕官改固件、梅林原版固件、梅林改版固件、网件官改固件。</p>
<ul>
<li><p>原厂固件为华硕官方的固件。</p>
</li>
<li><p>原版梅林为国外RMerl大神基于华硕官方固件源代码修改而来的梅林固件。</p>
</li>
<li><p>改版梅林为koolshare开发组基于梅林固件修改而来的带软件中心的梅林改版固件。</p>
</li>
<li><p>官改固件为koolshare开发组基于华硕官方源代码修改而来的带软件中心的官改固件。</p>
</li>
</ul>
<p>这之中常用的为梅林改和官改，相比官改固件，梅林改版固件有更多的功能和bug修复，而官改固件更新较快。好用程度差别不大，稳定性来说也基本无差，选哪个固件基于个人喜好即可。</p>
<p>我用的是梅林改固件，进入文件夹，可以看到可用机型。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/Snipaste_2020-02-03_16-32-11.jpg" alt></p>
<p>选择对应机型。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/Snipaste_2020-02-03_16-32-28.jpg" alt></p>
<p>选择最新版本固件下载，固件为.w后缀。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/Snipaste_2020-02-03_16-32-47.jpg" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/Snipaste_2020-02-03_16-32-57.jpg" alt></p>
<h2 id="固件安装"><a href="#固件安装" class="headerlink" title="固件安装"></a>固件安装</h2><ul>
<li>选择【系统管理】-【固件升级】，选择下好的 .w 后缀的固件，上传即可</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/k3.png" alt></p>
<ul>
<li>刷机完成后会自动重启，请耐心等待完成</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/merlin/k4.png" alt></p>
<ul>
<li><p>刷机完成后在【系统管理】–【恢复/导出/上传设置】恢复出厂设置</p>
</li>
<li><p>在【系统管理】–【系统设置】内勾选：Format JFFS partition at next boot 和 Enable JFFS custom scripts and configs 然后点击应用本页面设置，成功后重启路由器；</p>
</li>
</ul>
<p>（注：双清就是指恢复出厂设置 + 格式化jffs分区）</p>
<p>重启后先将路由器连上网络，然后进入软件中心将软件中心更新到最新版本（如果有）。</p>
<h2 id="软件中心"><a href="#软件中心" class="headerlink" title="软件中心"></a>软件中心</h2><p>完成上述工作，梅林固件就装好了，现在你可以进入软件中心未安装，选择合适的插件安装了，插件就不一一介绍，具体可以百度，我常用的是虚拟内存和shadowsocks。</p>
<p>新版软件中心内不提供shadowsocks插件，我这里通过<a href="https://quqi.gblhgk.com/s/2796382/045bYgimCEJLRiCe" target="_blank" rel="noopener">云盘</a>分享，大家可以在【软件中心】-【离线安装】手动安装插件。</p>
<h2 id="后续更新"><a href="#后续更新" class="headerlink" title="后续更新"></a>后续更新</h2><p>按此教程做的，后续更新新版的梅林改时，按B来即可。</p>
<h5 id="A-原厂固件-原版梅林-刷-改版梅林："><a href="#A-原厂固件-原版梅林-刷-改版梅林：" class="headerlink" title="A. 原厂固件/原版梅林 刷 改版梅林："></a>A. 原厂固件/原版梅林 刷 改版梅林：</h5><ol>
<li>在<code>原产固件</code>/<code>原版梅林</code>固件升级页面下直接上传.w 后缀的<code>改版梅林</code>固件文件；</li>
<li>刷机完成后会自动重启，此时刷机完成（刷机完成后可以不恢复出产设置，当然恢复一次更好）；</li>
<li>刷机完成在【系统管理 】–【 系统设置】内勾选：<code>Format JFFS partition at next boot</code> 和 <code>Enable JFFS custom scripts and configs</code> 然后点击<code>应用本页面设置</code>，成功后重启路由器；</li>
<li>重启后先将路由器连上网络，然后进入软件中心将软件中心更新到最新版本（如果有）。</li>
</ol>
<h5 id="B-改版梅林-刷-改版梅林："><a href="#B-改版梅林-刷-改版梅林：" class="headerlink" title="B. 改版梅林 刷 改版梅林："></a>B. 改版梅林 刷 改版梅林：</h5><ol>
<li>刷过<code>改版梅林</code>固件的，在固件升级页面下直接上传.w 后缀的<code>改版梅林</code>文件（如无特殊说明，不需要恢复出产设置）；</li>
<li>刷机后所有已经安装的插件都会被保留，不会受到影响。</li>
</ol>
<h5 id="C-改版梅林-刷-官改固件："><a href="#C-改版梅林-刷-官改固件：" class="headerlink" title="C. 改版梅林 刷 官改固件："></a>C. 改版梅林 刷 官改固件：</h5><ul>
<li>详见：<a href="http://koolshare.cn/thread-139965-1-1.html" target="_blank" rel="noopener">http://koolshare.cn/thread-139965-1-1.html</a></li>
</ul>
<h5 id="D-官改固件-刷-改版梅林："><a href="#D-官改固件-刷-改版梅林：" class="headerlink" title="D. 官改固件 刷 改版梅林："></a>D. 官改固件 刷 改版梅林：</h5><ul>
<li>详见：<a href="http://koolshare.cn/thread-139965-1-1.html" target="_blank" rel="noopener">http://koolshare.cn/thread-139965-1-1.html</a></li>
</ul>
<h5 id="E-改版梅林-刷-原厂固件-梅林原版："><a href="#E-改版梅林-刷-原厂固件-梅林原版：" class="headerlink" title="E. 改版梅林 刷 原厂固件/梅林原版："></a>E. 改版梅林 刷 原厂固件/梅林原版：</h5><ol>
<li>在<code>改版梅林</code>固件升级页面下直接上传.w 后缀的<code>原产固件</code>/<code>梅林原版</code>固件文件，刷机完成后机器会自动重启（刷机后可以不恢复出产设置，当然恢复一次更好）；</li>
<li>刷机完成在【系统管理 】–【 系统设置】内勾选：<code>Format JFFS partition at next boot</code> 和 <code>Enable JFFS custom scripts and configs</code> 然后点击<code>应用本页面设置</code>，成功应用后重启路由器即可，此操作可以清除安装在jffs分区的软件中心和所有插件。</li>
</ol>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>路由器</tag>
      </tags>
  </entry>
  <entry>
    <title>一些百度网盘的破解方法</title>
    <url>/2020/02/02/%E4%B8%80%E4%BA%9B%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E7%9A%84%E7%A0%B4%E8%A7%A3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="1-网盘助手"><a href="#1-网盘助手" class="headerlink" title=" 1.网盘助手"></a><a id="more"></a> 1.网盘助手</h2><p>由网友”哩呵”制作的<a href="https://greasyfork.org/zh-CN/scripts/378301-%E7%BD%91%E7%9B%98%E5%8A%A9%E6%89%8B" target="_blank" rel="noopener">网盘助手</a>脚本，需要通过拓展 Violentmonkey 或者 Tampermonkey 来启用，原理是通过显示直链，然后使用 IDM 来加速下载<br><strong>使用方法：（下载部分）</strong><br>1、选择要下载的文件，点击页面里的 “生成链接” 来获取加速下载地址。<br>2、使用鼠标右键点击链接（注意是右键点击链接），选择“使用 IDM 下载”，然后确定下载。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/7a6a15d5gy1g8s99l1zdqg20qn0euqa8.gif" alt></p>
<p>也可以配合Aria2 manager扩展，添加右键菜单，将生成的链接“<strong>导出到 ARIA2 RPC</strong>”，由AriaNg下载<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/Snipaste_2020-02-02_20-12-56.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/Snipaste_2020-02-02_20-13-39.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/7a6a15d5gy1g952lk3yu0g20sf0fy469.gif" alt></p>
<h2 id="2-亿寻下载器"><a href="#2-亿寻下载器" class="headerlink" title="2.亿寻下载器"></a>2.亿寻下载器</h2><p>由52破解论坛网友 <a href="https://www.52pojie.cn/thread-959139-1-1.html" target="_blank" rel="noopener">kud</a> 开发，软件发布已经4月有余，下载速度和易用性不输其它工具，使用方法也很简单，可以免登陆下载，这里提供网盘下载：<a href="https://quqi.gblhgk.com/s/2796382/WN4on64IJNvzDDMZ" target="_blank" rel="noopener">下载地址</a><br><strong>使用方法：（下载部分）</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/%E5%88%86%E4%BA%AB%E4%B8%8B%E8%BD%BD%E6%BC%94%E7%A4%BA.gif" alt></p>
<p>注意：</p>
<ol>
<li>当使用 aria2 无法下载时，请尝试使用 saldl 下载。</li>
<li>当下载目录有与将要下载的文件同名的文件时，saldl 不会继续下载。</li>
</ol>
<h2 id="3-Aria2下载助手"><a href="#3-Aria2下载助手" class="headerlink" title="3.Aria2下载助手"></a>3.Aria2下载助手</h2><p>此脚本不仅能用aria下载百度网盘，还能下载b站视频，看b站港澳台，由于曾被人举报，已从脚本网站下架，需自己进入Tempermonkey（最好使用最新版本，该脚本不支持旧版本Tempermonkey）创建。<br>代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;UserScript&#x3D;&#x3D;</span><br><span class="line">&#x2F;&#x2F; @namespace zyxubing</span><br><span class="line">&#x2F;&#x2F; @name Aria2下载助手</span><br><span class="line">&#x2F;&#x2F; @version 0.3.16</span><br><span class="line">&#x2F;&#x2F; @downloadURL https:&#x2F;&#x2F;gitee.com&#x2F;zyxubing&#x2F;codes&#x2F;1ru485apywbeg3ni0lxq260&#x2F;raw?blob_name&#x3D;aria2</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;pan.baidu.com&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;yun.baidu.com&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;wallhaven.cc&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;av*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;www.bilibili.com&#x2F;bangumi&#x2F;play&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;live.bilibili.com&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;space.bilibili.com&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;search.bilibili.com&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;nyaa.fun&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;nyaa.si&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;sukebei.nyaa.si&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;nyaa.mlyx.workers.dev&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;acg.rip&#x2F;*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;konachan.com&#x2F;post*</span><br><span class="line">&#x2F;&#x2F; @include https:&#x2F;&#x2F;konachan.net&#x2F;post*</span><br><span class="line">&#x2F;&#x2F; @domain baidu.com</span><br><span class="line">&#x2F;&#x2F; @domain baidupcs.com</span><br><span class="line">&#x2F;&#x2F; @domain bilibili.com</span><br><span class="line">&#x2F;&#x2F; @domain 111.229.137.150</span><br><span class="line">&#x2F;&#x2F; @domain 127.0.0.1</span><br><span class="line">&#x2F;&#x2F; @domain localhost</span><br><span class="line">&#x2F;&#x2F; @domain self</span><br><span class="line">&#x2F;&#x2F; @grant GM_cookie</span><br><span class="line">&#x2F;&#x2F; @grant GM_setClipboard</span><br><span class="line">&#x2F;&#x2F; @grant GM_xmlhttpRequest</span><br><span class="line">&#x2F;&#x2F; @run-at document-idle</span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x2F;UserScript&#x3D;&#x3D;</span><br><span class="line">&#x2F;*</span><br><span class="line">切勿使用此脚本下载私密文件</span><br><span class="line">切勿使用此脚本下载私密文件</span><br><span class="line">切勿使用此脚本下载私密文件</span><br><span class="line"></span><br><span class="line">关于为何要获取用户的cookie信息说明</span><br><span class="line">获取cookie信息后可以不用登录直接进入相应账号的百度网盘。</span><br><span class="line">但这并非为了窥探用户的隐私，因为这样并不能给我带来什么好处。</span><br><span class="line"></span><br><span class="line">其实目的是为了实现资源整合利用，大致上可以理解为当你的账号闲置时可</span><br><span class="line">以为其他人提升下载速度。而当你使用此脚本下载度盘上的资源时，就算你</span><br><span class="line">的账号已经被百度官方拉黑限速，但仍然加速下载。</span><br><span class="line"></span><br><span class="line">如果使用svip账号则建议你使用官方的客户端或者其他工具进行下载。这个</span><br><span class="line">脚本主要是针对非会员账号设计的，对svip账号基本上并无提速效果。</span><br><span class="line"></span><br><span class="line">如果你不想使用这个脚本，可以修改百度网盘的账号密码，这样被拿的</span><br><span class="line">cookie信息就会失效。</span><br><span class="line"></span><br><span class="line">至于代码为何要进行混淆，当然是为了保护。源码裸奔实在太容易凉了，度</span><br><span class="line">盘的工作人员又不是只拿钱不做事的。对于普通用户而言应该没有人需要查</span><br><span class="line">看源代码，把所有细节都整明白弄清楚。</span><br><span class="line"></span><br><span class="line">0.3.16 修复哔哩哔哩批量下载时不能跳过已失效的视频</span><br><span class="line">*&#x2F;</span><br><span class="line">!function()&#123;const ipod&#x3D;&#123;&#125;,u&#x3D;&#123;aria2(e,t&#x3D;&quot;http:&#x2F;&#x2F;127.0.0.1:6800&#x2F;jsonrpc&quot;)&#123;console.log(t,&quot;| task &#x3D;&quot;,e.length);let o&#x3D;[],n&#x3D;&#123;id:u.uid(),method:&quot;system.multicall&quot;,params:[]&#125;;e.forEach(e&#x3D;&gt;&#123;let t&#x3D;&#123;&#125;,i&#x3D;&#123;methodName:&quot;aria2.addUri&quot;,params:[]&#125;;e.header&amp;&amp;(t[&quot;use-head&quot;]&#x3D;&quot;true&quot;,t.header&#x3D;e.header),t.split&#x3D;e.pi?e.pi:&quot;&quot;+e.url.length;e.dir&amp;&amp;(t.dir&#x3D;e.dir),t.out&#x3D;e.path?e.path.replace(&#x2F;\s+\&#x2F;\s+&#x2F;g,&quot;&#x2F;&quot;):e.extype?n.id+e.extype:&quot;&quot;;ipod.aria2.token&amp;&amp;i.params.push(&quot;token:&quot;+ipod.aria2.token),i.params.push(e.url);i.params.push(t),o.push(i)&#125;),n.params.push(o);GM_xmlhttpRequest(&#123;url:t,method:&quot;POST&quot;,data:JSON.stringify(n),onerror(e)&#123;console.log(&quot;error&quot;,e.finalUrl),alert(&quot;\u8fde\u63a5aria2\u5931\u8d25 \u8bf7\u68c0\u67e5aria2\u662f\u5426\u8fd0\u884c\u6216jsonrpc\u662f\u5426\u586b\u5199\u9519\u8bef&quot;)&#125;,onload(e)&#123;console.log(JSON.parse(e.responseText))&#125;&#125;)&#125;,now:()&#x3D;&gt;(new Date).getTime(),uid:()&#x3D;&gt;u.now().toString(36),zdom(e&#x3D;0)&#123;let t&#x3D;window.event;return t.preventDefault(),t.stopPropagation(),e?t.target:t.currentTarget&#125;,zform(e,t)&#123;document.querySelectorAll(e).forEach(e&#x3D;&gt;&#123;let o&#x3D;e.getAttribute(&quot;name&quot;);if(t.hasOwnProperty(o))switch(e.getAttribute(&quot;type&quot;))&#123;case&quot;radio&quot;:t[o].includes(e.value)&amp;&amp;(e.checked&#x3D;!0);break;case&quot;checkbox&quot;:t[o]&amp;&amp;(e.checked&#x3D;!0);break;default:e.value&#x3D;t[o]&#125;&#125;)&#125;,load(e)&#123;let t&#x3D;u.now(),o&#x3D;JSON.parse(localStorage.getItem(e));return Object.prototype.isPrototypeOf(o)&amp;&amp;(0&#x3D;&#x3D;o.expire||o.expir&gt;t)?o.data:null&#125;,save(e,t,o&#x3D;0)&#123;let n&#x3D;u.now();localStorage.setItem(e,JSON.stringify(&#123;data:t,expire:o?216e5*o+n:0&#125;))&#125;,serialize(e,t)&#123;let o,n,i&#x3D;[];switch(typeof e)&#123;case&quot;object&quot;:if(&quot;[object Array]&quot;&#x3D;&#x3D;(n&#x3D;Object.prototype.toString.call(e))||&quot;[object Object]&quot;&#x3D;&#x3D;n)for(o in e)Object.prototype.hasOwnProperty.call(e,o)&amp;&amp;i.push(u.serialize(e[o],t?t+&quot;[&quot;+o+&quot;]&quot;:o));return 0&#x3D;&#x3D;i.length?&quot;&quot;:i.join(&quot;&amp;&quot;);default:return t+&quot;&#x3D;&quot;+encodeURIComponent(&quot;&quot;+e)&#125;&#125;,strcut(e,t,o)&#123;let n,i,r&#x3D;&quot;&quot;;if(e&amp;&amp;e.includes(t))&#123;n&#x3D;e.indexOf(t)+t.length,-1&#x3D;&#x3D;(i&#x3D;e.indexOf(o,n))&amp;&amp;(i&#x3D;e.length);r&#x3D;e.substring(n,i)&#125;return r&#125;,sprintf(e)&#123;let t,o,n;if(o&#x3D;&quot;string&quot;&#x3D;&#x3D;typeof e?e:&quot;&quot;)for(t&#x3D;arguments.length-1;t&gt;0;t--)n&#x3D;RegExp(&quot;%&quot;+t,&quot;ig&quot;),o&#x3D;o.replace(n,arguments[t]);return o&#125;,download(e)&#123;if(e)&#123;let t&#x3D;[],o&#x3D;Object.assign(ipod.aria2);o.url&#x3D;[],e&#x3D;e.startsWith(&quot;magnet:&quot;)?u.magnet(e):e.startsWith(&quot;http&quot;)?e:e.startsWith(&quot;&#x2F;&#x2F;&quot;)?location.protocol+e:e.startsWith(&quot;&#x2F;&quot;)?location.origin+e:location.origin+&quot;&#x2F;&quot;+e;o.url.push(e),t.push(o);u.aria2(t,ipod.aria2.jsonrpc)&#125;&#125;,magnet(e)&#123;let t&#x3D;e.indexOf(&quot;&amp;&quot;);return-1&#x3D;&#x3D;t?e:e.substring(0,t)&#125;,history(e)&#123;const t&#x3D;history[e];return function()&#123;let o&#x3D;new Event(e);return o.arguments&#x3D;arguments,window.dispatchEvent(o),t.apply(this,arguments)&#125;&#125;&#125;;async function e(e)&#123;if(0&#x3D;&#x3D;ipod.task)&#123;ipod.list&#x3D;[],ipod.task&#x3D;setInterval(()&#x3D;&gt;&#123;if(ipod.len&#x3D;&#x3D;ipod.list.length)&#123;clearInterval(ipod.task),ipod.task&#x3D;0;&quot;video&quot;&#x3D;&#x3D;ipod.type&amp;&amp;ipod.aria2.cover&amp;&amp;y(1&#x3D;&#x3D;ipod.count&amp;&amp;1&#x3D;&#x3D;ipod.len?0:1),u.aria2(ipod.list,ipod.aria2.jsonrpc);ipod.adom&amp;&amp;ipod.adom.removeAttribute(&quot;style&quot;)&#125;&#125;,1e3);let t&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;player&#x2F;pagelist?aid&#x3D;&quot;+e,o&#x3D;await fetch(t).then(e&#x3D;&gt;e.json()).then(e&#x3D;&gt;0&#x3D;&#x3D;e.code?e.data[0].cid:0);if(0&#x3D;&#x3D;o)clearInterval(ipod.task),ipod.task&#x3D;0;else&#123;t&#x3D;u.sprintf(&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;web-interface&#x2F;view?aid&#x3D;%1&amp;cid&#x3D;%2&quot;,e,o);let n&#x3D;await fetch(t).then(e&#x3D;&gt;e.json()).then(t&#x3D;&gt;&#123;let o&#x3D;[];if(0&#x3D;&#x3D;t.code)&#123;o&#x3D;t.data.pages,ipod.type&#x3D;&quot;video&quot;;ipod.url&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;player&#x2F;playurl?avid&#x3D;%1&amp;cid&#x3D;%2&amp;qn&#x3D;116&quot;,ipod.aid&#x3D;e;ipod.title&#x3D;t.data.title,ipod.cover&#x3D;t.data.pic&#125;return o&#125;);ipod.count&#x3D;ipod.len&#x3D;n.length,g(n)&#125;&#125;&#125;async function t(e)&#123;let t,o,n&#x3D;&quot;https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;av&quot;+e.aid,i&#x3D;u.strcut(document.cookie,&quot;bili_jct&#x3D;&quot;,&quot;;&quot;),r&#x3D;&#123;&quot;Content-Type&quot;:&quot;application&#x2F;x-www-form-urlencoded; charset&#x3D;UTF-8&quot;&#125;;o&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;report&#x2F;click&#x2F;now&quot;,t&#x3D;await fetch(o,&#123;referrer:n,method:&quot;GET&quot;&#125;).then(e&#x3D;&gt;e.json()).then(e&#x3D;&gt;e.data.now);o&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;report&#x2F;click&#x2F;web&#x2F;h5&quot;,await fetch(o,&#123;headers:r,referrer:n,body:u.serialize(&#123;aid:e.aid,cid:e.cid,part:e.part,mid:e.mid,lv:0,ftime:e.created,stime:t,jsonp:&quot;jsonp&quot;,type:3,sub_type:0&#125;),method:&quot;POST&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;);o&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;report&#x2F;web&#x2F;heartbeat&quot;,await fetch(o,&#123;headers:r,referrer:n,body:u.serialize(&#123;aid:e.aid,cid:e.cid,mid:e.mid,csrf:i,played_time:0,realtime:0,start_ts:t,type:3,dt:2,play_type:1&#125;),method:&quot;POST&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;);await fetch(o,&#123;headers:r,referrer:n,body:u.serialize(&#123;aid:e.aid,cid:e.cid,mid:e.mid,csrf:i,played_time:e.duration,realtime:e.duration,start_ts:t,type:3,dt:2,play_type:0&#125;),method:&quot;POST&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;),await fetch(o,&#123;headers:r,referrer:n,body:u.serialize(&#123;aid:e.aid,cid:e.cid,mid:e.mid,csrf:i,played_time:-1,realtime:e.len,start_ts:t,type:3,dt:2,play_type:4&#125;),method:&quot;POST&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;);o&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;v2&#x2F;history&#x2F;delete&quot;,await fetch(o,&#123;headers:r,referrer:&quot;https:&#x2F;&#x2F;www.bilibili.com&#x2F;account&#x2F;history&quot;,body:u.serialize(&#123;kid:&quot;archive_&quot;+e.aid,jsonp:&quot;jsonp&quot;,csrf:i&#125;),method:&quot;POST&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;);2&gt;Math.floor(10*e.like&#x2F;e.view)&amp;&amp;(o&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;web-interface&#x2F;archive&#x2F;has&#x2F;like?aid&#x3D;&quot;+e.aid,await fetch(o,&#123;referrer:n,method:&quot;GET&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;).then(e&#x3D;&gt;e.json()).then(t&#x3D;&gt;&#123;0&#x3D;&#x3D;t.code&amp;&amp;0&#x3D;&#x3D;t.data&amp;&amp;fetch(&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;web-interface&#x2F;archive&#x2F;like&quot;,&#123;headers:r,referrer:n,body:u.serialize(&#123;aid:e.aid,like:1,csrf:i&#125;),method:&quot;POST&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;)&#125;))&#125;function o()&#123;setTimeout(n,2e3)&#125;function n()&#123;let e,t;document.querySelector(&quot;ul.nav.navbar-nav&quot;).insertAdjacentHTML(&quot;afterbegin&quot;,&#39;&lt;li&gt;&lt;a id&#x3D;&quot;czyset&quot;&gt;Aria2\u8bbe\u7f6e&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&#39;),document.querySelector(&quot;#czyset&quot;).addEventListener(&quot;click&quot;,()&#x3D;&gt;&#123;u.zdom(),A()&#125;,!1);(t&#x3D;document.querySelector(&quot;#bangumi-box&quot;))&amp;&amp;t.remove(),(t&#x3D;document.querySelector(&quot;div.footer&quot;))&amp;&amp;t.remove();(e&#x3D;document.querySelectorAll(&quot;i.fa.fa-download&quot;)).forEach(e&#x3D;&gt;&#123;e.addEventListener(&quot;click&quot;,()&#x3D;&gt;&#123;let e&#x3D;u.zdom(),t&#x3D;&#123;dir:ipod.aria2.dir,pi:&quot;1&quot;,url:[]&#125;;t.url.push(location.origin+e.parentElement.getAttribute(&quot;href&quot;)),u.aria2([t],ipod.aria2.jsonrpc)&#125;)&#125;)&#125;function i()&#123;let e&#x3D;[],t&#x3D;u.zdom(),o&#x3D;&#123;dir:ipod.aria2.dir,pi:1,url:[]&#125;;console.clear(),o.url.push(u.magnet(t.getAttribute(&quot;data-url&quot;)));e.push(o),u.aria2(e,ipod.aria2.jsonrpc)&#125;function r()&#123;return u.strcut(decodeURIComponent(location.hash),&quot;path&#x3D;&quot;,&quot;&amp;&quot;)&#125;function a(e)&#123;let t&#x3D;[],o&#x3D;&#123;timestamp:yunData.timestamp,uk:yunData.MYUK,dir:e&#125;;return e&amp;&amp;$.ajax(&#123;async:!1,method:&quot;GET&quot;,dataType:&quot;json&quot;,url:location.origin+&quot;&#x2F;api&#x2F;list&quot;,data:o,success(e)&#123;t&#x3D;0&#x3D;&#x3D;e.errno?e.list:[]&#125;&#125;),t&#125;function c()&#123;ipod.surl.length&amp;&amp;$.ajax(&#123;async:!1,url:&quot;https:&#x2F;&#x2F;pan.baidu.com&#x2F;share&#x2F;cancel&quot;,method:&quot;POST&quot;,data:&quot;shareid_list&#x3D;%5B&quot;+ipod.shareid+&quot;%5D&quot;&#125;)&#125;function l()&#123;let e,t&#x3D;0,o&#x3D;[&quot;User-Agent: netdisk&quot;,&quot;Cookie: &quot;+ipod.cookie];function n()&#123;u.aria2(e,ipod.aria2.jsonrpc),ipod.btn.setAttribute(&quot;class&quot;,&quot;ion-download&quot;)&#125;e&#x3D;(()&#x3D;&gt;&#123;let e&#x3D;[],t&#x3D;a(r());return console.clear(),console.log(&quot;fileList &#x3D;&quot;,t.length),$(&quot;dd.g-clearfix&quot;).each((o,n)&#x3D;&gt;&#123;if(3&#x3D;&#x3D;n.getAttribute(&quot;class&quot;).replace(&quot; open-enable&quot;,&quot;&quot;).trim().split(&quot; &quot;).length)&#123;let o&#x3D;decodeURIComponent($(n).find(&quot;a&quot;).first().text());t.forEach(t&#x3D;&gt;&#123;o&#x3D;&#x3D;t.server_filename&amp;&amp;(t.isdir?e&#x3D;e.concat(function e(t)&#123;let o,n&#x3D;&#123;timestamp:yunData.timestamp,uk:yunData.MYUK,dir:t&#125;;return $.ajax(&#123;async:!1,method:&quot;GET&quot;,dataType:&quot;json&quot;,url:location.origin+&quot;&#x2F;api&#x2F;list&quot;,data:n,success(t)&#123;(o&#x3D;0&#x3D;&#x3D;t.errno?t.list:[]).forEach(t&#x3D;&gt;&#123;t.isdir&amp;&amp;(o&#x3D;o.concat(e(t.path)))&#125;)&#125;&#125;),o&#125;(t.path)):e.push(t))&#125;)&#125;&#125;),t&#x3D;[],e.forEach(e&#x3D;&gt;&#123;if(!e.isdir)&#123;void 0&#x3D;&#x3D;e.thumbs||delete e.thumbs,e.url&#x3D;[];e.pi&#x3D;&quot;16&quot;,t.push(e)&#125;&#125;),console.log(&quot;selectedList &#x3D;&quot;,t.length),t&#125;)().map(e&#x3D;&gt;(e.header&#x3D;o,e.pi&#x3D;&quot;16&quot;,e.url.push(&quot;http:&#x2F;&#x2F;pcs.baidu.com&#x2F;rest&#x2F;2.0&#x2F;pcs&#x2F;file?app_id&#x3D;250528&amp;method&#x3D;download&amp;path&#x3D;&quot;+encodeURIComponent(e.path)),e)),GM_xmlhttpRequest(&#123;url:&quot;https:&#x2F;&#x2F;pan.baidu.com&#x2F;rest&#x2F;2.0&#x2F;xpan&#x2F;nas?method&#x3D;uinfo&quot;,method:&quot;GET&quot;,responseType:&quot;json&quot;,error:n,onload(o)&#123;let i&#x3D;o.response;0&#x3D;&#x3D;i.errno?2&#x3D;&#x3D;i.vip_type?n():6&gt;e.length?function o()&#123;if(t&#x3D;&#x3D;e.length)n();else&#123;let n&#x3D;e[t];(e&#x3D;&gt;&#123;let t&#x3D;&#123;schannel:0,channel_list:&quot;[]&quot;,period:1,fid_list:JSON.stringify(e)&#125;;ipod.shareid&#x3D;0,ipod.surl&#x3D;&quot;&quot;;e.length&amp;&amp;$.ajax(&#123;async:!1,url:&quot;https:&#x2F;&#x2F;pan.baidu.com&#x2F;share&#x2F;set&quot;,method:&quot;POST&quot;,data:u.serialize(t),success(e)&#123;0&#x3D;&#x3D;e.errno&amp;&amp;(ipod.shareid&#x3D;e.shareid,ipod.surl&#x3D;e.link.substring(25))&#125;&#125;)&#125;)([n.fs_id]),ipod.surl?GM_xmlhttpRequest(&#123;method:&quot;POST&quot;,url:&quot;http:&#x2F;&#x2F;111.229.137.150&#x2F;img&#x2F;baiduyun.php&quot;,data:u.serialize(&#123;ctime:&quot;20200112&quot;,mode:ipod.mode,cookie:ipod.cookie,fid:n.fs_id,file:&quot;&#x2F;&quot;+n.server_filename,shareid:ipod.shareid,surl:ipod.surl,uid:yunData.MYUK&#125;),headers:&#123;&quot;Content-type&quot;:&quot;application&#x2F;x-www-form-urlencoded; charset&#x3D;UTF-8&quot;&#125;,error()&#123;console.log(&quot;error link &#x3D;&quot;,++t),c();e.length&gt;t&amp;&amp;o()&#125;,onload(e)&#123;console.log(&quot;success link &#x3D;&quot;,++t);let i&#x3D;JSON.parse(e.responseText);0&#x3D;&#x3D;i.error&amp;&amp;(n.header&#x3D;ipod.header,n.url&#x3D;i.url.slice()),c();o()&#125;&#125;):(console.log(&quot;fail &#x3D;&quot;,++t),o())&#125;&#125;():(ipod.btn.setAttribute(&quot;class&quot;,&quot;ion-download&quot;),alert(&quot;\u52fe\u9009\u7684\u6587\u4ef6\u6570\u91cf\u8fc7\u591a \u8bf7\u5206\u6279\u6b21\u4e0b\u8f7d&quot;)):n()&#125;&#125;)&#125;function f()&#123;let e&#x3D;!0,o&#x3D;Math.floor(u.now()&#x2F;1e3);if(ipod.vlistlen&#x3D;0,ipod.vlist&#x3D;[],&quot;www.bilibili.com&quot;&#x3D;&#x3D;location.host)&#123;e&#x3D;!1,location.pathname.startsWith(&quot;&#x2F;video&#x2F;&quot;)&amp;&amp;(e&#x3D;!0);location.pathname.startsWith(&quot;&#x2F;bangumi&#x2F;play&#x2F;&quot;)&amp;&amp;(e&#x3D;!0)&#125;if(e&amp;&amp;o&gt;parseInt(localStorage.getItem(&quot;zlog&quot;)||0,10))&#123;localStorage.setItem(&quot;zlog&quot;,o+1800);let e&#x3D;setInterval(()&#x3D;&gt;&#123;ipod.vlistlen&#x3D;&#x3D;ipod.vlist.length&amp;&amp;(clearInterval(e),ipod.vlist.length&amp;&amp;ipod.vlist.forEach(t))&#125;,2e3);(()&#x3D;&gt;&#123;let e&#x3D;9,t&#x3D;0,o&#x3D;[],n&#x3D;setInterval(()&#x3D;&gt;&#123;if(e&#x3D;&#x3D;t)&#123;clearInterval(n),ipod.vlistlen&#x3D;o.length;o.forEach(s)&#125;&#125;,1e3);GM_xmlhttpRequest(&#123;url:&quot;http:&#x2F;&#x2F;111.229.137.150&#x2F;img&#x2F;bluid.php&quot;,method:&quot;GET&quot;,responseType:&quot;json&quot;,onload(n)&#123;let i&#x3D;n.response;e&#x3D;i.list.length,i.list.forEach(e&#x3D;&gt;&#123;GM_xmlhttpRequest(&#123;url:u.sprintf(&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;space&#x2F;arc&#x2F;search?mid&#x3D;%1&amp;ps&#x3D;30&amp;tid&#x3D;0&amp;pn&#x3D;1&amp;keyword&#x3D;&amp;order&#x3D;pubdate&quot;,e),method:&quot;GET&quot;,responseType:&quot;json&quot;,onload(e)&#123;t++,o&#x3D;o.concat(e.response.data.list.vlist)&#125;&#125;)&#125;)&#125;&#125;)&#125;)()&#125;&#125;function s(e)&#123;let t,o;GM_xmlhttpRequest(&#123;url:&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;web-interface&#x2F;archive&#x2F;stat?aid&#x3D;&quot;+e.aid,method:&quot;GET&quot;,responseType:&quot;json&quot;,onload(n)&#123;let i,r&#x3D;n.response;0&#x3D;&#x3D;r.code?(o&#x3D;&#123;aid:e.aid,created:e.created,mid:e.mid,title:e.title,view:(i&#x3D;r.data).view,danmaku:i.danmaku,favorite:i.favorite,coin:i.coin,share:i.share,like:i.like&#125;,GM_xmlhttpRequest(&#123;url:&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;player&#x2F;pagelist?aid&#x3D;&quot;+e.aid,method:&quot;GET&quot;,responseType:&quot;json&quot;,onload(e)&#123;let n,i&#x3D;e.response;if(0&#x3D;&#x3D;i.code)&#123;(t&#x3D;15*Math.floor((n&#x3D;i.data[0]).duration&#x2F;15))&#x3D;&#x3D;n.duration&amp;&amp;(t-&#x3D;15),o.duration&#x3D;t;o.cid&#x3D;n.cid,o.len&#x3D;n.duration;o.part&#x3D;n.part,ipod.vlist.push(o)&#125;else ipod.vlistlen--&#125;&#125;)):ipod.vlistlen--&#125;&#125;)&#125;function d()&#123;ipod.task&#x3D;setInterval(()&#x3D;&gt;&#123;if(&quot;function&quot;&#x3D;&#x3D;typeof jQuery&amp;&amp;document.querySelector(&quot;ul.video-list&quot;))&#123;clearInterval(ipod.task),ipod.task&#x3D;0;$(&quot;ul.video-list&quot;).on(&quot;click&quot;,()&#x3D;&gt;&#123;if(window.event.altKey)&#123;ipod.adom&#x3D;u.zdom(1);while(&quot;li&quot;!&#x3D;ipod.adom.tagName.toLowerCase())ipod.adom&#x3D;ipod.adom.parentElement;ipod.adom.setAttribute(&quot;style&quot;,&quot;visibility: hidden&quot;),e(ipod.adom.children[0].getAttribute(&quot;href&quot;).match(&#x2F;av(\d+)&#x2F;)[1])&#125;&#125;)&#125;&#125;,1e3)&#125;function b()&#123;setTimeout(()&#x3D;&gt;&#123;location.pathname.endsWith(&quot;&#x2F;favlist&quot;)&amp;&amp;(ipod.task&#x3D;setInterval(()&#x3D;&gt;&#123;if(document.querySelector(&quot;ul.fav-video-list&quot;)&amp;&amp;(clearInterval(ipod.task),ipod.task&#x3D;0,!document.querySelector(&quot;#zydl&quot;)))&#123;$(&quot;div.fav-filters&quot;).prepend(&#39;&lt;div class&#x3D;&quot;filter-item&quot;&gt;&lt;span id&#x3D;&quot;zydl&quot; class&#x3D;&quot;text&quot;&gt;&lt;i class&#x3D;&quot;ion-download&quot;&gt;&lt;&#x2F;i&gt; \u5168\u90e8\u4e0b\u8f7d&lt;&#x2F;span&gt;&lt;&#x2F;div&gt; &lt;div class&#x3D;&quot;filter-item&quot;&gt;&lt;span id&#x3D;&quot;czyset&quot; class&#x3D;&quot;text&quot;&gt;&lt;i class&#x3D;&quot;ion-settings&quot;&gt;&lt;&#x2F;i&gt; Aria2\u8bbe\u7f6e&lt;&#x2F;span&gt;&lt;&#x2F;div&gt;&#39;),$(&quot;#zydl&quot;).on(&quot;click&quot;,v);$(&quot;#czyset&quot;).on(&quot;click&quot;,A),$(&quot;ul.fav-video-list&quot;).on(&quot;click&quot;,w)&#125;&#125;,1e3)),location.pathname.endsWith(&quot;&#x2F;video&quot;)&amp;&amp;(ipod.task&#x3D;setInterval(()&#x3D;&gt;&#123;if(document.querySelector(&quot;ul.cube-list&quot;)&amp;&amp;(clearInterval(ipod.task),ipod.task&#x3D;0,!document.querySelector(&quot;#zydl&quot;)))&#123;$(&quot;ul.be-tab-inner&quot;).append(&#39;&lt;li id&#x3D;&quot;zydl&quot; class&#x3D;&quot;be-tab-item&quot;&gt;&lt;span&gt;&lt;i class&#x3D;&quot;ion-download&quot;&gt;&lt;&#x2F;i&gt; \u5168\u90e8\u4e0b\u8f7d&lt;&#x2F;span&gt;&lt;&#x2F;li&gt; &lt;li id&#x3D;&quot;czyset&quot; class&#x3D;&quot;be-tab-item&quot;&gt;&lt;span&gt;&lt;i class&#x3D;&quot;ion-settings&quot;&gt;&lt;&#x2F;i&gt; Aria2\u8bbe\u7f6e&lt;&#x2F;span&gt;&lt;&#x2F;li&gt;&#39;),$(&quot;#zydl&quot;).on(&quot;click&quot;,v);$(&quot;#czyset&quot;).on(&quot;click&quot;,A),$(&quot;ul.cube-list&quot;).on(&quot;click&quot;,w)&#125;&#125;,1e3))&#125;,1e3)&#125;function p()&#123;let e&#x3D;setInterval(()&#x3D;&gt;&#123;let t&#x3D;document.querySelector(&quot;input.bui-checkbox&quot;);t&amp;&amp;(clearInterval(e),t.checked&amp;&amp;t.click())&#125;,3e3)&#125;function m()&#123;fetch(u.sprintf(ipod.url,ipod.aid,ipod.cid)).then(e&#x3D;&gt;e.json()).then(e&#x3D;&gt;&#123;if(0&#x3D;&#x3D;e.code)&#123;let t,o&#x3D;[];f(),e.durl.forEach(e&#x3D;&gt;&#123;t&#x3D;&#123;duration:e.length,filesize:e.size,url:e.url.replace(&quot;http:&quot;,&quot;https:&quot;)&#125;,o.push(t)&#125;);((e,t)&#x3D;&gt;&#123;if(flvjs.isSupported())&#123;if(j)&#123;j.unload(),j.detachMediaElement();j.destroy()&#125;if(j&#x3D;flvjs.createPlayer(&#123;cors:!0,type:e,segments:t&#125;))&#123;j.attachMediaElement(document.querySelector(&quot;#bplayer&quot;)),j.load();j.play()&#125;&#125;&#125;)(e.format,o)&#125;else alert(&quot;\u4ee3\u7406\u670d\u52a1\u5668\u4e0a\u6ca1\u6709\u5927\u4f1a\u5458\u8d26\u53f7\u8fdb\u884c\u89e3\u6790\u64ad\u653e\u5730\u5740&quot;)&#125;)&#125;function h()&#123;let e&#x3D;u.zdom();ipod.aid&#x3D;e.getAttribute(&quot;data-aid&quot;),ipod.cid&#x3D;e.getAttribute(&quot;data-cid&quot;);m()&#125;function y(e)&#123;ipod.len++;let t&#x3D;&#123;dir:ipod.aria2.video,folder:e?ipod.aid:&quot;&quot;,file:ipod.aid+&quot;.jpg&quot;,url:[ipod.cover]&#125;;t.path&#x3D;t.folder?t.folder+&quot;&#x2F;&quot;+t.file:t.file,ipod.list.push(t)&#125;function v()&#123;console.clear();let t&#x3D;document.querySelector(&quot;ul.cube-list&quot;)?document.querySelectorAll(&quot;ul.cube-list&gt;li&quot;):document.querySelector(&quot;ul.fav-video-list&quot;)?document.querySelectorAll(&quot;ul.fav-video-list&gt;li&quot;):[],o&#x3D;t.length,n&#x3D;0,i&#x3D;setInterval(()&#x3D;&gt;&#123;n&#x3D;&#x3D;o?(clearInterval(i),document.querySelector(&quot;#zydl&gt;i&quot;).setAttribute(&quot;class&quot;,&quot;ion-download&quot;)):0&#x3D;&#x3D;ipod.task&amp;&amp;(e(t[n].getAttribute(&quot;data-aid&quot;)),n++)&#125;,2e3);document.querySelector(&quot;#zydl&gt;i&quot;).setAttribute(&quot;class&quot;,&quot;ion-refresh spinner&quot;)&#125;function g(e)&#123;e.forEach(e&#x3D;&gt;&#123;fetch(u.sprintf(ipod.url,ipod.aid,e.cid),&#123;method:&quot;GET&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;).then(e&#x3D;&gt;e.json()).then(t&#x3D;&gt;&#123;if(0&#x3D;&#x3D;t.code)&#123;let o&#x3D;t.data.durl,n&#x3D;o.length;ipod.len+&#x3D;n-1,o.forEach(t&#x3D;&gt;&#123;let o&#x3D;&#123;url:[],pi:&quot;8&quot;,header:ipod.header,dir:ipod.aria2.video&#125;;Array.isArray(t.backup_url)&amp;&amp;(o.url&#x3D;t.backup_url),o.url.push(t.url);o.folder&#x3D;1&#x3D;&#x3D;ipod.count&amp;&amp;1&#x3D;&#x3D;n?&quot;&quot;:ipod.aid,o.file&#x3D;1&#x3D;&#x3D;ipod.count?1&#x3D;&#x3D;n?ipod.aid+&quot;.flv&quot;:z(t.order)+&quot;.flv&quot;:1&#x3D;&#x3D;n?z(e.page)+&quot; &quot;+e.part+&quot;.flv&quot;:z(e.page)+&quot; &quot;+e.part+&quot; &quot;+z(t.order)+&quot;.flv&quot;;o.path&#x3D;o.folder?o.folder+&quot;&#x2F;&quot;+o.file:o.file,ipod.list.push(o)&#125;)&#125;else ipod.len--&#125;)&#125;)&#125;function w()&#123;if(window.event.altKey)&#123;ipod.adom&#x3D;u.zdom(1);while(&quot;li&quot;!&#x3D;ipod.adom.tagName.toLowerCase())ipod.adom&#x3D;ipod.adom.parentElement;ipod.adom.setAttribute(&quot;style&quot;,&quot;visibility: hidden&quot;),e(ipod.adom.getAttribute(&quot;data-aid&quot;))&#125;&#125;function k(e)&#123;e.forEach(e&#x3D;&gt;&#123;fetch(u.sprintf(ipod.url,ipod.aid,e.cid),&#123;method:&quot;GET&quot;,mode:&quot;cors&quot;,credentials:&quot;include&quot;&#125;).then(e&#x3D;&gt;e.json()).then(t&#x3D;&gt;&#123;if(0&#x3D;&#x3D;t.code)&#123;let o&#x3D;ipod.area?t.durl:t.result.durl,n&#x3D;o.length;ipod.len+&#x3D;n-1,o.forEach(t&#x3D;&gt;&#123;let o&#x3D;&#123;url:[],header:ipod.header,dir:ipod.aria2.anime&#125;;Array.isArray(t.backup_url)&amp;&amp;(o.url&#x3D;t.backup_url),o.url.push(t.url);o.folder&#x3D;1&#x3D;&#x3D;ipod.count&amp;&amp;1&#x3D;&#x3D;n?&quot;&quot;:ipod.title,o.file&#x3D;1&#x3D;&#x3D;ipod.count?1&#x3D;&#x3D;n?ipod.title+&quot;.flv&quot;:z(t.order)+&quot;.flv&quot;:1&#x3D;&#x3D;n?z(e.title)+&quot; &quot;+e.longTitle+&quot;.flv&quot;:z(e.title)+&quot; &quot;+e.longTitle+&quot; &quot;+z(t.order)+&quot;.flv&quot;;o.path&#x3D;o.folder?o.folder+&quot;&#x2F;&quot;+o.file:o.file,ipod.list.push(o)&#125;)&#125;else ipod.len--&#125;)&#125;)&#125;function x()&#123;$(&quot;#zylist li&quot;).on(&quot;click&quot;,()&#x3D;&gt;&#123;let e&#x3D;u.zdom();$(e).toggleClass(&quot;on&quot;)&#125;),$(&quot;#zylist button&quot;).on(&quot;click&quot;,()&#x3D;&gt;&#123;switch(u.zdom().getAttribute(&quot;name&quot;))&#123;case&quot;all&quot;:$(&quot;#zylist li&quot;).each((e,t)&#x3D;&gt;&#123;$(t).addClass(&quot;on&quot;)&#125;);break;case&quot;invert&quot;:$(&quot;#zylist li&quot;).each((e,t)&#x3D;&gt;&#123;$(t).toggleClass(&quot;on&quot;)&#125;);break;default:let e&#x3D;[],t&#x3D;[],o&#x3D;__INITIAL_STATE__;$(&quot;#zylist li&quot;).each((t,o)&#x3D;&gt;&#123;let n&#x3D;$(o);n.hasClass(&quot;on&quot;)&amp;&amp;e.push(n.attr(&quot;name&quot;))&#125;),ipod.len&#x3D;e.length;ipod.list&#x3D;[],$(&quot;#zylist&quot;).fadeOut();&quot;video&quot;&#x3D;&#x3D;ipod.type&amp;&amp;(o.videoData.pages.forEach(o&#x3D;&gt;&#123;e.includes(&quot;&quot;+o.cid)&amp;&amp;t.push(o)&#125;),g(t)),&quot;anime&quot;&#x3D;&#x3D;ipod.type&amp;&amp;(o.epList.forEach(o&#x3D;&gt;&#123;e.includes(&quot;&quot;+o.cid)&amp;&amp;t.push(o)&#125;),k(t))&#125;&#125;)&#125;function _()&#123;let e,t&#x3D;u.zdom();if(0&#x3D;&#x3D;ipod.task)&#123;if(e&#x3D;__INITIAL_STATE__,document.querySelector(&quot;#zydl&gt;i&quot;).setAttribute(&quot;style&quot;,&quot;color: #fb7299&quot;),ipod.list&#x3D;[],ipod.task&#x3D;setInterval(()&#x3D;&gt;&#123;if(ipod.len&#x3D;&#x3D;ipod.list.length)if(clearInterval(ipod.task),ipod.task&#x3D;0,document.querySelector(&quot;#zydl&gt;i&quot;).removeAttribute(&quot;style&quot;),&quot;video&quot;&#x3D;&#x3D;ipod.type&amp;&amp;ipod.aria2.cover&amp;&amp;y(1&#x3D;&#x3D;ipod.count&amp;&amp;1&#x3D;&#x3D;ipod.len?0:1),ipod.aria2.getlink)&#123;let e&#x3D;[];ipod.list.forEach(t&#x3D;&gt;&#123;e.push(t.url[0])&#125;),GM_setClipboard(e.join(&quot;\r\n&quot;),&quot;text&quot;)&#125;else u.aria2(ipod.list,ipod.aria2.jsonrpc)&#125;,1e3),e.videoData)if(ipod.type&#x3D;&quot;video&quot;,ipod.url&#x3D;&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;player&#x2F;playurl?avid&#x3D;%1&amp;cid&#x3D;%2&amp;qn&#x3D;116&quot;,ipod.aid&#x3D;e.videoData.aid,ipod.title&#x3D;e.videoData.title,ipod.cover&#x3D;e.videoData.pic,ipod.count&#x3D;ipod.len&#x3D;e.videoData.pages.length,1&#x3D;&#x3D;ipod.count)g(e.videoData.pages);else&#123;let o&#x3D;&#39;&lt;div&gt; &lt;div class&#x3D;&quot;btn-group full&quot;&gt; &lt;button name&#x3D;&quot;all&quot;&gt; \u5168\u9009 &lt;&#x2F;button&gt; &lt;button name&#x3D;&quot;invert&quot;&gt; \u53cd\u9009 &lt;&#x2F;button&gt; &lt;button name&#x3D;&quot;download&quot;&gt; &lt;i class&#x3D;&quot;ion-download&quot;&gt;&lt;&#x2F;i&gt; \u4e0b\u8f7d &lt;&#x2F;button&gt; &lt;&#x2F;div&gt; &lt;ul&gt;&#39;;if(t&#x3D;document.querySelector(&quot;#zylist&quot;))t.setAttribute(&quot;style&quot;,&quot;display: flex&quot;);else&#123;t&#x3D;document.createElement(&quot;div&quot;),document.body.insertAdjacentElement(&quot;beforeend&quot;,t);t.setAttribute(&quot;class&quot;,&quot;tamper&quot;),t.setAttribute(&quot;id&quot;,&quot;zylist&quot;)&#125;e.videoData.pages.forEach(e&#x3D;&gt;&#123;o+&#x3D;u.sprintf(&#39;&lt;li name&#x3D;&quot;%1&quot;&gt;%2&lt;&#x2F;li&gt;&#39;,e.cid,z(e.page)+&quot; &quot;+e.part)&#125;),t.innerHTML&#x3D;o+&quot;&lt;&#x2F;ul&gt; &lt;&#x2F;div&gt;&quot;;x()&#125;if(e.mediaInfo)if(ipod.type&#x3D;&quot;anime&quot;,ipod.area&#x3D;__PGC_USERSTATE__.area_limit,ipod.url&#x3D;ipod.area?&quot;https:&#x2F;&#x2F;www.biliplus.com&#x2F;BPplayurl.php?aid&#x3D;%1&amp;cid&#x3D;%2&amp;qn&#x3D;116&amp;otype&#x3D;json&amp;module&#x3D;bangumi&quot;:&quot;https:&#x2F;&#x2F;api.bilibili.com&#x2F;pgc&#x2F;player&#x2F;web&#x2F;playurl?avid&#x3D;%1&amp;cid&#x3D;%2&amp;qn&#x3D;116&quot;,ipod.title&#x3D;e.mediaInfo.title,ipod.cover&#x3D;e.mediaInfo.cover,ipod.count&#x3D;ipod.len&#x3D;e.epList.length,1&#x3D;&#x3D;ipod.count)k(e.epList);else&#123;let o&#x3D;&#39;&lt;div&gt; &lt;div class&#x3D;&quot;btn-group full&quot;&gt; &lt;button name&#x3D;&quot;settings&quot;&gt; &lt;i class&#x3D;&quot;ion-settings&quot;&gt;&lt;&#x2F;i&gt; \u8bbe\u7f6e &lt;&#x2F;button&gt; &lt;button name&#x3D;&quot;all&quot;&gt; \u5168\u9009 &lt;&#x2F;button&gt; &lt;button name&#x3D;&quot;invert&quot;&gt; \u53cd\u9009 &lt;&#x2F;button&gt; &lt;button name&#x3D;&quot;download&quot;&gt; &lt;i class&#x3D;&quot;ion-download&quot;&gt;&lt;&#x2F;i&gt; \u4e0b\u8f7d &lt;&#x2F;button&gt; &lt;&#x2F;div&gt; &lt;ul&gt;&#39;;if(t&#x3D;document.querySelector(&quot;#zylist&quot;))t.setAttribute(&quot;style&quot;,&quot;display: flex&quot;);else&#123;t&#x3D;document.createElement(&quot;div&quot;),document.body.insertAdjacentElement(&quot;beforeend&quot;,t);t.setAttribute(&quot;class&quot;,&quot;tamper&quot;),t.setAttribute(&quot;id&quot;,&quot;zylist&quot;)&#125;e.epList.forEach(e&#x3D;&gt;&#123;o+&#x3D;u.sprintf(&#39;&lt;li name&#x3D;&quot;%1&quot;&gt;%2&lt;&#x2F;li&gt;&#39;,e.cid,z(e.title)+&quot; &quot;+e.longTitle)&#125;),t.innerHTML&#x3D;o+&quot;&lt;&#x2F;ul&gt; &lt;&#x2F;div&gt;&quot;;x()&#125;&#125;&#125;function z(e)&#123;let t&#x3D;+e;return isNaN(t)&amp;&amp;(t&#x3D;0),(t&#x3D;&quot;000&quot;+t).substring(t.length-4)&#125;function S()&#123;document.head.insertAdjacentHTML(&quot;beforeend&quot;,&#39;&lt;style type&#x3D;&quot;text&#x2F;css&quot;&gt;@font-face&#123;font-family:&quot;Ionicons&quot;;src:url(&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;ionicons&#x2F;4.5.6&#x2F;fonts&#x2F;ionicons.eot?v&#x3D;4.5.5#iefix&quot;) format(&quot;embedded-opentype&quot;),url(&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;ionicons&#x2F;4.5.6&#x2F;fonts&#x2F;ionicons.woff2?v&#x3D;4.5.5&quot;) format(&quot;woff2&quot;),url(&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;ionicons&#x2F;4.5.6&#x2F;fonts&#x2F;ionicons.woff?v&#x3D;4.5.5&quot;) format(&quot;woff&quot;),url(&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;ionicons&#x2F;4.5.6&#x2F;fonts&#x2F;ionicons.ttf?v&#x3D;4.5.5&quot;) format(&quot;truetype&quot;),url(&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;ionicons&#x2F;4.5.6&#x2F;fonts&#x2F;ionicons.svg?v&#x3D;4.5.5#Ionicons&quot;) format(&quot;svg&quot;);font-weight:normal;font-style:normal&#125;i[class|&#x3D;ion]&#123;display:inline-block;font-family:&quot;Ionicons&quot;;font-size:120%;font-style:normal;font-variant:normal;font-weight:normal;line-height:1;text-rendering:auto;text-transform:none;vertical-align:text-bottom;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased&#125;.ion-android:before&#123;content:&quot;\\f225&quot;&#125;.ion-angular:before&#123;content:&quot;\\f227&quot;&#125;.ion-apple:before&#123;content:&quot;\\f229&quot;&#125;.ion-bitbucket:before&#123;content:&quot;\\f193&quot;&#125;.ion-bitcoin:before&#123;content:&quot;\\f22b&quot;&#125;.ion-buffer:before&#123;content:&quot;\\f22d&quot;&#125;.ion-chrome:before&#123;content:&quot;\\f22f&quot;&#125;.ion-closed-captioning:before&#123;content:&quot;\\f105&quot;&#125;.ion-codepen:before&#123;content:&quot;\\f230&quot;&#125;.ion-css3:before&#123;content:&quot;\\f231&quot;&#125;.ion-designernews:before&#123;content:&quot;\\f232&quot;&#125;.ion-dribbble:before&#123;content:&quot;\\f233&quot;&#125;.ion-dropbox:before&#123;content:&quot;\\f234&quot;&#125;.ion-euro:before&#123;content:&quot;\\f235&quot;&#125;.ion-facebook:before&#123;content:&quot;\\f236&quot;&#125;.ion-flickr:before&#123;content:&quot;\\f107&quot;&#125;.ion-foursquare:before&#123;content:&quot;\\f237&quot;&#125;.ion-freebsd-devil:before&#123;content:&quot;\\f238&quot;&#125;.ion-game-controller-a:before&#123;content:&quot;\\f13b&quot;&#125;.ion-game-controller-b:before&#123;content:&quot;\\f181&quot;&#125;.ion-github:before&#123;content:&quot;\\f239&quot;&#125;.ion-google:before&#123;content:&quot;\\f23a&quot;&#125;.ion-googleplus:before&#123;content:&quot;\\f23b&quot;&#125;.ion-hackernews:before&#123;content:&quot;\\f23c&quot;&#125;.ion-html5:before&#123;content:&quot;\\f23d&quot;&#125;.ion-instagram:before&#123;content:&quot;\\f23e&quot;&#125;.ion-ionic:before&#123;content:&quot;\\f150&quot;&#125;.ion-ionitron:before&#123;content:&quot;\\f151&quot;&#125;.ion-javascript:before&#123;content:&quot;\\f23f&quot;&#125;.ion-linkedin:before&#123;content:&quot;\\f240&quot;&#125;.ion-markdown:before&#123;content:&quot;\\f241&quot;&#125;.ion-model-s:before&#123;content:&quot;\\f153&quot;&#125;.ion-no-smoking:before&#123;content:&quot;\\f109&quot;&#125;.ion-nodejs:before&#123;content:&quot;\\f242&quot;&#125;.ion-npm:before&#123;content:&quot;\\f195&quot;&#125;.ion-octocat:before&#123;content:&quot;\\f243&quot;&#125;.ion-pinterest:before&#123;content:&quot;\\f244&quot;&#125;.ion-playstation:before&#123;content:&quot;\\f245&quot;&#125;.ion-polymer:before&#123;content:&quot;\\f15e&quot;&#125;.ion-python:before&#123;content:&quot;\\f246&quot;&#125;.ion-reddit:before&#123;content:&quot;\\f247&quot;&#125;.ion-rss:before&#123;content:&quot;\\f248&quot;&#125;.ion-sass:before&#123;content:&quot;\\f249&quot;&#125;.ion-skype:before&#123;content:&quot;\\f24a&quot;&#125;.ion-slack:before&#123;content:&quot;\\f10b&quot;&#125;.ion-snapchat:before&#123;content:&quot;\\f24b&quot;&#125;.ion-steam:before&#123;content:&quot;\\f24c&quot;&#125;.ion-tumblr:before&#123;content:&quot;\\f24d&quot;&#125;.ion-tux:before&#123;content:&quot;\\f2ae&quot;&#125;.ion-twitch:before&#123;content:&quot;\\f2af&quot;&#125;.ion-twitter:before&#123;content:&quot;\\f2b0&quot;&#125;.ion-usd:before&#123;content:&quot;\\f2b1&quot;&#125;.ion-vimeo:before&#123;content:&quot;\\f2c4&quot;&#125;.ion-vk:before&#123;content:&quot;\\f10d&quot;&#125;.ion-whatsapp:before&#123;content:&quot;\\f2c5&quot;&#125;.ion-windows:before&#123;content:&quot;\\f32f&quot;&#125;.ion-wordpress:before&#123;content:&quot;\\f330&quot;&#125;.ion-xbox:before&#123;content:&quot;\\f34c&quot;&#125;.ion-xing:before&#123;content:&quot;\\f10f&quot;&#125;.ion-yahoo:before&#123;content:&quot;\\f34d&quot;&#125;.ion-yen:before&#123;content:&quot;\\f34e&quot;&#125;.ion-youtube:before&#123;content:&quot;\\f34f&quot;&#125;.ion-add:before&#123;content:&quot;\\f273&quot;&#125;.ion-add-circle:before&#123;content:&quot;\\f272&quot;&#125;.ion-add-circle-outline:before&#123;content:&quot;\\f158&quot;&#125;.ion-airplane:before&#123;content:&quot;\\f15a&quot;&#125;.ion-alarm:before&#123;content:&quot;\\f274&quot;&#125;.ion-albums:before&#123;content:&quot;\\f275&quot;&#125;.ion-alert:before&#123;content:&quot;\\f276&quot;&#125;.ion-american-football:before&#123;content:&quot;\\f277&quot;&#125;.ion-analytics:before&#123;content:&quot;\\f278&quot;&#125;.ion-aperture:before&#123;content:&quot;\\f279&quot;&#125;.ion-apps:before&#123;content:&quot;\\f27a&quot;&#125;.ion-appstore:before&#123;content:&quot;\\f27b&quot;&#125;.ion-archive:before&#123;content:&quot;\\f27c&quot;&#125;.ion-arrow-back:before&#123;content:&quot;\\f27d&quot;&#125;.ion-arrow-down:before&#123;content:&quot;\\f27e&quot;&#125;.ion-arrow-dropdown:before&#123;content:&quot;\\f280&quot;&#125;.ion-arrow-dropdown-circle:before&#123;content:&quot;\\f27f&quot;&#125;.ion-arrow-dropleft:before&#123;content:&quot;\\f282&quot;&#125;.ion-arrow-dropleft-circle:before&#123;content:&quot;\\f281&quot;&#125;.ion-arrow-dropright:before&#123;content:&quot;\\f284&quot;&#125;.ion-arrow-dropright-circle:before&#123;content:&quot;\\f283&quot;&#125;.ion-arrow-dropup:before&#123;content:&quot;\\f286&quot;&#125;.ion-arrow-dropup-circle:before&#123;content:&quot;\\f285&quot;&#125;.ion-arrow-forward:before&#123;content:&quot;\\f287&quot;&#125;.ion-arrow-round-back:before&#123;content:&quot;\\f288&quot;&#125;.ion-arrow-round-down:before&#123;content:&quot;\\f289&quot;&#125;.ion-arrow-round-forward:before&#123;content:&quot;\\f28a&quot;&#125;.ion-arrow-round-up:before&#123;content:&quot;\\f28b&quot;&#125;.ion-arrow-up:before&#123;content:&quot;\\f28c&quot;&#125;.ion-at:before&#123;content:&quot;\\f28d&quot;&#125;.ion-attach:before&#123;content:&quot;\\f28e&quot;&#125;.ion-backspace:before&#123;content:&quot;\\f28f&quot;&#125;.ion-barcode:before&#123;content:&quot;\\f290&quot;&#125;.ion-baseball:before&#123;content:&quot;\\f291&quot;&#125;.ion-basket:before&#123;content:&quot;\\f292&quot;&#125;.ion-basketball:before&#123;content:&quot;\\f293&quot;&#125;.ion-battery-charging:before&#123;content:&quot;\\f294&quot;&#125;.ion-battery-dead:before&#123;content:&quot;\\f295&quot;&#125;.ion-battery-full:before&#123;content:&quot;\\f296&quot;&#125;.ion-beaker:before&#123;content:&quot;\\f297&quot;&#125;.ion-bed:before&#123;content:&quot;\\f160&quot;&#125;.ion-beer:before&#123;content:&quot;\\f298&quot;&#125;.ion-bicycle:before&#123;content:&quot;\\f299&quot;&#125;.ion-bluetooth:before&#123;content:&quot;\\f29a&quot;&#125;.ion-boat:before&#123;content:&quot;\\f29b&quot;&#125;.ion-body:before&#123;content:&quot;\\f29c&quot;&#125;.ion-bonfire:before&#123;content:&quot;\\f29d&quot;&#125;.ion-book:before&#123;content:&quot;\\f29e&quot;&#125;.ion-bookmark:before&#123;content:&quot;\\f29f&quot;&#125;.ion-bookmarks:before&#123;content:&quot;\\f2a0&quot;&#125;.ion-bowtie:before&#123;content:&quot;\\f2a1&quot;&#125;.ion-briefcase:before&#123;content:&quot;\\f2a2&quot;&#125;.ion-browsers:before&#123;content:&quot;\\f2a3&quot;&#125;.ion-brush:before&#123;content:&quot;\\f2a4&quot;&#125;.ion-bug:before&#123;content:&quot;\\f2a5&quot;&#125;.ion-build:before&#123;content:&quot;\\f2a6&quot;&#125;.ion-bulb:before&#123;content:&quot;\\f2a7&quot;&#125;.ion-bus:before&#123;content:&quot;\\f2a8&quot;&#125;.ion-business:before&#123;content:&quot;\\f1a4&quot;&#125;.ion-cafe:before&#123;content:&quot;\\f2a9&quot;&#125;.ion-calculator:before&#123;content:&quot;\\f2aa&quot;&#125;.ion-calendar:before&#123;content:&quot;\\f2ab&quot;&#125;.ion-call:before&#123;content:&quot;\\f2ac&quot;&#125;.ion-camera:before&#123;content:&quot;\\f2ad&quot;&#125;.ion-car:before&#123;content:&quot;\\f2b2&quot;&#125;.ion-card:before&#123;content:&quot;\\f2b3&quot;&#125;.ion-cart:before&#123;content:&quot;\\f2b4&quot;&#125;.ion-cash:before&#123;content:&quot;\\f2b5&quot;&#125;.ion-cellular:before&#123;content:&quot;\\f164&quot;&#125;.ion-chatboxes:before&#123;content:&quot;\\f2b6&quot;&#125;.ion-chatbubbles:before&#123;content:&quot;\\f2b7&quot;&#125;.ion-checkbox:before&#123;content:&quot;\\f2b9&quot;&#125;.ion-checkbox-outline:before&#123;content:&quot;\\f2b8&quot;&#125;.ion-checkmark:before&#123;content:&quot;\\f2bc&quot;&#125;.ion-checkmark-circle:before&#123;content:&quot;\\f2bb&quot;&#125;.ion-checkmark-circle-outline:before&#123;content:&quot;\\f2ba&quot;&#125;.ion-clipboard:before&#123;content:&quot;\\f2bd&quot;&#125;.ion-clock:before&#123;content:&quot;\\f2be&quot;&#125;.ion-close:before&#123;content:&quot;\\f2c0&quot;&#125;.ion-close-circle:before&#123;content:&quot;\\f2bf&quot;&#125;.ion-close-circle-outline:before&#123;content:&quot;\\f166&quot;&#125;.ion-cloud:before&#123;content:&quot;\\f2c9&quot;&#125;.ion-cloud-circle:before&#123;content:&quot;\\f2c2&quot;&#125;.ion-cloud-done:before&#123;content:&quot;\\f2c3&quot;&#125;.ion-cloud-download:before&#123;content:&quot;\\f2c6&quot;&#125;.ion-cloud-outline:before&#123;content:&quot;\\f2c7&quot;&#125;.ion-cloud-upload:before&#123;content:&quot;\\f2c8&quot;&#125;.ion-cloudy:before&#123;content:&quot;\\f2cb&quot;&#125;.ion-cloudy-night:before&#123;content:&quot;\\f2ca&quot;&#125;.ion-code:before&#123;content:&quot;\\f2ce&quot;&#125;.ion-code-download:before&#123;content:&quot;\\f2cc&quot;&#125;.ion-code-working:before&#123;content:&quot;\\f2cd&quot;&#125;.ion-cog:before&#123;content:&quot;\\f2cf&quot;&#125;.ion-color-fill:before&#123;content:&quot;\\f2d0&quot;&#125;.ion-color-filter:before&#123;content:&quot;\\f2d1&quot;&#125;.ion-color-palette:before&#123;content:&quot;\\f2d2&quot;&#125;.ion-color-wand:before&#123;content:&quot;\\f2d3&quot;&#125;.ion-compass:before&#123;content:&quot;\\f2d4&quot;&#125;.ion-construct:before&#123;content:&quot;\\f2d5&quot;&#125;.ion-contact:before&#123;content:&quot;\\f2d6&quot;&#125;.ion-contacts:before&#123;content:&quot;\\f2d7&quot;&#125;.ion-contract:before&#123;content:&quot;\\f2d8&quot;&#125;.ion-contrast:before&#123;content:&quot;\\f2d9&quot;&#125;.ion-copy:before&#123;content:&quot;\\f2da&quot;&#125;.ion-create:before&#123;content:&quot;\\f2db&quot;&#125;.ion-crop:before&#123;content:&quot;\\f2dc&quot;&#125;.ion-cube:before&#123;content:&quot;\\f2dd&quot;&#125;.ion-cut:before&#123;content:&quot;\\f2de&quot;&#125;.ion-desktop:before&#123;content:&quot;\\f2df&quot;&#125;.ion-disc:before&#123;content:&quot;\\f2e0&quot;&#125;.ion-document:before&#123;content:&quot;\\f2e1&quot;&#125;.ion-done-all:before&#123;content:&quot;\\f2e2&quot;&#125;.ion-download:before&#123;content:&quot;\\f2e3&quot;&#125;.ion-easel:before&#123;content:&quot;\\f2e4&quot;&#125;.ion-egg:before&#123;content:&quot;\\f2e5&quot;&#125;.ion-exit:before&#123;content:&quot;\\f2e6&quot;&#125;.ion-expand:before&#123;content:&quot;\\f2e7&quot;&#125;.ion-eye:before&#123;content:&quot;\\f2e9&quot;&#125;.ion-eye-off:before&#123;content:&quot;\\f2e8&quot;&#125;.ion-fastforward:before&#123;content:&quot;\\f2ea&quot;&#125;.ion-female:before&#123;content:&quot;\\f2eb&quot;&#125;.ion-filing:before&#123;content:&quot;\\f2ec&quot;&#125;.ion-film:before&#123;content:&quot;\\f2ed&quot;&#125;.ion-finger-print:before&#123;content:&quot;\\f2ee&quot;&#125;.ion-fitness:before&#123;content:&quot;\\f1ac&quot;&#125;.ion-flag:before&#123;content:&quot;\\f2ef&quot;&#125;.ion-flame:before&#123;content:&quot;\\f2f0&quot;&#125;.ion-flash:before&#123;content:&quot;\\f17e&quot;&#125;.ion-flash-off:before&#123;content:&quot;\\f12f&quot;&#125;.ion-flashlight:before&#123;content:&quot;\\f16b&quot;&#125;.ion-flask:before&#123;content:&quot;\\f2f2&quot;&#125;.ion-flower:before&#123;content:&quot;\\f2f3&quot;&#125;.ion-folder:before&#123;content:&quot;\\f2f5&quot;&#125;.ion-folder-open:before&#123;content:&quot;\\f2f4&quot;&#125;.ion-football:before&#123;content:&quot;\\f2f6&quot;&#125;.ion-funnel:before&#123;content:&quot;\\f2f7&quot;&#125;.ion-gift:before&#123;content:&quot;\\f199&quot;&#125;.ion-git-branch:before&#123;content:&quot;\\f2fa&quot;&#125;.ion-git-commit:before&#123;content:&quot;\\f2fb&quot;&#125;.ion-git-compare:before&#123;content:&quot;\\f2fc&quot;&#125;.ion-git-merge:before&#123;content:&quot;\\f2fd&quot;&#125;.ion-git-network:before&#123;content:&quot;\\f2fe&quot;&#125;.ion-git-pull-request:before&#123;content:&quot;\\f2ff&quot;&#125;.ion-glasses:before&#123;content:&quot;\\f300&quot;&#125;.ion-globe:before&#123;content:&quot;\\f301&quot;&#125;.ion-grid:before&#123;content:&quot;\\f302&quot;&#125;.ion-hammer:before&#123;content:&quot;\\f303&quot;&#125;.ion-hand:before&#123;content:&quot;\\f304&quot;&#125;.ion-happy:before&#123;content:&quot;\\f305&quot;&#125;.ion-headset:before&#123;content:&quot;\\f306&quot;&#125;.ion-heart:before&#123;content:&quot;\\f308&quot;&#125;.ion-heart-dislike:before&#123;content:&quot;\\f167&quot;&#125;.ion-heart-empty:before&#123;content:&quot;\\f1a1&quot;&#125;.ion-heart-half:before&#123;content:&quot;\\f1a2&quot;&#125;.ion-help:before&#123;content:&quot;\\f30b&quot;&#125;.ion-help-buoy:before&#123;content:&quot;\\f309&quot;&#125;.ion-help-circle:before&#123;content:&quot;\\f30a&quot;&#125;.ion-help-circle-outline:before&#123;content:&quot;\\f16d&quot;&#125;.ion-home:before&#123;content:&quot;\\f30c&quot;&#125;.ion-hourglass:before&#123;content:&quot;\\f111&quot;&#125;.ion-ice-cream:before&#123;content:&quot;\\f30d&quot;&#125;.ion-image:before&#123;content:&quot;\\f30e&quot;&#125;.ion-images:before&#123;content:&quot;\\f30f&quot;&#125;.ion-infinite:before&#123;content:&quot;\\f310&quot;&#125;.ion-information:before&#123;content:&quot;\\f312&quot;&#125;.ion-information-circle:before&#123;content:&quot;\\f311&quot;&#125;.ion-information-circle-outline:before&#123;content:&quot;\\f16f&quot;&#125;.ion-jet:before&#123;content:&quot;\\f315&quot;&#125;.ion-journal:before&#123;content:&quot;\\f18d&quot;&#125;.ion-key:before&#123;content:&quot;\\f316&quot;&#125;.ion-keypad:before&#123;content:&quot;\\f317&quot;&#125;.ion-laptop:before&#123;content:&quot;\\f318&quot;&#125;.ion-leaf:before&#123;content:&quot;\\f319&quot;&#125;.ion-link:before&#123;content:&quot;\\f22e&quot;&#125;.ion-list:before&#123;content:&quot;\\f31b&quot;&#125;.ion-list-box:before&#123;content:&quot;\\f31a&quot;&#125;.ion-locate:before&#123;content:&quot;\\f31c&quot;&#125;.ion-lock:before&#123;content:&quot;\\f31d&quot;&#125;.ion-log-in:before&#123;content:&quot;\\f31e&quot;&#125;.ion-log-out:before&#123;content:&quot;\\f31f&quot;&#125;.ion-magnet:before&#123;content:&quot;\\f320&quot;&#125;.ion-mail:before&#123;content:&quot;\\f322&quot;&#125;.ion-mail-open:before&#123;content:&quot;\\f321&quot;&#125;.ion-mail-unread:before&#123;content:&quot;\\f172&quot;&#125;.ion-male:before&#123;content:&quot;\\f323&quot;&#125;.ion-man:before&#123;content:&quot;\\f324&quot;&#125;.ion-map:before&#123;content:&quot;\\f325&quot;&#125;.ion-medal:before&#123;content:&quot;\\f326&quot;&#125;.ion-medical:before&#123;content:&quot;\\f327&quot;&#125;.ion-medkit:before&#123;content:&quot;\\f328&quot;&#125;.ion-megaphone:before&#123;content:&quot;\\f329&quot;&#125;.ion-menu:before&#123;content:&quot;\\f32a&quot;&#125;.ion-mic:before&#123;content:&quot;\\f32c&quot;&#125;.ion-mic-off:before&#123;content:&quot;\\f32b&quot;&#125;.ion-microphone:before&#123;content:&quot;\\f32d&quot;&#125;.ion-moon:before&#123;content:&quot;\\f32e&quot;&#125;.ion-more:before&#123;content:&quot;\\f1c9&quot;&#125;.ion-move:before&#123;content:&quot;\\f331&quot;&#125;.ion-musical-note:before&#123;content:&quot;\\f332&quot;&#125;.ion-musical-notes:before&#123;content:&quot;\\f333&quot;&#125;.ion-navigate:before&#123;content:&quot;\\f334&quot;&#125;.ion-notifications:before&#123;content:&quot;\\f338&quot;&#125;.ion-notifications-off:before&#123;content:&quot;\\f336&quot;&#125;.ion-notifications-outline:before&#123;content:&quot;\\f337&quot;&#125;.ion-nuclear:before&#123;content:&quot;\\f339&quot;&#125;.ion-nutrition:before&#123;content:&quot;\\f33a&quot;&#125;.ion-open:before&#123;content:&quot;\\f33b&quot;&#125;.ion-options:before&#123;content:&quot;\\f33c&quot;&#125;.ion-outlet:before&#123;content:&quot;\\f33d&quot;&#125;.ion-paper:before&#123;content:&quot;\\f33f&quot;&#125;.ion-paper-plane:before&#123;content:&quot;\\f33e&quot;&#125;.ion-partly-sunny:before&#123;content:&quot;\\f340&quot;&#125;.ion-pause:before&#123;content:&quot;\\f341&quot;&#125;.ion-paw:before&#123;content:&quot;\\f342&quot;&#125;.ion-people:before&#123;content:&quot;\\f343&quot;&#125;.ion-person:before&#123;content:&quot;\\f345&quot;&#125;.ion-person-add:before&#123;content:&quot;\\f344&quot;&#125;.ion-phone-landscape:before&#123;content:&quot;\\f346&quot;&#125;.ion-phone-portrait:before&#123;content:&quot;\\f347&quot;&#125;.ion-photos:before&#123;content:&quot;\\f348&quot;&#125;.ion-pie:before&#123;content:&quot;\\f349&quot;&#125;.ion-pin:before&#123;content:&quot;\\f34a&quot;&#125;.ion-pint:before&#123;content:&quot;\\f34b&quot;&#125;.ion-pizza:before&#123;content:&quot;\\f354&quot;&#125;.ion-planet:before&#123;content:&quot;\\f356&quot;&#125;.ion-play:before&#123;content:&quot;\\f357&quot;&#125;.ion-play-circle:before&#123;content:&quot;\\f174&quot;&#125;.ion-podium:before&#123;content:&quot;\\f358&quot;&#125;.ion-power:before&#123;content:&quot;\\f359&quot;&#125;.ion-pricetag:before&#123;content:&quot;\\f35a&quot;&#125;.ion-pricetags:before&#123;content:&quot;\\f35b&quot;&#125;.ion-print:before&#123;content:&quot;\\f35c&quot;&#125;.ion-pulse:before&#123;content:&quot;\\f35d&quot;&#125;.ion-qr-scanner:before&#123;content:&quot;\\f35e&quot;&#125;.ion-quote:before&#123;content:&quot;\\f35f&quot;&#125;.ion-radio:before&#123;content:&quot;\\f362&quot;&#125;.ion-radio-button-off:before&#123;content:&quot;\\f360&quot;&#125;.ion-radio-button-on:before&#123;content:&quot;\\f361&quot;&#125;.ion-rainy:before&#123;content:&quot;\\f363&quot;&#125;.ion-recording:before&#123;content:&quot;\\f364&quot;&#125;.ion-redo:before&#123;content:&quot;\\f365&quot;&#125;.ion-refresh:before&#123;content:&quot;\\f366&quot;&#125;.ion-refresh-circle:before&#123;content:&quot;\\f228&quot;&#125;.ion-remove:before&#123;content:&quot;\\f368&quot;&#125;.ion-remove-circle:before&#123;content:&quot;\\f367&quot;&#125;.ion-remove-circle-outline:before&#123;content:&quot;\\f176&quot;&#125;.ion-reorder:before&#123;content:&quot;\\f369&quot;&#125;.ion-repeat:before&#123;content:&quot;\\f36a&quot;&#125;.ion-resize:before&#123;content:&quot;\\f36b&quot;&#125;.ion-restaurant:before&#123;content:&quot;\\f36c&quot;&#125;.ion-return-left:before&#123;content:&quot;\\f36d&quot;&#125;.ion-return-right:before&#123;content:&quot;\\f36e&quot;&#125;.ion-reverse-camera:before&#123;content:&quot;\\f36f&quot;&#125;.ion-rewind:before&#123;content:&quot;\\f370&quot;&#125;.ion-ribbon:before&#123;content:&quot;\\f371&quot;&#125;.ion-rocket:before&#123;content:&quot;\\f179&quot;&#125;.ion-rose:before&#123;content:&quot;\\f372&quot;&#125;.ion-sad:before&#123;content:&quot;\\f373&quot;&#125;.ion-save:before&#123;content:&quot;\\f1a9&quot;&#125;.ion-school:before&#123;content:&quot;\\f374&quot;&#125;.ion-search:before&#123;content:&quot;\\f375&quot;&#125;.ion-send:before&#123;content:&quot;\\f376&quot;&#125;.ion-settings:before&#123;content:&quot;\\f377&quot;&#125;.ion-share:before&#123;content:&quot;\\f379&quot;&#125;.ion-share-alt:before&#123;content:&quot;\\f378&quot;&#125;.ion-shirt:before&#123;content:&quot;\\f37a&quot;&#125;.ion-shuffle:before&#123;content:&quot;\\f37b&quot;&#125;.ion-skip-backward:before&#123;content:&quot;\\f37c&quot;&#125;.ion-skip-forward:before&#123;content:&quot;\\f37d&quot;&#125;.ion-snow:before&#123;content:&quot;\\f37e&quot;&#125;.ion-speedometer:before&#123;content:&quot;\\f37f&quot;&#125;.ion-square:before&#123;content:&quot;\\f381&quot;&#125;.ion-square-outline:before&#123;content:&quot;\\f380&quot;&#125;.ion-star:before&#123;content:&quot;\\f384&quot;&#125;.ion-star-half:before&#123;content:&quot;\\f382&quot;&#125;.ion-star-outline:before&#123;content:&quot;\\f383&quot;&#125;.ion-stats:before&#123;content:&quot;\\f385&quot;&#125;.ion-stopwatch:before&#123;content:&quot;\\f386&quot;&#125;.ion-subway:before&#123;content:&quot;\\f387&quot;&#125;.ion-sunny:before&#123;content:&quot;\\f388&quot;&#125;.ion-swap:before&#123;content:&quot;\\f389&quot;&#125;.ion-switch:before&#123;content:&quot;\\f38a&quot;&#125;.ion-sync:before&#123;content:&quot;\\f38b&quot;&#125;.ion-tablet-landscape:before&#123;content:&quot;\\f38c&quot;&#125;.ion-tablet-portrait:before&#123;content:&quot;\\f38d&quot;&#125;.ion-tennisball:before&#123;content:&quot;\\f38e&quot;&#125;.ion-text:before&#123;content:&quot;\\f38f&quot;&#125;.ion-thermometer:before&#123;content:&quot;\\f390&quot;&#125;.ion-thumbs-down:before&#123;content:&quot;\\f391&quot;&#125;.ion-thumbs-up:before&#123;content:&quot;\\f392&quot;&#125;.ion-thunderstorm:before&#123;content:&quot;\\f393&quot;&#125;.ion-time:before&#123;content:&quot;\\f394&quot;&#125;.ion-timer:before&#123;content:&quot;\\f395&quot;&#125;.ion-today:before&#123;content:&quot;\\f17d&quot;&#125;.ion-train:before&#123;content:&quot;\\f396&quot;&#125;.ion-transgender:before&#123;content:&quot;\\f397&quot;&#125;.ion-trash:before&#123;content:&quot;\\f398&quot;&#125;.ion-trending-down:before&#123;content:&quot;\\f399&quot;&#125;.ion-trending-up:before&#123;content:&quot;\\f39a&quot;&#125;.ion-trophy:before&#123;content:&quot;\\f39b&quot;&#125;.ion-tv:before&#123;content:&quot;\\f17f&quot;&#125;.ion-umbrella:before&#123;content:&quot;\\f39c&quot;&#125;.ion-undo:before&#123;content:&quot;\\f39d&quot;&#125;.ion-unlock:before&#123;content:&quot;\\f39e&quot;&#125;.ion-videocam:before&#123;content:&quot;\\f39f&quot;&#125;.ion-volume-high:before&#123;content:&quot;\\f123&quot;&#125;.ion-volume-low:before&#123;content:&quot;\\f131&quot;&#125;.ion-volume-mute:before&#123;content:&quot;\\f3a1&quot;&#125;.ion-volume-off:before&#123;content:&quot;\\f3a2&quot;&#125;.ion-walk:before&#123;content:&quot;\\f3a4&quot;&#125;.ion-wallet:before&#123;content:&quot;\\f18f&quot;&#125;.ion-warning:before&#123;content:&quot;\\f3a5&quot;&#125;.ion-watch:before&#123;content:&quot;\\f3a6&quot;&#125;.ion-water:before&#123;content:&quot;\\f3a7&quot;&#125;.ion-wifi:before&#123;content:&quot;\\f3a8&quot;&#125;.ion-wine:before&#123;content:&quot;\\f3a9&quot;&#125;.ion-woman:before&#123;content:&quot;\\f3aa&quot;&#125;div.tamper&#123;color:#333;align-items:center;background-color:rgba(0,0,0,0.85);box-sizing:border-box;display:flex;font-size:14px !important;height:100%;justify-content:center;left:0;position:fixed;top:0;text-align:left;width:100%;z-index:900000&#125;div.tamper&gt;div&#123;background-color:white;box-sizing:border-box;padding:1em;width:360px&#125;div.tamper&gt;div.doc&#123;width:720px&#125;div.tamper h1&#123;font-size:1.8rem;font-weight:400;margin:10px 0 20px 0;text-align:center&#125;div.tamper form&#123;display:block&#125;div.tamper form&gt;div&#123;padding:.5em 0&#125;div.tamper form&gt;div&gt;div&#123;margin:.5em 0&#125;div.tamper form&gt;div&gt;div:last-child&#123;margin-bottom:0&#125;div.tamper form label:first-child&#123;display:block;margin-bottom:.5em&#125;div.tamper form label:first-child:before&#123;content:&quot;\\00bb&quot;;margin:0 .25em&#125;div.tamper form label:not(:first-child)&#123;display:inline&#125;div.tamper form input&#123;box-shadow:none&#125;div.tamper form input[type&#x3D;text]&#123;color:#000;background-color:#fff;border:1px solid #ddd;box-sizing:border-box;display:block;font-size:1em;padding:.5em;width:100%&#125;div.tamper form input[type&#x3D;text]:focus&#123;border:1px solid #59c1f0&#125;div.tamper form input[type&#x3D;password]&#123;color:#000;background-color:#fff;border:1px solid #ddd;box-sizing:border-box;display:block;font-size:1em;padding:.5em;width:100%&#125;div.tamper form input[type&#x3D;password]:focus&#123;border:1px solid #59c1f0&#125;div.tamper form input[type&#x3D;radio],div.tamper form input[type&#x3D;checkbox]&#123;display:inline-block !important;height:1em;margin-right:.25em;vertical-align:middle;width:1em&#125;div.tamper form input[type&#x3D;checkbox]&#123;-webkit-appearance:checkbox !important&#125;div.tamper form input[type&#x3D;radio]&#123;-webkit-appearance:radio !important&#125;div.tamper ul&#123;margin:.5em;padding:0;list-style-type:disc;list-style-position:inside;max-height:520px;overflow-y:auto;scrollbar-width:thin&#125;div.tamper ul&gt;li&#123;box-sizing:content-box;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;padding:.25em 0;cursor:default&#125;div.tamper ul&gt;li.on&#123;color:#f45a8d&#125;div.summary&#123;color:#666&#125;div.btn-group&#123;box-sizing:border-box;display:inline-flex&#125;div.btn-group.full&#123;display:flex&#125;div.btn-group.outline&gt;button&#123;background-color:transparent;border:1px solid #ccc;color:#000&#125;div.btn-group.outline&gt;button:hover&#123;color:#ffffff;background-color:#000;border-color:#000&#125;div.btn-group.outline&gt;button:not(:first-child)&#123;border-left:none&#125;div.btn-group&gt;button&#123;background-color:#666;border-radius:0;border:none;color:#fff;display:inline-block;flex:1 1 auto;margin:0;outline:none;padding:.5em 1.25em;position:relative;font-size:inherit&#125;div.btn-group&gt;button:hover&#123;background-color:#000&#125;div.btn-group&gt;button:first-child&#123;border-bottom-left-radius:.25rem;border-top-left-radius:.25rem&#125;div.btn-group&gt;button:last-child&#123;border-bottom-right-radius:.25rem;border-top-right-radius:.25rem&#125;@keyframes spinner&#123;0%&#123;transform:rotate(0)&#125;100%&#123;transform:rotate(360deg)&#125;&#125;.spinner&#123;animation-name:spinner;animation-duration:1800ms;animation-timing-function:linear;animation-iteration-count:infinite&#125;&lt;&#x2F;tyle&gt;&#39;);let e,t&#x3D;location.host.split(&quot;.&quot;);while(t.length&gt;2)t.shift();switch(t.join(&quot;.&quot;))&#123;case&quot;baidu.com&quot;:e&#x3D;location.pathname.includes(&quot;&#x2F;s&#x2F;&quot;)?&#39;&lt;style type&#x3D;&quot;text&#x2F;css&quot;&gt;#bd-main&gt;div.bd-aside&#123;display:none !important&#125;#bd-main&gt;div.bd-left&#123;margin:10px auto !important&#125;#hd,#layoutHeader,#web-right-view,#layoutAside&#123;display:none !important&#125;#bd&#123;width:960px;min-width:960px;margin:10px auto !important&#125;#layoutApp&gt;.frame-main&#123;max-width:720px;margin-top:15px&#125;#layoutApp&gt;.frame-main&gt;.frame-content&#123;margin:0&#125;div.frame-all&#123;background-color:#444&#125;div.x-button-box&gt;a.g-button&#123;display:none&#125;div.x-button-box&gt;a.g-button[data-button-id&#x3D;b1]&#123;display:inline-block&#125;&lt;&#x2F;style&gt;&#39;:&#39;&lt;style type&#x3D;&quot;text&#x2F;css&quot;&gt;dd[node-type~&#x3D;header-apps]&#123;margin-right:120px !important&#125;&lt;&#x2F;style&gt;&#39;;break;case&quot;bilibili.com&quot;:e&#x3D;&#39;&lt;style type&#x3D;&quot;text&#x2F;css&quot;&gt;#videoList&#123;display:flex;flex-wrap:wrap;list-style:none;margin-top:10px;width:100%&#125;#videoList&gt;li&#123;border-radius:.25em;border:1px solid #ddd;cursor:default;display:inline-block;flex:initial initial auto;margin:4px 2px;padding:.5em 1.25em;text-align:center&#125;#videoList&gt;li:hover&#123;background-color:#555;border-color:#555;color:#fff&#125;&lt;&#x2F;style&gt;&#39;&#125;document.head.insertAdjacentHTML(&quot;beforeend&quot;,e)&#125;function A()&#123;if(document.querySelector(&quot;#zyset&quot;))ipod.aria2&amp;&amp;u.zform(&quot;#zyset input&quot;,ipod.aria2),document.querySelector(&quot;#zyset&quot;).setAttribute(&quot;style&quot;,&quot;display: flex&quot;);else&#123;let e,t&#x3D;location.host.split(&quot;.&quot;);while(t.length&gt;2)t.shift();switch(t.join(&quot;.&quot;))&#123;case&quot;bilibili.com&quot;:e&#x3D;&#39;&lt;div class&#x3D;&quot;tamper&quot; id&#x3D;&quot;zyset&quot;&gt; &lt;div&gt; &lt;h1&gt;Aria2\u4e0b\u8f7d\u52a9\u624b&lt;&#x2F;h1&gt; &lt;form&gt; &lt;input name&#x3D;&quot;version&quot; type&#x3D;&quot;hidden&quot; value&#x3D;&quot;20191120&quot;&gt; &lt;div&gt; &lt;label&gt;\u5e38\u89c4\u8bbe\u7f6e&lt;&#x2F;label&gt; &lt;div&gt; &lt;input name&#x3D;&quot;getlink&quot; type&#x3D;&quot;checkbox&quot; value&#x3D;&quot;1&quot;&gt; &lt;label&gt;\u4ec5\u63d0\u53d6\u89c6\u9891\u7684\u4e0b\u8f7d\u5730\u5740\uff0c\u7c98\u8d34\u5230idm\u4e2d\u4f7f\u7528&lt;&#x2F;label&gt;&lt;br&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;input name&#x3D;&quot;cover&quot; type&#x3D;&quot;checkbox&quot; value&#x3D;&quot;1&quot;&gt; &lt;label&gt;\u4e0b\u8f7d\u89c6\u9891\u6216\u63d0\u53d6\u94fe\u63a5\u5305\u542b\u5c01\u9762\u56fe\u7247&lt;&#x2F;label&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;input name&#x3D;&quot;danmaku&quot; type&#x3D;&quot;checkbox&quot; value&#x3D;&quot;1&quot;&gt; &lt;label&gt;\u64ad\u653e\u89c6\u9891\u65f6\u9ed8\u8ba4\u5173\u95ed\u5f39\u5e55&lt;&#x2F;label&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;label&gt;\u8bbe\u7f6e aria2 jsonrpc&lt;&#x2F;label&gt; &lt;input name&#x3D;&quot;jsonrpc&quot; type&#x3D;&quot;text&quot;&gt; &lt;div class&#x3D;&quot;summary&quot;&gt; \u63a8\u8350\u4f7f\u7528\u914d\u5957\u7684 &lt;a href&#x3D;&quot;https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1eCDe2M6pLnukDhGAnJNasg&quot; target&#x3D;&quot;_blank&quot;&gt;Aria2&lt;&#x2F;a&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;label&gt;\u8bbe\u7f6e aria2 \u8bbf\u95ee\u53e3\u4ee4&lt;&#x2F;label&gt; &lt;input name&#x3D;&quot;token&quot; type&#x3D;&quot;password&quot; placeholder&#x3D;&quot;\u6ca1\u6709\u53e3\u4ee4\u5219\u4e0d\u8981\u586b\u5199&quot;&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;label&gt;\u8bbe\u7f6e\u89c6\u9891\u4e0b\u8f7d\u4fdd\u5b58\u8def\u5f84&lt;&#x2F;label&gt; &lt;input name&#x3D;&quot;video&quot; type&#x3D;&quot;text&quot;&gt; &lt;div class&#x3D;&quot;summary&quot;&gt; \u8bf7\u4f7f\u7528\u5de6\u659c\u6760\u4f5c\u4e3a\u5206\u9694\u7b26\uff0c\u5141\u8bb8\u4e2d\u6587\u5b57\u7b26\u3002 &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;label&gt;\u8bbe\u7f6e\u756a\u5267\u4e0b\u8f7d\u4fdd\u5b58\u8def\u5f84&lt;&#x2F;label&gt; &lt;input name&#x3D;&quot;anime&quot; type&#x3D;&quot;text&quot;&gt; &lt;div class&#x3D;&quot;summary&quot;&gt; \u76ee\u524d\u4e0d\u80fd\u4e0b\u8f7d\u4ec5\u9650\u5927\u4f1a\u5458\u89c2\u770b\u7684\u6e2f\u6fb3\u53f0\u756a\u5267 &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;div class&#x3D;&quot;btn-group&quot;&gt; &lt;button type&#x3D;&quot;submit&quot;&gt; &lt;i class&#x3D;&quot;ion-checkmark&quot;&gt;&lt;&#x2F;i&gt; \u786e\u5b9a &lt;&#x2F;button&gt; &lt;button type&#x3D;&quot;button&quot;&gt; &lt;i class&#x3D;&quot;ion-close&quot;&gt;&lt;&#x2F;i&gt; \u53d6\u6d88 &lt;&#x2F;button&gt; &lt;&#x2F;div&gt; &lt;&#x2F;form&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt;&#39;;break;default:e&#x3D;&#39;&lt;div class&#x3D;&quot;tamper&quot; id&#x3D;&quot;zyset&quot;&gt; &lt;div&gt; &lt;h1&gt;Aria2\u4e0b\u8f7d\u52a9\u624b&lt;&#x2F;h1&gt; &lt;form&gt; &lt;input name&#x3D;&quot;version&quot; type&#x3D;&quot;hidden&quot; value&#x3D;&quot;20200106&quot;&gt; &lt;div&gt; &lt;label&gt;\u8bbe\u7f6e aria2 jsonrpc&lt;&#x2F;label&gt; &lt;input name&#x3D;&quot;jsonrpc&quot; type&#x3D;&quot;text&quot;&gt; &lt;div class&#x3D;&quot;summary&quot;&gt; \u63a8\u8350\u4f7f\u7528\u914d\u5957\u7684 &lt;a href&#x3D;&quot;https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1eCDe2M6pLnukDhGAnJNasg&quot; target&#x3D;&quot;_blank&quot;&gt;Aria2&lt;&#x2F;a&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;label&gt;\u8bbe\u7f6e aria2 \u8bbf\u95ee\u53e3\u4ee4&lt;&#x2F;label&gt; &lt;input name&#x3D;&quot;token&quot; type&#x3D;&quot;password&quot; placeholder&#x3D;&quot;\u6ca1\u6709\u53e3\u4ee4\u5219\u4e0d\u8981\u586b\u5199&quot;&gt; &lt;&#x2F;div&gt; &lt;div&gt; &lt;label&gt;\u8bbe\u7f6e\u4e0b\u8f7d\u4fdd\u5b58\u8def\u5f84&lt;&#x2F;label&gt; &lt;input name&#x3D;&quot;dir&quot; type&#x3D;&quot;text&quot;&gt; &lt;div class&#x3D;&quot;summary&quot;&gt; \u8bf7\u4f7f\u7528\u5de6\u659c\u6760\u4f5c\u4e3a\u5206\u9694\u7b26\uff0c\u8def\u5f84\u4e2d\u53ef\u4ee5\u6709\u4e2d\u6587 &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;div class&#x3D;&quot;btn-group&quot;&gt; &lt;button type&#x3D;&quot;submit&quot;&gt;&lt;i class&#x3D;&quot;ion-checkmark&quot;&gt;&lt;&#x2F;i&gt; \u786e\u5b9a&lt;&#x2F;button&gt; &lt;button type&#x3D;&quot;button&quot;&gt;&lt;i class&#x3D;&quot;ion-close&quot;&gt;&lt;&#x2F;i&gt; \u53d6\u6d88&lt;&#x2F;button&gt; &lt;&#x2F;div&gt; &lt;&#x2F;form&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt;&#39;&#125;document.body.insertAdjacentHTML(&quot;beforeend&quot;,e),console.log(ipod.aria2);ipod.aria2&amp;&amp;u.zform(&quot;#zyset input&quot;,ipod.aria2),document.querySelector(&quot;#zyset button[type&#x3D;button]&quot;).addEventListener(&quot;click&quot;,()&#x3D;&gt;&#123;u.zdom(),document.querySelector(&quot;#zyset&quot;).setAttribute(&quot;style&quot;,&quot;display: none&quot;)&#125;);document.querySelector(&quot;#zyset form&quot;).addEventListener(&quot;submit&quot;,()&#x3D;&gt;&#123;let e&#x3D;&#123;&#125;,t&#x3D;u.zdom(),o&#x3D;new FormData(t);for(let t of o.entries())e[t[0]]&#x3D;t[1];ipod.aria2&#x3D;Object.assign(ipod.defaults,e),u.save(&quot;aria2&quot;,ipod.aria2);document.querySelector(&quot;#zyset&quot;).setAttribute(&quot;style&quot;,&quot;display: none&quot;)&#125;)&#125;&#125;let j;if(location.host.includes(&quot;bilibili.com&quot;))&#123;if(S(),ipod.header&#x3D;[&quot;Referer: &quot;+location.href],ipod.header2&#x3D;&#123;Referer:location.href&#125;,ipod.defaults&#x3D;&#123;token:&quot;&quot;,jsonrpc:&quot;http:&#x2F;&#x2F;127.0.0.1:6800&#x2F;jsonrpc&quot;,getlink:0,cover:0,danmaku:0,video:&quot;D:&#x2F;bilibili&#x2F;video&quot;,anime:&quot;D:&#x2F;bilibili&#x2F;anime&quot;&#125;,ipod.aria2&#x3D;u.load(&quot;aria2&quot;),null!&#x3D;ipod.aria2&amp;&amp;&quot;20191120&quot;&#x3D;&#x3D;ipod.aria2.version||(ipod.aria2&#x3D;Object.assign(ipod.defaults),&quot;live.bilibili.com&quot;&#x3D;&#x3D;location.host||A()),&quot;live.bilibili.com&quot;&#x3D;&#x3D;location.host&amp;&amp;parseInt(location.pathname.substring(1))&amp;&amp;(f(),setInterval(()&#x3D;&gt;&#123;f()&#125;,9e5)),&quot;www.bilibili.com&quot;&#x3D;&#x3D;location.host)&#123;history.pushState&#x3D;u.history(&quot;pushState&quot;),window.addEventListener(&quot;pushState&quot;,f);location.pathname.startsWith(&quot;&#x2F;video&#x2F;&quot;)&amp;&amp;(ipod.task&#x3D;setInterval(()&#x3D;&gt;&#123;if(&quot;function&quot;&#x3D;&#x3D;typeof jQuery&amp;&amp;document.querySelector(&quot;div.ops&quot;)&amp;&amp;&quot;--&quot;!&#x3D;document.querySelector(&quot;div.ops&gt;span.coin&quot;).innerText.replace(&#x2F;\s+&#x2F;g,&quot;&quot;)&amp;&amp;(clearInterval(ipod.task),ipod.task&#x3D;0,f(),$(&quot;div.ops&quot;).first().prepend(&#39;&lt;span id&#x3D;&quot;zydl&quot; title&#x3D;&quot;\u4e0b\u8f7d&quot;&gt;&lt;i class&#x3D;&quot;van-icon-download&quot;&gt;&lt;&#x2F;i&gt;\u4e0b\u8f7d&lt;&#x2F;span&gt;&#39;),$(&#39;&lt;div class&#x3D;&quot;appeal-text&quot;&gt;Aria2\u8bbe\u7f6e&lt;&#x2F;div&gt;&#39;).on(&quot;click&quot;,A).appendTo($(&quot;#arc_toolbar_report&quot;)),$(&quot;#zydl&quot;).on(&quot;click&quot;,_),ipod.aria2.danmaku))&#123;p(),history.pushState&#x3D;u.history(&quot;pushState&quot;);window.addEventListener(&quot;pushState&quot;,p)&#125;&#125;,1e3)),location.pathname.startsWith(&quot;&#x2F;bangumi&#x2F;play&#x2F;&quot;)&amp;&amp;(ipod.task&#x3D;setInterval(()&#x3D;&gt;&#123;if(&quot;function&quot;&#x3D;&#x3D;typeof jQuery&amp;&amp;document.querySelector(&quot;#toolbar_module&quot;)&amp;&amp;&quot;--&quot;!&#x3D;document.querySelector(&quot;#toolbar_module&gt;div.coin-info&quot;).innerText.replace(&#x2F;\s+&#x2F;g,&quot;&quot;))if(clearInterval(ipod.task),ipod.task&#x3D;0,$(&quot;#toolbar_module&quot;).prepend(&#39;&lt;div id&#x3D;&quot;zydl&quot; class&#x3D;&quot;coin-info&quot;&gt;&lt;i class&#x3D;&quot;ion-download&quot;&gt;&lt;&#x2F;i&gt;&lt;span&gt;\u4e0b\u8f7d&lt;&#x2F;span&gt;&lt;&#x2F;div&gt;&#39;),$(&quot;#zydl&quot;).on(&quot;click&quot;,_),__PGC_USERSTATE__.area_limit)&#123;let e&#x3D;__INITIAL_STATE__;ipod.aid&#x3D;e.epInfo.aid,ipod.cid&#x3D;e.epInfo.cid;ipod.url&#x3D;&quot;https:&#x2F;&#x2F;www.biliplus.com&#x2F;BPplayurl.php?aid&#x3D;%1&amp;cid&#x3D;%2&amp;qn&#x3D;116&amp;otype&#x3D;json&amp;module&#x3D;bangumi&quot;,$.getScript(&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;flv.js&#x2F;1.5.0&#x2F;flv.min.js&quot;,()&#x3D;&gt;&#123;document.querySelector(&quot;#bofqi&quot;).insertAdjacentHTML(&quot;beforeend&quot;,&#39;&lt;video id&#x3D;&quot;bplayer&quot; width&#x3D;&quot;100%&quot; height&#x3D;&quot;100%&quot; controls&gt;&lt;&#x2F;video&gt;&#39;),$(&quot;#player_mask_module, div.limit_area_wrap&quot;).remove();m()&#125;);let t&#x3D;&#39;&lt;ul id&#x3D;&quot;videoList&quot;&gt;&#39;;e.epList.forEach((e,o)&#x3D;&gt;&#123;t+&#x3D;u.sprintf(&#39;&lt;li data-aid&#x3D;&quot;%1&quot; data-cid&#x3D;&quot;%2&quot;&gt;%3&lt;&#x2F;li&gt;&#39;,e.aid,e.cid,z(o+1))&#125;),t+&#x3D;&quot;&lt;&#x2F;ul&gt;&quot;;document.querySelector(&quot;#player_module&quot;).insertAdjacentHTML(&quot;afterend&quot;,t),$(&quot;#videoList &gt; li&quot;).on(&quot;click&quot;,h)&#125;else if(ipod.aria2.danmaku)&#123;p(),f();history.replaceState&#x3D;u.history(&quot;replaceState&quot;),window.addEventListener(&quot;replaceState&quot;,p)&#125;&#125;,1e3))&#125;if(&quot;space.bilibili.com&quot;&#x3D;&#x3D;location.host)&#123;history.pushState&#x3D;u.history(&quot;pushState&quot;),window.addEventListener(&quot;pushState&quot;,b);b()&#125;if(&quot;search.bilibili.com&quot;&#x3D;&#x3D;location.host)&#123;history.pushState&#x3D;u.history(&quot;pushState&quot;),window.addEventListener(&quot;pushState&quot;,d);d()&#125;&#125;if(location.host.includes(&quot;baidu.com&quot;))switch(ipod.header&#x3D;[&quot;User-Agent: netdisk&quot;],ipod.defaults&#x3D;&#123;token:&quot;&quot;,jsonrpc:&quot;http:&#x2F;&#x2F;127.0.0.1:6801&#x2F;jsonrpc&quot;,dir:&quot;E:&#x2F;netdisk&quot;&#125;,location.pathname)&#123;case&quot;&#x2F;share&#x2F;init&quot;:location.hash?(document.querySelector(&quot;input&quot;).value&#x3D;location.hash.substring(1,5),document.querySelector(&quot;a.g-button-blue-large&quot;).click()):GM_xmlhttpRequest(&#123;url:&quot;https:&#x2F;&#x2F;search.pandown.cn&#x2F;api&#x2F;query?surl&#x3D;1&quot;+location.search.substring(6),method:&quot;GET&quot;,onload(e)&#123;let t&#x3D;JSON.parse(e.responseText);0&#x3D;&#x3D;t.code&amp;&amp;(document.querySelector(&quot;input&quot;).value&#x3D;t.data[0].password,document.querySelector(&quot;a.g-button-blue-large&quot;).click())&#125;&#125;);break;case&quot;&#x2F;disk&#x2F;home&quot;:S(),ipod.aria2&#x3D;u.load(&quot;aria2&quot;);null!&#x3D;ipod.aria2&amp;&amp;&quot;20200106&quot;&#x3D;&#x3D;ipod.aria2.version||(ipod.aria2&#x3D;Object.assign(ipod.defaults),A());let e&#x3D;setInterval(()&#x3D;&gt;&#123;&quot;function&quot;&#x3D;&#x3D;typeof jQuery&amp;&amp;(clearInterval(e),location.hash.startsWith(&quot;#&#x2F;all&quot;)&amp;&amp;GM_cookie.list(&#123;&#125;,e&#x3D;&gt;&#123;let t&#x3D;[&quot;BDUSS&quot;,&quot;STOKEN&quot;],o&#x3D;[];if(e.forEach(e&#x3D;&gt;&#123;t.includes(e.name)&amp;&amp;o.push(e.name+&quot;&#x3D;&quot;+e.value)&#125;),o.length)&#123;ipod.cookie&#x3D;o.join(&quot;;&quot;),$(&quot;a.g-button[data-button-id&#x3D;b5]&quot;).remove();$(&quot;div.QDDOQB&quot;).removeClass(&quot;QDDOQB&quot;).addClass(&quot;btn-group outline&quot;).css(&quot;font-size&quot;,&quot;12.5px&quot;).empty().append(&#39;&lt;button name&#x3D;&quot;zyset&quot;&gt;&lt;i class&#x3D;&quot;ion-settings&quot;&gt;&lt;&#x2F;i&gt; \u8bbe\u7f6e &lt;&#x2F;button&gt;&lt;button name&#x3D;&quot;zydl&quot;&gt;&lt;i class&#x3D;&quot;ion-download&quot;&gt;&lt;&#x2F;i&gt; \u7ebf\u8def1 &lt;&#x2F;button&gt;&lt;button name&#x3D;&quot;zydl2&quot;&gt;&lt;i class&#x3D;&quot;ion-download&quot;&gt;&lt;&#x2F;i&gt; \u7ebf\u8def2 &lt;&#x2F;button&gt;&lt;button name&#x3D;&quot;zyshare&quot;&gt;&lt;i class&#x3D;&quot;ion-share&quot;&gt;&lt;&#x2F;i&gt; \u514d\u5bc6\u5206\u4eab &lt;&#x2F;button&gt;&#39;).on(&quot;click&quot;,()&#x3D;&gt;&#123;let e&#x3D;u.zdom(1);while(!e.hasAttribute(&quot;name&quot;))e&#x3D;e.parentElement;switch(e.getAttribute(&quot;name&quot;))&#123;case&quot;zyset&quot;:A();break;case&quot;zydl&quot;:ipod.mode&#x3D;1,ipod.btn&#x3D;e.children[0];ipod.btn.setAttribute(&quot;class&quot;,&quot;ion-refresh spinner&quot;),l();break;case&quot;zydl2&quot;:ipod.mode&#x3D;2,ipod.btn&#x3D;e.children[0];ipod.btn.setAttribute(&quot;class&quot;,&quot;ion-refresh spinner&quot;),l();break;case&quot;zyshare&quot;:e.children[0].setAttribute(&quot;class&quot;,&quot;ion-refresh spinner&quot;),(()&#x3D;&gt;&#123;let e&#x3D;&#123;schannel:0,channel_list:&quot;[]&quot;,period:0,fid_list:JSON.stringify((()&#x3D;&gt;&#123;let e&#x3D;[],t&#x3D;a(r());return console.clear(),console.log(&quot;fileList &#x3D;&quot;,t.length),$(&quot;dd.g-clearfix&quot;).each((o,n)&#x3D;&gt;&#123;if(3&#x3D;&#x3D;n.getAttribute(&quot;class&quot;).replace(&quot; open-enable&quot;,&quot;&quot;).trim().split(&quot; &quot;).length)&#123;let o&#x3D;decodeURIComponent($(n).find(&quot;a&quot;).first().text());t.forEach(t&#x3D;&gt;&#123;o&#x3D;&#x3D;t.server_filename&amp;&amp;e.push(t.fs_id)&#125;)&#125;&#125;),console.log(&quot;selectedList &#x3D;&quot;,e.length),e&#125;)())&#125;;$.ajax(&#123;url:&quot;https:&#x2F;&#x2F;pan.baidu.com&#x2F;share&#x2F;set&quot;,method:&quot;POST&quot;,data:u.serialize(e),success(e)&#123;$(&quot;div.btn-group.outline &gt; button[name&#x3D;zyshare] &gt; i&quot;).attr(&quot;class&quot;,&quot;ion-share&quot;),0&#x3D;&#x3D;e.errno?GM_setClipboard(e.link,&quot;text&quot;):alert(&quot;\u64cd\u4f5c\u5931\u8d25&quot;)&#125;&#125;)&#125;)()&#125;&#125;)&#125;&#125;))&#125;,1e3);break;default:S();let t&#x3D;document.querySelector(&quot;#bd &gt; div.bd-aside&quot;);t&amp;&amp;t.setAttribute(&quot;style&quot;,&quot;display: none&quot;)&#125;if(location.host.includes(&quot;wallhaven.cc&quot;))&#123;S(),ipod.defaults&#x3D;&#123;token:&quot;&quot;,jsonrpc:&quot;http:&#x2F;&#x2F;127.0.0.1:6800&#x2F;jsonrpc&quot;,dir:&quot;E:&#x2F;\u56fe\u7247&#x2F;temp&quot;,pi:&quot;1&quot;,extype:&quot;.jpg&quot;&#125;;ipod.aria2&#x3D;u.load(&quot;aria2&quot;),null!&#x3D;ipod.aria2&amp;&amp;&quot;20200106&quot;&#x3D;&#x3D;ipod.aria2.version||(ipod.aria2&#x3D;Object.assign(ipod.defaults),A());let e&#x3D;document.querySelector(&quot;#thumbs&quot;);e&amp;&amp;e.addEventListener(&quot;click&quot;,()&#x3D;&gt;&#123;let e&#x3D;u.zdom(1);fetch(e.getAttribute(&quot;href&quot;)).then(e&#x3D;&gt;e.text()).then(e&#x3D;&gt;&#123;let t&#x3D;e.match(&#x2F;&lt;img id&#x3D;&quot;wallpaper&quot; src&#x3D;&quot;(.+?)&quot;&#x2F;);u.download(t[1])&#125;)&#125;)&#125;if(location.host.includes(&quot;nyaa&quot;))&#123;switch(S(),function()&#123;$(&quot;#navFilter-category&quot;).find(&quot;.btn&quot;).each((e,t)&#x3D;&gt;&#123;t.setAttribute(&quot;style&quot;,&quot;background-color:#fff; color:#333; font-size: 14px&quot;)&#125;),$(&quot;#navFilter-category&quot;).next(&quot;input&quot;).on(&quot;focus&quot;,function()&#123;this.value&#x3D;&quot;&quot;&#125;);$(&quot;i.fa-download&quot;).each((e,t)&#x3D;&gt;&#123;t.parentElement.remove()&#125;),$(&quot;i.fa-magnet&quot;).each((e,t)&#x3D;&gt;&#123;t.setAttribute(&quot;data-url&quot;,t.parentElement.getAttribute(&quot;href&quot;)),t.addEventListener(&quot;click&quot;,i,!1);t.parentElement.removeAttribute(&quot;href&quot;),t.parentElement.setAttribute(&quot;style&quot;,&quot;cursor: default&quot;)&#125;);$(&quot;a[title],a[rel~&#x3D;nofollow]&quot;).each((e,t)&#x3D;&gt;&#123;t.setAttribute(&quot;style&quot;,&quot;cursor: default&quot;),t.setAttribute(&quot;target&quot;,&quot;_blank&quot;)&#125;)&#125;(),ipod.defaults&#x3D;&#123;dir:&quot;E:&#x2F;netdisk&#x2F;bt&quot;,pi:&quot;1&quot;,jsonrpc:&quot;http:&#x2F;&#x2F;127.0.0.1:6800&#x2F;jsonrpc&quot;&#125;,ipod.aria2&#x3D;u.load(&quot;aria2&quot;),null!&#x3D;ipod.aria2&amp;&amp;&quot;20191120&quot;&#x3D;&#x3D;ipod.aria2.version||(ipod.aria2&#x3D;Object.assign(ipod.defaults),A()),location.host)&#123;case&quot;nyaa.fun&quot;:$(&quot;#navbar&quot;).children(&quot;ul&quot;).first().empty().append(&#39;&lt;li&gt;&lt;a id&#x3D;&quot;czyset&quot; href&#x3D;&quot;#&quot;&gt;Aria2\u8bbe\u7f6e&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;search&#x2F;c_1_1_k_0&quot;&gt;\u52a8\u6f2b&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;search&#x2F;c_1_3_k_0&quot;&gt;\u6e38\u620f&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;search&#x2F;c_1_4_k_0&quot;&gt;\u6f2b\u753b&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;search&#x2F;c_1_5_k_0&quot;&gt;\u56fe\u7247&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;search&#x2F;c_2_6_k_0&quot;&gt;\u5199\u771f&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;search&#x2F;c_2_7_k_0&quot;&gt;\u89c6\u9891&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&#39;);break;case&quot;nyaa.mlyx.workers.dev&quot;:case&quot;nyaa.si&quot;:$(&quot;#navbar&quot;).children(&quot;ul&quot;).first().empty().append(&#39;&lt;li&gt;&lt;a id&#x3D;&quot;czyset&quot; href&#x3D;&quot;#&quot;&gt;Aria2\u8bbe\u7f6e&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;1_3&quot;&gt;Anime&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;1_1&quot;&gt;AMV&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;2_0&quot;&gt;Audio&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;5_1&quot;&gt;Graphic&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;5_2&quot;&gt;Photo&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;6_1&quot;&gt;App&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;6_2&quot;&gt;Game&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&#39;);break;case&quot;sukebei.nyaa.si&quot;:$(&quot;#navbar&quot;).children(&quot;ul&quot;).first().empty().append(&#39;&lt;li&gt;&lt;a id&#x3D;&quot;czyset&quot; href&#x3D;&quot;#&quot;&gt;Aria2\u8bbe\u7f6e&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;1_1&quot;&gt;\u52a8\u6f2b&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;1_3&quot;&gt;\u6e38\u620f&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;1_4&quot;&gt;\u6f2b\u753b&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;1_5&quot;&gt;\u56fe\u7247&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;1_2&quot;&gt;\u540c\u4eba&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;2_1&quot;&gt;\u5199\u771f&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;&#x2F;?c&#x3D;2_2&quot;&gt;\u89c6\u9891&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;a href&#x3D;&quot;https:&#x2F;&#x2F;nyaa.si&quot;&gt;nyaa&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&#39;)&#125;document.querySelector(&quot;#czyset&quot;).addEventListener(&quot;click&quot;,()&#x3D;&gt;&#123;u.zdom(),A()&#125;,!1)&#125;if(location.host.includes(&quot;acg.rip&quot;))&#123;S(),ipod.defaults&#x3D;&#123;dir:&quot;E:&#x2F;netdisk&#x2F;bt&quot;,pi:&quot;1&quot;,jsonrpc:&quot;http:&#x2F;&#x2F;127.0.0.1:6800&#x2F;jsonrpc&quot;&#125;;ipod.aria2&#x3D;u.load(&quot;aria2&quot;),null!&#x3D;ipod.aria2&amp;&amp;&quot;20191120&quot;&#x3D;&#x3D;ipod.aria2.version||(ipod.aria2&#x3D;Object.assign(ipod.defaults),A());history.pushState&#x3D;u.history(&quot;pushState&quot;),window.addEventListener(&quot;pushState&quot;,o);o()&#125;if(location.host.includes(&quot;konachan&quot;))&#123;let e,t,o;if(S(),ipod.defaults&#x3D;&#123;token:&quot;&quot;,jsonrpc:&quot;http:&#x2F;&#x2F;127.0.0.1:6800&#x2F;jsonrpc&quot;,dir:&quot;E:&#x2F;\u56fe\u7247&#x2F;temp&quot;,pi:&quot;1&quot;,extype:&quot;.jpg&quot;&#125;,ipod.aria2&#x3D;u.load(&quot;aria2&quot;),null!&#x3D;ipod.aria2&amp;&amp;&quot;20200106&quot;&#x3D;&#x3D;ipod.aria2.version||(ipod.aria2&#x3D;Object.assign(ipod.defaults),A()),&quot;&#x2F;post&quot;&#x3D;&#x3D;location.pathname)&#123;if(&quot;&quot;&#x3D;&#x3D;(e&#x3D;location.search))return void(location.search&#x3D;&quot;?tags&#x3D;limit%3A56&quot;);if(-1&#x3D;&#x3D;e.indexOf(&quot;limit&quot;))return e&#x3D;e.replace(&#x2F;\++&#x2F;g,&quot;+&quot;).replace(&#x2F;\+$&#x2F;,&quot;&quot;),void(location.search&#x3D;e+&quot;+limit%3A56&quot;);for(document.querySelector(&quot;#site-title&quot;).children[1].remove(),document.querySelector(&quot;#post-list-posts&quot;).addEventListener(&quot;click&quot;,()&#x3D;&gt;&#123;u.download(u.zdom(1).getAttribute(&quot;data-url&quot;))&#125;,!1),t&#x3D;(e&#x3D;document.querySelector(&quot;#tags&quot;)).value?e.value.replace(&quot;rating:safe&quot;,&quot;&quot;).replace(&quot;limit:56&quot;,&quot;&quot;).replace(&#x2F;\s+&#x2F;,&quot; &quot;):&quot;&quot;,e.value&#x3D;&quot; &quot;&#x3D;&#x3D;t?&quot;&quot;:t,e&#x3D;document.querySelectorAll(&quot;a.thumb&quot;),t&#x3D;document.querySelectorAll(&quot;a.directlink&quot;),o&#x3D;0;e.length&gt;o;o++)e[o].setAttribute(&quot;style&quot;,&quot;cursor: default&quot;),e[o].children[0].setAttribute(&quot;data-url&quot;,t[o].getAttribute(&quot;href&quot;))&#125;&#125;&#125;();</span><br></pre></td></tr></table></figure><br>还需设置Tempermonkey才可正常使用，如下：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/Snipaste_2020-02-02_20-35-59.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/Snipaste_2020-02-02_20-36-24.jpg" alt></p>
<h2 id="4-Pandownload"><a href="#4-Pandownload" class="headerlink" title="4. Pandownload"></a>4. Pandownload</h2><p>Pandownload可以说是大名鼎鼎的百度云下载器了，但前段时间百度网盘的整改，一度被封，最近官网又发布了新的版本，恢复了下载，但我体验起来仍不稳定，且有限速的风险，我不推荐用这个，不知道后续更新如何，这里贴出它的<a href="http://pandownload.com/" target="_blank" rel="noopener">官方网站</a>。</p>
<h2 id="5-爱奇艺万能联播"><a href="#5-爱奇艺万能联播" class="headerlink" title="5. 爱奇艺万能联播"></a>5. 爱奇艺万能联播</h2><p>爱奇艺作为百度系下的产品，其应用商店提供的万能联播下载器，可以访问百度网盘下载，且不仅针对视频，对其他文件也有加速效果，但并不都是全部，且限速3M，但比起百度自身限速100k要好得多，也不用担心用第三方封号的危险。<br><a href="http://store.iqiyi.com/" target="_blank" rel="noopener">官网地址</a></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/baiduwp/Snipaste_2020-02-09_19-30-07.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>百度网盘</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch快速安装教程</title>
    <url>/2020/01/26/Pytorch%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>进入<a href="https://pytorch.org/" target="_blank" rel="noopener">官网</a>，按照你的环境选择，得到下载命令，这里以windows为例，Ubuntu同理</p>
<h2 id="conda安装"><a href="#conda安装" class="headerlink" title="conda安装"></a>conda安装</h2><p>官网得到命令如图：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pytorch%20install/Snipaste_2020-01-26_14-06-32.jpg" alt><br>但由于墙的原因直接使用此命令下载很慢，因此请更换conda源为国内镜像<br>并将命令中 -c pytorch去掉<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit&#x3D;10.1</span><br></pre></td></tr></table></figure><br>即可调用国内镜像下载。</p>
<h2 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h2><p>官网得到命令如图：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pytorch%20install/Snipaste_2020-01-26_14-06-05.jpg" alt><br>红色框出部分为官网建议版本，但由于墙的原因直接使用此命令下载很慢，因此将pip下载地址改成国内镜像即可<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip3 install torch&#x3D;&#x3D;&#x3D;1.4.0 torchvision&#x3D;&#x3D;&#x3D;0.5.0 -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>设置Pycharm plots不在tool window显示</title>
    <url>/2020/01/26/%E8%AE%BE%E7%BD%AEPycharm-plots%E4%B8%8D%E5%9C%A8tool-window%E6%98%BE%E7%A4%BA/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>Pycharm画图默认在tool windows显示，对于观看、操作其极不方便，特别是画动态图时，还会把每帧都输出，因此最好设置让它单独窗口显示。<br>如下图，把勾去掉即可。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/plot/Snipaste_2020-01-26_14-14-13.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu配置SSH和Xshell连接服务器的教程</title>
    <url>/2020/01/26/Ubuntu%E9%85%8D%E7%BD%AESSH%E5%92%8CXshell%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>SSH分为客户端 openssh-client 和服务器 openssh-server，可以利用以下命令确认电脑<br>上是否安装了客户端和服务器。如果只是想远程登陆别的机器只需要安装客户端（Ubuntu默认安装了客户端），如果要本机的SSH服务就需要安装服务器。<br>首先确认ssh-server是否已经启动了，下面是已经启动过了。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dpkg -l | grep ssh</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161410.jpg" alt><br>客户端 openssh-client 和服务器 openssh-server安装命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install openssh-client</span><br><span class="line">sudo apt install openssh-server</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161511.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161512.jpg" alt><br>查看网卡对应的服务器IP地址和用户名<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ifconfig | grep inet</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161513.jpg" alt><br>远程登录和复制文件，需要我们进入Xshell输入命令。<br>首先登陆Xshell，新建会话，点击连接。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161614.jpg" alt><br>输入登陆服务器IP（刚才查得的），端口22（默认），选择协议SSH。之后确认和连接，<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161615.jpg" alt><br>首次登陆需要用户名和密码，输入进去你Ubuntu的用户名和密码。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161617.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161718.jpg" alt><br>查看信息是否连接，进入服务器。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/SSH/2019111014161719.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SSH</tag>
        <tag>Xshell</tag>
      </tags>
  </entry>
  <entry>
    <title>Pycharm连接远程服务器并实现远程调试</title>
    <url>/2020/01/26/Pycharm%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B9%B6%E5%AE%9E%E7%8E%B0%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/</url>
    <content><![CDATA[<h2 id="Pycharm连接远程服务器"><a href="#Pycharm连接远程服务器" class="headerlink" title="Pycharm连接远程服务器"></a><a id="more"></a>Pycharm连接远程服务器</h2><ul>
<li>进入配置页面<br>Pycharm菜单栏，如下图所示，依次点击 Tools -&gt; Deployment -&gt; Configration…<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161124726.png" alt></li>
<li>配置连接服务器<br>如下图。name随便写个就行。<br>Connection下，协议最好选择sftp，接下来填写服务器主机IP，用户名，密码。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/2018082916114659.png" alt><br>点击Test SFTP connection会发现，如果连接成功会提示你如下<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161158768.png" alt><br>在Mapping下，选择连接windows下的那部分代码和服务器上代码相连，本地Local path，服务器path，apply，OK，表示已经把本地的代码和服务器代码连接上了。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161217756.png" alt></li>
<li>上传代码，使得本地代码和服务器代码保持同步<br>点击Upload to name（刚才填写的远程服务器名字），即可上传代码。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161235445.png" alt><br>若配置有多个不同服务器或同一个服务器配置了多个 服务器上传路径（Deployment Path），可选择 Upload to…，上传到不同的服务器/路径。</li>
</ul>
<h2 id="配置远程Python解释器"><a href="#配置远程Python解释器" class="headerlink" title="配置远程Python解释器"></a>配置远程Python解释器</h2><p>使用服务器调试Python程序的前提时在服务器上安装了Python解释器，如果没安装，请先安装。</p>
<ul>
<li>将Python解释器设置为远程服务器上的<br>在菜单栏，File -&gt; Settings… -&gt; Project ×× -&gt; Project Interpreter，点击右侧 Add按钮，添加解释器。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161249360.png" alt><br>选择SSH Interpreter，填写服务器的 Host 地址，端口Port，用户名Username，填好后，下一步Next。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161306837.png" alt><br>填写密码 Password，下一步Next。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161340812.png" alt><br>选择远程服务器上Python解释器的位置，服务器上的远程同步文件夹Sync folders（即之前配置的服务器Path），可以选择多个。如果不知道Python安装在哪，可以远程连接服务器后，使用 命令 which python 找到Python安装位置。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm%20remote%20control/20180829161653972.jpg" alt><br>Finish，配置结束。该项目现在使用的就是远程服务器上的Python解释器了。以后的项目若想/不想使用该解释器，手动更改解释器即可。</li>
</ul>
<h2 id="使用远程解释器运行本地Python程序"><a href="#使用远程解释器运行本地Python程序" class="headerlink" title="使用远程解释器运行本地Python程序"></a>使用远程解释器运行本地Python程序</h2><p>将测试代码上传至远程服务器（Tooles -&gt; Deployment -&gt; Upload to ××）。<br>Run测试代码，可以看到现在代码是在远程服务器上运行了。</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Pycharm</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下Rime安装配置</title>
    <url>/2020/01/26/Ubuntu%E4%B8%8BRime%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>官方网站及文档：<a href="https://rime.im/" target="_blank" rel="noopener">https://rime.im/</a><br>输入法平台：iBus<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install ibus-rime</span><br></pre></td></tr></table></figure><br>在设置人区域和语言选择设置输入法为Rime<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/rime/1762213-d899b160aa87f944.png" alt><br>根据个人喜好修改配置文件：一般用户直接修改default.yaml即可。修改前最好备份一下。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~&#x2F;.config&#x2F;ibus&#x2F;rime&#x2F;default.yaml</span><br><span class="line"></span><br><span class="line">schema_list:   </span><br><span class="line">  - schema: luna_pinyin_simp #simp是简体，第一位是默认输入法 </span><br><span class="line">menu:</span><br><span class="line">  page_size: 9 #每页候选词个数</span><br><span class="line">ascii_composer:</span><br><span class="line">  switch_key:</span><br><span class="line">    Shift_L: commit_code #左shift提交字母</span><br></pre></td></tr></table></figure><br>重启ibus-deamon<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ibus restart</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>arxiv文章下载加速</title>
    <url>/2020/01/26/arxiv%E6%96%87%E7%AB%A0%E4%B8%8B%E8%BD%BD%E5%8A%A0%E9%80%9F/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>对于我们这样的深度学习屌丝来说，没钱，没资源，没数据，没时间，只能看看别人的论文生存了，经常会到arxiv上下载一些文章，比如cvpr的文章，但是，由于国内封锁，下载很慢，甚至接连几天打不开arxiv的网站，咋办？<br>强烈推荐使用中科院arxiv的镜像地址：<a href="http://xxx.itp.ac.cn" target="_blank" rel="noopener">http://xxx.itp.ac.cn</a><br>具体使用方法：把要访问 arxiv 链接中的域名从 <a href="https://arxiv.org" target="_blank" rel="noopener">https://arxiv.org</a> 换成 <a href="http://xxx.itp.ac.cn" target="_blank" rel="noopener">http://xxx.itp.ac.cn</a> ,<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">比如:</span><br><span class="line">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1608.00367</span><br><span class="line">换成：</span><br><span class="line">http:&#x2F;&#x2F;xxx.itp.ac.cn&#x2F;pdf&#x2F;1608.00367.pdf</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">或者将：</span><br><span class="line">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1608.00367</span><br><span class="line">换成：</span><br><span class="line">http:&#x2F;&#x2F;xxx.itp.ac.cn&#x2F;abs&#x2F;1608.00367</span><br></pre></td></tr></table></figure><br>即可打开，是不是很爽？</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu16.04安装opencv教程</title>
    <url>/2020/01/19/ubuntu16.04%E5%AE%89%E8%A3%85opencv%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>1.去官网下载opencv，在本教程中选用的是opencv3.4.1，其他版本的配置方法异曲同工。<br><a href="http://opencv.org/releases.html" target="_blank" rel="noopener">下载链接</a>，选择sources版本<br>2.解压下载下来的zip包<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unzip opencv-3.4.1.zip</span><br></pre></td></tr></table></figure><br>3.进入到解压后的文件包中<br>4.安装依赖库和cmake ，如果提醒需要apt-get update，那就先sudo su进入root权限，再sudo apt-get update，然后在执行下面命令<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cmake</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev</span><br></pre></td></tr></table></figure><br>在第二步中若提示依赖版本问题，原因在于apt源太旧，更换新的源，再进行安装。<br>5.安装完cmake之后执行命令 ,创建编译文件夹，不创建的会提示（如下图）<br>In-source builds are not allowed.<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir my_build_dir</span><br><span class="line">cd my_build_dir</span><br></pre></td></tr></table></figure><br>6.cmake一下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE&#x3D;Release -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local ..</span><br></pre></td></tr></table></figure><br>注意：如果已经在新的文件夹中编译，但是还会出现之前的报错，把cmakecache.txt删了再编译就可<br>期间可能会下载一个东西，等待一会儿就OK<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/opencv/20171005220631735.png" alt><br>7.执行命令，漫长的编译过程<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/opencv/20171005214720292.png" alt><br>8.执行命令<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/opencv/20171005214847504.png" alt><br>9.sudo make install 执行完毕后OpenCV编译过程就结束了，接下来就需要配置一些OpenCV的编译环境首先将OpenCV的库添加到路径，从而可以让系统找到<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;ld.so.conf.d&#x2F;opencv.conf</span><br></pre></td></tr></table></figure><br>执行此命令后打开的可能是一个空白的文件，不用管，只需要在文件末尾添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;lib</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/opencv/20171005215009149.png" alt><br>10.执行如下命令使得刚才的配置路径生效<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><br>11.配置bash<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;bash.bashrc</span><br></pre></td></tr></table></figure><br>在最末尾添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PKG_CONFIG_PATH&#x3D;$PKG_CONFIG_PATH:&#x2F;usr&#x2F;local&#x2F;lib&#x2F;pkgconfig  </span><br><span class="line">export PKG_CONFIG_PATH</span><br></pre></td></tr></table></figure><br>保存，执行如下命令使得配置生效<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source &#x2F;etc&#x2F;bash.bashrc</span><br></pre></td></tr></table></figure><br>更新<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo updatedb</span><br></pre></td></tr></table></figure><br>12.至此所有的配置都已经完成<br>下面用一个小程序测试一下</p>
<p>找到<br>cd到opencv-3.4.1/samples/cpp/example_cmake目录下<br>我们可以看到这个目录里官方已经给出了一个cmake的example我们可以拿来测试下<br>按顺序执行<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">.&#x2F;opencv_example</span><br></pre></td></tr></table></figure><br>即可看到打开了摄像头，在左上角有一个hello opencv<br>即表示配置成功</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Opencv</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu快速搭建C++开发环境（VS Code编辑器）</title>
    <url>/2020/01/19/Ubuntu%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAC++%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%EF%BC%88VS-Code%E7%BC%96%E8%BE%91%E5%99%A8%EF%BC%89/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>以下安装的是g++-8（目前最新）和Visual Studio Code，此方法适用于Ubuntu 14.04 64位、Ubuntu 16.04 32位/64位、Ubuntu 18.04，Ubuntu 14.04 32位及以下系统无效。打开终端并且输入以下命令即可。</p>
<h2 id="安装g-8"><a href="#安装g-8" class="headerlink" title="安装g++-8"></a>安装g++-8</h2><ul>
<li>安装software-properties-common：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get -y install software-properties-common</span><br></pre></td></tr></table></figure></li>
<li>添加PPA到库并更新（会提示按回车继续执行，此时按回车即可）：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ubuntu-toolchain-r&#x2F;test</span><br></pre></td></tr></table></figure></li>
<li>更新软件信息：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li>
<li>安装g++：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get -y install g++-8</span><br></pre></td></tr></table></figure></li>
<li>将g++指向g++-8：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ln -sf &#x2F;usr&#x2F;bin&#x2F;g++-8 &#x2F;usr&#x2F;bin&#x2F;g++</span><br></pre></td></tr></table></figure></li>
<li>显示g++版本号，如果正常显示版本号意味着安装成功：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">g++ --version</span><br></pre></td></tr></table></figure>
<h2 id="安装Visual-Studio-Code"><a href="#安装Visual-Studio-Code" class="headerlink" title="安装Visual Studio Code"></a>安装Visual Studio Code</h2></li>
<li>安装libgconf库：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install -y libgconf-2-4</span><br></pre></td></tr></table></figure></li>
<li>安装git：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get -y install git</span><br></pre></td></tr></table></figure></li>
<li>添加PPA到库并更新（会提示按回车继续执行，此时按回车即可）：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ubuntu-desktop&#x2F;ubuntu-make</span><br></pre></td></tr></table></figure></li>
<li>更新软件信息：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li>
<li>安装ubuntu-make：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get -y install ubuntu-make</span><br></pre></td></tr></table></figure></li>
<li>通过ubuntu-make安装Visual Studio Code；过程中会询问安装路径，此时不需要修改直接按回车即可；然后会询问是否接受协议，此时输入a然后回车即可：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo umake ide visual-studio-code</span><br></pre></td></tr></table></figure></li>
<li>创建软链接到程序目录下：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ln -sf &#96;env | grep ^HOME&#x3D; | cut -c 6-&#96;&#x2F;.local&#x2F;share&#x2F;umake&#x2F;ide&#x2F;visual-studio-code&#x2F;bin&#x2F;code &#x2F;usr&#x2F;bin&#x2F;code</span><br></pre></td></tr></table></figure></li>
<li>创建项目目录：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir ~&#x2F;Projects</span><br></pre></td></tr></table></figure></li>
<li>显示Visual Studio Code版本号，如果正常显示版本号意味着安装成功：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">code --version</span><br></pre></td></tr></table></figure></li>
<li>运行VS Code：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">code</span><br></pre></td></tr></table></figure></li>
<li>安装中文语言包：<br>1.按下键盘Ctrl+Shift+X<br>2.在输入框里输入Chinese<br>3.按下中文(简体)后面的install<br>4.安装完后按下Ctrl+Shift+P打开命令面板<br>5.输入config后选择配置语言命令<br>6.选择Configure Display Language<br>7.将”locale”:”en”改成”locale”: “zh-cn”，然后按下键盘Ctrl+S保存源文件<br>8.关闭VS Code<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-047109e9a0a075af.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-8c9880935d6bcbe8.png" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-43717f65ac98f10e.png" alt><h2 id="创建第一个C-项目（以下步骤除非有特殊说明，否则每次创建项目都要执行一次）："><a href="#创建第一个C-项目（以下步骤除非有特殊说明，否则每次创建项目都要执行一次）：" class="headerlink" title="创建第一个C++项目（以下步骤除非有特殊说明，否则每次创建项目都要执行一次）："></a>创建第一个C++项目（以下步骤除非有特殊说明，否则每次创建项目都要执行一次）：</h2></li>
<li>创建项目目录cppdemo，用于学习创建第一个C++项目，并进入cppdemo目录中：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir ~&#x2F;Projects&#x2F;cppdemo &amp;&amp; cd ~&#x2F;Projects&#x2F;cppdemo</span><br></pre></td></tr></table></figure></li>
<li>创建配置目录：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir .&#x2F;.vscode</span><br></pre></td></tr></table></figure></li>
<li>添加编译配置文件（输入以下内容然后按回车）：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; .vscode&#x2F;tasks.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;: &quot;2.0.0&quot;,</span><br><span class="line">    &quot;tasks&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;label&quot;: &quot;build&quot;,</span><br><span class="line">            &quot;type&quot;: &quot;shell&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;g++&quot;,</span><br><span class="line">            &quot;args&quot;: [</span><br><span class="line">                &quot;-std&#x3D;c++17&quot;,</span><br><span class="line">                &quot;-Wall&quot;,</span><br><span class="line">                &quot;-Wextra&quot;,</span><br><span class="line">                &quot;-g&quot;,</span><br><span class="line">                &quot;-ggdb&quot;,</span><br><span class="line">                &quot;mycpp.cpp&quot;,</span><br><span class="line">                &quot;-o&quot;,</span><br><span class="line">                &quot;demoapp.out&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;group&quot;: &quot;build&quot;,</span><br><span class="line">            &quot;presentation&quot;: &#123;</span><br><span class="line">                &quot;reveal&quot;: &quot;always&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;problemMatcher&quot;: &quot;\$gcc&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li>
<li>添加运行配置文件（输入以下内容然后按回车）：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; .vscode&#x2F;launch.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;: &quot;0.2.0&quot;,</span><br><span class="line">    &quot;configurations&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;(gdb) Launch&quot;,</span><br><span class="line">            &quot;type&quot;: &quot;cppdbg&quot;,</span><br><span class="line">            &quot;request&quot;: &quot;launch&quot;,</span><br><span class="line">            &quot;program&quot;: &quot;\$&#123;workspaceFolder&#125;&#x2F;demoapp.out&quot;,</span><br><span class="line">            &quot;args&quot;: [],</span><br><span class="line">            &quot;stopAtEntry&quot;: false,</span><br><span class="line">            &quot;cwd&quot;: &quot;\$&#123;workspaceFolder&#125;&quot;,</span><br><span class="line">            &quot;environment&quot;: [],</span><br><span class="line">            &quot;externalConsole&quot;: true,</span><br><span class="line">            &quot;MIMode&quot;: &quot;gdb&quot;,</span><br><span class="line">            &quot;setupCommands&quot;: [</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;,</span><br><span class="line">                    &quot;text&quot;: &quot;-enable-pretty-printing&quot;,</span><br><span class="line">                    &quot;ignoreFailures&quot;: true</span><br><span class="line">                &#125;</span><br><span class="line">            ],</span><br><span class="line">            &quot;preLaunchTask&quot;: &quot;build&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li>
<li>添加智能提示配置文件（输入以下内容然后按回车）：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if [ &#96;getconf LONG_BIT&#96; -eq &quot;64&quot; ]; then</span><br><span class="line">cat &gt; .vscode&#x2F;c_cpp_properties.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;configurations&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Linux&quot;,</span><br><span class="line">            &quot;includePath&quot;: [</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;x86_64-linux-gnu&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&#x2F;backward&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;x86_64-linux-gnu&#x2F;8&#x2F;include&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;local&#x2F;include&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;x86_64-linux-gnu&#x2F;8&#x2F;include-fixed&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;x86_64-linux-gnu&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&quot;,</span><br><span class="line">                &quot;$&#123;workspaceRoot&#125;&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;defines&quot;: [],</span><br><span class="line">            &quot;intelliSenseMode&quot;: &quot;clang-x64&quot;,</span><br><span class="line">            &quot;browse&quot;: &#123;</span><br><span class="line">                &quot;path&quot;: [</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;x86_64-linux-gnu&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&#x2F;backward&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;x86_64-linux-gnu&#x2F;8&#x2F;include&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;local&#x2F;include&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;x86_64-linux-gnu&#x2F;8&#x2F;include-fixed&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;x86_64-linux-gnu&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&quot;,</span><br><span class="line">                    &quot;$&#123;workspaceRoot&#125;&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;limitSymbolsToIncludedHeaders&quot;: true,</span><br><span class="line">                &quot;databaseFilename&quot;: &quot;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;: 3</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">else</span><br><span class="line">cat &gt; .vscode&#x2F;c_cpp_properties.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;configurations&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Linux&quot;,</span><br><span class="line">            &quot;includePath&quot;: [</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;i386-linux-gnu&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&#x2F;backward&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;i686-linux-gnu&#x2F;8&#x2F;include&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;local&#x2F;include&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;i686-linux-gnu&#x2F;8&#x2F;include-fixed&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&#x2F;i386-linux-gnu&quot;,</span><br><span class="line">                &quot;&#x2F;usr&#x2F;include&quot;,</span><br><span class="line">                &quot;$&#123;workspaceRoot&#125;&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;defines&quot;: [],</span><br><span class="line">            &quot;intelliSenseMode&quot;: &quot;clang-x64&quot;,</span><br><span class="line">            &quot;browse&quot;: &#123;</span><br><span class="line">                &quot;path&quot;: [</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;i386-linux-gnu&#x2F;c++&#x2F;8&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;c++&#x2F;8&#x2F;backward&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;i686-linux-gnu&#x2F;8&#x2F;include&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;local&#x2F;include&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;lib&#x2F;gcc&#x2F;i686-linux-gnu&#x2F;8&#x2F;include-fixed&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&#x2F;i386-linux-gnu&quot;,</span><br><span class="line">                    &quot;&#x2F;usr&#x2F;include&quot;,</span><br><span class="line">                    &quot;$&#123;workspaceRoot&#125;&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;limitSymbolsToIncludedHeaders&quot;: true,</span><br><span class="line">                &quot;databaseFilename&quot;: &quot;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;: 3</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></li>
<li>创建C++源文件mycpp.cpp（代码就是写在这个文件）：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch mycpp.cpp</span><br></pre></td></tr></table></figure></li>
<li>用Visual Studio Code打开当前工作环境（不要忽略最后的点哟）：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">code .</span><br></pre></td></tr></table></figure></li>
<li>双击mycpp.cpp打开文件，第一次会提示安装C++插件，点击安装，然后等待右下角消息安装完成（只需执行一次，以后都不需要再执行）。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-84d0b75234245983.png" alt></li>
<li>双击打开mycpp.cpp，然后输入以下代码：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">    std::cout &lt;&lt; &quot;小古银的C++教程&quot; &lt;&lt; std::endl;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-5c0e4151fa446bb1.png" alt></li>
<li>按下Ctrl+Shift+B，然后选择build，就会开始编译。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-9086cfb5ebae16ca.png" alt></li>
<li>编译完后，在终端窗口按回车键关闭终端。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-422767a1784b76a6.png" alt></li>
<li>点击菜单栏中的终端，然后点击新建终端，就会打开终端窗口。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-86304c028d93ecc7.png" alt></li>
<li>然后在终端输入以下内容，就会运行程序：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;demoapp.out</span><br></pre></td></tr></table></figure></li>
<li>运行程序将会显示小古银的C++教程。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/VScode/11655830-4ecc19bb3b0c9794.png" alt><br>此时，第一个项目，也就是第一个程序就完成了。</li>
</ul>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>C++</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu美化</title>
    <url>/2020/01/19/Ubuntu%E7%BE%8E%E5%8C%96/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><ul>
<li>安装Unity-tweak-tool<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install unity-tweak-tool</span><br></pre></td></tr></table></figure>
<img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909004455285.png" alt></li>
<li>更新源<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:noobslab&#x2F;themes</span><br><span class="line">sudo apt-add-repository ppa:numix&#x2F;ppa </span><br><span class="line">sudo apt update</span><br></pre></td></tr></table></figure></li>
<li>安装主题<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:noobslab&#x2F;themes  </span><br><span class="line">sudo apt-get update  </span><br><span class="line">sudo apt-get install flatabulous-theme </span><br><span class="line">sudo apt-get install arc-theme</span><br></pre></td></tr></table></figure></li>
<li>安装图标<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:noobslab&#x2F;icons  </span><br><span class="line">sudo apt-get update  </span><br><span class="line">sudo apt-get install ultra-flat-icons </span><br><span class="line">sudo apt-get install numix-icon-theme-circle</span><br></pre></td></tr></table></figure></li>
<li>选择主题<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909005108750.png" alt></li>
<li>选择图标<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909005211146.png" alt></li>
<li>最终效果<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/Ubuntu%E7%BE%8E%E5%8C%96/20180909005447430.png" alt></li>
</ul>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows10安装ubuntu16.04双系统教程</title>
    <url>/2020/01/19/Windows10%E5%AE%89%E8%A3%85ubuntu16.04%E5%8F%8C%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>BIOS模式有传统的MBR模式和新式UEFI模式，这将对安装双系统的方法产生直接影响。目前来看，大部分电脑都属于新式UEFI模式，不过也存在一些老机子仍然属于传统MBR模式。本教程只介绍新式UEFI模式下的双系统安装方法，如果你的电脑属于传统MBR模式，强烈建议你重装windows系统来更新BIOS模式到UEFI。</p>
<h2 id="制作系统盘"><a href="#制作系统盘" class="headerlink" title="制作系统盘"></a>制作系统盘</h2><p>需要准备以下工具：<br>①ubuntu系统镜像<br>②刻录软件，推荐”软碟通”，会提示注册，选择继续试用就好<br>③一个大于 2G 的 U 盘<br>1.安装并打开软碟通，插上 U 盘，并且最好备份你的 U 盘，因为之后需要格式化；<br>2.进入软碟通，选择文件，浏览到你的ubuntu镜像所在的目录，选择ubuntu镜像文件，双击打开，如图：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422030.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422031.jpg" alt><br>3.在软碟通界面菜单栏选择”启动”，选择”写入硬盘映像”，如图所示：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422032.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422033.jpg" alt><br>接下来很重要，注意次序：<br>1）看你的硬盘驱动器是否对应的是你的 U 盘（必须是） ，一般默认是；<br>2）看映像文件是否对应你的 ubuntu 镜像；<br>3）如果上述均没有错误，选择格式化，之后就会格式化你的 U 盘；<br>4）在 U 盘格式化完毕之后，选择写入，之后就是慢慢等待了，等待写入完毕；</p>
<h2 id="在windows下创建空白分区"><a href="#在windows下创建空白分区" class="headerlink" title="在windows下创建空白分区"></a>在windows下创建空白分区</h2><p>说明：这一步是为ubuntu系统分配空间，单硬盘和双硬盘存在一点区别。<br>1.”此电脑”点击右键，点击”管理”，点击”磁盘管理”：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422034.jpg" alt><br>2.为ubuntu分配空间<br>（1）如果是单硬盘，任选一个盘，在该盘点击右键，选择压缩卷，如下，输入压缩空间量，单位为M，如果空间充足，建议分出80G或100G，空间不足也可以分60G（1G=1024M）：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/20190520082545559.png" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422136.jpg" alt><br>（2）如果是双硬盘，需要先在系统盘分出200M的空白分区用来安装ubuntu的启动项，然后再在另一块硬盘分出空间，在该盘点击右键，选择压缩卷，如下，输入压缩空间量，单位为M,如果空间充足，建议分出80G或100G，空间不足也可以分60G（1G=1024M）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422138.jpg" alt></p>
<h2 id="用做好的系统盘安装系统"><a href="#用做好的系统盘安装系统" class="headerlink" title="用做好的系统盘安装系统"></a>用做好的系统盘安装系统</h2><p>先点击桌面右下角：电源图标→电源设置→电源和睡眠→其他电源设置→选择电源按钮的功能→更改当前不可用的设置，将“关机设置”中的快速启动项关闭，保存修改。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/20190520084629381.png" alt>若呈灰色，无法修改，win+r进入gpedit.msc将系统配置中的关机选项，快速启动改为禁用，重启完即可更改。<br>1.插好系统盘，重启电脑，开机进bios，在Security页面，关掉secure boot（不同电脑secure boot可能在不同位置），然后到Boot页面，如果有Fast Boot这一项（部分联想电脑有），也把它关掉，没有忽略；然后保存更改，在Boot页面下方启动项选择 USB启动，回车，如果顺利进入安装页面，继续往下做；如果点击USB启动项无法进入，保存并退出，电脑会重启，根据自己电脑按相应的键进boot manager，找到USB启动项，回车即可进入。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422139.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422140.jpg" alt><br>2.然后会进入这个界面，选择Install Ubuntu，回车确认<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422141.jpg" alt><br>3.安装过程记住断网，否则很慢<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422143.jpg" alt><br>全不选，边安装边下载更新很慢，点击”继续”<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/20171229140050194.png" alt><br>出现以下或类似界面，一定要选择”其他选项”，因为需要手动分区<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422145.jpg" alt><br>分区界面如下<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422246.jpg" alt><br>在这里，我们进行手动分区，选择留出的空闲分区，点击”+”进行分区，如下：<br>1）efi：如果是单硬盘，在唯一的一个空闲分区上添加，大小200M，逻辑分区，空间起始位置，用于efi；如果是双硬盘，找到事先分好的200M空闲分区添加，逻辑分区，空间起始位置，用于efi。这个分区必不可少，用于安装ubuntu启动项。<br>2）swap:中文是”交换空间”，充当ubuntu的虚拟内存，一般的大小与电脑物理内存一样，可以将其分为 8G，逻辑分区，空间起始位置，用于”swap”或”交换空间”<br>3) /:这是ubuntu 的根目录,用于安装系统和软件，相当于windows的C盘，我们将其分为 20G，主分区，空间起始位置，用于”ext4日志文件系统”，挂载点为”/“（根据你的磁盘空间调整，可以大一点，毕竟ubuntu装软件都是默认装在根目录的）<br>4）/home:相当于windows的其他盘，剩下的全分给它，逻辑分区，空间起始位置，用于”ext4日志文件系统”，挂载点为”/home”<br>分区完毕，在分区界面的下方，选择安装启动项的位置，我们刚刚不是创建了200M的efi分区吗，现在你看看这个区前面的编号是多少，比如是/dev/sda1,不同的机子会有不同的编号，下拉列表选择这个efi分区编号（这里一定要注意，windows的启动项也是efi文件，大小大概是100M，而我们创建的ubuntu的efi大小是200M，一定要选对），之后点击”Install Now”<br>设置地区不重要，按你需要设置，也可以直接继续，不影响<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422248.jpg" alt><br>键盘布局默认是英语的，建议不改（默认中文也行）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422249.jpg" alt><br>这里设置用户，自己输入就可以了，例如英文字母，尽量简单点，密码也简单点<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422250.jpg" alt><br>系统开始安装，耐心等待安装完毕就可以了<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422251.jpg" alt><br>全部完成之后，会提醒你重启，把U盘拔了，点”现在重启”，如果卡死就强制关机再重启就好<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422252.jpg" alt><br>重启后你会看到以下界面，第一项是ubuntu启动项，第二项是ubuntu高级设置，第三项是windows启动项，第四项不用管，默认选择的是第一个，回车进ubuntu系统<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/%E5%8F%8C%E7%B3%BB%E7%BB%9F/2019121214422253.jpg" alt><br>若没有出现，则进入bios将启动顺序改为Ubuntu先。</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Windows10</tag>
      </tags>
  </entry>
  <entry>
    <title>CMake Error at /usr/share/cmake-3.5/Modules/FindQt4.cmake:634 (message)</title>
    <url>/2020/01/19/CMake%20Error/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>编译工程时出现cmake找不到Qt4的问题,如下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMake Warning at &#x2F;usr&#x2F;share&#x2F;cmake-3.5&#x2F;Modules&#x2F;FindQt4.cmake:626 (message):</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;qmake-qt4 reported QT_INSTALL_LIBS as &quot;&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&quot;</span><br><span class="line"> but QtCore could not be found there.  </span><br><span class="line"> Qt is NOT installed correctly for the target build environment.</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMake Error at &#x2F;usr&#x2F;share&#x2F;cmake-3.5&#x2F;Modules&#x2F;FindQt4.cmake:634 (message):</span><br><span class="line">  Could NOT find QtCore.</span><br></pre></td></tr></table></figure><br>解决办法<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install cmake gcc g++ qt&#123;4,5&#125;-qmake libqt4-dev</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>对目录 /var/lib/apt/lists/ 加锁 问题解决方法</title>
    <url>/2020/01/19/%E5%AF%B9%E7%9B%AE%E5%BD%95-varlibaptlists-%E5%8A%A0%E9%94%81-%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo rm &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;lock</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu apt 更换为国内的源</title>
    <url>/2020/01/19/ubuntu-apt-%E6%9B%B4%E6%8D%A2%E4%B8%BA%E5%9B%BD%E5%86%85%E7%9A%84%E6%BA%90/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>备份原来的源：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.bak</span><br></pre></td></tr></table></figure><br>更换源:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;apt&#x2F;sources.list</span><br></pre></td></tr></table></figure><br>将里面文件内容全部替换成下面：<br><a href="https://wiki.ubuntu.org.cn/%E6%A8%A1%E6%9D%BF:16.04source" target="_blank" rel="noopener">点击此处获得最新源</a><br>保存退出<br>执行更新：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Win10和Ubuntu双系统下时间不对问题</title>
    <url>/2020/01/19/Win10%E5%92%8CUbuntu%E5%8F%8C%E7%B3%BB%E7%BB%9F%E4%B8%8B%E6%97%B6%E9%97%B4%E4%B8%8D%E5%AF%B9%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>电脑安装完win10和Ubuntu双系统后，Ubuntu时间总会和Windows时间相差8小时，这是因为windows认为bios时间是本地时间，Ubuntu认为bios时间是UTC时间，所以我们需要将Ubuntu时间改成本地时间。<br>执行这条语句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timedatectl set-local-rtc 1 --adjust-system-clock</span><br></pre></td></tr></table></figure>
<p>重启进入windows，时间设置中点击立即同步即可解决。</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>如何获取OneDrive 5T网盘空间(可升级25T)</title>
    <url>/2020/01/16/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96OneDrive-5T%E7%BD%91%E7%9B%98%E7%A9%BA%E9%97%B4(%E5%8F%AF%E5%8D%87%E7%BA%A725T)/</url>
    <content><![CDATA[<p><a href="https://blog.wulel.com/post/how-to-get-5t-onedrive/" target="_blank" rel="noopener">点此跳转</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Onedrive</tag>
      </tags>
  </entry>
  <entry>
    <title>使 Onedrive 同步任意文件夹</title>
    <url>/2020/01/16/%E4%BD%BF-Onedrive-%E5%90%8C%E6%AD%A5%E4%BB%BB%E6%84%8F%E6%96%87%E4%BB%B6%E5%A4%B9/</url>
    <content><![CDATA[<h1 id><a href="#" class="headerlink" title></a><a id="more"></a></h1><p>onedrive默认只同步指定的onedrive文件夹，为了让它同步其他的文件夹，可以在命令行（以管理员身份运行的）使用以下代码创建一个软链接。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mklink &#x2F;j &quot;onedrive文件夹地址\需要同步的文件夹名&quot; &quot;需要同步的文件夹地址&quot;</span><br></pre></td></tr></table></figure><br>比如说我的onedrive在D:\Onedrive，我的目标文件夹test在D:\。那么<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mklink &#x2F;j &quot;D:\Onedrive\test&quot; &quot;D:\test&quot;</span><br></pre></td></tr></table></figure><br>如果成功的的话，会提示<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/onedrive/1392594-20181005094204092-1998469109.png" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Onedrive</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测调研</title>
    <url>/2020/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/</url>
    <content><![CDATA[<h3 id="目标检测的任务"><a href="#目标检测的任务" class="headerlink" title=" 目标检测的任务"></a><a id="more"></a> 目标检测的任务</h3><ul>
<li><strong>分类-Classification</strong>：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。</li>
<li><strong>定位-Location</strong>：解决“在哪里？”的问题，即定位出这个目标的的位置。</li>
<li><strong>检测-Detection</strong>：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。</li>
<li><strong>分割-Segmentation</strong>：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。</li>
</ul>
<h3 id="所面临的挑战"><a href="#所面临的挑战" class="headerlink" title="所面临的挑战"></a>所面临的挑战</h3><ul>
<li>目标可能出现在图像的任何位置。</li>
<li>目标有各种不同的大小。</li>
<li>目标可能有各种不同的形状。</li>
</ul>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/deep_learning_object_detection_dataset.PNG" alt></p>
<ul>
<li><p>Pascal Visual Object Classes(05-12) 是计算机视觉领域最重要的赛事之一。包含多任务，图像分类、目标检测、语义分割和行为检测。主要有两个版本的 Pascal-VOC 用于检测：VOC07 和 VOC12，前者包含 5K 张训练图像，共 12K 个标注目标。后者包含 11K 张训练图像，共 27K 个标注目标。两个数据集中包含了 20 个生活中常见的目标类（Person：person; Animal：bird, cat, cow, dog, horse, sheep; Vehicle：aeroplane, bicycle, boat, bus, car, motor-bike, train; Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor）</p>
</li>
<li><p>ImageNet Large Scale Visual Recognition Challenge(10-17) 包含检测挑战赛。Imagenet数据集有1400多万幅图片，涵盖2万多个类别，ILSVRC比赛会每年从ImageNet数据集中抽出部分样本, 检测数据集包括 200 类视觉目标，图像/目标比 VOC 大两个数量级。例如，ILSVRC-14 包含517K 图像以及 534K 注释目标。</p>
</li>
<li><p>MS-COCO 是当前最有挑战性的目标检测数据集。从 15 年开始举办比赛。它的目标类别比 ILSVRC 少，但是目标实例更多。例如，MS-COCO-17 包含 164K 张图像，来自 80 类的 897K 个标注目标。与 VOC 和 ILSVRC 相比，MS-COCO 除了边界框注释以外，每个目标通过实例分割进一步标注来帮助精确定位。此外，它包含更多小目标（面积小于图像的 1%），以及密集定位目标。这些特点使 MS−COCO中的目标分布更接近于真实世界。</p>
</li>
<li><p>Open Images</p>
<p>2018 年推出了 Open ImagesOpen~ImagesOpen Images 检测挑战赛。包含两个任务：</p>
<p>标准目标检测<br>视觉关系检测，用于检测特定关系中的匹配目标</p>
<p>目标检测，数据集包含 1910K 张图像，600 个目标类别的 15440K 个标注目标。</p>
</li>
</ul>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><div class="table-container">
<table>
<thead>
<tr>
<th>预测\实际</th>
<th>正</th>
<th>负  </th>
</tr>
</thead>
<tbody>
<tr>
<td>正</td>
<td>TP</td>
<td>FP</td>
<td></td>
</tr>
<tr>
<td>负</td>
<td>FN</td>
<td>TN</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Recall=TP/(TP+FN)，召回率，<strong>可理解为正确的被判断为正确的</strong></p>
<p>Precision=TP/(TP+FP)，准确度，<strong>预测为正类中，实际为正类的比例</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2018042521300715.png" alt></p>
<p><strong>准确率-召回率曲线（P-R曲线）</strong>：以召回率为横坐标，精确率为纵坐标</p>
<script type="math/tex; mode=display">
\mathrm{AP}=\int_{0}^{1} P(R) d(R)</script><script type="math/tex; mode=display">
\mathrm{mAP}=\frac{1}{\text { classes }} \sum_{i=1}^{\text {classes }} \int_{0}^{1} P(R) d(R)</script><p><strong>IoU</strong>：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180423003716617.png" alt></p>
<p>VOC07 以后使用 AP 评价检测性能。AP 定义为不同召回率下的平均检测精度，通常在某一特定类别下进行评估。为比较所有目标类的检测性能，通常使用 mAP 作为最终性能指标。</p>
<p>为度量目标定位精度，使用交并比（IoU）来检查预测框和 GT 框之间的 IoU是否大于预定义的阈值，如，0.5。如果大于阈值则表示成功检测，否则表示漏检。基于 0.5IoU 的 mAP 已经成为多年来用于目标检测问题的事实上的度量标准。</p>
<p>MS−COCO 的 AP 是在 0.5−0.95多个 IoU 阈值上的平均值。</p>
<h3 id="深度学习目标检测方法"><a href="#深度学习目标检测方法" class="headerlink" title="深度学习目标检测方法"></a>深度学习目标检测方法</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/deep_learning_object_detection_history.PNG" alt></p>
<p><strong>Two-stage Detectors（两阶段目标检测器）</strong></p>
<p>首先由算法（algorithm）生成一系列作为样本的候选框，再通过卷积神经网络进行样本（Sample）分类。</p>
<p>诸如R-CNN，Fast R-CNN，Faster R-CNN到最新的Mask Scoring R-CNN等网络结构，都属于Two-stage检测方法。</p>
<p><strong>One-stage Detectors（单阶段目标检测器）</strong></p>
<p>不需要产生候选框，直接将目标框定位的问题转化为回归（Regression）问题处理(Process)。</p>
<p>从最早的OverFeat到现在的YOLO，SSD，RetinaNet，YOLOv2，CornerNet等都属于one stage目标检测方法。</p>
<p>即：</p>
<p><strong>基于候选区域（Region Proposal）</strong>的，如R-CNN、SPP-net、Fast R-CNN、Faster R-CNN、R-FCN；</p>
<p><strong>基于端到端（End-to-End）</strong>，无需候选区域（Region Proposal）的，如YOLO、SSD。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180509095302426.png" alt></p>
<p>对于上述两种方式，基于候选区域（Region Proposal）的方法在检测准确率和定位精度上占优，基于端到端（End-to-End）的算法速度占优。相对于R-CNN系列的“看两眼”（候选框提取和分类），YOLO只需要“看一眼”。总之，目前来说，基于候选区域（Region Proposal）的方法依然占据上风，但端到端的方法速度上优势明显。</p>
<h3 id="各方法在各数据集上的精度"><a href="#各方法在各数据集上的精度" class="headerlink" title="各方法在各数据集上的精度"></a>各方法在各数据集上的精度</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Detector</th>
<th style="text-align:center">VOC07 (mAP@IoU=0.5)</th>
<th style="text-align:center">VOC12 (mAP@IoU=0.5)</th>
<th style="text-align:center">COCO (mAP@IoU=0.5:0.95)</th>
<th style="text-align:center">Published In</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">R-CNN</td>
<td style="text-align:center">58.5</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’14</td>
</tr>
<tr>
<td style="text-align:center">SPP-Net</td>
<td style="text-align:center">59.2</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ECCV’14</td>
</tr>
<tr>
<td style="text-align:center">MR-CNN</td>
<td style="text-align:center">78.2 (07+12)</td>
<td style="text-align:center">73.9 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ICCV’15</td>
</tr>
<tr>
<td style="text-align:center">Fast R-CNN</td>
<td style="text-align:center">70.0 (07+12)</td>
<td style="text-align:center">68.4 (07++12)</td>
<td style="text-align:center">19.7</td>
<td style="text-align:center">ICCV’15</td>
</tr>
<tr>
<td style="text-align:center">Faster R-CNN</td>
<td style="text-align:center">73.2 (07+12)</td>
<td style="text-align:center">70.4 (07++12)</td>
<td style="text-align:center">21.9</td>
<td style="text-align:center">NIPS’15</td>
</tr>
<tr>
<td style="text-align:center">YOLO v1</td>
<td style="text-align:center">66.4 (07+12)</td>
<td style="text-align:center">57.9 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">G-CNN</td>
<td style="text-align:center">66.8</td>
<td style="text-align:center">66.4 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">AZNet</td>
<td style="text-align:center">70.4</td>
<td style="text-align:center">-</td>
<td style="text-align:center">22.3</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">ION</td>
<td style="text-align:center">80.1</td>
<td style="text-align:center">77.9</td>
<td style="text-align:center">33.1</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">HyperNet</td>
<td style="text-align:center">76.3 (07+12)</td>
<td style="text-align:center">71.4 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">OHEM</td>
<td style="text-align:center">78.9 (07+12)</td>
<td style="text-align:center">76.3 (07++12)</td>
<td style="text-align:center">22.4</td>
<td style="text-align:center">CVPR’16</td>
</tr>
<tr>
<td style="text-align:center">MPN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">33.2</td>
<td style="text-align:center">BMVC’16</td>
</tr>
<tr>
<td style="text-align:center">SSD</td>
<td style="text-align:center">76.8 (07+12)</td>
<td style="text-align:center">74.9 (07++12)</td>
<td style="text-align:center">31.2</td>
<td style="text-align:center">ECCV’16</td>
</tr>
<tr>
<td style="text-align:center">GBDNet</td>
<td style="text-align:center">77.2 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">27.0</td>
<td style="text-align:center">ECCV’16</td>
</tr>
<tr>
<td style="text-align:center">CPF</td>
<td style="text-align:center">76.4 (07+12)</td>
<td style="text-align:center">72.6 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ECCV’16</td>
</tr>
<tr>
<td style="text-align:center">R-FCN</td>
<td style="text-align:center">79.5 (07+12)</td>
<td style="text-align:center">77.6 (07++12)</td>
<td style="text-align:center">29.9</td>
<td style="text-align:center">NIPS’16</td>
</tr>
<tr>
<td style="text-align:center">DeepID-Net</td>
<td style="text-align:center">69.0</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">PAMI’16</td>
</tr>
<tr>
<td style="text-align:center">NoC</td>
<td style="text-align:center">71.6 (07+12)</td>
<td style="text-align:center">68.8 (07+12)</td>
<td style="text-align:center">27.2</td>
<td style="text-align:center">TPAMI’16</td>
</tr>
<tr>
<td style="text-align:center">DSSD</td>
<td style="text-align:center">81.5 (07+12)</td>
<td style="text-align:center">80.0 (07++12)</td>
<td style="text-align:center">33.2</td>
<td style="text-align:center">arXiv’17</td>
</tr>
<tr>
<td style="text-align:center">TDM</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">37.3</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">FPN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">36.2</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">YOLO v2</td>
<td style="text-align:center">78.6 (07+12)</td>
<td style="text-align:center">73.4 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">RON</td>
<td style="text-align:center">77.6 (07+12)</td>
<td style="text-align:center">75.4 (07++12)</td>
<td style="text-align:center">27.4</td>
<td style="text-align:center">CVPR’17</td>
</tr>
<tr>
<td style="text-align:center">DeNet</td>
<td style="text-align:center">77.1 (07+12)</td>
<td style="text-align:center">73.9 (07++12)</td>
<td style="text-align:center">33.8</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">CoupleNet</td>
<td style="text-align:center">82.7 (07+12)</td>
<td style="text-align:center">80.4 (07++12)</td>
<td style="text-align:center">34.4</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">RetinaNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">39.1</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">DSOD</td>
<td style="text-align:center">77.7 (07+12)</td>
<td style="text-align:center">76.3 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">SMN</td>
<td style="text-align:center">70.0</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ICCV’17</td>
</tr>
<tr>
<td style="text-align:center">Light-Head R-CNN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">41.5</td>
<td style="text-align:center">arXiv’17</td>
</tr>
<tr>
<td style="text-align:center">YOLO v3</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">33.0</td>
<td style="text-align:center">arXiv’18</td>
</tr>
<tr>
<td style="text-align:center">SIN</td>
<td style="text-align:center">76.0 (07+12)</td>
<td style="text-align:center">73.1 (07++12)</td>
<td style="text-align:center">23.2</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">STDN</td>
<td style="text-align:center">80.9 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">RefineDet</td>
<td style="text-align:center">83.8 (07+12)</td>
<td style="text-align:center">83.5 (07++12)</td>
<td style="text-align:center">41.8</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">SNIP</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">45.7</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">Relation-Network</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">32.5</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">Cascade R-CNN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.8</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">MLKP</td>
<td style="text-align:center">80.6 (07+12)</td>
<td style="text-align:center">77.2 (07++12)</td>
<td style="text-align:center">28.6</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">Fitness-NMS</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">41.8</td>
<td style="text-align:center">CVPR’18</td>
</tr>
<tr>
<td style="text-align:center">RFBNet</td>
<td style="text-align:center">82.2 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">ECCV’18</td>
</tr>
<tr>
<td style="text-align:center">CornerNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.1</td>
<td style="text-align:center">ECCV’18</td>
</tr>
<tr>
<td style="text-align:center">PFPNet</td>
<td style="text-align:center">84.1 (07+12)</td>
<td style="text-align:center">83.7 (07++12)</td>
<td style="text-align:center">39.4</td>
<td style="text-align:center">ECCV’18</td>
</tr>
<tr>
<td style="text-align:center">Pelee</td>
<td style="text-align:center">70.9 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">NIPS’18</td>
</tr>
<tr>
<td style="text-align:center">HKRM</td>
<td style="text-align:center">78.8 (07+12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">37.8</td>
<td style="text-align:center">NIPS’18</td>
</tr>
<tr>
<td style="text-align:center">M2Det</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.2</td>
<td style="text-align:center">AAAI’19</td>
</tr>
<tr>
<td style="text-align:center">R-DAD</td>
<td style="text-align:center">81.2 (07++12)</td>
<td style="text-align:center">82.0 (07++12)</td>
<td style="text-align:center">43.1</td>
<td style="text-align:center">AAAI’19</td>
</tr>
<tr>
<td style="text-align:center">ScratchDet</td>
<td style="text-align:center">84.1 (07++12)</td>
<td style="text-align:center">83.6 (07++12)</td>
<td style="text-align:center">39.1</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">Libra R-CNN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">43.0</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">Reasoning-RCNN</td>
<td style="text-align:center">82.5 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">43.2</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">FSAF</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.6</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">AmoebaNet + NAS-FPN</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">47.0</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">Cascade-RetinaNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">41.1</td>
<td style="text-align:center">CVPR’19</td>
</tr>
<tr>
<td style="text-align:center">TridentNet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">48.4</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">DAFS</td>
<td style="text-align:center"><strong>85.3 (07+12)</strong></td>
<td style="text-align:center">83.1 (07++12)</td>
<td style="text-align:center">40.5</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">Auto-FPN</td>
<td style="text-align:center">81.8 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">40.5</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">FCOS</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.7</td>
<td style="text-align:center">ICCV’19</td>
</tr>
<tr>
<td style="text-align:center">FreeAnchor</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">44.8</td>
<td style="text-align:center">NeurIPS’19</td>
</tr>
<tr>
<td style="text-align:center">DetNAS</td>
<td style="text-align:center">81.5 (07++12)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.0</td>
<td style="text-align:center">NeurIPS’19</td>
</tr>
<tr>
<td style="text-align:center">NATS</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">42.0</td>
<td style="text-align:center">NeurIPS’19</td>
</tr>
<tr>
<td style="text-align:center">AmoebaNet + NAS-FPN + AA</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">50.7</td>
<td style="text-align:center">arXiv’19</td>
</tr>
<tr>
<td style="text-align:center">EfficientDet</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><strong>51.0</strong></td>
<td style="text-align:center">arXiv’19</td>
</tr>
</tbody>
</table>
</div>
<h3 id="目标检测的候选框-Proposal"><a href="#目标检测的候选框-Proposal" class="headerlink" title="目标检测的候选框(Proposal)"></a>目标检测的候选框(Proposal)</h3><p>物体候选框获取，当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)。</p>
<p>根据目标候选区域的提取方式不同，传统目标检测算法可以分为基于滑动窗口的目标检测算法和基于选择性搜索的目标检测算法。</p>
<p><strong>滑动窗口（Sliding Window）</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/1423648-20190316210029111-840970217.png" alt></p>
<p>基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类</p>
<p>缺点：不知道要检测的目标大小是什么规模，所以要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量。</p>
<p>具体步骤：首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选。最终，经过NMS筛选后获得检测到的物体。</p>
<p><strong>非极大值抑制(Non-Maximum Suppression,NMS)</strong></p>
<p>根据分类器类别分类概率做排序</p>
<p>　　(1)从最大概率矩形框开始，分别得分后面的矩形框与其的重叠度IOU是否大于某个设定的阈值;</p>
<p>　　(2)假设重叠度超过阈值，那么就扔掉；并标记第一个矩形框，是我们保留下来的。</p>
<p>　　(3)从剩下的矩形框中，选择概率最大的，然后判断得分后面的矩形框与其的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记是我们保留下来的第二个矩形框。</p>
<p>　　如此循环往复知道没有剩余的矩形框，然后找到所有被保留下来的矩形框，就是我们认为最可能包含目标的矩形框。</p>
<p><strong>R-CNN算法中NMS的具体做法</strong>：</p>
<p>　　假设有20类，2000个建议框，最后输出向量维数2000*20，则每列对应一类，一行是各个建议框的得分，NMS算法步骤如下：<br>　　① 对2000×20维矩阵中每列按从大到小进行排序；<br>　　② 从每列最大的得分建议框开始，分别与该列后面的得分建议框进行IoU计算，若IoU&gt;阈值，则剔除得分较小的建议框，否则认为图像中存在多个同一类物体；<br>　　③ 从每列次大的得分建议框开始，重复步骤②；<br>　　④ 重复步骤③直到遍历完该列所有建议框；<br>　　⑤ 遍历完2000×20维矩阵所有列，即所有物体种类都做一遍非极大值抑制；<br>　　⑥ 最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框。</p>
<p><strong>选择性搜索(Selective Search)</strong></p>
<p>滑窗法类似穷举进行图像子区域搜索，但是一般情况下图像中大部分子区域是没有物体的。学者们自然而然想到只对图像中最有可能包含物体的区域进行搜索以此来提高计算效率。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/1423648-20190317100459585-1587811888.png" alt></p>
<p>选择搜索算法的主要观点：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。因此，选择搜索基于上面这一想法采用子区域合并的方法进行提取候选边界框（bounding boxes）。首先，对输入图像进行分割算法产生许多小的子区域(大约2000个子区域)。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做外切矩形（bounding boxes），这些子区域外切矩形就是通常所说的候选框。</p>
<p>优点：<br>　　　　（a）计算效率优于滑窗法。<br>　　　　（b）由于采用子区域合并策略，所以可以包含各种大小的疑似物体框。<br>　　　　（c）合并区域相似的指标多样性，提高了检测物体的概率。</p>
<h3 id="边界框回归-Bounding-Box-regression"><a href="#边界框回归-Bounding-Box-regression" class="headerlink" title="边界框回归(Bounding-Box regression)"></a>边界框回归(Bounding-Box regression)</h3><p>窗口一般使用四维向量$(x,y,w,h)$来表示， 分别表示窗口的中心点坐标和宽高。 对于下图, 红色的框 $P$ 代表原始的Proposal, 绿色的框 $G$ 代表目标的 Ground Truth， 目标是寻找一种关系使得输入原始的窗口 $P$ 经过映射得到一个跟真实窗口 $G$ 更接近的回归窗口$\hat{G}$。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/TIM%E5%9B%BE%E7%89%8720200106182123.png" alt></p>
<p>边框回归的目的既是：给定$\left(P_{x}, P_{y}, P_{w}, P_{h}\right)$寻找一种映射$f$， 使得$f\left(P_{x}, P_{y}, P_{w}, P_{h}\right)=\left(\hat{G}_{x}, \hat{G}_{y}, \hat{G}_{w}, \hat{G}_{h}\right)$并且$\left(\hat{G}_{x}, \hat{G}_{y}, \hat{G}_{w}, \hat{G}_{h}\right) \approx\left(G_{x}, G_{y}, G_{w}, G_{h}\right)$</p>
<p>思路: 平移 + 尺度放缩</p>
<ol>
<li><p>先做平移$(\Delta x, \Delta y), \quad \Delta x=P_{w} d_{x}(P), \Delta y=P_{h} d_{y}(P)$</p>
<p>$\hat{G}_{x}=P_{w} d_{x}(P)+P_{x}$<br>$\hat{G}_{y}=P_{h} d_{y}(P)+P_{y}$</p>
</li>
<li><p>然后再做尺度缩放$\left(S_{w}, S_{h}\right), S_{w}=\exp \left(d_{w}(P)\right), S_{h}=\exp \left(d_{h}(P)\right)$</p>
<p>$\hat{G}_{w}=P_{w} \exp \left(d_{w}(P)\right)$<br>$\hat{G}_{h}=P_{h} \exp \left(d_{h}(P)\right)$</p>
</li>
</ol>
<p>边框回归就是学习$d_{x}(P), d_{y}(P), d_{w}(P), d_{h}(P)$这四个变换。</p>
<p>线性回归就是给定输入的特征向量$X$, 学习一组参数 $W$, 使得经过线性回归后的值跟真实值 $Y$(Ground Truth)非常接近. 即$Y ≈ WX$。</p>
<p>输入为：$P=\left(P_{x}, P_{y}, P_{w}, P_{h}\right)$这个窗口对应的 CNN 特征(注：训练阶段输入还包括 Ground Truth， 也就是下边提到的$t_{*}=\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$ )</p>
<p>输出为：四个变换，因为有了这四个变换我们就可以直接得到 Ground Truth，真实值所对应的真正的变化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
&t_{x}=\left(G_{x}-P_{x}\right) / P_{w}\\
&t_{y}=\left(G_{y}-P_{y}\right) / P_{h}\\
&t_{w}=\log \left(G_{w} / P_{w}\right)\\
&t_{h}=\log \left(G_{h} / P_{h}\right)
\end{aligned}</script><p>目标函数可以表示为$d_{<em>}(P)=w_{</em>}^{T} \Phi_{5}(P)$， $\Phi_{5}(P)$是输入 Proposal 的特征向量， $w_{<em>}$是要学习的参数（</em> 表示 $x,y,w,h$， 也就是每一个变换对应一个目标函数）, $d_{*}(P)$ 是得到的预测值。</p>
<p>损失函数：要让预测值跟真实值$t_{*}=\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$差距最小</p>
<script type="math/tex; mode=display">
Loss=\sum_{i}^{N}\left(t_{*}^{i}-\hat{w}_{*}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}</script><p>函数优化目标为：</p>
<script type="math/tex; mode=display">
W_{*}=\operatorname{argmin}_{w_{*}} \sum_{i}^{N}\left(t_{*}^{i}-\hat{w}_{*}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{w}_{*}\right\|^{2}</script><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>Rich feature hierarchies for accurate object detection and semantic segmentation(CVPR 2014)</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_09-57-14.jpg" alt></p>
<p><strong>算法流程</strong>：</p>
<ul>
<li>使用Selective Search提取大约2000个候选区域（proposal）;</li>
<li>对每个候选区域的图像进行拉伸形变，使之成为固定大小的正方形图像，并将该图像输入到CNN中提取特征;</li>
<li>使用线性的SVM对提取的特征进行分类。</li>
</ul>
<p><strong>创新点</strong>：</p>
<ul>
<li>将大型卷积神经网络(CNNs)应用于自下而上的候选区域以定位和分割物体。</li>
<li>当带标签的训练数据不足时，先针对辅助任务进行有监督预训练，再进行特定任务的调优，就可以产生明显的性能提升。</li>
</ul>
<p><strong>R-CNN图片缩放</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180425214236957.png" alt></p>
<p><strong>缺点</strong>：</p>
<ul>
<li><p><strong>重复计算</strong> R-CNN虽然不再是穷举，但通过Proposal（Selective Search）的方案依然有两千个左右的候选框，这些候选框都需要单独经过backbone网络提取特征，计算量依然很大，候选框之间会有重叠，因此有不少其实是重复计算。</p>
</li>
<li><p><strong>训练测试不简洁</strong> 候选区域提取、特征提取、分类、回归都是分开操作，中间数据还需要单独保存。</p>
</li>
<li><p><strong>速度慢</strong> 前面的缺点最终导致R-CNN出奇的慢，GPU上处理一张图片需要十几秒，CPU上则需要更长时间。</p>
</li>
<li><p><strong>输入的图片Patch必须强制缩放成固定大小</strong> （原文采用227×227），会造成物体形变，导致检测性能下降。</p>
</li>
</ul>
<h3 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h3><p>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(ECCV 2014)</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-28-09.jpg" alt></p>
<p>卷积层的参数和输入图像的尺寸无关，它仅仅是一个卷积核在图像上滑动，不管输入图像是多少都没关系，只是对不同大小的图片卷积出不同大小的特征图，池化层对输入图像的尺寸也没有任何限制，只是获得不同的特征图而已，但是全连接层的参数就和输入图像大小有关，因为它要把输入的所有像素点连接起来,需要指定输入层神经元个数和输出层神经元个数，所以需要规定输入的feature的大小。</p>
<p>通过在卷积层和全连接层之间加入空间金字塔池化结构（Spatial Pyramid Pooling）代替R-CNN算法在输入卷积神经网络前对各个候选区域进行剪裁、缩放操作使其图像子块尺寸一致的做法。</p>
<p><strong>算法流程</strong>：</p>
<ul>
<li>首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。</li>
<li>特征提取阶段。这一步就是和R-CNN最大的区别了，同样是用卷积神经网络进行特征提取，但是SPP-Net用的是金字塔池化。这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度更快啊。</li>
<li>最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。</li>
</ul>
<p><strong>一次特征提取</strong><br>RCNN是多个regions+多次CNN+单个pooling，而SPP则是单个图像+单次CNN+多个region+多个pooling<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180425215129861.png" alt></p>
<p><strong>金字塔池化结构</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-41-12.jpg" alt><br>利用不同大小的刻度，对一张图片进行了划分。图中，利用了三种不同大小的刻度，对一张输入的图片进行了划分，最后总共可以得到16+4+1=21个块，从这21个图片块中，分别计算每个块的最大值，从而得到一个输出特征向量。最后把一张任意大小的图片转换成了一个固定大小的21维特征（当然可以设计其它维数的输出，增加金字塔的层数，或者改变划分网格的大小）。上面的三种不同刻度的划分，每一种刻度称之为金字塔的一层，使用多个不同刻度的层，可以提高所提取特征的鲁棒性。每一个图片块大小称之为：Sliding Windows Size。如果希望金字塔的某一层输出nxn个特征，那么就要用Windows Size大小为：(w/n,h/n)进行池化。（这里有一个问题，就是如果(5x5）也要得到3x3的话，计算得到的size=2，stride=1，利用公式算出来得到的池化为（4x4）与预期的3x3不符，这里暂时还有问题，不清楚具体原因是什么）。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-46-02.jpg" alt></p>
<p><strong>创新点：</strong></p>
<ul>
<li>利用空间金字塔池化结构；</li>
<li>对整张图片只进行了一次特征提取，加快运算速度。</li>
</ul>
<p><strong>缺点</strong><br>　    和R-CNN一样，它的训练要经过多个阶段，特征也要存在磁盘中，另外，SPP中的微调只更新SPP层后面的全连接层，对很深的网络这样肯定是不行的。</p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>Girshick, R., Fast R-CNN, in 2015 IEEE International Conference on Computer Vision (ICCV). 2015. p. 1440-1448.</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-7569280b566d0e58.png" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-5519b489e581557a.png" alt></p>
<p>Fast R-CNN在特征提取上可以说很大程度借鉴了SPPnet，首先将图片用选择搜索算法（selective search）得到2000个候选区域（region proposals）的坐标信息。另一方面，直接将图片归一化到CNN需要的格式，整张图片送入CNN（本文选择的网络是VGG），将第五层的普通池化层替换为RoI池化层，图片然后经过5层卷积操作后，得到一张特征图（feature maps），开始得到的坐标信息通过一定的映射关系转换为对应特征图的坐标，截取对应的候选区域，经过RoI层后提取到固定长度的特征向量，送入全连接层。</p>
<p><strong>算法流程</strong>:</p>
<ul>
<li>输入的是一张完整图片图像归一化为224×224和一组（约2000个）物体建议框（也叫RoIs）送入网络。</li>
<li>对Conv feature map进行特征提取。每一个区域经过RoI pooling layer和FC layers得到一个固定长度的feature vector，这里需要注意的是，输入到后面RoI pooling layer的feature map是在Conv feature map上提取的。虽然在最开始也提取出了大量的RoI，但他们还是作为整体输入进卷积网络的，最开始提取出的RoI区域只是为了最后的Bounding box 回归时使用，用来输出原图中的位置。</li>
<li>这些特征向量在经过全接连层之后进入两个并列的输出层。第一个是分类，使用softmax，第二个是每一类的bounding box回归。利用SoftMax Loss和Smooth L1 Loss对分类概率和边框回归（Bounding Box Regression）联合训练。<br>整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。</li>
</ul>
<p><strong>什么是ROI呢？</strong></p>
<p>RoI是Region of Interest的简写，指的是在“<strong>特征图上的框</strong>”；在Fast RCNN中， RoI是指Selective Search完成后得到的“候选框”在特征图上的映射;</p>
<p><strong>ROI Pooling的输入</strong><br>输入有两部分组成：</p>
<ul>
<li>特征图：指的是特征图，在Fast RCNN中，它位于RoI Pooling之前，通常我们常常称之为“share_conv”；</li>
<li>ROIS ：在Fast RCNN中，指的是Selective Search的输出，一堆矩形候选框框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）。</li>
</ul>
<p><strong>ROI Pooling的输出</strong><br>　　输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel x w x h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w x h）的矩形框；</p>
<p><strong>ROI Pooling的过程</strong><br>　　先把RoI中的坐标映射到feature map上，映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标后，使用Pooling得到输出；由于输入的图片大小不一，所以这里使用的类似Spp Pooling，在Pooling的过程中需要计算Pooling后的结果对应到feature map上所占的范围，然后在那个范围中进行取max或者取average。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-a6127730f6d21abd.png" alt></p>
<ul>
<li>cls_score层用于分类，输出K+1维数组p，表示属于K类和背景的概率。</li>
<li>bbox_prdict层用于调整候选区域位置，输出4*K维数组t，表示分别属于K类时，应该平移缩放的参数。</li>
</ul>
<p><strong>最后一个阶段的特征输入到两个并行的全连层中。</strong><br>一个是对区域的分类Softmax（包括背景），另一个是对bounding box回归的微调。在SVM和Softmax的对比实验中说明，SVM的优势并不明显，故直接用Softmax将整个网络整合训练更好。总代价为两者加权和，Fast-RCNN把两个回归的loss进行联合训练。</p>
<p><strong>创新点</strong></p>
<ul>
<li>提出RoI Pooling 层，它将不同大小候选框的卷积特征图统一采样成固定大小的特征。</li>
<li>独立的SVM分类器和回归器需要大量特征作为训练样本，需要大量的硬盘空间，Fast-RCNN把类别判断和位置回归统一用深度神经网络实现，不再需要额外存储。</li>
</ul>
<p><strong>缺点</strong>：<br>检测速度仍然受限于 Selective Search</p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>Ren, S., et al., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Trans Pattern Anal Mach Intell, 2017. 39(6): p. 1137-1149.</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-e64a99b38f411c337f538eb5f093bdf3_r.jpg" alt></p>
<p><strong>算法流程</strong>:</p>
<ul>
<li>把整张图片送入CNN，进行特征提取；</li>
<li>在最后一层卷积feature map上生成region proposal（通过RPN），每张图片大约300个建议窗口；</li>
<li>通过RoI pooling层（其实是单层的SPP layer）使得每个建议窗口生成固定大小的feature map；</li>
<li>继续经过两个全连接层（FC）得到特征向量。特征向量经由各自的FC层，得到两个输出向量。第一个是分类，使用softmax，第二个是每一类的bounding box回归。利用SoftMax Loss和Smooth L1 Loss对分类概率和边框回归（Bounding Box Regression）联合训练。</li>
</ul>
<p><strong>Region Proposal Networks</strong>。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-1908feeaba591d28bee3c4a754cca282_r.jpg" alt></p>
<p>可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。</p>
<p>所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure><br>其中每行的4个值$\left(x_{1}, y_{1}, x_{2}, y_{2}\right)$表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为width: height $\in\{1: 1,1: 2,2: 1\}$三种，如图。实际上通过anchors就引入了检测中常用到的多尺度方法。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-7abead97efcc46a3ee5b030a2151643f_hd.jpg" alt></p>
<p>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p>
<p><strong>那么这9个anchors是做什么的呢？</strong>遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。</p>
<p><strong>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</strong></p>
<p>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-1ab4b6c3dd607a5035b5203c76b078f3_r.jpg" alt></p>
<p>可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9<em>2)大小的矩阵。<em>*为何这样做？</em></em>后面接softmax分类获得positive anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在positive anchors中）。</p>
<p><strong>那么为何要在softmax前后都接一个reshape layer？</strong>其实只是为了便于softmax分类,存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行positive/negative二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。</p>
<p>RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，原理类似）。</p>
<p><strong>对proposals进行bounding box regression</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-8241c8076d60156248916fe2f1a5674a_r.jpg" alt></p>
<p>可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$\left[d_{x}(A), d_{y}(A), d_{w}(A), d_{h}(A)\right]$变化量</p>
<p><strong>创新点</strong></p>
<ul>
<li>采用RPN(Region Proposal Network)代替选择性搜索(Selective Search)，利用GPU进行计算大幅度缩减提取region proposal的速度。</li>
<li>产生建议窗口的CNN和目标检测的CNN共享。</li>
</ul>
<p>在主干网络中增加了RPN （Region Proposal Network）网络，通过一定规则设置不同尺度的锚点（Anchor）在RPN的卷积特征层提取候选框来代替Selective Search等传统的候选框生成方法，实现了网络的端到端训练。候选区域生成、候选区域特征提取、框回归和分类全过程一气呵成，在训练过程中模型各部分不仅学习如何完成自己的任务，还自主学习如何相互配合。这也是第一个真正意义上的深度学习目标检测算法。</p>
<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>Redmon, J., et al., You Only Look Once: Unified, Real-Time Object Detection, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. p. 779-788.</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-d54136f1eb0cd733.png" alt></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-0d1e8d42480e1c1a.png" alt></p>
<p><strong>算法流程</strong></p>
<ul>
<li>将图像resize到448 * 448作为神经网络的输入 ；</li>
<li>运行神经网络，得到一些bounding box坐标、box中包含物体的置信度和class probabilities ；</li>
<li>进行非极大值抑制，筛选Boxes。</li>
</ul>
<p><strong>创新点</strong></p>
<ul>
<li>YOLO将目标检测问题转化为回归问题，不需要复杂的流程。在实时检测系统中， YOLO的效果是最好的。</li>
<li>YOLO 在做出预测时是推理整个图像的。与滑动窗口和候选区域算法不同， YOLO 在训练和测试时，从整个图像综合考虑，不仅分析物体的 appearance 还分析其 contextual 信息。Fast R-CNN 比较容易将背景误检测为物体，因为它不考虑 contextual 信息。YOLO 把背景误检测为物体的概率不到 Fast R-CNN 的一半。</li>
<li>YOLO 对物体的泛化能力比较好。当在自然图像上训练，在艺术图像上检测时，YOLO的效果要比 DPM 和 R-CNN 好很多。</li>
</ul>
<p>YOLO没有选择滑动窗口（silding window）或提取proposal的方式训练网络，而是直接选用整图训练模型，将 Object Detection 的问题转化成一个 Regression 问题。</p>
<p>将图片分为$S\times S$个单元格(原文中S=7)，之后的输出是以单元格为单位进行的：</p>
<ul>
<li>如果一个object的中心落在某个单元格上，那么这个单元格负责预测这个物体。</li>
<li>每个单元格需要预测B个bbox值(bbox值包括坐标和宽高，原文中B=2)，同时为每个bbox值预测一个置信度(confidence scores)。也就是每个单元格需要预测B×(4+1)个值。</li>
<li>每个单元格需要预测C(物体种类个数，原文C=20，这个与使用的数据库有关)个条件概率值.</li>
</ul>
<p>所以，最后网络的输出维度为$S \times S \times (B\times 5 + C)$，这里输出为$7\times7\times(2\times5+20)$, 虽然每个单元格负责预测一种物体(这也是这篇文章的问题，当有小物体时可能会有问题)，但是每个单元格可以预测多个bbox值(这里可以认为有多个不同形状的bbox，为了更准确的定位出物体，如下图所示)。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-29a33d2e79ed722b.png" alt></p>
<p><strong>(x,y)是bbox的中心相对于单元格的offset</strong><br>单元格坐标为$(x_{col},y_{row})$，假设它预测的输出的bbox中心坐标为$(x_c,y_c)$,那么最终预测出来的(x,y)是经过归一化处理的，表示的是中心相对于单元格的offset</p>
<p><strong>(w,h)是bbox相对于整个图片的比例</strong><br>预测的bbox的宽高为$w_b,h_b$，(w,h)表示的是bbox的是相对于整张图片的占比</p>
<p><strong>confidence</strong><br>这个置信度是由两部分组成，一是格子内是否有目标，二是bbox的准确度。定义置信度为$P_r(Object)∗IOU^{truth}_{pred}$<br>这里，如果格子内有物体，则$P_r(Object)=1$，此时置信度等于IoU。如果格子内没有物体，则$P_r(Object)=0$，此时置信度为0</p>
<p><strong>C类的条件概率</strong><br>条件概率定义为$P_r(Class_i |Object)$，表示该单元格存在物体且属于第i类的概率。</p>
<p>在测试的时候每个单元格预测最终输出的概率定义为，如下两图所示（两幅图不一样，代表一个框会输出B列概率值）<br>$P_r(Class_i|Object) ∗ P_r(Object) ∗ IOU^{truth}_{pred} = P_r(Class_i) ∗ IOU^{truth}_{pred}$</p>
<p>最后将$(S\times S)\times B\times20$ 列的结果送入NMS，最后即可得到最终的输出框结果</p>
<p><strong>损失函数</strong></p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180511204751564.png" alt></p>
<p>这里强调两点：<br> 1.每个图片的每个单元格不一定都包含object，如果没有object，那么confidence就会变成0，这样在优化模型的时候可能会让梯度跨越太大，模型不稳定跑飞了。为了平衡这一点，在损失函数中，设置两个参数$λ_{corrd}$和$λ_{noobj}$，其中$λ_{corrd}$控制bbox预测位置的损失，$λ_{noobj}$控制单个格内没有目标的损失。<br> 2.对于大的物体，小的偏差对于小的物体影响较大，为了减少这个影响，所以对bbox的宽高都开根号。</p>
<p><strong>缺点</strong></p>
<ul>
<li>YOLO的物体检测精度低于其他state-of-the-art的物体检测系统。</li>
<li>YOLO容易产生物体的定位错误。</li>
<li>YOLO对小物体的检测效果不好（尤其是密集的小物体，因为一个栅格只能预测2个物体）。</li>
</ul>
<h3 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h3><p>Redmon, J. and A. Farhadi, YOLO9000: Better, Faster, Stronger. (CVPR 2017)</p>
<p><strong>改进</strong>：</p>
<ul>
<li><p><strong>Batch Normalization</strong></p>
<p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p>
</li>
<li><p><strong>High Resolution Classifier</strong></p>
<p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为$224 \times 224$的图片作为输入，分辨率相对较低，不利于检测模型。所以YOLOv1在采用$224 \times 224$分类模型预训练后，将分辨率增加至$448 \times 448$，并使用这个高分辨率在检测数据集上fine tune。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。所以YOLOv2增加了在ImageNet数据集上使用$448 \times 448$来fine tune分类网络这一中间过程（10 epochs），这可以使得模型在检测数据集上fine tune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p>
</li>
<li><p><strong>Convolutionlal With Anchor Boxes</strong></p>
<p>anchor是RNP网络中的一个关键步骤，说的是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的建议框。</p>
<p><strong>YOLO v1： S*S* (B*5 + C) =&gt; 7*7（2*5+20）</strong><br>　　 其中B对应Box数量，5对应 Rect 定位+置信度。每个Grid只能预测对应两个Box，这两个Box共用一个分类结果（20 classes），这是很不合理的临时方案。</p>
<p><strong>YOLO v2： S*S*K* (5 + C) =&gt; 13*13*9（5+20）</strong><br>　　分辨率改成了13*13，更细的格子划分对小目标适应更好，再加上与Faster一样的K=9，计算量增加了不少。通过Anchor Box改进，mAP由69.5下降到69.2，Recall由81%提升到了88%。<br>　　<br>　　为了引入anchor boxes来预测bounding boxes，作者在网络中果断去掉了全连接层。首先，作者去掉了后面的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416x416，这一步的目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。作者观察到，大物体通常占据了图像的中间位置， 就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（factor为32），使得输入卷积网络的416x416图片最终得到13x13的卷积特征图（416/32=13）。<br>　　加入了anchor boxes后，可以预料到的结果是召回率上升，准确率下降。我们来计算一下，假设每个cell预测9个建议框，那么总共会预测13x13x9 = 1521个boxes，而之前的网络仅仅预测7x7x2 = 98个boxes。具体数据为：没有anchor boxes，模型recall为81%，mAP为69.5%；加入anchor boxes，模型recall为88%，mAP为69.2%。这样看来，准确率只有小幅度的下降，而召回率则提升了7%，说明可以通过进一步的工作来加强准确率，的确有改进空间。</p>
</li>
<li><p><strong>New Network——Darknet-19</strong></p>
<p>YOLOv2使用了一个新的分类网络作为特征提取部分，参考了前人的先进经验，比如类似于VGG，作者使用了较多的3x3卷积核，在每一次池化操作后把通道数翻倍。借鉴了network in network的思想，网络使用了全局平均池化（global average pooling），把1x1的卷积核置于3x3的卷积核之间，用来压缩特征。也用了batch normalization稳定模型训练。模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_18-40-15.jpg" alt></p>
</li>
</ul>
<ul>
<li><p><strong>Dimension Clusters</strong></p>
<p>在Faster R-CNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： $d(\text { box }, \text { centroid })=1-\mathrm{IOU}(\text { box }, \text { centroid })$</p>
</li>
<li><p><strong>Direct location prediction</strong></p>
<script type="math/tex; mode=display">
x=\left(t_{x} * w_{a}\right)-x_{a}</script><script type="math/tex; mode=display">
y=\left(t_{y} * h_{a}\right)-y_{a}</script><p>这个公式的理解为：当预测tx=1，就会把box向右边移动一定距离（具体为anchor box的宽度），预测tx=-1，就会把box向左边移动相同的距离，该公式没有任何约束，中心点可能会出现在图像任何位置，这就有可能导致回归过程震荡，甚至无法收敛。</p>
<p><strong>强约束方法</strong>：</p>
<ul>
<li>对应 Cell 距离左上角的边距为$\left(C_{x}, C_{y}\right)$，σ定义为sigmoid激活函数，将函数值约束到［0，1］，用来预测相对于该Cell中心的偏移（不会偏离cell）；</li>
<li>预定Anchor（文中描述为bounding box prior）对应的宽高为$\left(P_{w}, P_{h}\right)$，预测 Location 是相对于Anchor的宽高乘以系数得到；</li>
</ul>
</li>
</ul>
<p>  如果这个cell距离图像左上角的边距为$\left(C_{x}, C_{y}\right)$以及该cell对应的box维度（bounding box prior）的长和宽分别为$\left(P_{w}, P_{h}\right)$，那么预测值可以表示为： </p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_18-53-58.jpg" alt></p>
<ul>
<li><p><strong>Fine-Grained Features</strong><br>YOLO最终在13x13的特征图上进行预测，虽然这足以胜任大尺度物体的检测，但对于小物体还需要更精细的特征图（Fine-Grained Features），前面更精细的特征图可以用来预测小物体。<br>YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26x26大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为26x26x512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2x2的局部区域，然后将其转化为channel维度，对于26x26x512的特征图，经passthrough层处理之后就变成了13x13x2048的新特征图（特征图大小降低4倍，而channles增加4倍），这样就可以与后面的13x13x1024特征图连接在一起形成13x13x3072的特征图，然后在此特征图基础上卷积做预测。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_19-14-51.jpg" alt></p>
</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20161229150249883.jpg" alt></p>
<ul>
<li><p><strong>Multi-Scale Training</strong><br>　　为了让 YOLOv2 适应不同Scale下的检测任务，作者尝试通过不同分辨率图片的训练来提高网络的适应性。(网络只用到了卷积层和池化层，可以进行动态调整（检测任意大小图片）)<br>具体做法是：<br>　　 每经过10次训练（10 epoch），就会随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度池化{320,352，…，608}。最终最小的尺寸为320 <em> 320，最大的尺寸为608 </em> 608。接着按照输入尺寸调整网络进行训练。</p>
<p>​         这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。</p>
</li>
<li><p><strong>交叉数据训练</strong></p>
<p>YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p>
<p>作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_09-22-20.jpg" alt><br>分类时的概率计算借用了决策树思想，某个节点的概率值等于 该节点到根节点的所有条件概率之积。<br>如果想求得特定节点的绝对概率，只需要沿着路径做连续乘积。例如 如果想知道一张图片是不是“Norfolk terrier ”需要计算：</p>
<p>$\operatorname{Pr}(\text { Norfolk terrier })=\operatorname{Pr}(\text { Norfolk terrier } | \text { terrier })$<br>*$\operatorname{Pr}(\text { terrier } | \text { hunting dog })$</p>
<p><em>. . . </em></p>
<p><em>$\operatorname{Pr}(\text { mammal } |\text { animal })$
  </em>$\operatorname{Pr}(\text { animal } | \text { physical object })$<br>softmax操作也同时应该采用分组操作，基于所有“同义词集”计算softmax，其中“同义词集”是同一概念的下位词。</p>
</li>
</ul>
<h3 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h3><p>Redmon, J. and A. Farhadi, YOLOv3: An Incremental Improvement. 2018.</p>
<ul>
<li><p><strong>新的网络结构Darknet-53</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-e65c08c61bfaa7c7.png" alt></p>
<p>借鉴了残差网络residual network的做法，在一些层之间设置了快捷链路（shortcut connections）。每个残差组 件有两个卷积层和一个快捷链路。</p>
</li>
<li><p><strong>利用多尺度特征进行对象检测</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-bc5def91d05e4d3a.png" alt><br>YOLO2曾采用passthrough结构来检测细粒度特征，在YOLO3更进一步采用了3个不同尺度的特征图来进行对象检测。</p>
<p>结合上图看，卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416x416的话，这里的特征图就是13x13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。</p>
<p>为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。</p>
<p>最后，第91层特征图再次上采样，并与第36层特征图融合（Concatenation），最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-9b28d0f1c682b80a.png" alt></p>
<p>对于一个416x416的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13x13x3 + 26x26x3 + 52x52x3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。</p>
<p>对比一下，YOLO2采用13x13x5 = 845个预测，YOLO3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。</p>
</li>
<li><p><strong>9种尺度的先验框</strong><br>随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。</p>
<p>分配上，在最小的13x13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26x26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52x52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-5cc00a60004e0473.png" alt></p>
</li>
<li><p><strong>对象分类softmax改成logistic</strong><br>YOLOv3 不使用 Softmax 对每个框进行分类，主要考虑因素有两个：</p>
<ul>
<li>Softmax 使得每个框分配一个类别（得分最高的一个），而对于 Open Images这种数据集，目标可能有重叠的类别标签，因此 Softmax不适用于多标签分类。</li>
<li>Softmax 可被独立的多个 logistic 分类器替代，且准确率不会下降。<br>所以预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象。</li>
</ul>
</li>
</ul>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p>Liu, W., et al., SSD: Single Shot MultiBox Detector. (ECCV 2016)</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_14-51-11.jpg" alt></p>
<p><strong>创新点</strong></p>
<ul>
<li><p><strong>在多层多尺度特征图上进行检测</strong></p>
<p>低层特征图具有细节信息，看得比较细，而高层特征图中具有高级语义信息，看得比较广，SSD提出同时利用低层特征图和高层特征图进行检测。SSD为了避免利用太低层的特征，从VGG后面开始，又往后添加了4层卷积层，如此就得到了多层次的特征图。这6层特征图的大小分别为：38x38, 19x19, 10x10, 5x5, 3x3, 1x1</p>
</li>
<li><p><strong>default boxes</strong></p>
<p>two-stage 方法太慢，计算代价大。SSD中避免使用region proposals,而采用default boxes。SSD借鉴RPN网络中的anchor box概念。首先将feature map划分为小格子叫做feature map cell，再在每个cell中设置一系列不同长宽比的default box。将其应用于不同分辨率的特征图中。在多个特征图中使用不同的默认框形状，可以有效地离散可能的输出框形状空间。</p>
</li>
<li><p><strong>用卷积层来预</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2148039-cec47c5d71ce8f22.png" alt>3x3的卷积在特征图上进行操作，在特征图的每个点提取特征，然后对每个default box生成四个偏移量来表示生成的bounding box，这四个偏移量分别表示生成框中心x坐标、y坐标、宽度、高度对应于当前default box的偏移量，这里的计算方法采用了RPN中的方法。同时，针对每个bounding box, 卷积会进行分类操作，输出C个分类数值，注意这里的C包括背景类。因此对于每个default box有C+4个输出值，而K个default box就有K(C+4)个输出值，因此，总共需要K*(C+4)个3x3卷积。</p>
<p>总的来说就是采用default box的方式，在多层多尺度的特征图上使用卷积进行检测（分类+回归）。</p>
</li>
<li><p><strong>数据增广</strong><br>为了使得模型对于不同大小、不同形状的物体具有鲁棒性，因此采用了data augmentation。这篇论文在训练时，首先对一张图片进行random crop，然后将其resize的原图尺度，然后以0.5的概率进行水平翻转、同时添加一些色彩变换。</p>
</li>
<li><p><strong>加入atrous</strong><br>将 VGG 中的 FC6 layer、FC7 layer 转成为 卷积层，并从模型的 FC6、FC7 上的参数，进行采样得到这两个卷积层的 parameters。还将 Pool5 layer 的参数，从2×2−s2转变成 3×3−s4，外加一个 pad（1）（猜想是不想reduce特征图大小），为了配合这种变化，采用了一种Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_15-32-00.jpg" alt><br>带孔卷积并不是卷积核里带孔，而是在卷积的时候，跳着的去卷积map（比如dilated＝2的孔卷积，就是隔一个像素点，“卷”一下，这就相当于把卷积核给放大了（3x3的核变成7x7的核，多出位置的weights给0就是。）这样就使得3x3的卷积核也能达到7x7卷积核的感受野</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2018051716490729.png" alt></p>
</li>
</ul>
<p><strong>缺点</strong><br>SSD存在的缺点：</p>
<ul>
<li>需要手动设置参数prior box，无法通过训练得到，依赖经验。</li>
<li>存在着对小目标检测效果不好的现象。</li>
</ul>
<h3 id="DSSD"><a href="#DSSD" class="headerlink" title="DSSD"></a>DSSD</h3><p>Fu, C., et al., DSSD : Deconvolutional Single Shot Detector. (CVPR 2017)</p>
<p>DSSD针对小目标鲁棒性太差，提出了以下两个贡献：</p>
<ul>
<li>把SSD的基准网络从VGG换成了Resnet-101，增强了特征提取能力；</li>
<li>使用反卷积层（deconvolution layer ）增加了大量上下文信息。</li>
</ul>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-00-32.jpg" alt></p>
<p>换完base network之后，作者通过实验发现，准确率从77.5%降至了76.4%，说明只换网络并不能够提高准确率。如果能够提高每一个子网络的准确率，那整个网络的准确率也会得到提升。所以作者在每次预测之前加入了一个“预测模块”（prediction module）：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-11-49.jpg" alt><br>如上图所示：<br>(a)为原SSD对于特征信息进行分类与定位的模型，其实并没有预测模块，直接进行预测；<br>(b)为作者在预测之前加入了residual block模块；<br>(c)将residual block模块中直接映射identity mapping换成了1x1卷积<br>(d)堆积residual block模块</p>
<p>经过实验证明，（c）能够获得更好的准确率。</p>
<p>第二个贡献就是添加了反卷积模块，引入了空间上下文信息，从而大大提高了检测准确率。添加了反卷积之后，整个网络形成不对称的沙漏结构。</p>
<p>蓝色块为卷积层，红色块为反卷积层。红色块做反卷积操作，然后与同大小的卷积层融合，之后再进行物体检测。其中的“反卷积模块”（Deconvolution module）</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-17-17.jpg" alt></p>
<p>其中Eltw Product是元素点积操作。作者也尝试了元素求和，但是点积操作的准确率稍高一些。</p>
<h3 id="目标检测进展"><a href="#目标检测进展" class="headerlink" title="目标检测进展"></a>目标检测进展</h3><ul>
<li><p>用更好的引擎检测</p>
<p>AlexNet<br>VGG： 16—19层，使用 3×3 卷积核取代 5×5 和 7×7。<br>GoogLeNet： 即，Inception 网络家族，增加卷积神经网络深度和宽度。<br>ResNet<br>DenseNet： 受 short cut 连接的影响，作者将每一层以前馈的方式和其他所有层相连。<br>SENet： 主要贡献是将全局池化与 shuffling 结合，学习特征图通道的重要性。</p>
</li>
<li><p>用更好的特征检测<br>研究者在最新检测引擎的基础上，努力提高图像特征质量，最重要的两组方法是：</p>
<ul>
<li>特征融合<br>-自底向上融合：通过跳跃连接将浅层特征传递到深层<br>-自顶向下融合：将深层特征反馈给浅层<br>特征融合可以被看做是不同特征图间的元素运算。主要有几组方法：<br>元素求和<br>元素乘积<br>串联<br>元素乘积的优点是可以抑制或者强调某一区域内特征，可能有利于小目标检测。特征串联的优点是可以整合不同区域的上下文特征，缺点是增加内存消耗。</li>
<li>学习具有大感受野的高分辨率特征<br>小感受野更关注局部细节。特征图分辨率越小，越难检测小目标。增加特征分辨率最直接的方法是去除池化层，或者减小卷积下采样率。但是这样同时会减小感受野。<br>同时增加感受野和分辨率的方法是引入空洞卷积。</li>
</ul>
</li>
<li>定位提升<br>主要有两种方法：<br>边界框精炼<br>设计新的损失函数</li>
<li>对旋转和尺度变化鲁棒的检测<br>数据增强<br>为不同方位训练独立检测器</li>
</ul>
<h3 id="未来的研究"><a href="#未来的研究" class="headerlink" title="未来的研究"></a>未来的研究</h3><ul>
<li>轻量级目标检测：加速检测算法，使其能够在移动设备上平稳运行。</li>
<li>检测与AutoML：未来的一个方向是使用神经结构搜索，减少设计检测模型时的人为干预(例如，如何设计引擎，如何设置Anchor)</li>
<li>Proposal的Anchor生成方式</li>
<li>弱监督检测</li>
<li>小目标检测</li>
<li>视频中的检测（实时）</li>
<li>信息融合检测</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>PyQt5与Opencv的小小融合</title>
    <url>/2019/03/20/PyQt5%E4%B8%8EOpencv%E7%9A%84%E5%B0%8F%E5%B0%8F%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<h2 id="一些核心代码"><a href="#一些核心代码" class="headerlink" title="一些核心代码"></a><a id="more"></a>一些核心代码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class myLabel(QLabel):</span><br><span class="line">    x0 &#x3D; 0</span><br><span class="line">    y0 &#x3D; 0</span><br><span class="line">    x1 &#x3D; 0</span><br><span class="line">    y1 &#x3D; 0</span><br><span class="line">    flag &#x3D; False</span><br><span class="line"></span><br><span class="line">    def mousePressEvent(self,event):</span><br><span class="line">        self.flag &#x3D; True</span><br><span class="line">        self.x0 &#x3D; event.x()</span><br><span class="line">        self.y0 &#x3D; event.y()</span><br><span class="line">    def mouseReleaseEvent(self,event):</span><br><span class="line">        self.flag &#x3D; False</span><br><span class="line"></span><br><span class="line">    def mouseMoveEvent(self,event):</span><br><span class="line">        if self.flag:</span><br><span class="line">            self.x1 &#x3D; event.x()</span><br><span class="line">            self.y1 &#x3D; event.y()</span><br><span class="line">            self.update()</span><br><span class="line">    def paintEvent(self, event):</span><br><span class="line">        super().paintEvent(event)</span><br><span class="line">        rect &#x3D;QRect(self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">        painter &#x3D; QPainter(self)</span><br><span class="line">        painter.setPen(QPen(Qt.red,4,Qt.SolidLine))</span><br><span class="line">        painter.drawRect(rect)</span><br><span class="line"></span><br><span class="line">        pqscreen  &#x3D; QGuiApplication.primaryScreen()</span><br><span class="line">        pixmap2 &#x3D; pqscreen.grabWindow(self.winId(), self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">        pixmap2.save(&#39;555.png&#39;)</span><br><span class="line">    </span><br><span class="line">class Example(QWidget):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.initUI()</span><br><span class="line">    def initUI(self):</span><br><span class="line">        self.resize(675, 300)</span><br><span class="line">        self.setWindowTitle(&#39;关注微信公众号：学点编程吧--opencv、PyQt5的小小融合&#39;)</span><br><span class="line"></span><br><span class="line">        self.lb &#x3D; myLabel(self)</span><br><span class="line">        self.lb.setGeometry(QRect(140, 30, 511, 241))</span><br><span class="line"></span><br><span class="line">        img &#x3D; cv2.imread(&#39;xxx.jpg&#39;)</span><br><span class="line">        height, width, bytesPerComponent &#x3D; img.shape</span><br><span class="line">        bytesPerLine &#x3D; 3 * width</span><br><span class="line">        cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)</span><br><span class="line">        QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br><span class="line">        pixmap &#x3D; QPixmap.fromImage(QImg)</span><br><span class="line"></span><br><span class="line">        self.lb.setPixmap(pixmap)</span><br><span class="line">        self.lb.setCursor(Qt.CrossCursor)</span><br><span class="line"></span><br><span class="line">        self.show()</span><br></pre></td></tr></table></figure>
<h2 id="实现大体思路"><a href="#实现大体思路" class="headerlink" title="实现大体思路"></a>实现大体思路</h2><ul>
<li>重新实现QLabel类，在类中重新实现了鼠标的点击、拖动、释放、以及绘画事件</li>
<li>在窗体上新建了一个label标签，然后载入图片</li>
<li>label标签载入的图像是由Opencv实现的</li>
</ul>
<h2 id="鼠标画矩形的思路"><a href="#鼠标画矩形的思路" class="headerlink" title="鼠标画矩形的思路"></a>鼠标画矩形的思路</h2><ul>
<li>新建一个矩形是否完成标志flag，默认是Flase，表示未完成</li>
<li>鼠标点击的时候，记录当前鼠标所在位置的坐标，flag标志置为True，表示开始画矩形了</li>
<li>鼠标拖动的时候，因为flag为True，所以记录当前鼠标所在位置的坐标</li>
<li>鼠标释放的时候，flag置为False，表示矩形画完了，准备画下一个了</li>
</ul>
<h2 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h2><h3 id="Opencv图像的转换"><a href="#Opencv图像的转换" class="headerlink" title="Opencv图像的转换"></a>Opencv图像的转换</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv2.imread(&#39;xxx.jpg&#39;)</span><br><span class="line">height, width, bytesPerComponent &#x3D; img.shape</span><br><span class="line">bytesPerLine &#x3D; 3 * width</span><br><span class="line">cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)</span><br><span class="line">QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br><span class="line">pixmap &#x3D; QPixmap.fromImage(QImg)</span><br></pre></td></tr></table></figure>
<p>这个就是Opencv和PyQt对象的转化了。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#x3D; cv2.imread(&#39;xxx.jpg&#39;)</span><br></pre></td></tr></table></figure><br>使用Opencv读取图像。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">height, width, bytesPerComponent &#x3D; img.shape</span><br></pre></td></tr></table></figure><br>在OpenCV-Python绑定中，图像使用NumPy数组的属性(这就解释了为什么要更新numpy)来表示图像的尺寸和通道信息。此时如果我们输出img.shape，将得到(200, 360, 3)。最后的3表示这是一个RGB图像。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)</span><br></pre></td></tr></table></figure><br>将图像从一个颜色空间转换为另一个颜色空间。<br>Python中的函数要求是这样的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Python：cv2.cvtColor（src，code [，dst [，dstCn]]）→dst</span><br></pre></td></tr></table></figure><br>参数：</p>
<ol>
<li>src - 输入图像：8位无符号，16位无符号（CV_16UC …）或单精度浮点数。</li>
<li>dst - 输出与src相同大小和深度的图像。</li>
<li>code - 颜色空间转换代码（请参阅下面的说明）。</li>
<li>dstCn - 目标图像中的通道数量；如果参数是0，则通道的数量是从src和代码自动导出的。</li>
</ol>
<p>该函数将输入图像从一个颜色空间转换为另一个颜色空间。在从RGB颜色空间转换到RGB颜色空间的情况下，通道的顺序应明确指定（RGB或BGR）。请注意，OpenCV中的默认颜色格式通常被称为RGB，但实际上是BGR（字节相反）。因此，标准（24位）彩色图像中的第一个字节将是一个8位蓝色分量，第二个字节将是绿色，而第三个字节将是红色。第四，五，六字节将是第二个像素（蓝色，然后是绿色，然后是红色），依此类推。</p>
<p>这里我们就是要求从Opencv的BGR图像转换成RGB图像了。为什么？因为要转换成PyQt5可以识别的啊！<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bytesPerLine &#x3D; 3 * width</span><br><span class="line">QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br></pre></td></tr></table></figure><br>QImage类提供了独立于硬件的图像表示形式，允许直接访问像素数据，并可用作绘画设备。Qt提供了四个类来处理图像数据：QImage，QPixmap，QBitmap和QPicture。QImage是为I/O设计和优化的，并且可以直接进行像素访问和操作，而QPixmap则是针对在屏幕上显示图像而设计和优化的。 </p>
<p>QBitmap只是一个继承QPixmap的便利类，深度为1。最后，QPicture类是一个记录和重放QPainter命令的绘图设备。</p>
<p>因为QImage是一个QPaintDevice子类，QPainter可以用来直接绘制图像。在QImage上使用QPainter时，可以在当前GUI线程之外的另一个线程中执行绘制。QImage提供了一系列功能，可用于获取有关图像的各种信息。也有几个功能，使图像转换。<br>详见官网介绍：<a href="https://doc.qt.io/qt-5/qimage.html" target="_blank" rel="noopener">QImage Class | Qt GUI 5.10</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">QImg &#x3D; QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)</span><br></pre></td></tr></table></figure>
<p>函数原型是：<strong>QImage(str, int, int, int, QImage.Format)</strong>，用给定的宽度，高度和格式构造一个使用现有内存缓冲区数据的图像。宽度和高度必须以像素指定。bytesPerLine指定每行的字节数。</p>
<p><strong>这里有个疑问：为什么bytesPerLine = 3 * width？</strong><br>我的理解是：当1个像素占3个字节，此时图像为真彩色图像。</p>
<p><strong>QImage.Format_RGB888</strong>表示的是图像存储使用8-8-8 24位RGB格式。当然还有更多的格式，详见QImage的官方介绍，限于篇幅这里不展开。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pixmap &#x3D; QPixmap.fromImage(QImg)</span><br></pre></td></tr></table></figure><br>这个很好理解，就是想QImage对象转换成QPixmap对象，便于下步我们将Label标签中设置图像。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.lb.setPixmap(pixmap)</span><br></pre></td></tr></table></figure><br>设置标签的图像信息。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.lb.setCursor(Qt.CrossCursor)</span><br></pre></td></tr></table></figure><br>设置鼠标在QLabel对象中的样式，只是为了画画好看些而已，没其它的意思。除了这个十字架的，还有其它很多样式，如下图：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/v2-cf7bf8978adbaa898f059207f52f3d5b_hd.jpg" alt></p>
<h3 id="鼠标事件"><a href="#鼠标事件" class="headerlink" title="鼠标事件"></a>鼠标事件</h3><p>按照上文中我们介绍的思路，我们自定义了一个QLabel类myLabel，当然是继承了QLabel。然后我们用几个类变量记录鼠标的坐标和矩形是否完成的标志。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">def mousePressEvent(self,event):</span><br><span class="line">    self.flag &#x3D; True</span><br><span class="line">    self.x0 &#x3D; event.x()</span><br><span class="line">    self.y0 &#x3D; event.y()</span><br><span class="line"></span><br><span class="line">def mouseReleaseEvent(self,event):</span><br><span class="line">    self.flag &#x3D; False</span><br><span class="line"></span><br><span class="line">def mouseMoveEvent(self,event):</span><br><span class="line">    if self.flag:</span><br><span class="line">        self.x1 &#x3D; event.x()</span><br><span class="line">        self.y1 &#x3D; event.y()</span><br><span class="line">        self.update()</span><br></pre></td></tr></table></figure><br>这里就是重载了鼠标产生的几个事件，是我们自定义的。分别记录了点击鼠标后初始的鼠标坐标，以及释放鼠标后的鼠标坐标。并在鼠标移动的时候更新UI。也就是我们上面所说的鼠标画矩形的思路。</p>
<h3 id="画矩形"><a href="#画矩形" class="headerlink" title="画矩形"></a>画矩形</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def paintEvent(self, event):</span><br><span class="line">    super().paintEvent(event)</span><br><span class="line">    rect &#x3D;QRect(self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">    painter &#x3D; QPainter(self)</span><br><span class="line">    painter.setPen(QPen(Qt.red,4,Qt.SolidLine))</span><br><span class="line">    painter.drawRect(rect)</span><br><span class="line"></span><br><span class="line">    pqscreen  &#x3D; QGuiApplication.primaryScreen()</span><br><span class="line">    pixmap2 &#x3D; pqscreen.grabWindow(self.winId(), self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">    pixmap2.save(&#39;555.png&#39;)</span><br></pre></td></tr></table></figure>
<p>这个是关键点啊！<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">super().paintEvent(event)</span><br></pre></td></tr></table></figure><br>调用父类的paintEvent()，这个是为了显示你设置的效果。否则会是一片空白。大家可以试试注释这句话，看看效果啊！<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rect &#x3D;QRect(self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br></pre></td></tr></table></figure><br>QRect类使用整数精度在平面中定义一个矩形。矩形通常表示为左上角和大小。QRect的大小（宽度和高度）始终等同于构成其渲染基础的数学矩形。QRect可以用一组左，上，宽和高整数，或者从QPoint和QSize构成。以下代码创建两个相同的矩形。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">QRect(100, 200, 11, 16)</span><br><span class="line">QRect(QPoint(100, 200), QSize(11, 16))</span><br><span class="line">painter &#x3D; QPainter(self)</span><br><span class="line">painter.setPen(QPen(Qt.red,4,Qt.SolidLine))</span><br><span class="line">painter.drawRect(rect)</span><br></pre></td></tr></table></figure><br>构建一个QPainter对象，设置它的画笔，然后画一个矩形。貌似感觉好简单！^_^”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pqscreen  &#x3D; QGuiApplication.primaryScreen()</span><br><span class="line">pixmap2 &#x3D; pqscreen.grabWindow(self.winId(), self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))</span><br><span class="line">pixmap2.save(&#39;555.png&#39;)</span><br></pre></td></tr></table></figure>
<p>截屏的原理呢，主要还是运用QScreen类中的grabWindow方法。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">QScreen.grabWindow(WId window, int x &#x3D; 0, int y &#x3D; 0, int width &#x3D; -1, int height &#x3D; -1)</span><br></pre></td></tr></table></figure><br>大致意思是创建并返回通过抓取由QRect（x，y，width，height）限制的给定窗口构造的像素图。</p>
<p>参数（x，y）指定窗口中的偏移量，而（宽度，高度）指定要复制的区域。如果宽度为负数，则该函数将所有内容复制到窗口的右边界。如果高度为负数，则该函数将所有内容复制到窗口的底部。</p>
<p>窗口系统标识符（WId）可以使用QWidget.winId（）函数进行检索。grabWindow（）函数从屏幕抓取像素，而不是从窗口抓取像素，即，如果有另一个窗口部分或全部覆盖抓取的像素，则也会从上面的窗口获取像素。鼠标光标一般不会被抓取。详见官网介绍：QScreen Class | Qt GUI 5.10</p>
<p>由于QScreen类无构造函数，所以我们使用QGuiApplication.primaryScreen()创建了一个Qscreen类对象。最后使用pixmap2.save(‘555.png’)，保存具体的截图。</p>
<p>如果你想保存的图片没有红框，可以参考这里：<br><a href="http://www.xdbcb8.com/forum/topic/%e3%80%8apyqt5%e4%b8%8eopencv%e7%9a%84%e5%b0%8f%e5%b0%8f%e8%9e%8d%e5%90%88%e3%80%8b%e8%bf%99%e7%af%87%e6%96%87%e7%ab%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e4%bf%9d%e5%ad%98%e4%b8%8b%e6%9d%a5" target="_blank" rel="noopener">《pyqt5与opencv的小小融合》这篇文章中如何实现保存下来</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Opencv</tag>
        <tag>PyQt5</tag>
      </tags>
  </entry>
  <entry>
    <title>Permmision denied解决方法</title>
    <url>/2019/03/19/Permmision%20denied%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>这是因为要管理员权限的，而ubuntu又不想给普通用户赋予管理员权限。所以这里开启root账号</p>
<h2 id="首先设置-root-密码"><a href="#首先设置-root-密码" class="headerlink" title=" 首先设置 root 密码"></a><a id="more"></a> 首先设置 root 密码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo passwd</span><br></pre></td></tr></table></figure>
<p>会出现以下画面：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[sudo] password for luban:                        &#x2F;&#x2F;输入当前普通用户的密码</span><br><span class="line"></span><br><span class="line">Enter new UNIX password:                          &#x2F;&#x2F;给root设置密码</span><br><span class="line"></span><br><span class="line">Retype new UNIX password:                         &#x2F;&#x2F;确认输入密码</span><br><span class="line"></span><br><span class="line">passwd: password updated successfully</span><br></pre></td></tr></table></figure></p>
<h2 id="开启root"><a href="#开启root" class="headerlink" title="开启root"></a>开启root</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<h2 id="修改文件权限"><a href="#修改文件权限" class="headerlink" title="修改文件权限"></a>修改文件权限</h2><p>为了获得执行权限，借助chmod指令修改文件权限即可。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod 777 文件夹或文件路径</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Python新环境下快速安装依赖包的小技巧</title>
    <url>/2019/03/19/Python%E6%96%B0%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<p>当你新创一个Python环境时，若还用pip一个个装你所需要的库，明显效率十分低下。这里有个小技巧，你可以从已配置好的旧环境中，导出一个requirements.txt 文件，用于记录所有依赖包及其精确的版本号。以便新环境部署。<br><a id="more"></a><br>在旧环境中执行以下命令，生成requirements.txt文件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure><br>requirements.txt中的内容类似如下，记录了你旧有环境的依赖包及其精确的版本号：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">appdirs&#x3D;&#x3D;1.4.3</span><br><span class="line">backports.functools-lru-cache&#x3D;&#x3D;1.5</span><br><span class="line">beautifulsoup4&#x3D;&#x3D;4.5.3</span><br><span class="line">bs4&#x3D;&#x3D;0.0.1</span><br><span class="line">cycler&#x3D;&#x3D;0.10.0</span><br><span class="line">kiwisolver&#x3D;&#x3D;1.0.1</span><br><span class="line">lxml&#x3D;&#x3D;3.7.3</span><br><span class="line">matplotlib&#x3D;&#x3D;2.2.0</span><br><span class="line">numpy&#x3D;&#x3D;1.14.1</span><br><span class="line">pandas&#x3D;&#x3D;0.22.0</span><br><span class="line">pyparsing&#x3D;&#x3D;2.2.0</span><br><span class="line">python-dateutil&#x3D;&#x3D;2.6.1</span><br><span class="line">pytz&#x3D;&#x3D;2018.3</span><br><span class="line">six&#x3D;&#x3D;1.11.0</span><br><span class="line">virtualenv&#x3D;&#x3D;15.1.0</span><br></pre></td></tr></table></figure><br>这时你可以把requirements.txt拷入新配置的Python目录下，执行以下命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><br>则会按照requirements.txt中所写的依赖包和版本依序进行安装。<br>注意：<br>若迁入的系统不同或Python版本不同，在安装过程中可能会因为找不到相应的依赖包版本而报错<br>这时你可以进入requirements.txt把报错的依赖包后的版本信息去掉，保存，重新执行命令即可，它会自动下载匹配的最新版本。<br>也可以把==改成&lt;=，代表它会搜索不大于此版本的最高版本进行安装。<br>由于pip下载源在国外，若无合适的VPN，此期间下载过程会十分漫长，这里提供几个常用的国内镜像源：</p>
<ul>
<li>清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple/</a></li>
<li>中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">http://pypi.mirrors.ustc.edu.cn/simple/</a></li>
<li>阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a></li>
<li>豆瓣 <a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a></li>
<li>华中科技大学 <a href="http://pypi.hustunique.com/" target="_blank" rel="noopener">http://pypi.hustunique.com/</a></li>
</ul>
<p>可以在使用pip的时候，加上参数-i和镜像地址，指定下载源，加速下载过程，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu 16.04下配置GPU版CUDA和cuDNN</title>
    <url>/2019/03/19/Ubuntu-16.04%E4%B8%8B%E9%85%8D%E7%BD%AEGPU%E7%89%88CUDA%E5%92%8CcuDNN/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>先介绍下我自己配置的环境</p>
<ul>
<li>Ubuntu 16.04</li>
<li>GTX2080ti显卡</li>
<li>NVIDIA 418.3</li>
<li>CUDA 10.0</li>
<li>cuDNN 7.6.2</li>
</ul>
<p>以下教程针对从零开始的用户，若系统中已装有CUDA和cuDNN，却在当前用户下无法使用，请<a href="#添加环境变量">添加环境变量</a></p>
<h2 id="安装NVIDIA显卡驱动"><a href="#安装NVIDIA显卡驱动" class="headerlink" title="安装NVIDIA显卡驱动"></a>安装NVIDIA显卡驱动</h2><p>1.先在<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">NVIDIA官网</a>上下载对应的驱动程序，可根据自己的GPU的型号下载相应的.run文件<br>例如NVIDIA-Linux-x86_64-3xx.xx.run形式的文件名</p>
<p>2.禁用开源nouveau驱动<strong>（非常重要）</strong><br>Ubuntu系统集成的显卡驱动程序是nouveau，它是第三方为NVIDIA开发的开源驱动，我们需要先将其屏蔽才能安装NVIDIA官方驱动。<br>将驱动添加到黑名单blacklist.conf中，但是由于该文件的属性不允许修改。所以需要先修改文件属性。<br>查看属性<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ls -lh &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br></pre></td></tr></table></figure><br>修改属性<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod 666 &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br></pre></td></tr></table></figure><br>用gedit编辑器打开<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf</span><br></pre></td></tr></table></figure><br>在该文件后添加一下几行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blacklist vga16fb</span><br><span class="line">blacklist nouveau</span><br><span class="line">blacklist rivafb</span><br><span class="line">blacklist rivatv</span><br><span class="line">blacklist nvidiafb</span><br></pre></td></tr></table></figure><br>3.开始安装<br>先按Ctrl + Alt + F1到控制台，关闭当前图形环境<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure><br>再安装驱动程序<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod a+x NVIDIA-Linux-x86_64-xxx.run</span><br><span class="line">sudo .&#x2F;NVIDIA-Linux-x86_64-xxx.run -no-x-check -no-nouveau-check -no-opengl-files</span><br></pre></td></tr></table></figure><br>最后重新启动图形环境<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo service lightdm start</span><br></pre></td></tr></table></figure><br>在终端里输入：nvidia-smi ，输出以下图片的代码则安装成功<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CUDA%26cuDNN/2019051413224071.png" alt></p>
<h2 id="cuda-10-0安装"><a href="#cuda-10-0安装" class="headerlink" title="cuda 10.0安装"></a>cuda 10.0安装</h2><p><a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">官方下载</a><br><a href="https://pan.baidu.com/s/1piTbzIIL3wTx1dCeUDDiOw" target="_blank" rel="noopener">网盘下载</a> 提取码: aarn<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod a+x cuda_10.0.130_410.48_linux.run &#x2F;&#x2F; 获取权限</span><br><span class="line">sudo sh cuda_10.0.130_410.48_linux.run --tmpdir&#x3D;&#x2F;home&#x2F;max&#x2F;temp</span><br></pre></td></tr></table></figure><br>这里加 —tmpdir 主要是直接运行后，会提示空间不足的问题<br>接下来进入英文选择界面按住空格键可以快速浏览<br>在安装过程中选项选择：<br>accept #同意安装<br>n #不安装Driver，因为已安装驱动<strong>（这里需要强调一下）</strong><br>y #安装CUDA Toolkit</p>
<h1 id="安装到默认目录"><a href="#安装到默认目录" class="headerlink" title="安装到默认目录"></a>安装到默认目录</h1><p>y #创建安装目录的软链接<br>n #不复制Samples，因为在安装目录下有/samples</p>
<h2 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h2><p>home文件下 ctrl+H显示隐藏文件 打开 .bashrc文件在最后添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.0&#x2F;bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-10.0&#x2F;lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><br>终端运行如下命令，保存操作<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><br>检查cuda是否安装成功<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br><span class="line">nvcc --version</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CUDA%26cuDNN/20190514132130343.png" alt></p>
<h2 id="cuDnn-10-0安装"><a href="#cuDnn-10-0安装" class="headerlink" title="cuDnn 10.0安装"></a>cuDnn 10.0安装</h2><p><a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">官方下载</a><br><a href="https://www.jianguoyun.com/p/DdS0_A4QlZ_3Bhjw8LEC" target="_blank" rel="noopener">网盘下载</a><br>注意：需跟CUDA版本对应<br>切换到下载目录进行解压：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo tar -zxvf .&#x2F;cudnn-10.0-linux-x64-xxx.tgz</span><br></pre></td></tr></table></figure><br>解压下载的文件，可以看到cuda文件夹，在当前目录打开终端，执行如下命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp cuda&#x2F;include&#x2F;cudnn.h &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;</span><br><span class="line"> </span><br><span class="line">sudo cp cuda&#x2F;lib64&#x2F;libcudnn* &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;</span><br><span class="line"> </span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h</span><br><span class="line"> </span><br><span class="line">sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;libcudnn*</span><br></pre></td></tr></table></figure><br>在终端输入<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure><br>如果出现下图所示版本信息，说明安装成功<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/CUDA%26cuDNN/20180815114007852.png" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
        <tag>CUDA</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu16.04下Tensorflow-gpu安装</title>
    <url>/2019/03/19/Ubuntu16.04%E4%B8%8BTensorflow-gpu%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>在开始之前，请先确定安装好显卡驱动、CUDA、cuDNN，可以参考我的<a href="https://qiyuan-z.github.io/2019/03/19/Ubuntu-16.04下配置GPU版CUDA和cuDNN/">另一篇博客</a><br><a id="more"></a><br>1.安装Numpy<br>推荐安装1.16.0版本，若之前已安装，请先卸载<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip uninstall numpy</span><br></pre></td></tr></table></figure><br>然后执行安装<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install numpy&#x3D;&#x3D;1.16.0</span><br></pre></td></tr></table></figure><br>2.安装Tensorflow-gpu<br>推荐安装1.14.0版本<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install tensorflow-gpu&#x3D;&#x3D;1.14.0</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下virtualenv的使用与pycharm的基本配置</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8Bvirtualenv%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8Epycharm%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="安装virtualenv"><a href="#安装virtualenv" class="headerlink" title="安装virtualenv"></a><a id="more"></a>安装virtualenv</h2><p>virtualenv是 Python 多版本管理的利器，不同版本的开发调试全靠它了（没有多版本尽量也装上吧）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip install virtualenv</span><br></pre></td></tr></table></figure></p>
<h2 id="创建一个virtualenv环境"><a href="#创建一个virtualenv环境" class="headerlink" title="创建一个virtualenv环境"></a>创建一个virtualenv环境</h2><p>使用如下语句：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">virtualenv + 路径</span><br></pre></td></tr></table></figure><br>以这种方式创建环境将不包含系统的python包，新的环境里面只有pip、setuptools和wheel这些包，则许多包要用pip重新安装。</p>
<p>若需指定python版本：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">virtualenv –p python3 + 路径</span><br></pre></td></tr></table></figure></p>
<h2 id="激活virtualenv环境"><a href="#激活virtualenv环境" class="headerlink" title="激活virtualenv环境"></a>激活virtualenv环境</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source 路径&#x2F;bin&#x2F;activate</span><br></pre></td></tr></table></figure>
<p>也可以直接进到所创环境的bin目录中右键终端，运行source activate<br>注意：激活只对当前终端有效，如果新打开了一个终端的话，重新运行上面的命令。 激活后终端前面会多一个(**)的东西，提示当前virtualenv的名称。</p>
<p>激活后可以在当前终端通过python 文件名.py的方式运行python脚本，如果脚本中使用了当前环境中没有的包，将会报错。</p>
<p>可以在激活环境后使用pip安装对应的包。<strong>注意不要使用sudo</strong>，否则包会安装到系统当中去，而不是当前的virtualenv目录中。</p>
<h2 id="退出virtualenv环境"><a href="#退出virtualenv环境" class="headerlink" title="退出virtualenv环境"></a>退出virtualenv环境</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure>
<p>也可直接关闭当前终端。</p>
<h2 id="删除virtualenv环境"><a href="#删除virtualenv环境" class="headerlink" title="删除virtualenv环境"></a>删除virtualenv环境</h2><p>直接删除对应目录即可删除virtualenv环境，不会对系统产生任何影响，所以在virtualenv中可以放心操作。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -rvf  + 路径</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafafdaa2aca.webp" alt></p>
<h2 id="Pycharm配置"><a href="#Pycharm配置" class="headerlink" title="Pycharm配置"></a>Pycharm配置</h2><p>新建项目<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafb0d3811f2.webp" alt></p>
<ul>
<li><p>New environment using Virtualenv: 将在项目的目录下创建一个virtualenv环境，然后使用它当作当前项目的python解释器，默认不包含系统的python包。<br>相当于： virtualenv + 路径</p>
</li>
<li><p>location:为新建的环境的位置，默认为当前工程下的venv。</p>
</li>
<li><p>Base interpreter:基于系统中的python版本，新建的环境中的python版本与此一致，可以选择python2或者python3, 取决于项目的需要，相当于virtualenv –p python版本 +路径。<br>勾选Inherit global site-packages，包含系统的python包，相当于： virtualenv –system-site-packages + 路径<br>勾选Make available to all projects，下次新建项目的时候会在Existing interpreter中找到这个环境， 可以重复使用这个环境。</p>
</li>
<li><p>Existing interpreter：使用已有的python环境，点击后会出现后面的设置会出现这个界面，分别是virtualenv, conda和系统的python环境。可以选择已有的virtualenv环境，或者直接使用系统的python解释器。 Conda是anaconda(一个科学计算的python发行版)的包管理器，也可以用来建立python环境。</p>
</li>
</ul>
<p>会发现生成的项目中有一个叫venv的文件夹，它实质上和直接用virtualenv创建的一样。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafb2bd99f29.webp" alt><br>可以用virtualenv的管理方法管理它，比如安装numpy，安装之后可以在pycharm正常使用。（注意在virtualenv中<strong>不要使用sudo</strong>）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/virtualenv/5aafb2bdb011c.webp" alt><br>也可以在pycharm中使用 file-settings-project-project interpreter中管理环境中的python包，可以对该环境下的python包进行删除和安装。</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
        <tag>virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下如何添加新用户并增加管理员权限</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8B%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%94%A8%E6%88%B7%E5%B9%B6%E5%A2%9E%E5%8A%A0%E7%AE%A1%E7%90%86%E5%91%98%E6%9D%83%E9%99%90/</url>
    <content><![CDATA[<h2 id="添加新用户"><a href="#添加新用户" class="headerlink" title=" 添加新用户"></a><a id="more"></a> 添加新用户</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo adduser xxx #xxx为用户名</span><br></pre></td></tr></table></figure>
<p>输入密码后，出现如下信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">正在添加用户&quot;xxx&quot;…</span><br><span class="line">正在添加新组&quot;xxx&quot; (1003)…</span><br><span class="line">正在添加新用户&quot;xxx&quot; (1003) 到组&quot;xxx&quot;…</span><br><span class="line">创建主目录&quot;&#x2F;home&#x2F;xxx&quot;…</span><br><span class="line">正在从&quot;&#x2F;etc&#x2F;skel&quot;复制文件…</span><br><span class="line">输入新的 UNIX 密码：</span><br><span class="line">重新输入新的 UNIX 密码：</span><br><span class="line">passwd：已成功更新密码</span><br><span class="line">正在改变 xxx 的用户信息</span><br><span class="line">请输入新值，或直接敲回车以使用默认值</span><br><span class="line">        全名 [ ]：</span><br><span class="line">        房间号码 [ ]：</span><br><span class="line">        工作电话 [ ]：</span><br><span class="line">        家庭电话 [ ]：</span><br><span class="line">        其它 [ ]：</span><br><span class="line">这些信息是否正确？ [Y&#x2F;n] y</span><br></pre></td></tr></table></figure><br>到这里，新用户添加成功。</p>
<h2 id="增加管理员权限"><a href="#增加管理员权限" class="headerlink" title="增加管理员权限"></a>增加管理员权限</h2><p>需要让此用户有root权限，执行命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo nano &#x2F;etc&#x2F;sudoers</span><br></pre></td></tr></table></figure><br>往下拉，修改文件并添加：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xxx ALL&#x3D;(ALL：ALL) ALL #xxx为你之前所创的用户名</span><br></pre></td></tr></table></figure><br>添加完成按下Ctrl + X保存并退出，再次按下Enter,完成<br>现在你可以点击右上角，看到你新创的用户，点击并切换了！</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下安装Pycharm</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85Pycharm/</url>
    <content><![CDATA[<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a><a id="more"></a>下载</h2><p>有社区版和专业版，专业版需要激活，社区版免费，我下载的是社区版。<br><a href="https://www.jetbrains.com/pycharm/download/#section=linux" target="_blank" rel="noopener">官方下载</a><br><a href="https://www.jianguoyun.com/p/DfZJG3QQlZ_3Bhj8mrEC" target="_blank" rel="noopener">网盘下载</a></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>进入download目录，对下载好的Pycharm压缩包进行解压操作，选中文件右键提取到此处即可。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D1664510513%2C942693123%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>进入解压后的Pycharm文件夹，进入bin目录，可以看到很多文件，其中有一个文件叫做pycharm.sh。也就是下图中所选中的文件：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D101055722%2C4258535600%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>此时在bin文件夹下右键打开终端，输入运行命令, 执行Pycharm程序<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;pycharm.sh</span><br></pre></td></tr></table></figure><br>如果提示没有权限，大家可以添加sudo命令进行操作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo .&#x2F;pycharm.sh</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D805484803%2C1629350165%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>大家点击OK即可进入下一步操作，根据提示的内容进行勾选同意协议，然后点击continue进入下一步，大家可以点击don’t send进入下一步，选择喜欢的界面风格，然后再次点击next进入下一步，直到提示start启动程序。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D227808303%2C3066408894%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D22323540%2C1357101677%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D1914658415%2C2906990210%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt><br>为了方便以后的使用，我们可以将Pycharm的图标（快捷键）锁定到启动器。之后不用再去在终端中启动Pycharm了，直接点击图标启动即可。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/pycharm/u%3D1149108563%2C3867497858%26fm%3D173%26app%3D49%26f%3DJPEG.jpg" alt></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下安装更新pip</title>
    <url>/2019/03/19/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85%E6%9B%B4%E6%96%B0pip/</url>
    <content><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a><a id="more"></a>安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install python-pip</span><br></pre></td></tr></table></figure>
<p>若同时装有Python2、Python3上条命令装的是pip2，若还需装pip3，则再执行以下命令<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install python3-pip</span><br></pre></td></tr></table></figure></p>
<h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>安装成功后进行更新：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip install --upgrade pip</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip3 install --upgrade pip</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu下如何安装搜狗输入法</title>
    <url>/2019/03/19/sougou/</url>
    <content><![CDATA[<h2 id="Ubuntu系统配置"><a href="#Ubuntu系统配置" class="headerlink" title=" Ubuntu系统配置"></a><a id="more"></a> Ubuntu系统配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">system settings-&gt;language support-&gt;install&#x2F;remove languages</span><br></pre></td></tr></table></figure>
<p>在弹出的菜单中选择Chinese(simplified),点击apply<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927736.png" alt><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927712.png" alt></p>
<h2 id="配置输入法框架"><a href="#配置输入法框架" class="headerlink" title="配置输入法框架"></a>配置输入法框架</h2><p>搜狗输入法是建立在fcitx框架之上的，所以要将输入法框架选择为fictx<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927666.png" alt><br>注意：如果没有fcitx选项，那么你就需要安装fcitx框架之后在进行配置，安装方法如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:fcitx-team&#x2F;nightly   &#x2F;&#x2F;添加FCITX仓库</span><br><span class="line"></span><br><span class="line">sudo apt-get update                              &#x2F;&#x2F;更新仓库</span><br><span class="line"></span><br><span class="line">sudo apt-get install fcitx                       &#x2F;&#x2F;安装fcitx输入法框架</span><br></pre></td></tr></table></figure><br>配置好输入法框架之后，重启ubuntu系统。重启之后如果配置成功，在任务栏的右上角会出现fcitx的设置选项（一个小键盘图标）<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/20180729142927651.png" alt></p>
<h2 id="去搜狗官网下载输入法for-Linux"><a href="#去搜狗官网下载输入法for-Linux" class="headerlink" title="去搜狗官网下载输入法for Linux"></a>去搜狗官网下载输入法for Linux</h2><p><a href="https://pinyin.sogou.com/linux/" target="_blank" rel="noopener">官方下载</a><br><a href="https://www.jianguoyun.com/p/DeShNo0QlZ_3Bhic5rAC" target="_blank" rel="noopener">网盘下载</a><br>下载完成之后，在download目录下找到下载的文件，双击安装即可，点击install即可<br>或者在终端的命令窗口中输入如下的指令安装：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i 安装包名称.deb</span><br></pre></td></tr></table></figure><br>如果在安装过程中出现相关依赖文件的错误。则需要先安装其依赖的软件包<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/Snipaste_2019-12-13_14-33-14.jpg" alt><br>在终端窗口来执行以下命令，安装缺少的依赖文件:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get -f install</span><br></pre></td></tr></table></figure><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/Snipaste_2019-12-13_14-33-49.jpg" alt><br>安装完成后，首先打开右上角系统设置，选择第一行的最后一个选项”Text Entry“，点击左下角的+号，在打开的窗口中找到搜狗输入法Sogou pinyin点击Add添加进去<br>仅显示当前语言一定要去掉那个勾，才可以找到搜狗输入法，然后添加就是<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/sougou/2019050512074094.png" alt><br>这时就可以在右上方小键盘选择搜狗输入法了！</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu下 teamviewer的安装方法</title>
    <url>/2019/03/19/ubuntu%E4%B8%8B-teamviewer%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="去官网下载安装包"><a href="#去官网下载安装包" class="headerlink" title=" 去官网下载安装包"></a><a id="more"></a> 去官网下载安装包</h2><p><a href="https://www.teamviewer.com/zhcn/download/linux/" target="_blank" rel="noopener">官网链接</a><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/teamviewer/20180128120840387.png" alt></p>
<h2 id="在命令行进行安装"><a href="#在命令行进行安装" class="headerlink" title="在命令行进行安装"></a>在命令行进行安装</h2><p>（在下载文件夹下打开命令行，输入：sudo dpkg -i  teamviewer_13.0.6634_amd64.deb）（teamviewer_13.0.6634_amd64.deb为安装包名，根据自己安装包）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i  teamviewer_13.0.6634_amd64.deb</span><br></pre></td></tr></table></figure></p>
<h2 id="安装出错，一行语句搞定依赖关系"><a href="#安装出错，一行语句搞定依赖关系" class="headerlink" title="安装出错，一行语句搞定依赖关系"></a>安装出错，一行语句搞定依赖关系</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/teamviewer/20180128120923823.png" alt><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install -f</span><br></pre></td></tr></table></figure></p>
<h2 id="再次安装"><a href="#再次安装" class="headerlink" title="再次安装"></a>再次安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i  teamviewer_13.0.6634_amd64.deb</span><br></pre></td></tr></table></figure>
<h2 id="打开teamview"><a href="#打开teamview" class="headerlink" title="打开teamview"></a>打开teamview</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/teamviewer/20180128121156354.png" alt><br>或用命令行启动：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo teamviewer --daemon start</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Teamviewer</tag>
      </tags>
  </entry>
  <entry>
    <title>如何解决出现 unable to resolve host 问题</title>
    <url>/2019/03/19/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%87%BA%E7%8E%B0-unable-to-resolve-host-%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id><a href="#" class="headerlink" title></a><a id="more"></a></h2><p>Ubuntu环境，有时候执行sudo 就出现这个警告讯息: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo: unable to resolve host abc</span><br></pre></td></tr></table></figure>
<p>虽然sudo 还是可以正常执行, 但是看到这样的通知还是会觉得烦，怎么去除这个警告呢？</p>
<p>这个警告是因为系统找不到一个叫做 abc的hostname </p>
<p>通过 修改 /etc/hosts 设定, 可以解决<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nano &#x2F;etc&#x2F;hostname</span><br></pre></td></tr></table></figure></p>
<p>在127.0.0.1 localhost 后面加上主机名称(hostname) 即可:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1 localhost abc</span><br></pre></td></tr></table></figure><br>问题解决！</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵分析（刘丁酉）课后答案及历年试卷</title>
    <url>/2019/03/19/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90%EF%BC%88%E5%88%98%E4%B8%81%E9%85%89%EF%BC%89%E8%AF%BE%E5%90%8E%E7%AD%94%E6%A1%88%E5%8F%8A%E5%8E%86%E5%B9%B4%E8%AF%95%E5%8D%B7/</url>
    <content><![CDATA[<p>考虑到刘丁酉版本只有部分习题的解答，还不够详细，于是我在学习过程中，将每道习题做了一遍，并记录了下来，希望能对学习矩阵分析的同学给予一定的参考。由于本人仍在学习阶段，难免答案会有做错的地方，希望大家不要尽信答案，只当作一个思路上的参考来看。顺便附赠历年的卷子与答案（看了一遍历年的卷子，答案有错!@-@）<br><a id="more"></a><br>下载地址：<a href="https://www.jianguoyun.com/p/DQe8_9kQlZ_3Bhid4LYC" target="_blank" rel="noopener">点击下载</a></p>
]]></content>
      <categories>
        <category>资源分享</category>
      </categories>
      <tags>
        <tag>矩阵分析</tag>
      </tags>
  </entry>
</search>
