<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://Qiyuan-Z.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="article">
<meta property="og:title" content="AlexeyAB DarkNet卷积层的前向传播解析">
<meta property="og:url" content="https://qiyuan-z.github.io/2020/02/23/AlexeyAB-DarkNet%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%A7%A3%E6%9E%90/index.html">
<meta property="og:site_name" content="Yuan">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/649.webp">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/650.webp">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/651.png">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/652.webp">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/653.webp">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/654.webp">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/655.webp">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/656.png">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/657.webp">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/658.webp">
<meta property="article:published_time" content="2020-02-23T05:57:41.249Z">
<meta property="article:modified_time" content="2020-02-23T06:52:43.566Z">
<meta property="article:author" content="Qiyuan-Z">
<meta property="article:tag" content="目标检测">
<meta property="article:tag" content="YOLOv3">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/649.webp">

<link rel="canonical" href="https://qiyuan-z.github.io/2020/02/23/AlexeyAB-DarkNet%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%A7%A3%E6%9E%90/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css"><style>
#needsharebutton-postbottom {
  cursor: pointer;
  height: 26px;
  margin-top: 10px;
  position: relative;
}
#needsharebutton-postbottom .btn {
  border: 1px solid $btn-default-border-color;
  border-radius: 3px;
  display: initial;
  padding: 1px 4px;
}
</style>
  <title>AlexeyAB DarkNet卷积层的前向传播解析 | Yuan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yuan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">记录学习中的点点滴滴</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://qiyuan-z.github.io/2020/02/23/AlexeyAB-DarkNet%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%A7%A3%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/wallhaven-915.png">
      <meta itemprop="name" content="Qiyuan-Z">
      <meta itemprop="description" content="偉大な魂は目的を持ち、そうでないものは願望を持つ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AlexeyAB DarkNet卷积层的前向传播解析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-02-23 13:57:41 / 修改时间：14:52:43" itemprop="dateCreated datePublished" datetime="2020-02-23T13:57:41+08:00">2020-02-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><a id="more"></a> 前言</h2><p>今天来介绍一下DarkNet中卷积层的前向传播和反向传播的实现，卷积层是卷积神经网络中的核心组件，了解它的底层代码实现对我们理解卷积神经网络以及优化卷积神经网络都有一些帮助。</p>
<h2 id="卷积层的构造"><a href="#卷积层的构造" class="headerlink" title="卷积层的构造"></a>卷积层的构造</h2><p>卷积层的构造主要在<code>src/convolutional_layer.c</code>中的<code>make_convolutional_layer</code>中进行实现，下面给出部分核心代码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** batch 每个batch含有的图片数</span><br><span class="line">** step</span><br><span class="line">** h 图像高度(行数)</span><br><span class="line">** w 图像宽度(列数)</span><br><span class="line">** c 输入图像通道数</span><br><span class="line">** n 卷积核个数</span><br><span class="line">** groups 分组数</span><br><span class="line">** size 卷积核尺寸</span><br><span class="line">** stride 步长</span><br><span class="line">** dilation 空洞卷积空洞率</span><br><span class="line">** padding 四周补0长度</span><br><span class="line">** activation 激活函数类别</span><br><span class="line">** batch_normalize 是否进行BN</span><br><span class="line">** binary 是否对权重进行二值化</span><br><span class="line">** xnor 是否对权重以及输入进行二值化</span><br><span class="line">** adam 优化方式</span><br><span class="line">** use_bin_output</span><br><span class="line">** index 分组卷积的时候分组索引</span><br><span class="line">** antialiasing 抗锯齿标志，如果为真强行设置所有的步长为1</span><br><span class="line">** share_layer 标志参数，表示这一个卷积层是否和其它卷积层共享权重</span><br><span class="line">** assisted_excitation</span><br><span class="line">** deform 暂时不知道</span><br><span class="line">** train 标志参数，是否在训练</span><br><span class="line">*&#x2F;</span><br><span class="line">convolutional_layer make_convolutional_layer(int batch, int steps, int h, int w, int c, int n, int groups, int size, int stride_x, int stride_y, int dilation, int padding, ACTIVATION activation,</span><br><span class="line"> int batch_normalize, int binary, int xnor, int adam, int use_bin_output, int index, int antialiasing, convolutional_layer *share_layer, int assisted_excitation, int deform, int train)</span><br><span class="line">&#123;</span><br><span class="line">    int total_batch &#x3D; batch*steps;</span><br><span class="line">    int i;</span><br><span class="line">    convolutional_layer l &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">    l.type &#x3D; CONVOLUTIONAL;</span><br><span class="line">    l.train &#x3D; train;</span><br><span class="line"></span><br><span class="line">    if (xnor) groups &#x3D; 1;   &#x2F;&#x2F;对于二值网络，不能使用分组卷积</span><br><span class="line">    if (groups &lt; 1) groups &#x3D; 1;</span><br><span class="line"></span><br><span class="line">    const int blur_stride_x &#x3D; stride_x;</span><br><span class="line">    const int blur_stride_y &#x3D; stride_y;</span><br><span class="line">    l.antialiasing &#x3D; antialiasing;</span><br><span class="line">    if (antialiasing) &#123;</span><br><span class="line">        stride_x &#x3D; stride_y &#x3D; l.stride &#x3D; l.stride_x &#x3D; l.stride_y &#x3D; 1; &#x2F;&#x2F; use stride&#x3D;1 in host-layer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    l.deform &#x3D; deform;</span><br><span class="line">    l.assisted_excitation &#x3D; assisted_excitation;</span><br><span class="line">    l.share_layer &#x3D; share_layer;</span><br><span class="line">    l.index &#x3D; index;</span><br><span class="line">    l.h &#x3D; h;</span><br><span class="line">    l.w &#x3D; w;</span><br><span class="line">    l.c &#x3D; c;</span><br><span class="line">    l.groups &#x3D; groups;</span><br><span class="line">    l.n &#x3D; n;</span><br><span class="line">    l.binary &#x3D; binary;</span><br><span class="line">    l.xnor &#x3D; xnor;</span><br><span class="line">    l.use_bin_output &#x3D; use_bin_output;</span><br><span class="line">    l.batch &#x3D; batch;</span><br><span class="line">    l.steps &#x3D; steps;</span><br><span class="line">    l.stride &#x3D; stride_x;</span><br><span class="line">    l.stride_x &#x3D; stride_x;</span><br><span class="line">    l.stride_y &#x3D; stride_y;</span><br><span class="line">    l.dilation &#x3D; dilation;</span><br><span class="line">    l.size &#x3D; size;</span><br><span class="line">    l.pad &#x3D; padding;</span><br><span class="line">    l.batch_normalize &#x3D; batch_normalize;</span><br><span class="line">    l.learning_rate_scale &#x3D; 1;</span><br><span class="line">	&#x2F;&#x2F; 该卷积层总的权重元素个数（权重元素个数等于输入数据的通道数&#x2F;分组数*卷积核个数*卷积核的二维尺寸，注意因为每一个卷积核是同时作用于输入数据</span><br><span class="line">    &#x2F;&#x2F; 的多个通道上的，因此实际上卷积核是三维的，包括两个维度的平面尺寸，以及输入数据通道数这个维度，每个通道上的卷积核参数都是独立的训练参数）</span><br><span class="line">    l.nweights &#x3D; (c &#x2F; groups) * n * size * size;</span><br><span class="line">	&#x2F;&#x2F; 如果是共享卷积层，可以直接用共享的卷积层来赋值（猜测是有预训练权重的时候可以直接赋值）</span><br><span class="line">    if (l.share_layer) &#123;</span><br><span class="line">        if (l.size !&#x3D; l.share_layer-&gt;size || l.nweights !&#x3D; l.share_layer-&gt;nweights || l.c !&#x3D; l.share_layer-&gt;c || l.n !&#x3D; l.share_layer-&gt;n) &#123;</span><br><span class="line">            printf(&quot;Layer size, nweights, channels or filters don&#39;t match for the share_layer&quot;);</span><br><span class="line">            getchar();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        l.weights &#x3D; l.share_layer-&gt;weights;</span><br><span class="line">        l.weight_updates &#x3D; l.share_layer-&gt;weight_updates;</span><br><span class="line"></span><br><span class="line">        l.biases &#x3D; l.share_layer-&gt;biases;</span><br><span class="line">        l.bias_updates &#x3D; l.share_layer-&gt;bias_updates;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">		&#x2F;&#x2F; 该卷积层总的权重元素(卷积核元素)个数&#x3D;输入图像通道数 &#x2F; 分组数*卷积核个数*卷积核尺寸</span><br><span class="line">        l.weights &#x3D; (float*)xcalloc(l.nweights, sizeof(float));</span><br><span class="line">		&#x2F;&#x2F; bias就是Wx+b中的b（上面的weights就是W），有多少个卷积核，就有多少个b（与W的个数一一对应，每个W的元素个数为c*size*size）</span><br><span class="line">        l.biases &#x3D; (float*)xcalloc(n, sizeof(float));</span><br><span class="line">		&#x2F;&#x2F; 训练期间，需要执行反向传播</span><br><span class="line">        if (train) &#123;</span><br><span class="line">			&#x2F;&#x2F; 敏感图和特征图的尺寸应该是一样的</span><br><span class="line">            l.weight_updates &#x3D; (float*)xcalloc(l.nweights, sizeof(float));</span><br><span class="line">			&#x2F;&#x2F; bias的敏感图，维度和bias一致</span><br><span class="line">            l.bias_updates &#x3D; (float*)xcalloc(n, sizeof(float));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; float scale &#x3D; 1.&#x2F;sqrt(size*size*c);</span><br><span class="line">	&#x2F;&#x2F; 初始化权重：缩放因子*标准正态分布随机数，缩放因子等于sqrt(2.&#x2F;(size*size*c))，随机初始化</span><br><span class="line">    &#x2F;&#x2F; 此处初始化权重为正态分布，而在全连接层make_connected_layer()中初始化权重是均匀分布的。</span><br><span class="line">    &#x2F;&#x2F; TODO：个人感觉，这里应该加一个if条件语句：if(weightfile)，因为如果导入了预训练权重文件，就没有必要这样初始化了（事实上在detector.c的train_detector()函数中，</span><br><span class="line">    &#x2F;&#x2F; 紧接着parse_network_cfg()函数之后，就添加了if(weightfile)语句判断是否导入权重系数文件，如果导入了权重系数文件，也许这里初始化的值也会覆盖掉，</span><br><span class="line">    &#x2F;&#x2F; 总之这里的权重初始化的处理方式还是值得思考的，也许更好的方式是应该设置专门的函数进行权重的初始化，同时偏置也是，不过这里似乎没有考虑偏置的初始化，在make_connected_layer()中倒是有。。。）</span><br><span class="line">    float scale &#x3D; sqrt(2.&#x2F;(size*size*c&#x2F;groups));</span><br><span class="line">    for(i &#x3D; 0; i &lt; l.nweights; ++i) l.weights[i] &#x3D; scale*rand_uniform(-1, 1);   &#x2F;&#x2F; rand_normal();</span><br><span class="line">	&#x2F;&#x2F; 根据该层输入图像的尺寸、卷积核尺寸以及跨度计算输出特征图的宽度和高度</span><br><span class="line">    int out_h &#x3D; convolutional_out_height(l);</span><br><span class="line">    int out_w &#x3D; convolutional_out_width(l);</span><br><span class="line">	&#x2F;&#x2F; 输出图像高度</span><br><span class="line">    l.out_h &#x3D; out_h;</span><br><span class="line">	&#x2F;&#x2F; 输出图像宽度	</span><br><span class="line">    l.out_w &#x3D; out_w;</span><br><span class="line">	&#x2F;&#x2F; 输出图像通道数(等于卷积核个数,有多少个卷积核，最终就得到多少张特征图，每张特征图是一个通道)</span><br><span class="line">    l.out_c &#x3D; n;</span><br><span class="line">    l.outputs &#x3D; l.out_h * l.out_w * l.out_c; &#x2F;&#x2F; 对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即l.out_c张特征图）</span><br><span class="line">    l.inputs &#x3D; l.w * l.h * l.c; &#x2F;&#x2F; mini-batch中每张输入图片的像素元素个数</span><br><span class="line">    l.activation &#x3D; activation;</span><br><span class="line"></span><br><span class="line">    l.output &#x3D; (float*)xcalloc(total_batch*l.outputs, sizeof(float)); &#x2F;&#x2F; l.output为该层所有的输出（包括mini-batch所有输入图片的输出）</span><br><span class="line">#ifndef GPU</span><br><span class="line">    if (train) l.delta &#x3D; (float*)xcalloc(total_batch*l.outputs, sizeof(float));  &#x2F;&#x2F; l.delta 该层的敏感度图，和输出的维度想同</span><br><span class="line">#endif  &#x2F;&#x2F; not GPU</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 卷积层三种指针函数，对应三种计算：前向，反向，更新</span><br><span class="line">    l.forward &#x3D; forward_convolutional_layer;</span><br><span class="line">    l.backward &#x3D; backward_convolutional_layer;</span><br><span class="line">    l.update &#x3D; update_convolutional_layer;</span><br></pre></td></tr></table></figure>
<h2 id="卷积层前向传播的代码解析"><a href="#卷积层前向传播的代码解析" class="headerlink" title="卷积层前向传播的代码解析"></a>卷积层前向传播的代码解析</h2><p>代码在<code>src/convolutional_layer.c</code>中，注释如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 卷积层的前向传播核心代码</span><br><span class="line">void forward_convolutional_layer(convolutional_layer l, network_state state)</span><br><span class="line">&#123;</span><br><span class="line">    int out_h &#x3D; convolutional_out_height(l);</span><br><span class="line">    int out_w &#x3D; convolutional_out_width(l);</span><br><span class="line">    int i, j;</span><br><span class="line">	&#x2F;&#x2F; l.outputs &#x3D; l.out_h * l.out_w * l.out_c在make各网络层函数中赋值（比如make_convolutional_layer()），</span><br><span class="line">    &#x2F;&#x2F; 对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即l.out_c张特征图）</span><br><span class="line">    &#x2F;&#x2F; 初始化输出l.output全为0.0；输入l.outputs*l.batch为输出的总元素个数，其中l.outputs为batch</span><br><span class="line">    &#x2F;&#x2F; 中一个输入对应的输出的所有元素的个数，l.batch为一个batch输入包含的图片张数；0表示初始化所有输出为0；</span><br><span class="line">    fill_cpu(l.outputs*l.batch, 0, l.output, 1);</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 是否进行二值化操作</span><br><span class="line">    if (l.xnor &amp;&amp; (!l.align_bit_weights || state.train)) &#123;</span><br><span class="line">        if (!l.align_bit_weights || state.train) &#123;</span><br><span class="line">            binarize_weights(l.weights, l.n, l.nweights, l.binary_weights);</span><br><span class="line">            &#x2F;&#x2F;printf(&quot;\n binarize_weights l.align_bit_weights &#x3D; %p \n&quot;, l.align_bit_weights);</span><br><span class="line">        &#125;</span><br><span class="line">        swap_binary(&amp;l);</span><br><span class="line">        binarize_cpu(state.input, l.c*l.h*l.w*l.batch, l.binary_input);</span><br><span class="line">        state.input &#x3D; l.binary_input;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int m &#x3D; l.n &#x2F; l.groups; &#x2F;&#x2F; 该层的卷积核个数</span><br><span class="line">    int k &#x3D; l.size*l.size*l.c &#x2F; l.groups; &#x2F;&#x2F; 该层每个卷积核的参数元素个数</span><br><span class="line">    int n &#x3D; out_h*out_w; &#x2F;&#x2F; 该层每个特征图的尺寸(元素个数)</span><br><span class="line"></span><br><span class="line">    static int u &#x3D; 0;</span><br><span class="line">    u++;</span><br><span class="line">    &#x2F;&#x2F; 该循环即为卷积计算核心代码：所有卷积核对batch中每张图片进行卷积运算</span><br><span class="line">    &#x2F;&#x2F; 每次循环处理一张输入图片（所有卷积核对batch中一张图片做卷积运算）</span><br><span class="line">    for(i &#x3D; 0; i &lt; l.batch; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">		&#x2F;&#x2F; 该循环是为了处理分组卷积</span><br><span class="line">        for (j &#x3D; 0; j &lt; l.groups; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">			&#x2F;&#x2F; 当前组卷积核(也即权重)，元素个数为l.n*l.c&#x2F;l.groups*l.size*l.size,</span><br><span class="line">            &#x2F;&#x2F; 共有l.n行，l.c&#x2F;l.gropus,l.c*l.size*l.size列</span><br><span class="line">            float *a &#x3D; l.weights +j*l.nweights &#x2F; l.groups;</span><br><span class="line">			&#x2F;&#x2F; 对输入图像进行重排之后的图像数据，所以内存空间申请为网络中最大占用内存</span><br><span class="line">            float *b &#x3D; state.workspace;</span><br><span class="line">			&#x2F;&#x2F; 存储一张输入图片（多通道）当前组的输出特征图（输入图片是多通道的，输出</span><br><span class="line">            &#x2F;&#x2F; 图片也是多通道的，有多少组卷积核就有多少组通道，每个分组后的卷积核得到一张特征图即为一个通道）</span><br><span class="line">            &#x2F;&#x2F; 这里似乎有点拗口，可以看下分组卷积原理。</span><br><span class="line">            float *c &#x3D; l.output +(i*l.groups + j)*n*m;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;gemm(0,0,m,n,k,1,a,k,b,n,1,c,n);</span><br><span class="line">            &#x2F;&#x2F;gemm_nn_custom(m, n, k, 1, a, k, b, n, c, n);</span><br><span class="line">			&#x2F;&#x2F;二值网络，特殊处理，里面还有一些优化，细节很多，这里暂时不管二值网络这部分，把注意力先放在普通卷积层的计算上</span><br><span class="line">            if (l.xnor &amp;&amp; l.align_bit_weights &amp;&amp; !state.train &amp;&amp; l.stride_x &#x3D;&#x3D; l.stride_y)</span><br><span class="line">            &#123;</span><br><span class="line">                memset(b, 0, l.bit_align*l.size*l.size*l.c * sizeof(float));</span><br><span class="line"></span><br><span class="line">                if (l.c % 32 &#x3D;&#x3D; 0)</span><br><span class="line">                &#123;</span><br><span class="line">                    &#x2F;&#x2F;printf(&quot; l.index &#x3D; %d - new XNOR \n&quot;, l.index);</span><br><span class="line"></span><br><span class="line">                    int ldb_align &#x3D; l.lda_align;</span><br><span class="line">                    size_t new_ldb &#x3D; k + (ldb_align - k%ldb_align); &#x2F;&#x2F; (k &#x2F; 8 + 1) * 8;</span><br><span class="line">                    &#x2F;&#x2F;size_t t_intput_size &#x3D; new_ldb * l.bit_align;&#x2F;&#x2F; n;</span><br><span class="line">                    &#x2F;&#x2F;size_t t_bit_input_size &#x3D; t_intput_size &#x2F; 8;&#x2F;&#x2F; +1;</span><br><span class="line"></span><br><span class="line">                    int re_packed_input_size &#x3D; l.c * l.w * l.h;</span><br><span class="line">                    memset(state.workspace, 0, re_packed_input_size * sizeof(float));</span><br><span class="line"></span><br><span class="line">                    const size_t new_c &#x3D; l.c &#x2F; 32;</span><br><span class="line">                    size_t in_re_packed_input_size &#x3D; new_c * l.w * l.h + 1;</span><br><span class="line">                    memset(l.bin_re_packed_input, 0, in_re_packed_input_size * sizeof(uint32_t));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;float *re_packed_input &#x3D; calloc(l.c * l.w * l.h, sizeof(float));</span><br><span class="line">                    &#x2F;&#x2F;uint32_t *bin_re_packed_input &#x3D; calloc(new_c * l.w * l.h + 1, sizeof(uint32_t));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; float32x4 by channel (as in cuDNN)</span><br><span class="line">                    repack_input(state.input, state.workspace, l.w, l.h, l.c);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; 32 x floats -&gt; 1 x uint32_t</span><br><span class="line">                    float_to_bit(state.workspace, (unsigned char *)l.bin_re_packed_input, l.c * l.w * l.h);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;free(re_packed_input);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; slow - convolution the packed inputs and weights: float x 32 by channel (as in cuDNN)</span><br><span class="line">                    &#x2F;&#x2F;convolution_repacked((uint32_t *)bin_re_packed_input, (uint32_t *)l.align_bit_weights, l.output,</span><br><span class="line">                    &#x2F;&#x2F;    l.w, l.h, l.c, l.n, l.size, l.pad, l.new_lda, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; &#x2F;&#x2F; then exit from if()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    im2col_cpu_custom((float *)l.bin_re_packed_input, new_c, l.h, l.w, l.size, l.stride, l.pad, state.workspace);</span><br><span class="line">                    &#x2F;&#x2F;im2col_cpu((float *)bin_re_packed_input, new_c, l.h, l.w, l.size, l.stride, l.pad, b);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;free(bin_re_packed_input);</span><br><span class="line"></span><br><span class="line">                    int new_k &#x3D; l.size*l.size*l.c &#x2F; 32;</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; good for (l.c &#x3D;&#x3D; 64)</span><br><span class="line">                    &#x2F;&#x2F;gemm_nn_bin_32bit_packed(m, n, new_k, 1,</span><br><span class="line">                    &#x2F;&#x2F;    l.align_bit_weights, l.new_lda&#x2F;32,</span><br><span class="line">                    &#x2F;&#x2F;    b, n,</span><br><span class="line">                    &#x2F;&#x2F;    c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; &#x2F;&#x2F; then exit from if()</span><br><span class="line"></span><br><span class="line">                    transpose_uint32((uint32_t *)state.workspace, (uint32_t*)l.t_bit_input, new_k, n, n, new_ldb);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; the main GEMM function</span><br><span class="line">                    gemm_nn_custom_bin_mean_transposed(m, n, k, 1, (unsigned char*)l.align_bit_weights, new_ldb, (unsigned char*)l.t_bit_input, new_ldb, c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; &#x2F;&#x2F; alternative GEMM</span><br><span class="line">                    &#x2F;&#x2F;gemm_nn_bin_transposed_32bit_packed(m, n, new_k, 1,</span><br><span class="line">                    &#x2F;&#x2F;    l.align_bit_weights, l.new_lda&#x2F;32,</span><br><span class="line">                    &#x2F;&#x2F;    t_bit_input, new_ldb &#x2F; 32,</span><br><span class="line">                    &#x2F;&#x2F;    c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;free(t_bit_input);</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">                else</span><br><span class="line">                &#123; &#x2F;&#x2F; else (l.c % 32 !&#x3D; 0)</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;--------------------------------------------------------</span><br><span class="line">                    &#x2F;&#x2F;printf(&quot; l.index &#x3D; %d - old XNOR \n&quot;, l.index);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;im2col_cpu_custom_align(state.input, l.c, l.h, l.w, l.size, l.stride, l.pad, b, l.bit_align);</span><br><span class="line">                    im2col_cpu_custom_bin(state.input, l.c, l.h, l.w, l.size, l.stride, l.pad, state.workspace, l.bit_align);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;size_t output_size &#x3D; l.outputs;</span><br><span class="line">                    &#x2F;&#x2F;float *count_output &#x3D; calloc(output_size, sizeof(float));</span><br><span class="line">                    &#x2F;&#x2F;size_t bit_output_size &#x3D; output_size &#x2F; 8 + 1;</span><br><span class="line">                    &#x2F;&#x2F;char *bit_output &#x3D; calloc(bit_output_size, sizeof(char));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;size_t intput_size &#x3D; n * k; &#x2F;&#x2F; (out_h*out_w) X (l.size*l.size*l.c) : after im2col()</span><br><span class="line">                    &#x2F;&#x2F;size_t bit_input_size &#x3D; intput_size &#x2F; 8 + 1;</span><br><span class="line">                    &#x2F;&#x2F;char *bit_input &#x3D; calloc(bit_input_size, sizeof(char));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;size_t weights_size &#x3D; k * m; &#x2F;&#x2F;l.size*l.size*l.c*l.n; &#x2F;&#x2F; l.nweights</span><br><span class="line">                    &#x2F;&#x2F;size_t bit_weights_size &#x3D; weights_size &#x2F; 8 + 1;</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;char *bit_weights &#x3D; calloc(bit_weights_size, sizeof(char));</span><br><span class="line">                    &#x2F;&#x2F;float *mean_arr &#x3D; calloc(l.n, sizeof(float));</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; transpose B from NxK to KxN (x-axis (ldb &#x3D; l.size*l.size*l.c) - should be multiple of 8 bits)</span><br><span class="line">                    &#123;</span><br><span class="line">                        &#x2F;&#x2F;size_t ldb_align &#x3D; 256; &#x2F;&#x2F; 256 bit for AVX2</span><br><span class="line">                        int ldb_align &#x3D; l.lda_align;</span><br><span class="line">                        size_t new_ldb &#x3D; k + (ldb_align - k%ldb_align);</span><br><span class="line">                        size_t t_intput_size &#x3D; binary_transpose_align_input(k, n, state.workspace, &amp;l.t_bit_input, ldb_align, l.bit_align);</span><br><span class="line"></span><br><span class="line">                        &#x2F;&#x2F; 5x times faster than gemm()-float32</span><br><span class="line">                        gemm_nn_custom_bin_mean_transposed(m, n, k, 1, (unsigned char*)l.align_bit_weights, new_ldb, (unsigned char*)l.t_bit_input, new_ldb, c, n, l.mean_arr);</span><br><span class="line"></span><br><span class="line">                        &#x2F;&#x2F;gemm_nn_custom_bin_mean_transposed(m, n, k, 1, bit_weights, k, t_bit_input, new_ldb, c, n, mean_arr);</span><br><span class="line"></span><br><span class="line">                        &#x2F;&#x2F;free(t_input);</span><br><span class="line">                        &#x2F;&#x2F;free(t_bit_input);</span><br><span class="line">                        &#x2F;&#x2F;&#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                add_bias(l.output, l.biases, l.batch, l.n, out_h*out_w);</span><br><span class="line"></span><br><span class="line">                &#x2F;&#x2F;activate_array(l.output, m*n*l.batch, l.activation);</span><br><span class="line">                if (l.activation &#x3D;&#x3D; SWISH) activate_array_swish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; MISH) activate_array_mish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; NORM_CHAN) activate_array_normalize_channels(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 0);</span><br><span class="line">                else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX_MAXVAL) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 1);</span><br><span class="line">                else activate_array_cpu_custom(l.output, m*n*l.batch, l.activation);</span><br><span class="line">                return;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                &#x2F;&#x2F;printf(&quot; l.index &#x3D; %d - FP32 \n&quot;, l.index);</span><br><span class="line">				&#x2F;&#x2F; 由于有分组卷积，所以获取属于当前组的输入im并按一定存储规则排列的数组b，</span><br><span class="line">				&#x2F;&#x2F; 以方便、高效地进行矩阵（卷积）计算，详细查看该函数注释（比较复杂）</span><br><span class="line">				&#x2F;&#x2F; 这里的im实际上只加载了一张图片的数据</span><br><span class="line">				&#x2F;&#x2F; 关于im2col的原理我会讲</span><br><span class="line">                float *im &#x3D; state.input + (i*l.groups + j)*(l.c &#x2F; l.groups)*l.h*l.w;</span><br><span class="line">				&#x2F;&#x2F; 如果这里卷积核尺寸为1，是不需要改变内存排布方式</span><br><span class="line">                if (l.size &#x3D;&#x3D; 1) &#123;</span><br><span class="line">                    b &#x3D; im;</span><br><span class="line">                &#125;</span><br><span class="line">                else &#123;</span><br><span class="line">                    &#x2F;&#x2F;im2col_cpu(im, l.c &#x2F; l.groups, l.h, l.w, l.size, l.stride, l.pad, b);</span><br><span class="line">					&#x2F;&#x2F; 将多通道二维图像im变成按一定存储规则排列的数组b，</span><br><span class="line">					&#x2F;&#x2F; 以方便、高效地进行矩阵（卷积）计算，详细查看该函数注释（比较复杂）</span><br><span class="line">					&#x2F;&#x2F; 进行重排，l.c&#x2F;groups为每张图片的通道数分组，l.h为每张图片的高度，l.w为每张图片的宽度，l.size为卷积核尺寸，l.stride为步长</span><br><span class="line">					&#x2F;&#x2F; 得到的b为一张图片重排后的结果，也是按行存储的一维数组（共有l.c&#x2F;l.groups*l.size*l.size行，l.out_w*l.out_h列）</span><br><span class="line">                    im2col_cpu_ext(im,   &#x2F;&#x2F; input</span><br><span class="line">                        l.c &#x2F; l.groups,     &#x2F;&#x2F; input channels</span><br><span class="line">                        l.h, l.w,           &#x2F;&#x2F; input size (h, w)</span><br><span class="line">                        l.size, l.size,     &#x2F;&#x2F; kernel size (h, w)</span><br><span class="line">                        l.pad, l.pad,       &#x2F;&#x2F; padding (h, w)</span><br><span class="line">                        l.stride_y, l.stride_x, &#x2F;&#x2F; stride (h, w)</span><br><span class="line">                        l.dilation, l.dilation, &#x2F;&#x2F; dilation (h, w)</span><br><span class="line">                        b);                 &#x2F;&#x2F; output</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">				&#x2F;&#x2F; 此处在im2col_cpu操作基础上，利用矩阵乘法c&#x3D;alpha*a*b+beta*c完成对图像卷积的操作</span><br><span class="line">				&#x2F;&#x2F; 0,0表示不对输入a,b进行转置，</span><br><span class="line">				&#x2F;&#x2F; m是输入a,c的行数，具体含义为每个卷积核的个数，</span><br><span class="line">				&#x2F;&#x2F; n是输入b,c的列数，具体含义为每个输出特征图的元素个数(out_h*out_w)，</span><br><span class="line">				&#x2F;&#x2F; k是输入a的列数也是b的行数，具体含义为卷积核元素个数乘以输入图像的通道数除以分组数（l.size*l.size*l.c&#x2F;l.groups），</span><br><span class="line">				&#x2F;&#x2F; a,b,c即为三个参与运算的矩阵（用一维数组存储）,alpha&#x3D;beta&#x3D;1为常系数，</span><br><span class="line">				&#x2F;&#x2F; a为所有卷积核集合,元素个数为l.n*l.c&#x2F;l.groups*l.size*l.size，按行存储，共有l*n行，l.c&#x2F;l.groups*l.size*l.size列，</span><br><span class="line">				&#x2F;&#x2F; 即a中每行代表一个可以作用在3通道上的卷积核，</span><br><span class="line">				&#x2F;&#x2F; b为一张输入图像经过im2col_cpu重排后的图像数据（共有l.c&#x2F;l.group*l.size*l.size行，l.out_w*l.out_h列），</span><br><span class="line">				&#x2F;&#x2F; c为gemm()计算得到的值，包含一张输入图片得到的所有输出特征图（每个卷积核得到一张特征图），c中一行代表一张特征图，</span><br><span class="line">				&#x2F;&#x2F; 各特征图铺排开成一行后，再将所有特征图并成一大行，存储在c中，因此c可视作有l.n行，l.out_h*l.out_w列。</span><br><span class="line">				&#x2F;&#x2F; 详细查看该函数注释（比较复杂）</span><br><span class="line">                gemm(0, 0, m, n, k, 1, a, k, b, n, 1, c, n);</span><br><span class="line">                &#x2F;&#x2F; bit-count to float</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F;c +&#x3D; n*m;</span><br><span class="line">            &#x2F;&#x2F;state.input +&#x3D; l.c*l.h*l.w;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	&#x2F;&#x2F; 如果卷积层使用了BatchNorm，那么执行forward_batchnorm，如果没有，则添加偏置</span><br><span class="line">    if(l.batch_normalize)&#123;</span><br><span class="line">        forward_batchnorm_layer(l, state);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        add_bias(l.output, l.biases, l.batch, l.n, out_h*out_w);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;activate_array(l.output, m*n*l.batch, l.activation);</span><br><span class="line">	&#x2F;&#x2F; 使用不同的激活函数</span><br><span class="line">    if (l.activation &#x3D;&#x3D; SWISH) activate_array_swish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; MISH) activate_array_mish(l.output, l.outputs*l.batch, l.activation_input, l.output);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN) activate_array_normalize_channels(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 0);</span><br><span class="line">    else if (l.activation &#x3D;&#x3D; NORM_CHAN_SOFTMAX_MAXVAL) activate_array_normalize_channels_softmax(l.output, l.outputs*l.batch, l.batch, l.out_c, l.out_w*l.out_h, l.output, 1);</span><br><span class="line">    else activate_array_cpu_custom(l.output, l.outputs*l.batch, l.activation);</span><br><span class="line">	&#x2F;&#x2F; 二值网络，前向传播结束之后转回float</span><br><span class="line">    if(l.binary || l.xnor) swap_binary(&amp;l);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;visualize_convolutional_layer(l, &quot;conv_visual&quot;, NULL);</span><br><span class="line">    &#x2F;&#x2F;wait_until_press_key_cv();</span><br><span class="line">	&#x2F;&#x2F; 暂时不懂</span><br><span class="line">    if(l.assisted_excitation &amp;&amp; state.train) assisted_excitation_forward(l, state);</span><br><span class="line">	&#x2F;&#x2F; 暂时不懂</span><br><span class="line">    if (l.antialiasing) &#123;</span><br><span class="line">        network_state s &#x3D; &#123; 0 &#125;;</span><br><span class="line">        s.train &#x3D; state.train;</span><br><span class="line">        s.workspace &#x3D; state.workspace;</span><br><span class="line">        s.net &#x3D; state.net;</span><br><span class="line">        s.input &#x3D; l.output;</span><br><span class="line">        forward_convolutional_layer(*(l.input_layer), s);</span><br><span class="line">        &#x2F;&#x2F;simple_copy_ongpu(l.outputs*l.batch, l.output, l.input_antialiasing);</span><br><span class="line">        memcpy(l.output, l.input_layer-&gt;output, l.input_layer-&gt;outputs * l.input_layer-&gt;batch * sizeof(float));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="im2col解析"><a href="#im2col解析" class="headerlink" title="im2col解析"></a>im2col解析</h2><p>从上面的代码可以知道，卷积层的前向传播核心点是im2col操作还有sgemm矩阵计算方法对使用im2col进行重排后的数据进行计算。现在来解析一下im2col算法，sgemm算法就是im2col运行后直接调用即可，就不细讲了。</p>
<p>这里考虑到结合图片更容易理解im2col的思想，我利用CSDN Tiger-Gao博主的图描述一下。首先，我们把一个单通道的长宽均为4的图片通过im2col重新排布后会变成什么样呢？看下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/649.webp" alt></p>
<p>来具体看一下变化过程：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/650.webp" alt></p>
<p>这是单通道的变化过程，那么多通道的呢？首先来看原图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/651.png" alt></p>
<p>多通道的im2col的过程，是首先im2col第一通道，然后再im2col第二通道，最后im2col第三通道。各通道im2col的数据在内存中也是连续存储的。看下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/652.webp" alt></p>
<p>这是原图经过im2col的变化，那么kernel呢？看原图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/653.webp" alt></p>
<p>kernel的通道数据在内存中也是连续存储的。所以上面的kernel图像经过im2col算法后可以表示为下图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/654.webp" alt></p>
<p>那么我们是如何得到前向传播的结果呢？在DarkNet中和Caffe的实现方式一样，都是Kernel*Img，即是在矩阵乘法中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">M&#x3D;1 </span><br><span class="line">N&#x3D;output_h * output_w</span><br><span class="line">K&#x3D;input_channels * kernel_h * kernel_w</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/655.webp" alt></p>
<p>图像数据是连续存储，因此输出图像也可以如下图所示【output_h x output_w】=【2 x 2】：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/656.png" alt></p>
<p>对于多通道图像过程类似：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/657.webp" alt></p>
<p>同样，多个输出通道图像的数据是连续存储，因此输出图像也可以如下图所示【output_channels x output_h x output_w】=【3 x 2 x 2】</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/658.webp" alt></p>
<p>im2col算法的实现在<code>src/im2col.c</code>中，即<code>im2col_cpu</code>函数。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 从输入的多通道数组im（存储图像数据）中获取指定行，列，通道数处的元素值</span><br><span class="line">** im:  函数的输入，所有的数据存成一个一维数组</span><br><span class="line">** height: 每一个通道的高度(即是输入图像的真正高度，补0之前)</span><br><span class="line">** width: 每一个通道的宽度(即是输入图像的真正宽度，补0之前)</span><br><span class="line">** channles：输入通道数</span><br><span class="line">** row: 要提取的元素所在的行(padding之后的行数)</span><br><span class="line">** col: 要提取的元素所在的列(padding之后的列数)</span><br><span class="line">** channel: 要提取的元素所在的通道</span><br><span class="line">** pad: 图像上下左右补0的个数，四周是一样的</span><br><span class="line">** 返回im中channel通道，row-pad行,col-pad列处的元素值</span><br><span class="line">** 在im中并没有存储补0的元素值，因此height，width都是没有补0时输入图像真正的高、宽；</span><br><span class="line">** 而row与col则是补0之后，元素所在的行列，因此，要准确获取在im中的元素值，首先需要</span><br><span class="line">** 减去pad以获取在im中真实的行列数</span><br><span class="line">*&#x2F;</span><br><span class="line">float im2col_get_pixel(float *im, int height, int width, int channels,</span><br><span class="line">                        int row, int col, int channel, int pad)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F;减去补0长度，获取像素真实的行列数</span><br><span class="line">    row -&#x3D; pad;</span><br><span class="line">    col -&#x3D; pad;</span><br><span class="line">	&#x2F;&#x2F; 如果行列数&lt;0，或者超过height&#x2F;width，则返回0(刚好是补0的效果)</span><br><span class="line">    if (row &lt; 0 || col &lt; 0 ||</span><br><span class="line">        row &gt;&#x3D; height || col &gt;&#x3D; width) return 0;</span><br><span class="line">	&#x2F;&#x2F; im存储多通道二维图像的数据格式为: 各个通道所有的所有行并成1行，再多通道依次并成一行</span><br><span class="line">    &#x2F;&#x2F; 因此width*height*channel首先移位到所在通道的起点位置，再加上width*row移位到所在指定</span><br><span class="line">    &#x2F;&#x2F; 通道行，再加上col移位到所在列</span><br><span class="line">    return im[col + width*(row + height*channel)];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;From Berkeley Vision&#39;s Caffe!</span><br><span class="line">&#x2F;&#x2F;https:&#x2F;&#x2F;github.com&#x2F;BVLC&#x2F;caffe&#x2F;blob&#x2F;master&#x2F;LICENSE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">** 将输入图片转为便于计算的数组格式</span><br><span class="line">** data_im: 输入图像</span><br><span class="line">** height: 输入图像的高度(行)</span><br><span class="line">** width: 输入图像的宽度(列)</span><br><span class="line">** ksize: 卷积核尺寸</span><br><span class="line">** stride: 卷积核跨度</span><br><span class="line">** pad: 四周补0的长度</span><br><span class="line">** data_col: 相当于输出，为进行格式重排后的输入图像数据</span><br><span class="line">** 输出data_col的元素个数与data_im个数不相等，一般比data_im个数多，因为stride较小，各个卷积核之间有很多重叠，</span><br><span class="line">** 实际data_col中的元素个数为channels*ksize*ksize*height_col*width_col，其中channels为data_im的通道数，</span><br><span class="line">** ksize为卷积核大小，height_col和width_col如下所注。data_col的还是按行排列，只是行数为channels*ksize*ksize,</span><br><span class="line">** 列数为height_col*width_col，即一张特征图总的元素个数，每整列包含与某个位置处的卷积核计算的所有通道上的像素，</span><br><span class="line">** （比如输入图像通道数为3,卷积核尺寸为3*3，则共有27行，每列有27个元素），不同列对应卷积核在图像上的不同位置做卷积</span><br><span class="line">*&#x2F;</span><br><span class="line">void im2col_cpu(float* data_im,</span><br><span class="line">     int channels,  int height,  int width,</span><br><span class="line">     int ksize,  int stride, int pad, float* data_col)</span><br><span class="line">&#123;</span><br><span class="line">    int c,h,w;</span><br><span class="line">	&#x2F;&#x2F; 计算该层神经网络的输出图像尺寸（其实没有必要再次计算的，因为在构建卷积层时，make_convolutional_layer()函数</span><br><span class="line">    &#x2F;&#x2F; 已经调用convolutional_out_width()，convolutional_out_height()函数求取了这两个参数，</span><br><span class="line">    &#x2F;&#x2F; 此处直接使用l.out_h,l.out_w即可，函数参数只要传入该层网络指针就可了，没必要弄这么多参数）</span><br><span class="line">    int height_col &#x3D; (height + 2*pad - ksize) &#x2F; stride + 1;</span><br><span class="line">    int width_col &#x3D; (width + 2*pad - ksize) &#x2F; stride + 1;</span><br><span class="line">	&#x2F;&#x2F; 卷积核大小：ksize*ksize是一个卷积核的大小，之所以乘以通道数channels，是因为输入图像有多通道，每个卷积核在做卷积时，</span><br><span class="line">    &#x2F;&#x2F; 是同时对同一位置多通道的图像进行卷积运算，这里为了实现这一目的，将三个通道将三通道上的卷积核并在一起以便进行计算，因此卷积核</span><br><span class="line">    &#x2F;&#x2F; 实际上并不是二维的，而是三维的，比如对于3通道图像，卷积核尺寸为3*3，该卷积核将同时作用于三通道图像上，这样并起来就得</span><br><span class="line">    &#x2F;&#x2F; 到含有27个元素的卷积核，且这27个元素都是独立的需要训练的参数。所以在计算训练参数个数时，一定要注意每一个卷积核的实际</span><br><span class="line">    &#x2F;&#x2F; 训练参数需要乘以输入通道数。</span><br><span class="line">    int channels_col &#x3D; channels * ksize * ksize;</span><br><span class="line">	&#x2F;&#x2F; 外循环次数为一个卷积核的尺寸数，循环次数即为最终得到的data_col的总行数</span><br><span class="line">    for (c &#x3D; 0; c &lt; channels_col; ++c) &#123;</span><br><span class="line">		&#x2F;&#x2F; 列偏移，卷积核是一个二维矩阵，并按行存储在一维数组中，利用求余运算获取对应在卷积核中的列数，比如对于</span><br><span class="line">        &#x2F;&#x2F; 3*3的卷积核（3通道），当c&#x3D;0时，显然在第一列，当c&#x3D;5时，显然在第2列，当c&#x3D;9时，在第二通道上的卷积核的第一列，</span><br><span class="line">        &#x2F;&#x2F; 当c&#x3D;26时，在第三列（第三通道上）</span><br><span class="line">        int w_offset &#x3D; c % ksize;</span><br><span class="line">		&#x2F;&#x2F; 行偏移，卷积核是一个二维的矩阵，且是按行（卷积核所有行并成一行）存储在一维数组中的，</span><br><span class="line">        &#x2F;&#x2F; 比如对于3*3的卷积核，处理3通道的图像，那么一个卷积核具有27个元素，每9个元素对应一个通道上的卷积核（互为一样），</span><br><span class="line">        &#x2F;&#x2F; 每当c为3的倍数，就意味着卷积核换了一行，h_offset取值为0,1,2，对应3*3卷积核中的第1, 2, 3行</span><br><span class="line">        int h_offset &#x3D; (c &#x2F; ksize) % ksize;</span><br><span class="line">		&#x2F;&#x2F; 通道偏移，channels_col是多通道的卷积核并在一起的，比如对于3通道，3*3卷积核，每过9个元素就要换一通道数，</span><br><span class="line">        &#x2F;&#x2F; 当c&#x3D;0~8时，c_im&#x3D;0;c&#x3D;9~17时，c_im&#x3D;1;c&#x3D;18~26时，c_im&#x3D;2</span><br><span class="line">        int c_im &#x3D; c &#x2F; ksize &#x2F; ksize;</span><br><span class="line">		&#x2F;&#x2F; 中循环次数等于该层输出图像行数height_col，说明data_col中的每一行存储了一张特征图，这张特征图又是按行存储在data_col中的某行中</span><br><span class="line">        for (h &#x3D; 0; h &lt; height_col; ++h) &#123;</span><br><span class="line">			&#x2F;&#x2F; 内循环等于该层输出图像列数width_col，说明最终得到的data_col总有channels_col行，height_col*width_col列</span><br><span class="line">            for (w &#x3D; 0; w &lt; width_col; ++w) &#123;</span><br><span class="line">				&#x2F;&#x2F; 由上面可知，对于3*3的卷积核，h_offset取值为0,1,2,当h_offset&#x3D;0时，会提取出所有与卷积核第一行元素进行运算的像素，</span><br><span class="line">                &#x2F;&#x2F; 依次类推；加上h*stride是对卷积核进行行移位操作，比如卷积核从图像(0,0)位置开始做卷积，那么最先开始涉及(0,0)~(3,3)</span><br><span class="line">                &#x2F;&#x2F; 之间的像素值，若stride&#x3D;2，那么卷积核进行一次行移位时，下一行的卷积操作是从元素(2,0)（2为图像行号，0为列号）开始</span><br><span class="line">                int im_row &#x3D; h_offset + h * stride;</span><br><span class="line">				&#x2F;&#x2F; 对于3*3的卷积核，w_offset取值也为0,1,2，当w_offset取1时，会提取出所有与卷积核中第2列元素进行运算的像素，</span><br><span class="line">                &#x2F;&#x2F; 实际在做卷积操作时，卷积核对图像逐行扫描做卷积，加上w*stride就是为了做列移位，</span><br><span class="line">                &#x2F;&#x2F; 比如前一次卷积其实像素元素为(0,0)，若stride&#x3D;2,那么下次卷积元素起始像素位置为(0,2)（0为行号，2为列号）</span><br><span class="line">                int im_col &#x3D; w_offset + w * stride;</span><br><span class="line">				&#x2F;&#x2F; col_index为重排后图像中的像素索引，等于c * height_col * width_col + h * width_col +w（还是按行存储，所有通道再并成一行），</span><br><span class="line">                &#x2F;&#x2F; 对应第c通道，h行，w列的元素</span><br><span class="line">                int col_index &#x3D; (c * height_col + h) * width_col + w;</span><br><span class="line">				&#x2F;&#x2F; im2col_get_pixel函数获取输入图像data_im中第c_im通道，im_row,im_col的像素值并赋值给重排后的图像，</span><br><span class="line">                &#x2F;&#x2F; height和width为输入图像data_im的真实高、宽，pad为四周补0的长度（注意im_row,im_col是补0之后的行列号，</span><br><span class="line">                &#x2F;&#x2F; 不是真实输入图像中的行列号，因此需要减去pad获取真实的行列号）</span><br><span class="line">                data_col[col_index] &#x3D; im2col_get_pixel(data_im, height, width, channels,</span><br><span class="line">                        im_row, im_col, c_im, pad);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
    </div>

    
    
    <div class="post-widgets">
      <div id="needsharebutton-postbottom">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    </div>
	<div>

	  

		<div>

    

        <div >-------------本文结束感谢您的阅读-------------</div>

    

</div>

	  

	</div>
      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag"><i class="fa fa-tag"></i> 目标检测</a>
              <a href="/tags/YOLOv3/" rel="tag"><i class="fa fa-tag"></i> YOLOv3</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/22/AlexeyAB-DarkNet%E7%BD%91%E7%BB%9C%E7%9A%84%E5%89%8D%E5%90%91%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8Alayer%E7%9A%84%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90/" rel="prev" title="AlexeyAB DarkNet网络的前向和反向传播介绍以及layer的详细解析">
      <i class="fa fa-chevron-left"></i> AlexeyAB DarkNet网络的前向和反向传播介绍以及layer的详细解析
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/23/Ubuntu%E9%85%8D%E7%BD%AEv2ray%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/" rel="next" title="Ubuntu配置v2ray详细教程">
      Ubuntu配置v2ray详细教程 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>



<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-text"> 前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积层的构造"><span class="nav-text">卷积层的构造</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积层前向传播的代码解析"><span class="nav-text">卷积层前向传播的代码解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#im2col解析"><span class="nav-text">im2col解析</span></a></li></ol></div>
      </div>
      <!--/noindex-->
	  
     
	  
      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Qiyuan-Z"
      src="/images/wallhaven-915.png">
  <p class="site-author-name" itemprop="name">Qiyuan-Z</p>
  <div class="site-description" itemprop="description">偉大な魂は目的を持ち、そうでないものは願望を持つ</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">103</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Qiyuan-Z" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Qiyuan-Z" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:601872068@qq.com" title="E-Mail → mailto:601872068@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://project-inkstone.github.io/project-inkstone/?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;project-inkstone.github.io&#x2F;project-inkstone&#x2F;?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">project-inkstone</a>
        </li>
    </ul>
  </div>

      </div>
	  
      <div id="music163player">
		   <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=29784463&auto=1&height=66"></iframe>
		   </iframe>
	  </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qiyuan-Z</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">567k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:36</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span> 
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("12/01/2019 13:14:21");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













	<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
	<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
	<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
	<script type="text/javascript">
    		var gitalk = new Gitalk({
		        clientID: '2d10cfb27783db577e70',
		        clientSecret: '154292876bb14966f6ae57304b67859617b08c94',
		        id: md5(location.pathname),
		        repo: 'gitalk',
		        owner: 'Qiyuan-Z',
		        admin: 'Qiyuan-Z',
			distractionFreeMode: '',

		    });
	    gitalk.render('gitalk-container');
	</script>



  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js"></script>
  <script>
      pbOptions = {};
        pbOptions.iconStyle = "box";
        pbOptions.boxForm = "horizontal";
        pbOptions.position = "bottomCenter";
        pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      new needShareButton('#needsharebutton-postbottom', pbOptions);
  </script>
	<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
	<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
	<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
	<script type="text/javascript">
    		var gitalk = new Gitalk({
		        clientID: '2d10cfb27783db577e70',
		        clientSecret: '154292876bb14966f6ae57304b67859617b08c94',
		        id: md5(location.pathname),
		        repo: 'gitalk',
		        owner: 'Qiyuan-Z',
		        admin: 'Qiyuan-Z',
			distractionFreeMode: '',

		    });
	    gitalk.render('gitalk-container');
	</script>


  <script type="text/javascript" src="/js/src/clicklove.js"></script>
</body>
</html>

