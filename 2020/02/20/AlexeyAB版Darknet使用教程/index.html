<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>AlexeyAB版Darknet使用教程 | Yuan</title><meta name="keywords" content="目标检测,YOLOv3"><meta name="author" content="Qiyuan-Z"><meta name="copyright" content="Qiyuan-Z"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言自从Joseph Redmon提出了yolov3后，其darknet仓库已经获得了16k的star，足以说明darknet的流行。该作者最新一次更新也是一年前了，没有继续维护。不过自来自俄国的大神AlexeyAB在不断地更新darknet, 不仅添加了darknet在window下的适配，而且实现了多种SOTA目标检测算法。AlexeyAB也在库中提供了一份详细的建议，从编译、配置、涉及网络到"><meta property="og:type" content="article"><meta property="og:title" content="AlexeyAB版Darknet使用教程"><meta property="og:url" content="https://qiyuan-z.github.io/2020/02/20/AlexeyAB%E7%89%88Darknet%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"><meta property="og:site_name" content="Yuan"><meta property="og:description" content="前言自从Joseph Redmon提出了yolov3后，其darknet仓库已经获得了16k的star，足以说明darknet的流行。该作者最新一次更新也是一年前了，没有继续维护。不过自来自俄国的大神AlexeyAB在不断地更新darknet, 不仅添加了darknet在window下的适配，而且实现了多种SOTA目标检测算法。AlexeyAB也在库中提供了一份详细的建议，从编译、配置、涉及网络到"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg"><meta property="article:published_time" content="2020-02-20T01:30:53.277Z"><meta property="article:modified_time" content="2022-06-07T02:53:22.681Z"><meta property="article:author" content="Qiyuan-Z"><meta property="article:tag" content="目标检测"><meta property="article:tag" content="YOLOv3"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qiyuan-z.github.io/2020/02/20/AlexeyAB%E7%89%88Darknet%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"bottom-right"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-06-07 10:53:22"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));const n=saveToLocal.get("global-font-size");void 0!==n&&document.documentElement.style.setProperty("--global-font-size",n+"px")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://unpkg.com/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">131</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yuan</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">AlexeyAB版Darknet使用教程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-02-20T01:30:53.277Z" title="发表于 2020-02-20 09:30:53">2020-02-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-06-07T02:53:22.681Z" title="更新于 2022-06-07 10:53:22">2022-06-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>33分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自从Joseph Redmon提出了yolov3后，其darknet仓库已经获得了16k的star，足以说明darknet的流行。该作者最新一次更新也是一年前了，没有继续维护。不过自来自俄国的大神AlexeyAB在不断地更新darknet, 不仅添加了darknet在window下的适配，而且实现了多种SOTA目标检测算法。AlexeyAB也在库中提供了一份详细的建议，从编译、配置、涉及网络到测量指标等，一应俱全。通过阅读和理解AlexeyAB的建议，可以为我们带来很多启发。本文是来自翻译AlexeyAB的darknet中的README。</p><p>下图是CSPNet中统计的目前的State of the Art的目标检测模型。其中csresnext50-panet-spp-optimal模型是CSPNet中提出来的，结合AlexeyAB版本的Darknet就可以实现。</p><p><img src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/ABDarknet/640.jpg" alt></p><h2 id="1-依赖"><a href="#1-依赖" class="headerlink" title="1. 依赖"></a>1. 依赖</h2><h3 id="1-1-环境要求"><a href="#1-1-环境要求" class="headerlink" title="1.1 环境要求"></a>1.1 环境要求</h3><ul><li>window系统或者linux系统。</li><li>CMake版本高于3.8。</li><li>CUDA 10.0，cuDNN&gt;=7.0。</li><li>OpenCV版本高于2.4。</li><li>Linux下需要GCC 或者Clang, Window下需要Visual Studio 15、17或19版。</li></ul><h3 id="1-2-数据集获取"><a href="#1-2-数据集获取" class="headerlink" title="1.2 数据集获取"></a>1.2 数据集获取</h3><ol><li>MS COCO数据集: 使用<code>./scripits/get_coco_dataset.sh</code>来获取数据集。</li><li>OpenImages数据集: 使用<code>./scripits/get_openimages_dataset.py</code>获取数据集,并按照规定的格式重排训练集。</li><li>Pascal VOC数据集: 使用<code>./scripits/voc_label.py</code>对数据集标注进行处理。</li><li>ILSVRC2012数据集(ImageNet Classification): 使用<code>./scripits/get_imagenet_train.sh</code>获取数据集，运行<code>./scripits/imagenet_label.sh</code>用于验证集。</li><li>German/Belgium/Russian/LISA/MASTIF 交通标志数据集。</li><li>其他数据集，请访问<code>https://github.com/AlexeyAB/darknet/tree/master/scripts#datasets</code></li></ol><p>结果示意：</p><p><img src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/ABDarknet/641.webp" alt></p><p>其他测试结果可以访问:<code>https://www.youtube.com/user/pjreddie/videos</code></p><h2 id="2-相比原作者Darknet的改进"><a href="#2-相比原作者Darknet的改进" class="headerlink" title="2. 相比原作者Darknet的改进"></a>2. 相比原作者Darknet的改进</h2><ul><li>添加了对windows下运行darknet的支持。</li><li>添加了SOTA模型： CSPNet, PRN, EfficientNet。</li><li>在官方Darknet的基础上添加了新的层：[conv_lstm], [scale_channels] SE/ASFF/BiFPN, [local_avgpool], [sam], [Gaussian_yolo], [reorg3d] (修复 [reorg]), 修复 [batchnorm]。</li><li>可以使用<code>[conv_lstm]</code>层或者<code>[crnn]</code>层来实现针对视频的目标检测。</li><li>添加了多种数据增强策略: <code>[net] mixup=1 cutmix=1 mosaic=1 blur=1</code>。</li><li>添加了多种激活函数: SWISH, MISH, NORM_CHAN, NORM\CHAN_SOFTMAX。</li><li>增加了使用CPU-RAM提高GPU处理训练的能力，以增加<code>mini_batch_size</code>和准确性。</li><li>提升了二值网络，让其在CPU和GPU上的训练和测试速度变为原来的2-4倍。</li><li>通过将Convolutional层和Batch-Norm层合并成一个层，提升了约7%速度。</li><li>如果在Makefile中使用CUDNN_HALF参数，可以让网络在TeslaV100，GeForce RTX等型号的GPU上的检测速度提升两倍。</li><li>针对视频的检测进行了优化，对高清视频检测速度可以提升1.2倍，对4k的视频检测速度可以提升2倍。</li><li>数据增强部分使用Opencv SSE/AVX指令优化了原来朴素实现的数据增强，数据增强速度提升为原来的3.5倍。</li><li>在CPU上使用AVX指令来提高了检测速度，yolov3提高了约85%。</li><li>在网络多尺度训练（<code>random=1</code>）的时候优化了内存分配。</li><li>优化了检测时的GPU初始化策略，在bacth=1的时候执行初始化而不是当batch=1的时候重新初始化。</li><li>添加了计算mAP,F1,IoU, Precision-Recall等指标的方法，只需要运行<code>darknet detector map</code>命令即可。</li><li>支持在训练的过程中画loss曲线和准确率曲线，只需要添加<code>-map</code>标志即可。</li><li>提供了<code>-json_port</code>,<code>-mjpeg_port</code>选项，支持作为json和mjpeg 服务器来在线获取的结果。可以使用你的编写的软件或者web浏览器与<strong>json和mjpeg服务器</strong>连接。</li><li>添加了Anchor的计算功能，可以根据数据集来聚类得到合适的Anchor。</li><li>添加了一些目标检测和目标跟踪的示例：<code>https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</code></li><li>在使用错误的cfg文件或者数据集的时候，添加了运行时的建议和警告。</li><li>其它一些代码修复。</li></ul><h2 id="3-命令行使用"><a href="#3-命令行使用" class="headerlink" title="3. 命令行使用"></a>3. 命令行使用</h2><p>Linux中使用./darknet，window下使用darknet.exe.</p><p>Linux中命令格式类似<code>./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights</code></p><p>Linux中的可执行文件在根目录下，Window下则在<code>\build\darknet\x64</code>文件夹中。以是不同情况下应该使用的命令：</p><ul><li>Yolo v3 COCO - <strong>图片测试</strong>: <code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25</code></li><li><strong>输出坐标</strong> of objects: <code>darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg</code></li><li>Yolo v3 COCO - <strong>视频测试</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4</code></li><li><strong>网络摄像头</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0</code></li><li><strong>网络视频摄像头</strong> - Smart WebCam: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg</code></li><li>Yolo v3 - <strong>保存视频结果为res.avi</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi</code></li><li>Yolo v3 <strong>Tiny版本</strong> COCO - video: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4</code></li><li><strong>JSON and MJPEG 服务器</strong> ：创建JSON和MJPEG服务器，允许软件或Web浏览器进行与服务器之间进行多个连接 。假设两者需要的端口为<code>ip-address:8070</code> 和 <code>8090</code>: <code>./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output</code></li><li>Yolo v3 <strong>Tiny</strong> <strong>on GPU</strong>: <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4</code></li><li>另一个可进行图片测试的命令 Yolo v3 COCO - <strong>图片测试</strong>: <code>darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25</code></li><li>在 <strong>Amazon EC2</strong>上训练, 如果想要看mAP和Loss曲线，运行以下命令: <code>http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090</code> (<strong>Darknet 必须使用OpenCV进行编译才能使用该功能</strong>): <code>./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code></li><li>186 MB Yolo9000 - <strong>图片分类</strong>: <code>darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights</code></li><li><strong>处理一系列图片，并保存结果为json文件</strong>：<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json &lt; data/train.txt</code></li><li><strong>处理一系列图片，并保存结果为txt文件</strong>:<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output &lt; data/train.txt &gt; result.txt</code></li><li><strong>伪标注：</strong> 处理一个list的图片 <code>data/new_train.txt</code> ，可以让结果保存为Yolo训练所需的格式，标注文件为 <code>.txt</code> 。通过这种方法可以迅速增加训练数据量。具体命令为:<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels &lt; data/new_train.txt</code></li><li><strong>如何计算anchor</strong>(通过聚类得到): <code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code></li><li><strong>计算mAP@IoU=50</strong>: <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li><li><strong>计算mAP@IoU=75</strong>: <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75</code></li></ul><p><strong>利用Video-Camera和Mjepg-Stream在Android智能设备中运行YOLOv3</strong></p><ol><li><p>下载 mjpeg-stream APP: IP Webcam / Smart WebCam:</p></li><li><ul><li>Smart WebCam - 从此处下载: <code>https://play.google.com/store/apps/details?id=com.acontech.android.SmartWebCam2</code></li><li>IP Webcam下载地址: <code>https://play.google.com/store/apps/details?id=com.pas.webcam</code></li></ul></li><li><p>将你的手机与电脑通过WIFI或者USB相连。</p></li><li><p>开启手机中的Smart WebCam APP。</p></li><li><p>将以下IP地址替换,在Smart WebCam APP中显示，并运行以下命令：</p></li></ol><p>Yolo v3 COCO-model: <code>darknet.exe detector demo data/coco.data yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0</code></p><h2 id="4-Linux下如何编译Darknet"><a href="#4-Linux下如何编译Darknet" class="headerlink" title="4. Linux下如何编译Darknet"></a>4. Linux下如何编译Darknet</h2><h3 id="4-1-使用CMake编译Darknet"><a href="#4-1-使用CMake编译Darknet" class="headerlink" title="4.1 使用CMake编译Darknet"></a>4.1 使用CMake编译Darknet</h3><p>CMakeList.txt是一个尝试发现所有安装过的、可选的依赖项(比如CUDA，cuDNN, ZED)的配置文件，然后使用这些依赖项进行编译。它将创建一个共享库文件，这样就可以使用Darknet进行代码开发。</p><p>在克隆了项目库以后按照以下命令进行执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build-release</span><br><span class="line">cd build-release</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><h3 id="4-2-使用make编译Darknet"><a href="#4-2-使用make编译Darknet" class="headerlink" title="4.2 使用make编译Darknet"></a>4.2 使用make编译Darknet</h3><p>在克隆了项目库以后，直接运行<code>make</code>命令，需要注意的是Makefile中有一些可选参数：</p><ul><li>GPU=1代表编译完成后将可以使用CUDA来进行GPU加速(CUDA应该在<code>/usr/local/cuda</code>中)。</li><li>CUDNN=1代表通过cuDNN v5-v7进行编译，这样将可以加速使用GPU训练过程(cuDNN应该在<code>/usr/local/cudnn</code>中)。</li><li>CUDNN_HALF=1代表在编译的过程中是否添加Tensor Cores, 编译完成后将可以将目标检测速度提升为原来的3倍，训练网络的速度提高为原来的2倍。</li><li>OPENCV=1代表编译的过程中加入OpenCV, 目前支持的OpenCV的版本有4.x/3.x/2.4.x， 编译结束后将允许Darknet对网络摄像头的视频流或者视频文件进行目标检测。</li><li>DEBUG=1 代表是否开启YOLO的debug模式。</li><li>OPENMP=1代表编译过程将引入openmp,编译结束后将代表可以使用多核CPU对yolo进行加速。</li><li>LIBSO=1 代表编译库darknet.so。</li><li>ZED_CAMERA=1 构建具有ZED-3D相机支持的库(应安装ZED SDK)，然后运行。</li></ul><h2 id="5-如何在Window下编译Darknet"><a href="#5-如何在Window下编译Darknet" class="headerlink" title="5. 如何在Window下编译Darknet"></a>5. 如何在Window下编译Darknet</h2><h3 id="5-1-使用CMake-GUI进行编译"><a href="#5-1-使用CMake-GUI进行编译" class="headerlink" title="5.1 使用CMake-GUI进行编译"></a>5.1 使用CMake-GUI进行编译</h3><p>建议使用以下方法来完成Window下Darknet的编译，需要环境有：Visual Studio 15/17/19, CUDA&gt;10.0, cuDNN&gt;7.0, OpenCV&gt;2.4</p><p>使用CMake-GUI编译流程：</p><ol><li>Configure.</li><li>Optional platform for generator (Set: x64) .</li><li>Finish.</li><li>Generate.</li><li>Open Project.</li><li>Set: x64 &amp; Release.</li><li>Build.</li><li>Build solution.</li></ol><h3 id="5-2-使用vcpkg进行编译"><a href="#5-2-使用vcpkg进行编译" class="headerlink" title="5.2 使用vcpkg进行编译"></a>5.2 使用vcpkg进行编译</h3><p>如果你已经满足Visual Studio 15/17/19 、CUDA&gt;10.0、 cuDNN&gt;7.0、OpenCV&gt;2.4的条件, 那么推荐使用通过CMake-GUI的方式进行编译。</p><p>否则按照以下步骤进行编译:</p><ul><li>安装或更新Visual Studio到17+,确保已经对其进行全面修补。</li><li>安装CUDA和cuDNN。</li><li>安装Git和CMake, 并将它们加入环境变量中。</li><li>安装vcpkg然后尝试安装一个测试库来确认安装是正确的，比如：<code>vcpkg install opengl</code>。</li><li>定义一个环境变量<code>VCPKG_ROOT</code>, 指向vcpkg的安装路径。</li><li>定义另一个环境变量<code>VCPKG_DEFAULT_TRIPLET</code>将其指向x64-windows。</li><li>打开Powershell然后运行以下命令：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PS \&gt;                  cd $env:VCPKG_ROOT</span><br><span class="line">PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] </span><br><span class="line">#replace with opencv[cuda,ffmpeg] in case you want to use cuda-accelerated openCV</span><br></pre></td></tr></table></figure><ul><li>打开Powershell, 切换到darknet文件夹，然后运行<code>.\build.ps1</code>进行编译。如果要使用Visual Studio，将在Build后找到CMake为您创建的两个自定义解决方案，一个在<code>build_win_debug</code>中，另一个在<code>build_win_release</code>中，其中包含适用于系统的所有配置标志。</li></ul><h3 id="5-3-使用legacy-way进行编译"><a href="#5-3-使用legacy-way进行编译" class="headerlink" title="5.3 使用legacy way进行编译"></a>5.3 使用legacy way进行编译</h3><ul><li><p>如果你有CUDA10.0、cuDNN 7.4 和OpenCV 3.x , 那么打开<code>build\darknet\darknet.sln</code>, 设置x64和Release 然后运行Build， 进行darknet的编译，将cuDNN加入环境变量中。</p><ul><li>在<code>C:\opencv_3.0\opencv\build\x64\vc14\bin</code>找到<code>opencv_world320.dll</code>和<code>opencv_ffmpeg320_64.dll</code>, 然后将其复制到<code>darknet.exe</code>同级目录中。</li><li>在<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</code>中检查是否含有bin和include文件夹。如果没有这两个文件夹，那就将他们从CUDA安装的地方复制到这个地方。</li><li>安装cuDNN 7.4.1 来匹配CUDA 10.0, 将cuDNN添加到环境变量<code>CUDNN</code>。将<code>cudnn64_7.dll</code>复制到<code>\build\darknet\x64</code>中。</li></ul></li><li><p>如果你是用的是其他版本的CUDA（不是CUDA 10.0）, 那么使用Notepad打开<code>build\darknet\darknet.vxcproj</code>, 将其中的CUDA 10.0替换为你的CUDA的版本。然后打开<code>\darknet.sln</code>, 然后右击工程，点击属性properties, 选择CUDA C/C++, 然后选择Device , 然后移除<code>compute_75,sm_75</code>。之后从第一步从头开始执行。</p></li><li><p>如果你没有GPU但是有OpenCV3.0， 那么打开<code>build\darknet\darknet_no_gpu.sln</code>, 设置x64和Release， 然后运行build -&gt; build darknet_no_gpu。</p></li><li><p>如果你只安装了OpenCV 2.4.14，那你应该修改<code>\darknet.sln</code>中的路径。</p></li><li><ul><li>(右键点击工程) -&gt; properties -&gt; C/C++ -&gt; General -&gt; Additional Include Directories: <code>C:\opencv_2.4.13\opencv\build\include</code></li><li>(右键点击工程)-&gt; properties -&gt; Linker -&gt; General -&gt; Additional Library Directories: <code>C:\opencv_2.4.13\opencv\build\x64\vc14\lib</code></li></ul></li><li><p>如果你的GPU有Tensor Cores(Nvidia Titan V/ Tesla V100/ DGX-2等型号)， 可以提升目标检测模型测试速度为原来的3倍，训练速度变为原来的2倍。<code>\darknet.sln</code> -&gt; (右键点击工程) -&gt; properties -&gt; C/C++ -&gt; Preprocessor -&gt; Preprocessor Definitions, and add here: <code>CUDNN_HALF;</code></p><p><strong>注意</strong>：CUDA 必须在Visual Studio安装后再安装。</p></li></ul><h2 id="6-如何训练"><a href="#6-如何训练" class="headerlink" title="6. 如何训练"></a>6. 如何训练</h2><h3 id="6-1-Pascal-VOC-dataset"><a href="#6-1-Pascal-VOC-dataset" class="headerlink" title="6.1 Pascal VOC dataset"></a>6.1 Pascal VOC dataset</h3><ol><li><p>下载预训练模型 (154 MB): <code>http://pjreddie.com/media/files/darknet53.conv.74</code> 将其放在 <code>build\darknet\x64</code>文件夹中。</p></li><li><p>下载pascal voc数据集并解压到 <code>build\darknet\x64\data\voc</code> 放在 <code>build\darknet\x64\data\voc\VOCdevkit\</code>文件夹中:</p><p>2.1 下载 <code>voc_label.py</code> 到 <code>build\darknet\x64\data\voc</code>，地址为: <code>http://pjreddie.com/media/files/voc_label.py。</code></p></li><li><ul><li><code>http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar</code>。</li><li><code>http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar</code>。</li><li><code>http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar</code>。</li></ul></li><li><p>下载并安装python: <code>https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe</code></p></li><li><p>运行命令: <code>python build\darknet\x64\data\voc\voc_label.py</code> (来生成文件: 2007_test.txt, 2007_train.txt, 2007_val.txt, 2012_train.txt, 2012_val.txt)。</p></li><li><p>运行命令: <code>type 2007_train.txt 2007_val.txt 2012_*.txt &gt; train.txt</code>。</p></li><li><p>在 <code>yolov3-voc.cfg</code>文件中设置 <code>batch=64</code> 和<code>subdivisions=8</code>。</p></li><li><p>使用 <code>train_voc.cmd</code> 或者使用以下命令开始训练:</p><p><code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code>。</p></li></ol><p>(<strong>Note:</strong> 如果想要停止loss显示，添加 <code>-dont_show</code>标志. 如果使用CPU运行, 用<code>darknet_no_gpu.exe</code> 代替 <code>darknet.exe</code>。)</p><p>如果想要改数据集路径的话，请修改 <code>build\darknet\cfg\voc.data</code>文件。</p><p><strong>Note:</strong> 在训练中如果你看到avg为nan，那证明训练出错。但是如果在其他部分出现nan，这属于正常现象，训练过程是正常的。</p><h3 id="6-2-如何使用多GPU训练？"><a href="#6-2-如何使用多GPU训练？" class="headerlink" title="6.2 如何使用多GPU训练？"></a>6.2 如何使用多GPU训练？</h3><ol><li>首先在一个GPU中训练大概1000个轮次: <code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code>。</li><li>然后停下来基于这个保存的模型 <code>/backup/yolov3-voc_1000.weights</code> 使用多GPU (最多4个GPU): <code>darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg /backup/yolov3-voc_1000.weights -gpus 0,1,2,3</code>。</li></ol><p>在多GPU训练的时候，<code>learning rate</code>需要进行修改，比如单<code>gpu使用0.001</code>，那么多gpu应该使用0.001/GPUS。然后<code>cfg</code>文件中的<code>burn_in</code>参数和<code>max_batches</code>参数要设置为原来的GPUS倍。</p><h3 id="6-3-训练自定义数据集-重点关注"><a href="#6-3-训练自定义数据集-重点关注" class="headerlink" title="6.3 训练自定义数据集(重点关注)"></a>6.3 训练自定义数据集(重点关注)</h3><p>训练较早提出的Yolo系列算法如<code>yolov2-voc.cfg</code>, <code>yolov2-tiny-voc.cfg</code>, <code>yolo-voc.cfg</code>, <code>yolo-voc.2.0.cfg</code>，请看<code>https://github.com/AlexeyAB/darknet/tree/47c7af1cea5bbdedf1184963355e6418cb8b1b4f#how-to-train-pascal-voc-data</code>。</p><p>Training Yolo v3:</p><ol><li>创建与 <code>yolov3.cfg</code>内容相同的 <code>yolo-obj.cfg</code> 或者直接复制然后重命名为<code>yolo-obj.cfg</code> 然后</li></ol><ul><li><p>设置<code>cfg</code>文件中 <code>batch=64</code>。</p></li><li><p>设置<code>cfg</code>文件中 <code>subdivisions=16</code>。</p></li><li><p>设置<code>cfg</code>文件中<code>max_batches</code>参数 (一般可以设置为<code>classes*2000</code> 但是不要低于 <code>4000</code>), 比如 如果你有三个类，那么设置<code>max_batches=6000</code>。</p></li><li><p>设置<code>steps</code>参数，一般为80%和90%的<code>max_batches</code>。比如 <code>steps=4800,5400</code></p></li><li><p>设置网络输入长宽必须能够整除32，比如 <code>width=416 height=416</code> `</p></li><li><p>修改yolo层中的 <code>classes=80</code> 改为你的类别的个数，比如<code>classes=3</code>:</p></li><li><p>修改yolo层前一个卷积层convolutional输出通道数。修改的<code>filter</code>个数有一定要求，按照公式<code>filters=(classes+5)×3</code>来设置。这里的<code>5</code>代表<code>x, y, w, h, conf</code>, 这里的<code>3</code>代表分配<code>3</code>个anchor。</p></li><li><p>如果使用 <code>[Gaussian_yolo]</code> (Gaussian_yolov3_BDD.cfg)，<code>filters</code>计算方式不太一样，按照 <code>filters=(classes + 9)x3</code>进行计算。</p></li><li><p>通常来讲，filters的个数计算依赖于类别个数，坐标以及<code>mask</code>的个数（<code>cfg</code>中的<code>mask</code>参数也就是<code>anchors</code>的个数）。</p><p>举个例子,对于两个目标,你的 <code>yolo-obj.cfg</code> 和<code>yolov3.cfg</code> 不同的地方应该在每个<code>[yolo]/[region]</code>层的下面几行:</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">filters=21</span><br><span class="line"></span><br><span class="line">[region]</span><br><span class="line">classes=2</span><br></pre></td></tr></table></figure><ol><li>在<code>build\darknet\x64\data\</code>创建文件 <code>obj.names</code> , 每行一个类别的名称。</li><li>在<code>build\darknet\x64\data\</code> 创建<code>obj.data</code>, 具体内容如下:</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classes= 2 # 你的类别的个数</span><br><span class="line">train  = data/train.txt # 存储用于训练的图片位置</span><br><span class="line">valid  = data/test.txt # 存储用于测试的图片的位置</span><br><span class="line">names = data/obj.names # 每行一个类别的名称</span><br><span class="line">backup = backup/</span><br></pre></td></tr></table></figure><ol><li>将你的图片放在 <code>build\darknet\x64\data\obj\</code>文件夹下。</li><li>你应该标注你的数据集中的每一张图片，使用<code>Yolo_mark</code>这个可视化GUI软件来标注出目标框并且产生标注文件。地址： <code>https://github.com/AlexeyAB/Yolo_mark</code>。</li></ol><p>软件将会为每一个图像创建一个<code>txt</code>文件，并将其放在同一个文件夹中，命名与原图片的名称相同，唯一不同的就是后缀是txt。txt标注文件中每一个目标独占一行，按照<code>&lt;object-class&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code>的格式排布。</p><p>具体参数解释：</p><ul><li><p><code>&lt;object-class&gt;</code>- 是从 <code>0</code> 到 <code>(classes-1)</code>的整数，代表具体的类别。</p></li><li><p><code>&lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> - 是归一化到<code>(0.0 to 1.0]</code>之间的浮点数，都是相对于图片整体的宽和高的一个相对值。</p></li><li><p>比如: <code>&lt;x&gt; = &lt;absolute_x&gt; / &lt;image_width&gt;</code> 或者 <code>&lt;height&gt; = &lt;absolute_height&gt; / &lt;image_height&gt;</code></p></li><li><p>需要注意的是: <code>&lt;x_center&gt; &lt;y_center&gt;</code> - 是标注框的中心点，而不是左上角。请注意格式。</p><p>举个例子，img1.txt中内容如下，代表有两个类别的三个目标：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 0.716797 0.395833 0.216406 0.147222</span><br><span class="line">0 0.687109 0.379167 0.255469 0.158333</span><br><span class="line">1 0.420312 0.395833 0.140625 0.166667</span><br></pre></td></tr></table></figure><ol><li>在<code>build\darknet\x64\data\</code>文件夹中创建train.txt文件，每行包含的是训练集图片的内容。其路径是相对于 <code>darknet.exe</code>的路径或者绝对路径：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data/obj/img1.jpg</span><br><span class="line">data/obj/img2.jpg</span><br><span class="line">data/obj/img3.jpg</span><br></pre></td></tr></table></figure><ol><li><p>下载预训练权重，并将其放在 <code>build\darknet\x64</code>文件夹中。</p><ul><li>对于<code>csresnext50-panet-spp.cfg</code> (133 MB)：请查看原工程。</li><li>对于<code>yolov3.cfg, yolov3-spp.cfg</code> (154 MB)：请查看原工程。</li><li>对于<code>yolov3-tiny-prn.cfg , yolov3-tiny.cfg</code> (6 MB)：请查看原工程。</li><li>对于<code>enet-coco.cfg (EfficientNetB0-Yolov3)</code>：请查看原工程。</li></ul></li><li><p>使用以下命令行开始训练: <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74</code>。</p><p>对于linux用户使用以下命令开始训练: <code>./darknet detector train data/obj.data yolo-obj.cfg darknet53.conv.74</code> (使用<code>./darknet</code> 而不是 <code>darknet.exe</code>)。</p><p>如果想训练的过程中同步显示mAP（每四个epoch进行一次更新），运行命令: <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map</code>。</p><ul><li>权重文件 <code>yolo-obj_last.weights</code> 将会保存在 <code>build\darknet\x64\backup\</code> 文件夹中，每100个迭代保存一次。</li><li>权重文件<code>yolo-obj_xxxx.weights</code> 将会保存在 <code>build\darknet\x64\backup\</code> 文件夹中，每1000个迭代保存一次。</li><li>如果不想在训练的过程中同步展示loss曲线，请执行以下命令 <code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show</code>。</li><li>如果想在训练过程中查看mAP和Loss曲线，可以使用以下命令：<code>darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code> ，然后在浏览器中打开 URL <code>http://ip-address:8090</code> 。</li></ul></li><li><p>训练结束以后，将会在文件夹<code>build\darknet\x64\backup\</code>中得到权重文件 <code>yolo-obj_final.weights</code> 。</p></li></ol><ul><li>在100次迭代以后，你可以停下来，然后从这个点加载模型继续训练。比如说, 你在2000次迭代以后停止训练，如果你之后想要恢复训练，只需要运行命令： <code>darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights</code>，而不需要重头开始训练。</li></ul><p><strong>注意</strong>：</p><ol><li>如果在训练的过程中，发现<code>avg</code>指标变为<code>nan</code>，那证明训练过程有误，可能是数据标注越界导致的问题。但是其他指标有<code>nan</code>是正常的。</li><li>修改<code>width</code>,<code>height</code>的时候必须要保证两者都能够被32整除。</li><li>训练结束后，可以使用以下命令来进行测试：<code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></li><li>如果出现<code>Ouf of memery</code>问题，那说明显卡的显存不够，你可以通过设置<code>subdivisions</code>参数，将其从原来的<code>16</code>提高为<code>32</code>或者<code>64</code>，这样就能降低使用的显存，保证程序正常运行。</li></ol><h3 id="6-4-训练tiny-yolo"><a href="#6-4-训练tiny-yolo" class="headerlink" title="6.4 训练tiny-yolo"></a>6.4 训练tiny-yolo</h3><p>训练tiny yolo与以上的训练过程并无明显区别，除了以下几点：</p><ul><li>下载tiny yolo的预训练权重：<code>https://pjreddie.com/media/files/yolov3-tiny.weights</code></li><li>使用以下命令行来获取预训练权重: <code>darknet.exe partial cfg/yolov3-tiny.cfg yolov3-tiny.weights yolov3-tiny.conv.15 15</code>， 这里的15代表前15个层，也就是backbone所在的层。</li><li>使用的配置文件应该是 <code>cfg/yolov3-tiny_obj.cfg</code> 而不是 <code>yolov3.cfg</code></li><li>使用以下命令开始训练: <code>darknet.exe detector train data/obj.data yolov3-tiny-obj.cfg yolov3-tiny.conv.15</code></li></ul><p>如果想使用其他backbone进行训练比如 DenseNet201-Yolo或者ResNet50-Yolo, 你可以在以下链接中找到:<code>https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd</code></p><p>如果你采用的是自己设计的backbone,那就无法进行迁移学习，backbone可以直接进行参数随机初始化。</p><h3 id="6-5-什么时候停止训练"><a href="#6-5-什么时候停止训练" class="headerlink" title="6.5 什么时候停止训练"></a>6.5 什么时候停止训练</h3><p>建议为每个类分配至少2000次迭代，但是整体迭代次数不应少于4000次。如果想要更加精准地定义什么时候该停止训练，需要使用以下方法：</p><ol><li>训练过程中，你将会看到日志中有很多错误的度量指标，你需要在avg指标不再下降的时候停止训练，如下图所示:</li></ol><blockquote><p>Region Avg IOU: 0.798363, Class: 0.893232, Obj: 0.700808, No Obj: 0.004567, Avg Recall: 1.000000, count: 8 Region Avg IOU: 0.800677, Class: 0.892181, Obj: 0.701590, No Obj: 0.004574, Avg Recall: 1.000000, count: 8</p><p><strong>9002</strong>: 0.211667, <strong>0.60730 avg</strong>, 0.001000 rate, 3.868000 seconds, 576128 images Loaded: 0.000000 seconds</p></blockquote><ul><li><p><strong>9002</strong> - 代表当前的迭代次数。</p></li><li><p><strong>0.60730 avg</strong> - average loss (error) - <strong>这个指标是平均loss, 其越低越好。</strong></p><p>在这个指标不再下降的时候就可以停止训练了。最终的值大概分布在0.05-3.0之间，小而简单的模型通常最终loss比较小，大而复杂的loss可能会比较大。</p></li></ul><p>训练完成后，你就可以从 <code>darknet\build\darknet\x64\backup</code> 文件夹中取出比较靠后的几个<code>weights</code>文件，并对他们进行测试，选择最好的权重文件。</p><p>举个例子，你在<code>9000</code>次迭代后停止训练，但最好的权重可能是<code>7000,8000,9000</code>次的值。这种情况的出现是由于<strong>过拟合</strong>导致的。<strong>过拟合</strong>是由于过度学习训练集的分布，而降低了模型在测试集的泛化能力。</p><p><strong>Early Stopping Point</strong>示意图:</p><p><img src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/ABDarknet/642.webp" alt></p><p>为了得到在early stopping point处的权重：</p><p>2.1 首先，你的obj.data文件中应该含有valid=valid.txt一项，用于测试在验证集的准确率。如果你没有验证集图片，那就直接复制train.txt重命名为valid.txt。</p><p>2.2 假如你选择在<code>9000</code>次迭代后停止，那可以通过以下命令测试<code>7000,8000,9000</code>三个模型的相关指标。选择最高<code>mAP</code>或者最高<code>IoU</code>的模型最为最终模型。</p><ul><li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li><li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_8000.weights</code></li><li><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_9000.weights</code></li></ul><p>或者你可以选择使用<code>-map</code>标志符来直接实时测试mAP值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map</span><br></pre></td></tr></table></figure><p>然后你就能得到loss曲线和mAP曲线，mAP每4个epoch对验证集进行一次测试，并将结果显示在图中。</p><p><img src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/ABDarknet/643.webp" alt></p><p>指标解释</p><ul><li><p><strong>IoU</strong> (intersect over union) - 平均交并比</p></li><li><p><strong>mAP</strong> (mean average precision) - 每个类的平均精度。</p></li></ul><p><strong>mAP</strong> 是Pascal VOC竞赛的默认指标，与MS COCO竞赛中的AP50指标是一致的。</p><p>Precision和Recall参数在Pascal VOC竞赛中略微不同，但 <strong>IoU 的意义都是相同的</strong>。</p><p><img src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/ABDarknet/644.jpg" alt></p><h3 id="6-6-如何在pascal-voc2007数据集上计算mAP指标"><a href="#6-6-如何在pascal-voc2007数据集上计算mAP指标" class="headerlink" title="6.6 如何在pascal voc2007数据集上计算mAP指标"></a>6.6 如何在pascal voc2007数据集上计算mAP指标</h3><ol><li>在VOC2007中计算mAP：</li></ol><ul><li>下载VOC数据集，安装python并且下载<code>`2007_test.txt</code>文件，具体可以参考链接：<code>https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data</code></li><li>下载文件 <code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/scripts/voc_label_difficult.py</code> 到 <code>build\darknet\x64\data\</code> 文件夹，然后运行 <code>voc_label_difficult.py</code> 从而得到 <code>difficult_2007_test.txt</code>。</li><li>将下面voc.data文件中的第四行#删除</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">classes= 20</span><br><span class="line">train  = data/train_voc.txt</span><br><span class="line">valid  = data/2007_test.txt</span><br><span class="line">#difficult = data/difficult_2007_test.txt</span><br><span class="line">names = data/voc.names</span><br><span class="line">backup = backup/</span><br></pre></td></tr></table></figure><p>然后就有两个方法来计算得到mAP:</p><ol><li>使用Darknet + Python: 运行 <code>build/darknet/x64/calc_mAP_voc_py.cmd</code> ，你将得到 <code>yolo-voc.cfg</code> 模型的mAP值, mAP = 75.9%</li><li>直接使用命令: 运行文件 <code>build/darknet/x64/calc_mAP.cmd</code> -你将得到 <code>yolo-voc.cfg</code> 模型, 得到mAP = 75.8%</li></ol><p>YOLOv3的论文：<code>https://arxiv.org/pdf/1612.08242v1.pdf</code>指出对于416x416的YOLOv2，Pascal Voc上的mAP值是76.8%。我们得到的值较低，可能是由于模型在进行检测时的代码略有不同。</p><p>如果你想为<code>tiny-yolo-voc</code>计算mAP值，将脚本中<code>tiny-yolo-voc.cfg</code>取消注释，将<code>yolo-voc.cfg</code>注释掉。</p><p>如果你是用的是python 2.x 而不是python 3.x, 而且你选择使用Darknet+Python的方式来计算mAP, 那你应该使用 <code>reval_voc.py</code> 和 <code>voc_eval.py</code> 而不是使用 <code>reval_voc_py3.py</code> 和 <code>voc_eval_py3.py</code> 。以上脚本来自以下目录：<code>https://github.com/AlexeyAB/darknet/tree/master/scripts</code>。</p><p>目标检测的例子：<code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></p><p><img src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/ABDarknet/645.webp" alt></p><p><img src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/ABDarknet/646.webp" alt></p><h2 id="7-如何提升目标检测性能？"><a href="#7-如何提升目标检测性能？" class="headerlink" title="7. 如何提升目标检测性能？"></a>7. 如何提升目标检测性能？</h2><ol><li><p>训练之前：</p><ul><li><code>train_network_width * train_obj_width / train_image_width ~= detection_network_width * detection_obj_width / detection_image_width</code></li><li><p><code>train_network_height * train_obj_height / train_image_height ~= detection_network_height * detection_obj_height / detection_image_height</code></p></li><li><p>完整模型（5个yolo层）：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg</code>。</p></li><li><p>Tiny模型（3个yolo层）：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny_3l.cfg</code>。</p></li><li><p>带空间金字塔池化的完整模型（3个yolo层）：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-spp.cfg</code>。</p></li><li><p>在<code>cfg</code>文件中将<code>random</code>设为1：这将在Yolo中使用多尺度训练，会提升检测模型准确率。</p></li><li><p>在<code>cfg</code>文件中把输入分辨率增大(<code>height=608</code>, <code>width=608</code>或者其他任意32的倍数)：这将提升检测模型准确率。</p></li><li><p>检查你要检测的每个目标在数据集中是否被标记，数据集中任何目标都不应该没有标签。在大多数训练出问题的情况中基本都是有错误的标签（通过使用某些转换脚本，使用第三方工具标注来获得标签），可以通过<code>https://github.com/AlexeyAB/Yolo_mark</code>来检查你的数据集是否全部标注正确。</p></li><li><p>我的损失函数很高并且mAP很低，训练出错了吗？在训练命令末端使用<code>-show_imgs</code> 标志来运行训练，你是否能看到有正确的边界预测框的目标（在窗口或者<code>aug_...jpg</code>）？如果没有，训练是发生错误了。</p></li><li><p>对于你要检测的每个目标，训练数据集中必须至少有一个相似的目标，它们具有大致相同的形状，物体侧面姿态，相对大小，旋转角度，倾斜度，照明度等。理想的是，你的训练数据集包括具有不同比例，旋转角度，照明度，物体侧面姿态和处于不同背景的目标图像，你最好拥有2000张不同的图像，并且至少训练<code>2000×classes</code>轮次。</p></li><li><p>希望你的训练数据集图片包含你不想检测的未标注的目标，也即是无边界框的负样本图片(空的<code>.txt</code>文件)，并且负样本图片的数量和带有目标的正样本图片数量最好一样多。</p></li><li><p>标注目标的最佳方法是：仅仅标记目标的可见部分或者标记目标的可见和重叠部分，或标记比整个目标多一点(有一点间隙)?根据你希望如何检测目标来进行标记。</p></li><li><p>为了对图片中包含大量目标的数据进行训练，添加<code>max=200</code>或者更高的值在你<code>cfg</code>文件中<code>yolo</code>层或者<code>region</code>层的最后一行（YOLOv3可以检测到的目标全局最大数量为<code>0,0615234375*(width*height)</code>其中<code>width</code>和<code>height</code>是在<code>cfg</code>文件中的<code>[net]</code>部分指定的）。</p></li><li><p>对于小目标的训练（把图像resize到416x416大小后小于16x16的目标）：设置<code>layers = -1, 11</code>而不是<code>layers=-1, 36</code>；设置<code>stride=4</code>而不是<code>stride=2</code>。</p></li><li><p>对于既有大目标又有小目标的训练使用下面的模型：</p></li><li><p>如果你要训练模型将左右目标分为单独的类别（左/右手，左/右交通标志），那就禁用翻转的数据扩充方式，即在数据输入部分添加<code>flip=0</code>。</p></li><li><p>一般规则：你的训练数据集应包括一组您想要检测的相对大小的目标，如下：</p><p>即，对于测试集中的每个目标，训练集中必须至少有一个同类目标和它具有大约相同的尺寸：</p><p><code>object width in percent from Training dataset</code> ~= <code>object width in percent from Test dataset</code></p><p>也就是说，如果训练集中仅存在占图像比例80%-90%的目标，则训练后的目标检测网络将无法检测到占图像比例为1-10%的目标。</p></li><li><p>为了加快训练速度（同时会降低检测精度）使用微调而不是迁移学习，在[net]下面设置<code>stopbackward=1</code>。然后执行下面的命令：<code>./darknet partial cfg/yolov3.cfg yolov3.weights yolov3.conv.81 81</code>这将会创建<code>yolov3.conv.81</code>文件，然后使用<code>yolov3.conv.81</code>文件进行训练而不是<code>darknet53.conv.74</code>。</p></li><li><p>在观察目标的时候，从不同的方向、不同的照明情况、不同尺度、不同的转角和倾斜角度来看，对神经网络来说，它们都是不同的目标。因此，要检测的目标越多，应使用越复杂的网络模型。</p></li><li><p>为了让检测框更准，你可以在每个<code>yolo</code>层添加下面三个参数<code>ignore_thresh = .9 iou_normalizer=0.5 iou_loss=giou</code>，这回提高map@0.9，但会降低map@0.5。</p></li><li><p>当你是神经网络方面的专家时，可以重新计算相对于<code>width</code>和<code>height</code>的<code>anchors</code>：<code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code>然后在3个<code>[yolo]</code>层放置这9个<code>anchors</code>。但是你需要修改每个<code>[yolo]</code>层的<code>masks</code>参数，让第一个<code>[yolo]</code>层的<code>anchors</code>尺寸大于60x60，第二个<code>[yolo]</code>层的<code>anchors</code>尺寸大于30x30，剩下就是第三个<code>[yolo]</code>层的<code>mask</code>。宁外，你需要修改每一个<code>[yolo]</code>层前面的<code>filters=(classes + 5)x</code>。如果很多计算的<code>anchors</code>都找不到合适的层，那还是使用Yolo的默认<code>anchors</code>吧。</p></li></ul></li><li><p>训练之后：</p><ul><li>没有必要重新训练模型，直接使用用<code>416x416</code>分辨率训练出来的<code>.weights</code>模型文件。</li><li>但是要获得更高的准确率，你应该使用<code>608x608</code>或者<code>832x832</code>来训练，注意如果<code>Out of memory</code>发生了，你应该在<code>.cfg</code>文件中增加<code>subdivisions=16，32，64</code>。</li><li>通过在<code>.cfg</code>文件中设置（<code>height=608</code> and <code>width=608</code>）或者（<code>height=832</code> and <code>width=832</code>）或者任何32的倍数，这会提升准确率并使得对小目标的检测更加容易。</li></ul></li></ol><h2 id="8-如何标注以及创建标注文件"><a href="#8-如何标注以及创建标注文件" class="headerlink" title="8. 如何标注以及创建标注文件"></a>8. 如何标注以及创建标注文件</h2><p>下面的工程提供了用于标记目标边界框并为YOLO v2&amp;v3 生成标注文件的带图像界面软件，地址为：<code>https://github.com/AlexeyAB/Yolo_mark</code>。</p><p>例如对于只有两类目标的数据集标注后有以下文件<code>train.txt</code>,<code>obj.names</code>,<code>obj.data</code>,<code>yolo-obj.cfg</code>,<code>air 1-6.txt</code>,<code>bird 1-4.txt</code>，接着配合<code>train_obj.cmd</code>就可以使用YOLO v2和YOLO v3来训练这个数据集了。</p><p>下面提供了5重常见的目标标注工具：</p><ul><li>C++实现的：<code>https://github.com/AlexeyAB/Yolo_mark</code></li><li>Python实现的：<code>https://github.com/tzutalin/labelImg</code></li><li>Python实现的：<code>https://github.com/Cartucho/OpenLabeling</code></li><li>C++实现的：<code>https://www.ccoderun.ca/darkmark/</code></li><li>JavaScript实现的：<code>https://github.com/opencv/cvat</code></li></ul><h2 id="9-使用YOLO9000"><a href="#9-使用YOLO9000" class="headerlink" title="9. 使用YOLO9000"></a>9. 使用YOLO9000</h2><p>同时检测和分类9000个目标：<code>darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights data/dog.jpg</code></p><ul><li><p><code>yolo9000.weights</code>：186Mb的YOLO9000模型需要4G GPU显存，训练好的模型下载地址：<code>http://pjreddie.com/media/files/yolo9000.weights</code>。</p></li><li><p><code>yolo9000.cfg</code>：YOLO9000的c网络结构文件，同时这里也有<code>9k.tree</code>和<code>coco9k.map</code>文件的路径。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree=data/9k.tree</span><br><span class="line">map = data/coco9k.map</span><br></pre></td></tr></table></figure><ul><li><code>9k.tree</code>：9418个类别的单词数，每一行的形式为<code>`，如果</code>parent_id==-1<code>那么这个标签没有父类别，地址为：</code><a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.tree`。">https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.tree`。</a></li><li><code>coco9k.map</code>：将MSCOCO的80个目标类别映射到<code>9k.tree</code>的文件，地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/coco9k.map</code>。</li></ul></li><li><p><code>combine9k.data</code>：数据文件，分别是<code>9k.labels</code>。<code>9k.names</code>，<code>inet9k.map</code>的路径（修改<code>combine9k.train.list</code>文件的路径为你自己的）。地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/combine9k.data</code>。</p></li><li><p><code>9k.labels</code>：9418类目标的标签。地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.labels</code>。</p></li><li><p><code>9k.names</code>：9418类目标的名字。地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/9k.names</code>。</p></li><li><p><code>inet9k.map</code>：将ImageNet的200个目标类别映射到<code>9k.tree</code>的文件，地址为：<code>https://raw.githubusercontent.com/AlexeyAB/darknet/master/build/darknet/x64/data/inet9k.map</code>。</p></li></ul><h2 id="10-如何将YOLO作为DLL和SO库进行使用？"><a href="#10-如何将YOLO作为DLL和SO库进行使用？" class="headerlink" title="10. 如何将YOLO作为DLL和SO库进行使用？"></a>10. 如何将YOLO作为DLL和SO库进行使用？</h2><ul><li><p>在Linux上。</p><ul><li>使用<code>build.sh</code> 或者</li><li>使用<code>cmake</code>编译<code>darknet</code> 或者</li><li>将<code>Makefile</code>重的<code>LIBSO=0</code>改为<code>LIBSO=1</code>，然后执行<code>make</code>编译<code>darknet</code></li></ul></li><li><p>在Windows上。</p><ul><li>使用<code>build.ps1</code> 或者</li><li>使用<code>cmake</code>编译<code>darknet</code> 或者</li><li>使用<code>build\darknet\yolo_cpp_dll.sln</code>或<code>build\darknet\yolo_cpp_dll_no_gpu.sln</code>解决方法编译<code>darknet</code></li></ul></li><li><p>这里有两个API：</p><ul><li>使用C++ API的C++例子：<code>https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</code></li><li>使用C API的Python例子：<br><code>https://github.com/AlexeyAB/darknet/blob/master/darknet.py</code><br><code>https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py</code></li><li>C API：<code>https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h</code></li><li>C++ API：<code>https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp</code></li></ul></li></ul><h2 id="11-附录"><a href="#11-附录" class="headerlink" title="11. 附录"></a>11. 附录</h2><ol><li><p>为了将Yolo编译成C++的DLL文件<code>yolo_cpp_dll.dll</code>：打开<code>build\darknet\yolo_cpp_dll.sln</code>解决方案，编译选项选<strong>X64</strong>和<strong>Release</strong>，然后执行Build-&gt;Build yolo_cpp_dll就，编译的一些前置条件为：</p><ul><li>安装<strong>CUDA 10.0</strong>。</li><li>为了使用cuDNN执行以下步骤：点击工程属性-&gt;properties-&gt;C++-&gt;Preprocessor-&gt;Preprocessor Definitions，然后在开头添加一行<code>CUDNN</code>。</li></ul></li><li><p>在自己的C++工程中将Yolo当成DLL文件使用：打开<code>build\darknet\yolo_console_dll.sln</code>解决方案，编译选项选<strong>X64</strong>和<strong>Release</strong>，然后执行Build-&gt;Build yolo_console_dll：</p><p><code>yolo_cpp_dll.dll</code>-API：<code>https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp</code></p><ul><li>你可以利用Windows资源管理器运行<code>build\darknet\x64\yolo_console_dll.exe</code>可执行程序并<strong>使用下面的命令</strong>: <code>yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4</code></li><li>启动控制台应用程序并输入图像文件名后，你将看到每个目标的信息：<code></code></li><li>如果要使用OpenCV-GUI你应该将<code>yolo_console_dll.cpp</code>中的<code>//#define OPENCV</code>取消注释。</li><li>你可以看到视频检测例子的源代码，地址为yolo_console_dll.cpp的第75行。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bbox_t</span> &#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> x, y, w, h;    <span class="comment">// (x,y) - top-left corner, (w, h) - width &amp; height of bounded box</span></span><br><span class="line">    <span class="keyword">float</span> prob;                    <span class="comment">// confidence - probability that the object was found correctly</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> obj_id;        <span class="comment">// class of object - from range [0, classes-1]</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> track_id;        <span class="comment">// tracking id for video (0 - untracked, 1 - inf - tracked object)</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> frames_counter;<span class="comment">// counter of frames on which the object was detected</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Detector</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">        <span class="built_in">Detector</span>(std::string cfg_filename, std::string weight_filename, <span class="keyword">int</span> gpu_id = <span class="number">0</span>);</span><br><span class="line">        ~<span class="built_in">Detector</span>();</span><br><span class="line"></span><br><span class="line">        <span class="function">std::vector&lt;<span class="keyword">bbox_t</span>&gt; <span class="title">detect</span><span class="params">(std::string image_filename, <span class="keyword">float</span> thresh = <span class="number">0.2</span>, <span class="keyword">bool</span> use_mean = <span class="literal">false</span>)</span></span>;</span><br><span class="line">        <span class="function">std::vector&lt;<span class="keyword">bbox_t</span>&gt; <span class="title">detect</span><span class="params">(<span class="keyword">image_t</span> img, <span class="keyword">float</span> thresh = <span class="number">0.2</span>, <span class="keyword">bool</span> use_mean = <span class="literal">false</span>)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">image_t</span> <span class="title">load_image</span><span class="params">(std::string image_filename)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">free_image</span><span class="params">(<span class="keyword">image_t</span> m)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> OPENCV</span></span><br><span class="line">        <span class="function">std::vector&lt;<span class="keyword">bbox_t</span>&gt; <span class="title">detect</span><span class="params">(cv::Mat mat, <span class="keyword">float</span> thresh = <span class="number">0.2</span>, <span class="keyword">bool</span> use_mean = <span class="literal">false</span>)</span></span>;</span><br><span class="line">	<span class="function">std::shared_ptr&lt;<span class="keyword">image_t</span>&gt; <span class="title">mat_to_image_resize</span><span class="params">(cv::Mat mat)</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li></ul></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">Qiyuan-Z</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://qiyuan-z.github.io/2020/02/20/AlexeyAB%E7%89%88Darknet%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/">https://qiyuan-z.github.io/2020/02/20/AlexeyAB%E7%89%88Darknet%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://Qiyuan-Z.github.io" target="_blank">Yuan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a><a class="post-meta__tags" href="/tags/YOLOv3/">YOLOv3</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/02/21/AlexeyAB-DarkNet%E6%A1%86%E6%9E%B6%E6%80%BB%E8%A7%88/"><img class="prev-cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">AlexeyAB DarkNet框架总览</div></div></a></div><div class="next-post pull-right"><a href="/2020/02/19/ECCV-2018-Convolutional-Block-Attention-Module/"><img class="next-cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ECCV 2018 Convolutional Block Attention Module</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/04/17/AlexeyAB-DarkNet-Dropout层代码详解/" title="AlexeyAB DarkNet Dropout层代码详解"><img class="cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-17</div><div class="title">AlexeyAB DarkNet Dropout层代码详解</div></div></a></div><div><a href="/2020/03/01/AlexeyAB-DarkNet-BN层代码详解(batchnorm_layer.c)/" title="AlexeyAB DarkNet BN层代码详解(batchnorm_layer.c)"><img class="cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-01</div><div class="title">AlexeyAB DarkNet BN层代码详解(batchnorm_layer.c)</div></div></a></div><div><a href="/2020/02/29/AlexeyAB-DarkNet池化层代码详解(maxpool_layer.c)/" title="AlexeyAB DarkNet池化层代码详解(maxpool_layer.c)"><img class="cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-29</div><div class="title">AlexeyAB DarkNet池化层代码详解(maxpool_layer.c)</div></div></a></div><div><a href="/2020/02/28/YOLOV3损失函数代码详解(yolo_layer.c)/" title="YOLOV3损失函数代码详解(yolo_layer.c)"><img class="cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-28</div><div class="title">YOLOV3损失函数代码详解(yolo_layer.c)</div></div></a></div><div><a href="/2020/02/27/YOLOV2损失函数代码详解(region_layer.c)/" title="YOLOV2损失函数代码详解(region_layer.c)"><img class="cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-27</div><div class="title">YOLOV2损失函数代码详解(region_layer.c)</div></div></a></div><div><a href="/2020/02/26/YOLOV1损失函数代码详解(detection_layer.c)/" title="YOLOV1损失函数代码详解(detection_layer.c)"><img class="cover" src="https://raw.githubusercontent.com/Qiyuan-Z/blog-image/main/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-26</div><div class="title">YOLOV1损失函数代码详解(detection_layer.c)</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BE%9D%E8%B5%96"><span class="toc-number">2.</span> <span class="toc-text">1. 依赖</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%8E%AF%E5%A2%83%E8%A6%81%E6%B1%82"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 环境要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E8%8E%B7%E5%8F%96"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 数据集获取</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E6%AF%94%E5%8E%9F%E4%BD%9C%E8%80%85Darknet%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="toc-number">3.</span> <span class="toc-text">2. 相比原作者Darknet的改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BD%BF%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text">3. 命令行使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Linux%E4%B8%8B%E5%A6%82%E4%BD%95%E7%BC%96%E8%AF%91Darknet"><span class="toc-number">5.</span> <span class="toc-text">4. Linux下如何编译Darknet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%BD%BF%E7%94%A8CMake%E7%BC%96%E8%AF%91Darknet"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 使用CMake编译Darknet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%BD%BF%E7%94%A8make%E7%BC%96%E8%AF%91Darknet"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 使用make编译Darknet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A6%82%E4%BD%95%E5%9C%A8Window%E4%B8%8B%E7%BC%96%E8%AF%91Darknet"><span class="toc-number">6.</span> <span class="toc-text">5. 如何在Window下编译Darknet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E4%BD%BF%E7%94%A8CMake-GUI%E8%BF%9B%E8%A1%8C%E7%BC%96%E8%AF%91"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 使用CMake-GUI进行编译</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BD%BF%E7%94%A8vcpkg%E8%BF%9B%E8%A1%8C%E7%BC%96%E8%AF%91"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 使用vcpkg进行编译</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%BD%BF%E7%94%A8legacy-way%E8%BF%9B%E8%A1%8C%E7%BC%96%E8%AF%91"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 使用legacy way进行编译</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83"><span class="toc-number">7.</span> <span class="toc-text">6. 如何训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-Pascal-VOC-dataset"><span class="toc-number">7.1.</span> <span class="toc-text">6.1 Pascal VOC dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%A4%9AGPU%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">7.2.</span> <span class="toc-text">6.2 如何使用多GPU训练？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86-%E9%87%8D%E7%82%B9%E5%85%B3%E6%B3%A8"><span class="toc-number">7.3.</span> <span class="toc-text">6.3 训练自定义数据集(重点关注)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E8%AE%AD%E7%BB%83tiny-yolo"><span class="toc-number">7.4.</span> <span class="toc-text">6.4 训练tiny-yolo</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%81%9C%E6%AD%A2%E8%AE%AD%E7%BB%83"><span class="toc-number">7.5.</span> <span class="toc-text">6.5 什么时候停止训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-%E5%A6%82%E4%BD%95%E5%9C%A8pascal-voc2007%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%AE%A1%E7%AE%97mAP%E6%8C%87%E6%A0%87"><span class="toc-number">7.6.</span> <span class="toc-text">6.6 如何在pascal voc2007数据集上计算mAP指标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">8.</span> <span class="toc-text">7. 如何提升目标检测性能？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%A6%82%E4%BD%95%E6%A0%87%E6%B3%A8%E4%BB%A5%E5%8F%8A%E5%88%9B%E5%BB%BA%E6%A0%87%E6%B3%A8%E6%96%87%E4%BB%B6"><span class="toc-number">9.</span> <span class="toc-text">8. 如何标注以及创建标注文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E4%BD%BF%E7%94%A8YOLO9000"><span class="toc-number">10.</span> <span class="toc-text">9. 使用YOLO9000</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E5%A6%82%E4%BD%95%E5%B0%86YOLO%E4%BD%9C%E4%B8%BADLL%E5%92%8CSO%E5%BA%93%E8%BF%9B%E8%A1%8C%E4%BD%BF%E7%94%A8%EF%BC%9F"><span class="toc-number">11.</span> <span class="toc-text">10. 如何将YOLO作为DLL和SO库进行使用？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E9%99%84%E5%BD%95"><span class="toc-number">12.</span> <span class="toc-text">11. 附录</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022<i id="heartbeat" class="fa fas fa-heartbeat"></i> Qiyuan-Z</div><div class="footer_custom_text"><p><a style="margin-inline:5px" target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"></a><a style="margin-inline:5px" target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"></a><a style="margin-inline:5px" target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用 Jsdelivr 为静态资源提供CDN加速" alt="Jsdelivr"></a><a style="margin-inline:5px" target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"></a><a style="margin-inline:5px" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a><br>昨日までの私は、もうどこにもいない<br></p></div></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd"}},tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""],addClass:[200,()=>{document.querySelectorAll("mjx-container:not([display='true']").forEach(t=>{const e=t.parentNode;e.classList.contains("has-jax")||e.classList.add("mathjax-overflow")})},"",!1]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>function addGitalkSource(){const e=document.createElement("link");e.rel="stylesheet",e.href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css",document.getElementsByTagName("head")[0].appendChild(e)}function loadGitalk(){function e(){new Gitalk({clientID:"2d10cfb27783db577e70",clientSecret:"154292876bb14966f6ae57304b67859617b08c94",repo:"gitalk",owner:"Qiyuan-Z",admin:["Qiyuan-Z"],id:"95a215bf4eb440fc94e8e7e20683804d",language:"zh-CN",perPage:10,distractionFreeMode:!1,pagerDirection:"last",createIssueManually:!1,updateCountCallback:commentCount}).render("gitalk-container")}"function"==typeof Gitalk?e():(addGitalkSource(),getScript("https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js").then(e))}function commentCount(e){let t=document.querySelector("#post-meta .gitalk-comment-count");t&&(t.innerHTML=e)}{function loadOtherComment(){loadGitalk()}loadGitalk()}</script></div><script defer src="https://cdn.jsdelivr.net/npm/akilar-live2d-widget/autoload.min.js"></script><script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script><script src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>