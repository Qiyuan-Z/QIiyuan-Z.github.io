<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://Qiyuan-Z.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="article">
<meta property="og:title" content="AlexeyAB DarkNet BN层代码详解(batchnorm_layer.c)">
<meta property="og:url" content="https://qiyuan-z.github.io/2020/03/01/AlexeyAB-DarkNet-BN%E5%B1%82%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(batchnorm_layer.c)/index.html">
<meta property="og:site_name" content="Yuan">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/667.png">
<meta property="og:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/668.webp">
<meta property="article:published_time" content="2020-03-01T02:59:33.332Z">
<meta property="article:modified_time" content="2020-03-01T04:24:36.879Z">
<meta property="article:author" content="Qiyuan-Z">
<meta property="article:tag" content="目标检测">
<meta property="article:tag" content="YOLOv3">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/667.png">

<link rel="canonical" href="https://qiyuan-z.github.io/2020/03/01/AlexeyAB-DarkNet-BN%E5%B1%82%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(batchnorm_layer.c)/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css"><style>
#needsharebutton-postbottom {
  cursor: pointer;
  height: 26px;
  margin-top: 10px;
  position: relative;
}
#needsharebutton-postbottom .btn {
  border: 1px solid $btn-default-border-color;
  border-radius: 3px;
  display: initial;
  padding: 1px 4px;
}
</style>
  <title>AlexeyAB DarkNet BN层代码详解(batchnorm_layer.c) | Yuan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yuan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">记录学习中的点点滴滴</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://qiyuan-z.github.io/2020/03/01/AlexeyAB-DarkNet-BN%E5%B1%82%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3(batchnorm_layer.c)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/wallhaven-915.png">
      <meta itemprop="name" content="Qiyuan-Z">
      <meta itemprop="description" content="偉大な魂は目的を持ち、そうでないものは願望を持つ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AlexeyAB DarkNet BN层代码详解(batchnorm_layer.c)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-01 10:59:33 / 修改时间：12:24:36" itemprop="dateCreated datePublished" datetime="2020-03-01T10:59:33+08:00">2020-03-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="BatchNorm原理"><a href="#BatchNorm原理" class="headerlink" title=" BatchNorm原理"></a><a id="more"></a> BatchNorm原理</h2><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/667.png" alt></p>
<p>这是论文中给出的对BatchNorm的算法流程解释，这篇推文的目的主要是推导和从源码角度解读BatchNorm的前向传播和反向传播，就不关注具体的原理了（实际上是因为BN层的原理非常复杂），我们暂时知道BN层是用来调整数据分布，降低过拟合的就够了。</p>
<h2 id="前向传播推导"><a href="#前向传播推导" class="headerlink" title="前向传播推导"></a>前向传播推导</h2><p>前向传播实际就是将Algorithm1的4个公式转化为编程语言，这里先贴一段CS231N官方提供的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def batchnorm_forward(x, gamma, beta, bn_param):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  Input:</span><br><span class="line">  - x: (N, D)维输入数据</span><br><span class="line">  - gamma: (D,)维尺度变化参数</span><br><span class="line">  - beta: (D,)维尺度变化参数</span><br><span class="line">  - bn_param: Dictionary with the following keys:</span><br><span class="line">    - mode: &#39;train&#39; 或者 &#39;test&#39;</span><br><span class="line">    - eps: 一般取1e-8~1e-4</span><br><span class="line">    - momentum: 计算均值、方差的更新参数</span><br><span class="line">    - running_mean: (D,)动态变化array存储训练集的均值</span><br><span class="line">    - running_var：(D,)动态变化array存储训练集的方差</span><br><span class="line"></span><br><span class="line">  Returns a tuple of:</span><br><span class="line">  - out: 输出y_i（N，D）维</span><br><span class="line">  - cache: 存储反向传播所需数据</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  mode &#x3D; bn_param[&#39;mode&#39;]</span><br><span class="line">  eps &#x3D; bn_param.get(&#39;eps&#39;, 1e-5)</span><br><span class="line">  momentum &#x3D; bn_param.get(&#39;momentum&#39;, 0.9)</span><br><span class="line"></span><br><span class="line">  N, D &#x3D; x.shape</span><br><span class="line">  # 动态变量，存储训练集的均值方差</span><br><span class="line">  running_mean &#x3D; bn_param.get(&#39;running_mean&#39;, np.zeros(D, dtype&#x3D;x.dtype))</span><br><span class="line">  running_var &#x3D; bn_param.get(&#39;running_var&#39;, np.zeros(D, dtype&#x3D;x.dtype))</span><br><span class="line"></span><br><span class="line">  out, cache &#x3D; None, None</span><br><span class="line">  # TRAIN 对每个batch操作</span><br><span class="line">  if mode &#x3D;&#x3D; &#39;train&#39;:</span><br><span class="line">    sample_mean &#x3D; np.mean(x, axis &#x3D; 0)</span><br><span class="line">    sample_var &#x3D; np.var(x, axis &#x3D; 0)</span><br><span class="line">    x_hat &#x3D; (x - sample_mean) &#x2F; np.sqrt(sample_var + eps)</span><br><span class="line">    out &#x3D; gamma * x_hat + beta</span><br><span class="line">    cache &#x3D; (x, gamma, beta, x_hat, sample_mean, sample_var, eps)</span><br><span class="line">    running_mean &#x3D; momentum * running_mean + (1 - momentum) * sample_mean</span><br><span class="line">    running_var &#x3D; momentum * running_var + (1 - momentum) * sample_var</span><br><span class="line">  # TEST：要用整个训练集的均值、方差</span><br><span class="line">  elif mode &#x3D;&#x3D; &#39;test&#39;:</span><br><span class="line">    x_hat &#x3D; (x - running_mean) &#x2F; np.sqrt(running_var + eps)</span><br><span class="line">    out &#x3D; gamma * x_hat + beta</span><br><span class="line">  else:</span><br><span class="line">    raise ValueError(&#39;Invalid forward batchnorm mode &quot;%s&quot;&#39; % mode)</span><br><span class="line"></span><br><span class="line">  bn_param[&#39;running_mean&#39;] &#x3D; running_mean</span><br><span class="line">  bn_param[&#39;running_var&#39;] &#x3D; running_var</span><br><span class="line"></span><br><span class="line">  return out, cache</span><br></pre></td></tr></table></figure>
<p>就是一个公式带入的问题，这里倒是没啥好说的，不过了为了和下面反向传播对比理解，这里我们明确每一个张量的维度：</p>
<ul>
<li><strong>x</strong> shape为(N,D)，可以将N看成batch size,D看成特征图展开为1列的元素个数</li>
<li><strong>gamma</strong> shape为(D,)</li>
<li><strong>beta</strong> shape为(D,)</li>
<li><strong>running_mean</strong> shape为(D,)</li>
<li><strong>running_var</strong> shape为(D,)</li>
</ul>
<p>请特别注意滑动平均(影子变量)这种Trick的引入，目的是为了控制变量更新的速度，防止变量的突然变化对变量的整体影响，这能提高模型的鲁棒性。</p>
<h2 id="反向传播推导"><a href="#反向传播推导" class="headerlink" title="反向传播推导"></a>反向传播推导</h2><p>这才是重点，现在做一些约定：</p>
<ul>
<li>$\delta$为一个Batch所有样本的方差</li>
<li>$\mu$为样本均值</li>
<li>$\hat{x}$为归一化后的样本数据</li>
<li>$y_{i}$为输入样本$x_{i}$经过尺度变化的输出量</li>
<li>$\gamma$和$\beta$为尺度变化系数</li>
<li>$\frac{\partial L}{\partial y}$是上一层的梯度，并假设$x$和$y$都是$(\mathrm{N}, \mathrm{D})$维，即有N个维度为D的样本在BN层的前向传播中$x_{i}$通过$\gamma, \beta, \hat{x}$将$x_{i}$变换为$y_{i}$，那么反向传播则是根据$\frac{\partial L}{\partial y_{i}}$求得$\frac{\partial L}{\partial \gamma}, \frac{\partial L}{\partial \beta}, \frac{\partial L}{\partial x_{i}}$</li>
<li>求解$\frac{\partial L}{\partial \gamma}$  $\frac{\partial L}{\partial \gamma}=\sum_{i} \frac{\partial L}{\partial y_{i}} \frac{\partial y_{i}}{\partial \gamma}=\sum_{i} \frac{\partial L}{\partial y_{i}} \hat{x}$</li>
<li>求解$\frac{\partial L}{\partial \beta}$  $\frac{\partial L}{\partial \beta}=\sum_{i} \frac{\partial L}{\partial y_{i}} \frac{\partial y_{i}}{\partial \beta}=\sum_{i} \frac{\partial L}{\partial y_{i}}$</li>
<li>求解$\frac{\partial L}{\partial x_{i}}$根据论文的公式和链式法则可得下面的等式: $\frac{\partial L}{\partial x_{i}}=\frac{\partial L}{\partial \widehat{x}_{i}} \frac{\partial \widehat{x}_{i}}{\partial x_{i}}+\frac{\partial L}{\partial \sigma} \frac{\partial \sigma}{\partial x_{i}}+\frac{\partial L}{\partial \mu} \frac{\partial \mu}{\partial x_{i}}$我们这里又可以先求$\frac{\partial L}{\partial \hat{x}}$</li>
<li>$\frac{\partial L}{\partial \hat{x}}=\frac{\partial L}{\partial y} \frac{\partial y}{\partial \hat{x}}=\frac{\partial L}{\partial y} \gamma$    (1)</li>
<li>$\frac{\partial L}{\partial \sigma}=\sum_{i} \frac{\partial L}{\partial y_{i}} \frac{\partial y_{i}}{\partial \hat{x}_{i}} \frac{\partial \hat{x}_{i}}{\partial \sigma}=-\frac{1}{2} \sum_{i} \frac{\partial L}{\partial \widehat{x}_{i}}\left(x_{i}-\mu\right)(\sigma+\varepsilon)^{-1.5}$    (2)</li>
<li>$\frac{\partial L}{\partial \mu}=\frac{\partial L}{\partial \hat{x}} \frac{\partial \hat{x}}{\partial \mu}+\frac{\partial L}{\partial \sigma} \frac{\partial \sigma}{\partial \mu}=\sum_{i} \frac{\partial L}{\partial \hat{x}_{i}} \frac{-1}{\sqrt{\sigma+\varepsilon}}+\frac{\partial L}{\partial \sigma} \frac{-2 \Sigma_{i}\left(x_{i}-\mu\right)}{N}$    (3)</li>
<li>有了(1) (2) (3)就可以求出$\frac{\partial L}{\partial x_{i}}$<br>$\frac{\partial L}{\partial x_{i}}=\frac{\partial L}{\partial \widehat{x}_{i}} \frac{\partial \widehat{x}_{i}}{\partial x_{i}}+\frac{\partial L}{\partial \sigma} \frac{\partial \sigma}{\partial x_{i}}+\frac{\partial L}{\partial \mu} \frac{\partial \mu}{\partial x_{i}}=\frac{\partial L}{\partial \hat{x}_{i}} \frac{1}{\sqrt{\sigma+\varepsilon}}+\frac{\partial L}{\partial \sigma} \frac{2\left(x_{i}-\mu\right)}{N}+\frac{\partial L}{\partial \mu} \frac{1}{N}$</li>
</ul>
<p>到这里就推到出了BN层的反向传播公式了，和论文中一样，截取一下论文中的结果图：</p>
<p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ABDarknet/668.webp" alt></p>
<p>贴一份CS231N反向传播代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def batchnorm_backward(dout, cache):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  Inputs:</span><br><span class="line">  - dout: 上一层的梯度，维度(N, D)，即 dL&#x2F;dy</span><br><span class="line">  - cache: 所需的中间变量，来自于前向传播</span><br><span class="line"></span><br><span class="line">  Returns a tuple of:</span><br><span class="line">  - dx: (N, D)维的 dL&#x2F;dx</span><br><span class="line">  - dgamma: (D,)维的dL&#x2F;dgamma</span><br><span class="line">  - dbeta: (D,)维的dL&#x2F;dbeta</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">    x, gamma, beta, x_hat, sample_mean, sample_var, eps &#x3D; cache</span><br><span class="line">  N &#x3D; x.shape[0]</span><br><span class="line"></span><br><span class="line">  dgamma &#x3D; np.sum(dout * x_hat, axis &#x3D; 0)</span><br><span class="line">  dbeta &#x3D; np.sum(dout, axis &#x3D; 0)</span><br><span class="line"></span><br><span class="line">  dx_hat &#x3D; dout * gamma</span><br><span class="line">  dsigma &#x3D; -0.5 * np.sum(dx_hat * (x - sample_mean), axis&#x3D;0) * np.power(sample_var + eps, -1.5)</span><br><span class="line">  dmu &#x3D; -np.sum(dx_hat &#x2F; np.sqrt(sample_var + eps), axis&#x3D;0) - 2 * dsigma*np.sum(x-sample_mean, axis&#x3D;0)&#x2F; N</span><br><span class="line">  dx &#x3D; dx_hat &#x2F;np.sqrt(sample_var + eps) + 2.0 * dsigma * (x - sample_mean) &#x2F; N + dmu &#x2F; N</span><br><span class="line"></span><br><span class="line">  return dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h2 id="DarkNet代码详解"><a href="#DarkNet代码详解" class="headerlink" title="DarkNet代码详解"></a>DarkNet代码详解</h2><h3 id="1-构造BN层"><a href="#1-构造BN层" class="headerlink" title="1. 构造BN层"></a>1. 构造BN层</h3><p>构造BN层的代码在<code>src/batchnorm_layer.c</code>中实现，详细代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">layer make_batchnorm_layer(int batch, int w, int h, int c, int train)</span><br><span class="line">&#123;</span><br><span class="line">    fprintf(stderr, &quot;Batch Normalization Layer: %d x %d x %d image\n&quot;, w,h,c);</span><br><span class="line">    layer layer &#x3D; &#123; (LAYER_TYPE)0 &#125;;</span><br><span class="line">    layer.type &#x3D; BATCHNORM; &#x2F;&#x2F; 网络层的名字</span><br><span class="line">    layer.batch &#x3D; batch; &#x2F;&#x2F;一个batch中包含的图片数</span><br><span class="line">    layer.train &#x3D; train;</span><br><span class="line">    layer.h &#x3D; layer.out_h &#x3D; h;  &#x2F;&#x2F; 当前层的输出高度等于输入高度h</span><br><span class="line">    layer.w &#x3D; layer.out_w &#x3D; w; &#x2F;&#x2F; 当前层的输出宽度等于输入宽度w</span><br><span class="line">    layer.c &#x3D; layer.out_c &#x3D; c; &#x2F;&#x2F; 当前层的输出通道数等于输入通道数</span><br><span class="line"></span><br><span class="line">    layer.n &#x3D; layer.c;</span><br><span class="line">    layer.output &#x3D; (float*)xcalloc(h * w * c * batch, sizeof(float)); &#x2F;&#x2F; layer.output为该层所有的输出（包括mini-batch所有输入图片的输出）</span><br><span class="line">    layer.delta &#x3D; (float*)xcalloc(h * w * c * batch, sizeof(float)); &#x2F;&#x2F;layer.delta 是该层的敏感度图，和输出的维度想同</span><br><span class="line">    layer.inputs &#x3D; w*h*c; &#x2F;&#x2F;mini-batch中每张输入图片的像素元素个数</span><br><span class="line">    layer.outputs &#x3D; layer.inputs; &#x2F;&#x2F; 对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即layer.out_c张特征图）</span><br><span class="line"></span><br><span class="line">    layer.biases &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; BN层特有参数，缩放系数</span><br><span class="line">    layer.bias_updates &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 缩放系数的敏感度图</span><br><span class="line"></span><br><span class="line">    layer.scales &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; BN层特有参数，偏置系数</span><br><span class="line">    layer.scale_updates &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 偏置系数的敏感度图</span><br><span class="line">    int i;</span><br><span class="line">    for(i &#x3D; 0; i &lt; c; ++i)&#123;</span><br><span class="line">        layer.scales[i] &#x3D; 1; &#x2F;&#x2F; 将缩放系数初始化为1</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    layer.mean &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; mean 一个batch中所有图片的均值，分通道求取</span><br><span class="line">    layer.variance &#x3D; (float*)xcalloc(c, sizeof(float));  &#x2F;&#x2F; variance 一个batch中所有图片的方差，分通道求取</span><br><span class="line"></span><br><span class="line">    layer.rolling_mean &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 均值的滑动平均，影子变量</span><br><span class="line">    layer.rolling_variance &#x3D; (float*)xcalloc(c, sizeof(float)); &#x2F;&#x2F; 方差的滑动平均，影子变量</span><br><span class="line"></span><br><span class="line">    layer.forward &#x3D; forward_batchnorm_layer; &#x2F;&#x2F; 前向传播函数</span><br><span class="line">    layer.backward &#x3D; backward_batchnorm_layer; &#x2F;&#x2F; 反向传播函数</span><br><span class="line">    layer.update &#x3D; update_batchnorm_layer;</span><br><span class="line">    ...</span><br><span class="line">    return layer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-前向传播公式实现"><a href="#2-前向传播公式实现" class="headerlink" title="2.前向传播公式实现"></a>2.前向传播公式实现</h3><p>DarkNet中在<code>src/blas.h</code>中实现了前向传播的几个公式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">** 计算输入数据x的平均值，输出的mean是一个矢量，比如如果x是多张三通道的图片，那么mean的维度就为通道3</span><br><span class="line">** 由于每次训练输入的都是一个batch的图片，因此最终会输出batch张三通道的图片，mean中的第一个元素就是第</span><br><span class="line">** 一个通道上全部batch张输出特征图所有元素的平均值，本函数的用处之一就是batch normalization的第一步了</span><br><span class="line">** x: 包含所有数据，比如l.output，其包含的元素个数为l.batch*l.outputs</span><br><span class="line">** batch: 一个batch中包含的图片张数，即l.batch</span><br><span class="line">** filters: 该层神经网络的滤波器个数，也即该层网络输出图片的通道数（比如对卷积网络来说，就是核的个数了）</span><br><span class="line">** spatial: 该层神经网络每张输出特征图的尺寸，也即等于l.out_w*l.out_h</span><br><span class="line">** mean: 求得的平均值，维度为filters，也即每个滤波器对应有一个均值（每个滤波器会处理所有图片）</span><br><span class="line">** x的内存排布？此处还是结合batchnorm_layer.c中的forward_batch_norm_layer()函数的调用来解释，其中x为l.output，其包含的元素个数为l</span><br><span class="line">** 有l.batch行，每行有l.out_c*l.out_w*l.out_h个元素，每一行又可以分成l.out_c行，l.out_w*l.out_h列，</span><br><span class="line">** 那么l.mean中的每一个元素，是某一个通道上所有batch的输出的平均值</span><br><span class="line">** （比如卷积层，有3个核，那么输出通道有3个，每张输入图片都会输出3张特征图，可以理解每张输出图片是3通道的，</span><br><span class="line">** 若每次输入batch&#x3D;64张图片，那么将会输出64张3通道的图片，而mean中的每个元素就是某个通道上所有64张图片</span><br><span class="line">** 所有元素的平均值，比如第1个通道上，所有64张图片像素平均值）</span><br><span class="line">*&#x2F;</span><br><span class="line">void mean_cpu(float *x, int batch, int filters, int spatial, float *mean)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; scale即是均值中的分母项</span><br><span class="line">    float scale &#x3D; 1.&#x2F;(batch * spatial);</span><br><span class="line">    int i,j,k;</span><br><span class="line">    &#x2F;&#x2F; 外循环次数为filters，也即mean的维度，每次循环将得到一个平均值</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        mean[i] &#x3D; 0;</span><br><span class="line">        &#x2F;&#x2F; 中间循环次数为batch，也即叠加每张输入图片对应的某一通道上的输出</span><br><span class="line">        for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">            &#x2F;&#x2F; 内层循环即叠加一张输出特征图的所有像素值</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                &#x2F;&#x2F; 计算偏移</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                mean[i] +&#x3D; x[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mean[i] *&#x3D; scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">** 计算输入x中每个元素的方差</span><br><span class="line">** 本函数的主要用处应该就是batch normalization的第二步了</span><br><span class="line">** x: 包含所有数据，比如l.output，其包含的元素个数为l.batch*l.outputs</span><br><span class="line">** batch: 一个batch中包含的图片张数，即l.batch</span><br><span class="line">** filters: 该层神经网络的滤波器个数，也即是该网络层输出图片的通道数</span><br><span class="line">** spatial: 该层神经网络每张特征图的尺寸，也即等于l.out_w*l.out_h</span><br><span class="line">** mean: 求得的平均值，维度为filters，也即每个滤波器对应有一个均值（每个滤波器会处理所有图片）</span><br><span class="line">*&#x2F;</span><br><span class="line">void variance_cpu(float *x, float *mean, int batch, int filters, int spatial, float *variance)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; 这里计算方差分母要减去1的原因是无偏估计，可以看：https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20983193</span><br><span class="line">    &#x2F;&#x2F; 事实上，在统计学中，往往采用的方差计算公式都会让分母减1,这时因为所有数据的方差是基于均值这个固定点来计算的，</span><br><span class="line">    &#x2F;&#x2F; 对于有n个数据的样本，在均值固定的情况下，其采样自由度为n-1（只要n-1个数据固定，第n个可以由均值推出）</span><br><span class="line">    float scale &#x3D; 1.&#x2F;(batch * spatial - 1);</span><br><span class="line">    int i,j,k;</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        variance[i] &#x3D; 0;</span><br><span class="line">        for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                &#x2F;&#x2F; 每个元素减去均值求平方</span><br><span class="line">                variance[i] +&#x3D; pow((x[index] - mean[i]), 2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        variance[i] *&#x3D; scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">void normalize_cpu(float *x, float *mean, float *variance, int batch, int filters, int spatial)</span><br><span class="line">&#123;</span><br><span class="line">    int b, f, i;</span><br><span class="line">    for(b &#x3D; 0; b &lt; batch; ++b)&#123;</span><br><span class="line">        for(f &#x3D; 0; f &lt; filters; ++f)&#123;</span><br><span class="line">            for(i &#x3D; 0; i &lt; spatial; ++i)&#123;</span><br><span class="line">                int index &#x3D; b*filters*spatial + f*spatial + i;</span><br><span class="line">                x[index] &#x3D; (x[index] - mean[f])&#x2F;(sqrt(variance[f]) + .000001f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;*</span><br><span class="line">** axpy 是线性代数中的一种基本操作(仿射变换)完成y&#x3D; alpha*x + y操作，其中x,y为矢量，alpha为实数系数，</span><br><span class="line">** 请看: https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;e3f386771c51</span><br><span class="line">** N: X中包含的有效元素个数</span><br><span class="line">** ALPHA: 系数alpha</span><br><span class="line">** X: 参与运算的矢量X</span><br><span class="line">** INCX: 步长(倍数步长)，即x中凡是INCX倍数编号的参与运算</span><br><span class="line">** Y: 参与运算的矢量，也相当于是输出</span><br><span class="line">*&#x2F;</span><br><span class="line">void axpy_cpu(int N, float ALPHA, float *X, int INCX, float *Y, int INCY)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">    for(i &#x3D; 0; i &lt; N; ++i) Y[i*INCY] +&#x3D; ALPHA*X[i*INCX];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void scal_cpu(int N, float ALPHA, float *X, int INCX)</span><br><span class="line">&#123;</span><br><span class="line">    int i;</span><br><span class="line">    for(i &#x3D; 0; i &lt; N; ++i) X[i*INCX] *&#x3D; ALPHA;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-前向传播和反向传播接口函数"><a href="#3-前向传播和反向传播接口函数" class="headerlink" title="3. 前向传播和反向传播接口函数"></a>3. 前向传播和反向传播接口函数</h3><p>DarkNet在<code>src/batchnorm_layer.c</code>中实现了前向传播和反向传播的接口函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; BN层的前向传播函数</span><br><span class="line">void forward_batchnorm_layer(layer l, network net)</span><br><span class="line">&#123;</span><br><span class="line">    if(l.type &#x3D;&#x3D; BATCHNORM) copy_cpu(l.outputs*l.batch, net.input, 1, l.output, 1);</span><br><span class="line">    copy_cpu(l.outputs*l.batch, l.output, 1, l.x, 1);</span><br><span class="line">    &#x2F;&#x2F; 训练阶段</span><br><span class="line">    if(net.train)&#123;</span><br><span class="line">        &#x2F;&#x2F; blas.c中有详细注释，计算输入数据的均值，保存为l.mean</span><br><span class="line">        mean_cpu(l.output, l.batch, l.out_c, l.out_h*l.out_w, l.mean);</span><br><span class="line">        &#x2F;&#x2F; blas.c中有详细注释，计算输入数据的方差，保存为l.variance</span><br><span class="line">        variance_cpu(l.output, l.mean, l.batch, l.out_c, l.out_h*l.out_w, l.variance);</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 计算滑动平均和方差，影子变量，可以参考https:&#x2F;&#x2F;blog.csdn.net&#x2F;just_sort&#x2F;article&#x2F;details&#x2F;100039418</span><br><span class="line">        scal_cpu(l.out_c, .99, l.rolling_mean, 1);</span><br><span class="line">        axpy_cpu(l.out_c, .01, l.mean, 1, l.rolling_mean, 1);</span><br><span class="line">        scal_cpu(l.out_c, .99, l.rolling_variance, 1);</span><br><span class="line">        axpy_cpu(l.out_c, .01, l.variance, 1, l.rolling_variance, 1);</span><br><span class="line">        &#x2F;&#x2F; 减去均值，除以方差得到x^，论文中的第3个公式</span><br><span class="line">        normalize_cpu(l.output, l.mean, l.variance, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">        &#x2F;&#x2F; BN层的输出</span><br><span class="line">        copy_cpu(l.outputs*l.batch, l.output, 1, l.x_norm, 1);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        &#x2F;&#x2F; 测试阶段，直接用滑动变量来计算输出</span><br><span class="line">        normalize_cpu(l.output, l.rolling_mean, l.rolling_variance, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 最后一个公式，对输出进行移位和偏置</span><br><span class="line">    scale_bias(l.output, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    add_bias(l.output, l.biases, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; BN层的反向传播函数</span><br><span class="line">void backward_batchnorm_layer(layer l, network net)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; 如果在测试阶段，均值和方差都可以直接用滑动变量来赋值</span><br><span class="line">    if(!net.train)&#123;</span><br><span class="line">        l.mean &#x3D; l.rolling_mean;</span><br><span class="line">        l.variance &#x3D; l.rolling_variance;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 在卷积层中定义了backward_bias，并有详细注释</span><br><span class="line">    backward_bias(l.bias_updates, l.delta, l.batch, l.out_c, l.out_w*l.out_h);</span><br><span class="line">    &#x2F;&#x2F; 这里是对论文中最后一个公式的缩放系数求梯度更新值</span><br><span class="line">    backward_scale_cpu(l.x_norm, l.delta, l.batch, l.out_c, l.out_w*l.out_h, l.scale_updates);</span><br><span class="line">    &#x2F;&#x2F; 也是在convlution_layer.c中定义的函数，先将敏感度图乘以l.scales</span><br><span class="line">    scale_bias(l.delta, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; 对应了https:&#x2F;&#x2F;blog.csdn.net&#x2F;just_sort&#x2F;article&#x2F;details&#x2F;100039418 中对均值求导数</span><br><span class="line">    mean_delta_cpu(l.delta, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.mean_delta);</span><br><span class="line">    &#x2F;&#x2F; 对应了https:&#x2F;&#x2F;blog.csdn.net&#x2F;just_sort&#x2F;article&#x2F;details&#x2F;100039418 中对方差求导数</span><br><span class="line">    variance_delta_cpu(l.x, l.delta, l.mean, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.variance_delta);</span><br><span class="line">    &#x2F;&#x2F; 计算敏感度图，对应了论文中的最后一部分</span><br><span class="line">    normalize_delta_cpu(l.x, l.mean, l.variance, l.mean_delta, l.variance_delta, l.batch, l.out_c, l.out_w*l.out_h, l.delta);</span><br><span class="line">    if(l.type &#x3D;&#x3D; BATCHNORM) copy_cpu(l.outputs*l.batch, l.delta, 1, net.delta, 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-反向传播函数公式实现"><a href="#4-反向传播函数公式实现" class="headerlink" title="4.反向传播函数公式实现"></a>4.反向传播函数公式实现</h3><p>其中反向传播的函数如下，就是利用推导出的公式来计算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 这里是对论文中最后一个公式的缩放系数求梯度更新值</span><br><span class="line">&#x2F;&#x2F; x_norm 代表BN层前向传播的输出值</span><br><span class="line">&#x2F;&#x2F; delta 代表上一层的梯度图</span><br><span class="line">&#x2F;&#x2F; batch 为l.batch，即一个batch的图片数</span><br><span class="line">&#x2F;&#x2F; n代表输出通道数，也即是输入通道数</span><br><span class="line">&#x2F;&#x2F; size 代表w * h</span><br><span class="line">&#x2F;&#x2F; scale_updates 代表scale的梯度更新值</span><br><span class="line">&#x2F;&#x2F; y &#x3D; gamma * x + beta</span><br><span class="line">&#x2F;&#x2F; dy &#x2F; d(gamma) &#x3D; x</span><br><span class="line">void backward_scale_cpu(float *x_norm, float *delta, int batch, int n, int size, float *scale_updates)</span><br><span class="line">&#123;</span><br><span class="line">    int i,b,f;</span><br><span class="line">    for(f &#x3D; 0; f &lt; n; ++f)&#123;</span><br><span class="line">        float sum &#x3D; 0;</span><br><span class="line">        for(b &#x3D; 0; b &lt; batch; ++b)&#123;</span><br><span class="line">            for(i &#x3D; 0; i &lt; size; ++i)&#123;</span><br><span class="line">                int index &#x3D; i + size*(f + n*b);</span><br><span class="line">                sum +&#x3D; delta[index] * x_norm[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        scale_updates[f] +&#x3D; sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 对均值求导</span><br><span class="line">&#x2F;&#x2F; 对应了论文中的求导公式3，不过Darknet特殊的点在于是先计算均值的梯度</span><br><span class="line">&#x2F;&#x2F; 这个时候方差是没有梯度的，所以公式3的后半部分为0，也就只保留了公式3的前半部分</span><br><span class="line">&#x2F;&#x2F; 不过我从理论上无法解释这种操作会带来什么影响，但从目标检测来看应该是没有影响的</span><br><span class="line">void mean_delta_cpu(float *delta, float *variance, int batch, int filters, int spatial, float *mean_delta)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    int i,j,k;</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        mean_delta[i] &#x3D; 0;</span><br><span class="line">        for (j &#x3D; 0; j &lt; batch; ++j) &#123;</span><br><span class="line">            for (k &#x3D; 0; k &lt; spatial; ++k) &#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                mean_delta[i] +&#x3D; delta[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mean_delta[i] *&#x3D; (-1.&#x2F;sqrt(variance[i] + .00001f));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 对方差求导</span><br><span class="line">&#x2F;&#x2F; 对应了论文中的求导公式2</span><br><span class="line">void  variance_delta_cpu(float *x, float *delta, float *mean, float *variance, int batch, int filters, int spatial, float *variance_delta)</span><br><span class="line">&#123;</span><br><span class="line">    int i,j,k;</span><br><span class="line">    for(i &#x3D; 0; i &lt; filters; ++i)&#123;</span><br><span class="line">        variance_delta[i] &#x3D; 0;</span><br><span class="line">        for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + i*spatial + k;</span><br><span class="line">                variance_delta[i] +&#x3D; delta[index]*(x[index] - mean[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        variance_delta[i] *&#x3D; -.5 * pow(variance[i] + .00001f, (float)(-3.&#x2F;2.));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 求出BN层的梯度敏感度图</span><br><span class="line">&#x2F;&#x2F; 对应了论文中的求导公式4，即是对x_i求导</span><br><span class="line">void normalize_delta_cpu(float *x, float *mean, float *variance, float *mean_delta, float *variance_delta, int batch, int filters, int spatial, float *delta)</span><br><span class="line">&#123;</span><br><span class="line">    int f, j, k;</span><br><span class="line">    for(j &#x3D; 0; j &lt; batch; ++j)&#123;</span><br><span class="line">        for(f &#x3D; 0; f &lt; filters; ++f)&#123;</span><br><span class="line">            for(k &#x3D; 0; k &lt; spatial; ++k)&#123;</span><br><span class="line">                int index &#x3D; j*filters*spatial + f*spatial + k;</span><br><span class="line">                delta[index] &#x3D; delta[index] * 1.&#x2F;(sqrt(variance[f] + .00001f)) + variance_delta[f] * 2. * (x[index] - mean[f]) &#x2F; (spatial * batch) + mean_delta[f]&#x2F;(spatial*batch);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

    </div>

    
    
    <div class="post-widgets">
      <div id="needsharebutton-postbottom">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    </div>
	<div>

	  

		<div>

    

        <div >-------------本文结束感谢您的阅读-------------</div>

    

</div>

	  

	</div>
      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag"><i class="fa fa-tag"></i> 目标检测</a>
              <a href="/tags/YOLOv3/" rel="tag"><i class="fa fa-tag"></i> YOLOv3</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/29/SSD/" rel="prev" title="SSD">
      <i class="fa fa-chevron-left"></i> SSD
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/03/01/YOLOv2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/" rel="next" title="YOLOv2损失函数详解">
      YOLOv2损失函数详解 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>



<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#BatchNorm原理"><span class="nav-text"> BatchNorm原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播推导"><span class="nav-text">前向传播推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播推导"><span class="nav-text">反向传播推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DarkNet代码详解"><span class="nav-text">DarkNet代码详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-构造BN层"><span class="nav-text">1. 构造BN层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-前向传播公式实现"><span class="nav-text">2.前向传播公式实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-前向传播和反向传播接口函数"><span class="nav-text">3. 前向传播和反向传播接口函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-反向传播函数公式实现"><span class="nav-text">4.反向传播函数公式实现</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->
	  
     
	  
      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Qiyuan-Z"
      src="/images/wallhaven-915.png">
  <p class="site-author-name" itemprop="name">Qiyuan-Z</p>
  <div class="site-description" itemprop="description">偉大な魂は目的を持ち、そうでないものは願望を持つ</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">107</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Qiyuan-Z" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Qiyuan-Z" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:601872068@qq.com" title="E-Mail → mailto:601872068@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://project-inkstone.github.io/project-inkstone/?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;project-inkstone.github.io&#x2F;project-inkstone&#x2F;?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">project-inkstone</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.jngwl.top/" title="https:&#x2F;&#x2F;www.jngwl.top" rel="noopener" target="_blank">清风与归</a>
        </li>
    </ul>
  </div>

      </div>
	  
      <div id="music163player">
		   <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=29784463&auto=1&height=66"></iframe>
		   </iframe>
	  </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qiyuan-Z</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">571k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:39</span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span> 
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("12/01/2019 13:14:21");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













	<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
	<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
	<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
	<script type="text/javascript">
    		var gitalk = new Gitalk({
		        clientID: '2d10cfb27783db577e70',
		        clientSecret: '154292876bb14966f6ae57304b67859617b08c94',
		        id: md5(location.pathname),
		        repo: 'gitalk',
		        owner: 'Qiyuan-Z',
		        admin: 'Qiyuan-Z',
			distractionFreeMode: '',

		    });
	    gitalk.render('gitalk-container');
	</script>



  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js"></script>
  <script>
      pbOptions = {};
        pbOptions.iconStyle = "box";
        pbOptions.boxForm = "horizontal";
        pbOptions.position = "bottomCenter";
        pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      new needShareButton('#needsharebutton-postbottom', pbOptions);
  </script>
	<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
	<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
	<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
	<script type="text/javascript">
    		var gitalk = new Gitalk({
		        clientID: '2d10cfb27783db577e70',
		        clientSecret: '154292876bb14966f6ae57304b67859617b08c94',
		        id: md5(location.pathname),
		        repo: 'gitalk',
		        owner: 'Qiyuan-Z',
		        admin: 'Qiyuan-Z',
			distractionFreeMode: '',

		    });
	    gitalk.render('gitalk-container');
	</script>


  <script type="text/javascript" src="/js/src/clicklove.js"></script>
</body>
</html>

