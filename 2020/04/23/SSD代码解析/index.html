<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>SSD代码解析 | Yuan</title><meta name="keywords" content="目标检测"><meta name="author" content="Qiyuan-Z"><meta name="copyright" content="Qiyuan-Z"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="article"><meta property="og:title" content="SSD代码解析"><meta property="og:url" content="https://qiyuan-z.github.io/2020/04/23/SSD%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/index.html"><meta property="og:site_name" content="Yuan"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://qiyuan-z.github.io/img/1097380.jpg"><meta property="article:published_time" content="2020-04-23T01:53:33.255Z"><meta property="article:modified_time" content="2020-04-24T02:26:34.191Z"><meta property="article:author" content="Qiyuan-Z"><meta property="article:tag" content="目标检测"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://qiyuan-z.github.io/img/1097380.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qiyuan-z.github.io/2020/04/23/SSD%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:200,languages:{author:"作者: Qiyuan-Z",link:"链接: ",source:"来源: Yuan",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"bottom-right"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2020-04-24 10:26:34"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));const n=saveToLocal.get("global-font-size");void 0!==n&&document.documentElement.style.setProperty("--global-font-size",n+"px")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">116</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">34</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yuan</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">SSD代码解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-04-23T01:53:33.255Z" title="发表于 2020-04-23 09:53:33">2020-04-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-04-24T02:26:34.191Z" title="更新于 2020-04-24 10:26:34">2020-04-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><span id="more"></span>前言</h2><p>本篇文章是在SSD算法原理解析的基础上做的代码解析，今天要解析的SSD源码来自于github一个非常火的Pytorch实现，已经有3K+星，地址为：<a target="_blank" rel="noopener" href="https://github.com/amdegroot/ssd.pytorch/">https://github.com/amdegroot/ssd.pytorch/</a></p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>为了比较好的对应SSD的结构来看代码，我们首先放出SSD的网络结构，如下图所示：</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ssd%20code/640.webp" alt></p><p>可以看到原始的SSD网络是以VGG-16作Backbone（骨干网络）的。为了更加清晰看到相比于VGG16，SSD的网络使用了哪些变化，带有特征图维度信息的更清晰的骨干网络和VGG16的对比图如下：</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ssd%20code/641.webp" alt></p><h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><p>OK，现在我们就要开始从源码剖析SSD了 。主要弄清楚三个方面，网络结构的搭建，Anchor还有损失函数，就算是理解这个源码了。</p><h3 id="网络搭建"><a href="#网络搭建" class="headerlink" title="网络搭建"></a>网络搭建</h3><p>从上面的图中我们可以清晰的看到在以VGG16做骨干网络时，在conv5后丢弃了VGG16中的全连接层改为了1024 x 3 x 3和1024 x 1 x 1 的卷积层。其中<code>conv4-1</code>卷积层前面的<code>maxpooling</code>层的<code>ceil_model=True</code>，使得输出特征图长宽为38 x 38。还有<code>conv5-3</code>后面的一层<code>maxpooling</code>层参数为$(\text {kernelsize}=3, \text {stride}=1, \text {padding}=1)$，不进行下采样。然后在<code>fc7</code>后面接上多尺度提取的另外4个卷积层就构成了完整的SSD网络。这里VGG16修改后的代码如下，来自ssd.py：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def vgg(cfg, i, batch_norm=False):</span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = i</span><br><span class="line">    for v in cfg:</span><br><span class="line">        if v == &#x27;M&#x27;:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]</span><br><span class="line">        elif v == &#x27;C&#x27;:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]</span><br><span class="line">        else:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)</span><br><span class="line">            if batch_norm:</span><br><span class="line">                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]</span><br><span class="line">            else:</span><br><span class="line">                layers += [conv2d, nn.ReLU(inplace=True)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)</span><br><span class="line">    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)</span><br><span class="line">    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)</span><br><span class="line">    layers += [pool5, conv6,</span><br><span class="line">               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]</span><br><span class="line">    return layers</span><br></pre></td></tr></table></figure><p>可以看到和我们上面的那张图是完全一致的。代码里面最后获得的<code>conv7</code>就是我们上面图里面的<code>fc7</code>，特征维度是：$[\text { None }, 1024,19,19]$。现在可以开始搭建SSD网络后面的多尺度提取网络了。也就是网络结构图中的Extra Feature Layers。我们从开篇的结构图中截取一下这一部分，方便我们对照代码。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ssd%20code/642.webp" alt></p><p>实现的代码如下（同样来自ssd.py）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def add_extras(cfg, i, batch_norm=False):</span><br><span class="line">    # Extra layers added to VGG for feature scaling</span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = i</span><br><span class="line">    flag = False #flag 用来控制 kernel_size= 1 or 3</span><br><span class="line">    for k, v in enumerate(cfg):</span><br><span class="line">        if in_channels != &#x27;S&#x27;:</span><br><span class="line">            if v == &#x27;S&#x27;:</span><br><span class="line">                layers += [nn.Conv2d(in_channels, cfg[k + 1],</span><br><span class="line">                           kernel_size=(1, 3)[flag], stride=2, padding=1)]</span><br><span class="line">            else:</span><br><span class="line">                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]</span><br><span class="line">            flag = not flag</span><br><span class="line">        in_channels = v</span><br><span class="line">return layers</span><br></pre></td></tr></table></figure><p>可以看到网络结构中除了魔改后的VGG16和Extra Layers还有6个横着的线，这代表的是对6个尺度的特征图进行卷积获得预测框的回归(loc)和类别(cls)信息，注意SSD将背景也看成类别了，所以对于VOC数据集类别数就是20+1=21。这部分的代码为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def multibox(vgg, extra_layers, cfg, num_classes):</span><br><span class="line">    loc_layers = []#多尺度分支的回归网络</span><br><span class="line">    conf_layers = []#多尺度分支的分类网络</span><br><span class="line">    # 第一部分，vgg 网络的 Conv2d-4_3(21层)， Conv2d-7_1(-2层)</span><br><span class="line">    vgg_source = [21, -2]</span><br><span class="line">    for k, v in enumerate(vgg_source):</span><br><span class="line">        # 回归 box*4(坐标)</span><br><span class="line">        loc_layers += [nn.Conv2d(vgg[v].out_channels,</span><br><span class="line">                                 cfg[k] * 4, kernel_size=3, padding=1)]</span><br><span class="line">        # 置信度 box*(num_classes)</span><br><span class="line">        conf_layers += [nn.Conv2d(vgg[v].out_channels,</span><br><span class="line">                        cfg[k] * num_classes, kernel_size=3, padding=1)]</span><br><span class="line">    # 第二部分，cfg从第三个开始作为box的个数，而且用于多尺度提取的网络分别为1,3,5,7层</span><br><span class="line">    for k, v in enumerate(extra_layers[1::2], 2):</span><br><span class="line">        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]</span><br><span class="line">                                 * 4, kernel_size=3, padding=1)]</span><br><span class="line">        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]</span><br><span class="line">                                  * num_classes, kernel_size=3, padding=1)]</span><br><span class="line">    return vgg, extra_layers, (loc_layers, conf_layers)</span><br><span class="line"># 用下面的测试代码测试一下</span><br><span class="line">if __name__  == &quot;__main__&quot;:</span><br><span class="line">    vgg, extra_layers, (l, c) = multibox(vgg(base[&#x27;300&#x27;], 3),</span><br><span class="line">                                         add_extras(extras[&#x27;300&#x27;], 1024),</span><br><span class="line">                                         [4, 6, 6, 6, 4, 4], 21)</span><br><span class="line">    print(nn.Sequential(*l))</span><br><span class="line">    print(&#x27;---------------------------&#x27;)</span><br><span class="line">    print(nn.Sequential(*c))</span><br></pre></td></tr></table></figure><p>在jupter notebook输出信息为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">loc layers:</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">Sequential(</span><br><span class="line">  (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">)</span><br><span class="line">---------------------------</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">conf layers:</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">Sequential(</span><br><span class="line">  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Anchor生成-Prior-Box层"><a href="#Anchor生成-Prior-Box层" class="headerlink" title="Anchor生成(Prior_Box层)"></a>Anchor生成(Prior_Box层)</h3><p>这个在前面SSD的原理篇中讲过了，这里不妨再回忆一下，SSD从魔改后的VGG16的<code>conv4_3</code>开始一共使用了6个不同大小的特征图，大小分别为<code>(38,28),(19,19),(10,10),(5,5),(3,3),(1,1)</code>，但每个特征图上设置的先验框(Anchor)的数量不同。先验框的设置包含尺度和长宽比两个方面。对于先验框的设置，公式如下： $s_{k}=s_{\min }+\frac{s_{m a x}-s_{\min }}{m-1}(k-1), k \in[1, m]$，其中$m$指的是特征图个数，这里为5，因为第一层<code>conv4_3</code>的Anchor是单独设置的，$s_{k}$代表先验框大小相对于原图的比例。最后，$s_{min}$和$s_{max}$表示比例的最小值和最大值，论文中分别取0.2和0.9。对于第一个特征图，它的先验框尺度比例设置为$s_{min}/2=0.1$，则他的尺度为300 x 0.1 = 30，后面的特征图带入公式计算，计算时$\frac{s_{m a x}-s_{\min }}{m-1}$保留两位小数为0.17，比如300 x (0.17 x 0 + 0.2) = 60，300 x (0.17 x 1 + 0.2) = 111，所以剩下的5个特征图的min_size尺度$s_{k}$为60，111，162，213，264。综合起来，6个特征图的min_size尺度$s_{k}$为30，60，111，162，213，264。有了Anchor的尺度，接下来设置Anchor的长宽，论文中长宽设置一般为$a_{r}=1,2,3, \frac{1}{2}, \frac{1}{3}$，根据面积和长宽比可以得到先验框的宽度和高度：$w_{k}^{a}=s_{k} \sqrt{a_{r}}, h_{k}^{a}=s_{k} / \sqrt{a_{r}}$ 。这里有一些值得注意的点，如下：</p><ul><li><p>上面的$s_{k}$是相对于原图的大小。</p></li><li><p>默认情况下，每个特征图除了上面5个比例的Anchor，还会设置一个尺度为$s_{k}^{\prime}=\sqrt{s_{k} s_{k+1}}$且$a_{r}=1$的先验框，这样每个特征图都设置了两个长宽比为1但大小不同的正方形先验框。最后一个特征图需要参考一个虚拟$s_{k+1}=300\times(0.17\times5 + 0.2) = 315$所以综合起来，6个特征图的max_size尺度$s_{k}$为60，111，162，213，264，315。</p></li><li><p>在实现<code>conv4_3</code>,<code>conv10_2</code>,<code>conv11_2</code>层时仅使用4个先验框，不使用长宽比为$3, \frac{1}{3}$的Anchor。</p></li><li><p>每个单元的先验框中心点分布在每个单元的中心，即：$\left[\frac{i+0.5}{\left|f_{k}\right|}, \frac{j+0.5}{\left|f_{k}\right|}\right], i, j \in\left[0,\left|f_{k}\right|\right]$ ，其中$f_{k}$是特征图的大小。</p></li></ul><p>从Anchor的值来看，越前面的特征图Anchor的尺寸越小，也就是说对小目标的效果越好。先验框的总数为<code>num_priors = 38x38x4+19x19x6+10x10x6+5x5x6+3x3x4+1x1x4=8732</code>。</p><p>生成先验框的代码如下（来自layers/functions/prior_box.py）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">class PriorBox(object):</span><br><span class="line">    &quot;&quot;&quot;Compute priorbox coordinates in center-offset form for each source</span><br><span class="line">    feature map.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, cfg):</span><br><span class="line">        super(PriorBox, self).__init__()</span><br><span class="line">        self.image_size = cfg[&#x27;min_dim&#x27;]</span><br><span class="line">        # number of priors for feature map location (either 4 or 6)</span><br><span class="line">        self.num_priors = len(cfg[&#x27;aspect_ratios&#x27;])</span><br><span class="line">        self.variance = cfg[&#x27;variance&#x27;] or [0.1]</span><br><span class="line">        self.feature_maps = cfg[&#x27;feature_maps&#x27;]</span><br><span class="line">        self.min_sizes = cfg[&#x27;min_sizes&#x27;]</span><br><span class="line">        self.max_sizes = cfg[&#x27;max_sizes&#x27;]</span><br><span class="line">        self.steps = cfg[&#x27;steps&#x27;]</span><br><span class="line">        self.aspect_ratios = cfg[&#x27;aspect_ratios&#x27;]</span><br><span class="line">        self.clip = cfg[&#x27;clip&#x27;]</span><br><span class="line">        self.version = cfg[&#x27;name&#x27;]</span><br><span class="line">        for v in self.variance:</span><br><span class="line">            if v &lt;= 0:</span><br><span class="line">                raise ValueError(&#x27;Variances must be greater than 0&#x27;)</span><br><span class="line"></span><br><span class="line">    def forward(self):</span><br><span class="line">        mean = []</span><br><span class="line">        # 遍历多尺度的 特征图: [38, 19, 10, 5, 3, 1]</span><br><span class="line">        for k, f in enumerate(self.feature_maps):</span><br><span class="line">            # 遍历每个像素</span><br><span class="line">            for i, j in product(range(f), repeat=2):</span><br><span class="line">                # k-th 层的feature map 大小</span><br><span class="line">                f_k = self.image_size / self.steps[k]</span><br><span class="line">                # # 每个框的中心坐标</span><br><span class="line">                cx = (j + 0.5) / f_k</span><br><span class="line">                cy = (i + 0.5) / f_k</span><br><span class="line"></span><br><span class="line">                # aspect_ratio: 1 当 ratio==1的时候，会产生两个 box</span><br><span class="line">                # r==1, size = s_k， 正方形</span><br><span class="line">                s_k = self.min_sizes[k]/self.image_size</span><br><span class="line">                mean += [cx, cy, s_k, s_k]</span><br><span class="line"></span><br><span class="line">                # r==1, size = sqrt(s_k * s_(k+1)), 正方形</span><br><span class="line">                # rel size: sqrt(s_k * s_(k+1))</span><br><span class="line">                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))</span><br><span class="line">                mean += [cx, cy, s_k_prime, s_k_prime]</span><br><span class="line"></span><br><span class="line">                # 当 ratio != 1 的时候，产生的box为矩形</span><br><span class="line">                for ar in self.aspect_ratios[k]:</span><br><span class="line">                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]</span><br><span class="line">                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]</span><br><span class="line">        # 转化为 torch的Tensor</span><br><span class="line">        output = torch.Tensor(mean).view(-1, 4)</span><br><span class="line">        #归一化，把输出设置在 [0,1]</span><br><span class="line">        if self.clip:</span><br><span class="line">            output.clamp_(max=1, min=0)</span><br><span class="line">return output</span><br></pre></td></tr></table></figure><h3 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h3><p>结合了前面介绍的魔改后的VGG16，还有Extra Layers，还有生成Anchor的Priobox策略，我们可以写出SSD的整体结构如下（代码在ssd.py）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">class SSD(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Single Shot Multibox Architecture</span><br><span class="line">    The network is composed of a base VGG network followed by the</span><br><span class="line">    added multibox conv layers.  Each multibox layer branches into</span><br><span class="line">        1) conv2d for class conf scores</span><br><span class="line">        2) conv2d for localization predictions</span><br><span class="line">        3) associated priorbox layer to produce default bounding</span><br><span class="line">           boxes specific to the layer&#x27;s feature map size.</span><br><span class="line">    See: https://arxiv.org/pdf/1512.02325.pdf for more details.</span><br><span class="line">    Args:</span><br><span class="line">        phase: (string) Can be &quot;test&quot; or &quot;train&quot;</span><br><span class="line">        size: input image size</span><br><span class="line">        base: VGG16 layers for input, size of either 300 or 500</span><br><span class="line">        extras: extra layers that feed to multibox loc and conf layers</span><br><span class="line">        head: &quot;multibox head&quot; consists of loc and conf conv layers</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, phase, size, base, extras, head, num_classes):</span><br><span class="line">        super(SSD, self).__init__()</span><br><span class="line">        self.phase = phase</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        # 配置config</span><br><span class="line">        self.cfg = (coco, voc)[num_classes == 21]</span><br><span class="line">        # 初始化先验框</span><br><span class="line">        self.priorbox = PriorBox(self.cfg)</span><br><span class="line">        self.priors = Variable(self.priorbox.forward(), volatile=True)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">        # SSD network</span><br><span class="line">        # backbone网络</span><br><span class="line">        self.vgg = nn.ModuleList(base)</span><br><span class="line">        # Layer learns to scale the l2 normalized features from conv4_3</span><br><span class="line">        # conv4_3后面的网络，L2 正则化</span><br><span class="line">        self.L2Norm = L2Norm(512, 20)</span><br><span class="line">        self.extras = nn.ModuleList(extras)</span><br><span class="line">        # 回归和分类网络</span><br><span class="line">        self.loc = nn.ModuleList(head[0])</span><br><span class="line">        self.conf = nn.ModuleList(head[1])</span><br><span class="line"></span><br><span class="line">        if phase == &#x27;test&#x27;:</span><br><span class="line">            self.softmax = nn.Softmax(dim=-1)</span><br><span class="line">            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;Applies network layers and ops on input image(s) x.</span><br><span class="line">        Args:</span><br><span class="line">            x: input image or batch of images. Shape: [batch,3,300,300].</span><br><span class="line">        Return:</span><br><span class="line">            Depending on phase:</span><br><span class="line">            test:</span><br><span class="line">                Variable(tensor) of output class label predictions,</span><br><span class="line">                confidence score, and corresponding location predictions for</span><br><span class="line">                each object detected. Shape: [batch,topk,7]</span><br><span class="line">            train:</span><br><span class="line">                list of concat outputs from:</span><br><span class="line">                    1: confidence layers, Shape: [batch*num_priors,num_classes]</span><br><span class="line">                    2: localization layers, Shape: [batch,num_priors*4]</span><br><span class="line">                    3: priorbox layers, Shape: [2,num_priors*4]</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        sources = list()</span><br><span class="line">        loc = list()</span><br><span class="line">        conf = list()</span><br><span class="line"></span><br><span class="line">        # apply vgg up to conv4_3 relu</span><br><span class="line">        # vgg网络到conv4_3</span><br><span class="line">        for k in range(23):</span><br><span class="line">            x = self.vgg[k](x)</span><br><span class="line">        # l2 正则化</span><br><span class="line">        s = self.L2Norm(x)</span><br><span class="line">        sources.append(s)</span><br><span class="line"></span><br><span class="line">        # apply vgg up to fc7</span><br><span class="line">        # conv4_3 到 fc</span><br><span class="line">        for k in range(23, len(self.vgg)):</span><br><span class="line">            x = self.vgg[k](x)</span><br><span class="line">        sources.append(x)</span><br><span class="line"></span><br><span class="line">        # apply extra layers and cache source layer outputs</span><br><span class="line">        # extras 网络</span><br><span class="line">        for k, v in enumerate(self.extras):</span><br><span class="line">            x = F.relu(v(x), inplace=True)</span><br><span class="line">            if k % 2 == 1:</span><br><span class="line">                # 把需要进行多尺度的网络输出存入 sources</span><br><span class="line">                sources.append(x)</span><br><span class="line"></span><br><span class="line">        # apply multibox head to source layers</span><br><span class="line">        # 多尺度回归和分类网络</span><br><span class="line">        for (x, l, c) in zip(sources, self.loc, self.conf):</span><br><span class="line">            loc.append(l(x).permute(0, 2, 3, 1).contiguous())</span><br><span class="line">            conf.append(c(x).permute(0, 2, 3, 1).contiguous())</span><br><span class="line"></span><br><span class="line">        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)</span><br><span class="line">        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)</span><br><span class="line">        if self.phase == &quot;test&quot;:</span><br><span class="line">            output = self.detect(</span><br><span class="line">                loc.view(loc.size(0), -1, 4),                   # loc preds</span><br><span class="line">                self.softmax(conf.view(conf.size(0), -1,</span><br><span class="line">                             self.num_classes)),                # conf preds</span><br><span class="line">                self.priors.type(type(x.data))                  # default boxes</span><br><span class="line">            )</span><br><span class="line">        else:</span><br><span class="line">            output = (</span><br><span class="line">                # loc的输出，size:(batch, 8732, 4)</span><br><span class="line">                loc.view(loc.size(0), -1, 4),</span><br><span class="line">                # conf的输出，size:(batch, 8732, 21)</span><br><span class="line">                conf.view(conf.size(0), -1, self.num_classes),</span><br><span class="line">                # 生成所有的候选框 size([8732, 4])</span><br><span class="line">                self.priors</span><br><span class="line">            )</span><br><span class="line">        return output</span><br><span class="line">    # 加载模型参数</span><br><span class="line">    def load_weights(self, base_file):</span><br><span class="line">        other, ext = os.path.splitext(base_file)</span><br><span class="line">        if ext == &#x27;.pkl&#x27; or &#x27;.pth&#x27;:</span><br><span class="line">            print(&#x27;Loading weights into state dict...&#x27;)</span><br><span class="line">            self.load_state_dict(torch.load(base_file,</span><br><span class="line">                                 map_location=lambda storage, loc: storage))</span><br><span class="line">            print(&#x27;Finished!&#x27;)</span><br><span class="line">        else:</span><br><span class="line">			print(&#x27;Sorry only .pth and .pkl files supported.&#x27;)</span><br></pre></td></tr></table></figure><p>然后为了增加可读性，重新封装了一下，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">base = &#123;</span><br><span class="line">    &#x27;300&#x27;: [64, 64, &#x27;M&#x27;, 128, 128, &#x27;M&#x27;, 256, 256, 256, &#x27;C&#x27;, 512, 512, 512, &#x27;M&#x27;,</span><br><span class="line">            512, 512, 512],</span><br><span class="line">    &#x27;512&#x27;: [],</span><br><span class="line">&#125;</span><br><span class="line">extras = &#123;</span><br><span class="line">    &#x27;300&#x27;: [256, &#x27;S&#x27;, 512, 128, &#x27;S&#x27;, 256, 128, 256, 128, 256],</span><br><span class="line">    &#x27;512&#x27;: [],</span><br><span class="line">&#125;</span><br><span class="line">mbox = &#123;</span><br><span class="line">    &#x27;300&#x27;: [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location</span><br><span class="line">    &#x27;512&#x27;: [],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def build_ssd(phase, size=300, num_classes=21):</span><br><span class="line">    if phase != &quot;test&quot; and phase != &quot;train&quot;:</span><br><span class="line">        print(&quot;ERROR: Phase: &quot; + phase + &quot; not recognized&quot;)</span><br><span class="line">        return</span><br><span class="line">    if size != 300:</span><br><span class="line">        print(&quot;ERROR: You specified size &quot; + repr(size) + &quot;. However, &quot; +</span><br><span class="line">              &quot;currently only SSD300 (size=300) is supported!&quot;)</span><br><span class="line">        return</span><br><span class="line">    # 调用multibox，生成vgg,extras,head</span><br><span class="line">    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),</span><br><span class="line">                                     add_extras(extras[str(size)], 1024),</span><br><span class="line">                                     mbox[str(size)], num_classes)</span><br><span class="line">	return SSD(phase, size, base_, extras_, head_, num_classes)</span><br></pre></td></tr></table></figure><h3 id="Loss解析"><a href="#Loss解析" class="headerlink" title="Loss解析"></a>Loss解析</h3><p>SSD的损失函数包含两个部分，一个是定位损失$L_{l o c}$，一个是分类损失$L_{conf}$，整个损失函数表达如下：$L(x, c, l, g)=\frac{1}{N}\left(L_{c o n f}(x, c)+\alpha L_{l o c}(x, l, g)\right)$ 其中，$N$是先验框的正样本数量，$c$是类别置信度预测值，$l$是先验框对应的边界框预测值，$g$是ground truth的位置参数，$x$代表网络的预测值。对于位置损失，采用Smooth L1 Loss，位置信息都是<code>encode</code>之后的数值，后面会讲这个encode的过程。而对于分类损失，首先需要使用<code>hard negtive mining</code>将正负样本按照<code>1:3</code> 的比例把负样本抽样出来，抽样的方法是：针对所有batch的confidence，按照置信度误差进行降序排列，取出前<code>top_k</code>个负样本。损失函数可以用下图表示：</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ssd%20code/643.png" alt></p><h4 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h4><ul><li>Reshape所有batch中的conf，即代码中的<code>batch_conf = conf_data.view(-1, self.num_classes)</code>，方便后续排序。</li><li>置信度误差越大，实际上就是预测背景的置信度越小。</li><li>把所有conf进行<code>logsoftmax</code>处理(均为负值)，预测的置信度越小，则<code>logsoftmax</code>越小，取绝对值，则<code>|logsoftmax|</code>越大，降序排列<code>-logsoftmax</code>，取前<code>top_k</code>的负样本。其中，log_sum_exp函数的代码如下：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def log_sum_exp(x):</span><br><span class="line">    x_max = x.detach().max()</span><br><span class="line">    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True))+x_max</span><br></pre></td></tr></table></figure><p>分类损失<code>conf_logP</code>函数如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf_logP = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))</span><br></pre></td></tr></table></figure><p>这样计算的原因主要是为了增强<code>logsoftmax</code>损失的数值稳定性。放一张手推图：</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/ssd%20code/644.webp" alt></p><p>损失函数完整代码实现，来自<code>layers/modules/multibox_loss.py</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">class MultiBoxLoss(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;SSD Weighted Loss Function</span><br><span class="line">    Compute Targets:</span><br><span class="line">        1) Produce Confidence Target Indices by matching  ground truth boxes</span><br><span class="line">           with (default) &#x27;priorboxes&#x27; that have jaccard index &gt; threshold parameter</span><br><span class="line">           (default threshold: 0.5).</span><br><span class="line">        2) Produce localization target by &#x27;encoding&#x27; variance into offsets of ground</span><br><span class="line">           truth boxes and their matched  &#x27;priorboxes&#x27;.</span><br><span class="line">        3) Hard negative mining to filter the excessive number of negative examples</span><br><span class="line">           that comes with using a large number of default bounding boxes.</span><br><span class="line">           (default negative:positive ratio 3:1)</span><br><span class="line">    Objective Loss:</span><br><span class="line">        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N</span><br><span class="line">        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss</span><br><span class="line">        weighted by α which is set to 1 by cross val.</span><br><span class="line">        Args:</span><br><span class="line">            c: class confidences,</span><br><span class="line">            l: predicted boxes,</span><br><span class="line">            g: ground truth boxes</span><br><span class="line">            N: number of matched default boxes</span><br><span class="line">        See: https://arxiv.org/pdf/1512.02325.pdf for more details.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, num_classes, overlap_thresh, prior_for_matching,</span><br><span class="line">                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,</span><br><span class="line">                 use_gpu=True):</span><br><span class="line">        super(MultiBoxLoss, self).__init__()</span><br><span class="line">        self.use_gpu = use_gpu</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.threshold = overlap_thresh</span><br><span class="line">        self.background_label = bkg_label</span><br><span class="line">        self.encode_target = encode_target</span><br><span class="line">        self.use_prior_for_matching = prior_for_matching</span><br><span class="line">        self.do_neg_mining = neg_mining</span><br><span class="line">        self.negpos_ratio = neg_pos</span><br><span class="line">        self.neg_overlap = neg_overlap</span><br><span class="line">        self.variance = cfg[&#x27;variance&#x27;]</span><br><span class="line"></span><br><span class="line">    def forward(self, predictions, targets):</span><br><span class="line">        &quot;&quot;&quot;Multibox Loss</span><br><span class="line">        Args:</span><br><span class="line">            predictions (tuple): A tuple containing loc preds, conf preds,</span><br><span class="line">            and prior boxes from SSD net.</span><br><span class="line">                conf shape: torch.size(batch_size,num_priors,num_classes)</span><br><span class="line">                loc shape: torch.size(batch_size,num_priors,4)</span><br><span class="line">                priors shape: torch.size(num_priors,4)</span><br><span class="line">            targets (tensor): Ground truth boxes and labels for a batch,</span><br><span class="line">                shape: [batch_size,num_objs,5] (last idx is the label).</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        loc_data, conf_data, priors = predictions</span><br><span class="line">        num = loc_data.size(0)# batch_size</span><br><span class="line">        priors = priors[:loc_data.size(1), :]</span><br><span class="line">        num_priors = (priors.size(0)) # 先验框个数</span><br><span class="line">        num_classes = self.num_classes #类别数</span><br><span class="line"></span><br><span class="line">        # match priors (default boxes) and ground truth boxes</span><br><span class="line">        # 获取匹配每个prior box的 ground truth</span><br><span class="line">        # 创建 loc_t 和 conf_t 保存真实box的位置和类别</span><br><span class="line">        loc_t = torch.Tensor(num, num_priors, 4)</span><br><span class="line">        conf_t = torch.LongTensor(num, num_priors)</span><br><span class="line">        for idx in range(num):</span><br><span class="line">            truths = targets[idx][:, :-1].data #ground truth box信息</span><br><span class="line">            labels = targets[idx][:, -1].data # ground truth conf信息</span><br><span class="line">            defaults = priors.data # priors的 box 信息</span><br><span class="line">            # 匹配 ground truth</span><br><span class="line">            match(self.threshold, truths, defaults, self.variance, labels,</span><br><span class="line">                  loc_t, conf_t, idx)</span><br><span class="line">        if self.use_gpu:</span><br><span class="line">            loc_t = loc_t.cuda()</span><br><span class="line">            conf_t = conf_t.cuda()</span><br><span class="line">        # wrap targets</span><br><span class="line">        loc_t = Variable(loc_t, requires_grad=False)</span><br><span class="line">        conf_t = Variable(conf_t, requires_grad=False)</span><br><span class="line">        # 匹配中所有的正样本mask,shape[b,M]</span><br><span class="line">        pos = conf_t &gt; 0</span><br><span class="line">        num_pos = pos.sum(dim=1, keepdim=True)</span><br><span class="line">        # Localization Loss,使用 Smooth L1</span><br><span class="line">        # shape[b,M]--&gt;shape[b,M,4]</span><br><span class="line">        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)</span><br><span class="line">        loc_p = loc_data[pos_idx].view(-1, 4) #预测的正样本box信息</span><br><span class="line">        loc_t = loc_t[pos_idx].view(-1, 4) #真实的正样本box信息</span><br><span class="line">        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) #Smooth L1 损失</span><br><span class="line">		</span><br><span class="line">		&#x27;&#x27;&#x27;</span><br><span class="line">        Target；</span><br><span class="line">            下面进行hard negative mining</span><br><span class="line">        过程:</span><br><span class="line">            1、 针对所有batch的conf，按照置信度误差(预测背景的置信度越小，误差越大)进行降序排列;</span><br><span class="line">            2、 负样本的label全是背景，那么利用log softmax 计算出logP,</span><br><span class="line">               logP越大，则背景概率越低,误差越大;</span><br><span class="line">            3、 选取误差较大的top_k作为负样本，保证正负样本比例接近1:3;</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        # Compute max conf across batch for hard negative mining</span><br><span class="line">        # shape[b*M,num_classes]</span><br><span class="line">        batch_conf = conf_data.view(-1, self.num_classes)</span><br><span class="line">        # 使用logsoftmax，计算置信度,shape[b*M, 1]</span><br><span class="line">        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))</span><br><span class="line"></span><br><span class="line">        # Hard Negative Mining</span><br><span class="line">        loss_c[pos] = 0  # 把正样本排除，剩下的就全是负样本，可以进行抽样</span><br><span class="line">        loss_c = loss_c.view(num, -1)# shape[b, M]</span><br><span class="line">        # 难预测的损失大</span><br><span class="line">        # 两次sort排序，能够得到每个元素在降序排列中的位置idx_rank</span><br><span class="line">        # 即得到原Tensor的元素按dim指定维度，排第几，索引变成了排名</span><br><span class="line">        # 比如[4, 9, 7, 8, 5]</span><br><span class="line">        # 第一次sort排序结果为[9,8,7,5,4] 对应索引为[1,3,2,4,0]</span><br><span class="line">        # 第二次sort排序结果为[0,1,2,3,4] 对应索引为[4,0,2,1,3]</span><br><span class="line">        # 即第一排的元素9，它是第一排（也就是按dim=1看）里面最大的，所以它的排名是0，原Tensor第一排的元素4，它是第一排里面最小的，所以它的排名是4</span><br><span class="line">        _, loss_idx = loss_c.sort(1, descending=True) </span><br><span class="line">        _, idx_rank = loss_idx.sort(1) </span><br><span class="line">        # 抽取负样本</span><br><span class="line">        # 每个batch中正样本的数目，shape[b,1]</span><br><span class="line">        num_pos = pos.long().sum(1, keepdim=True)</span><br><span class="line">        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)</span><br><span class="line">        # 小于num_neg为True，限制负样本的数量，shape[b, M]</span><br><span class="line">        # 排名靠前的留下</span><br><span class="line">        neg = idx_rank &lt; num_neg.expand_as(idx_rank)</span><br><span class="line"></span><br><span class="line">        # Confidence Loss Including Positive and Negative Examples</span><br><span class="line">        # shape[b,M] --&gt; shape[b,M,num_classes]</span><br><span class="line">        pos_idx = pos.unsqueeze(2).expand_as(conf_data)</span><br><span class="line">        neg_idx = neg.unsqueeze(2).expand_as(conf_data)</span><br><span class="line">        # 提取出所有筛选好的正负样本(预测的和真实的)</span><br><span class="line">        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)</span><br><span class="line">        targets_weighted = conf_t[(pos+neg).gt(0)]</span><br><span class="line">        # 计算conf交叉熵</span><br><span class="line">        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)</span><br><span class="line"></span><br><span class="line">        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N</span><br><span class="line">		# 正样本个数</span><br><span class="line">        N = num_pos.data.sum()</span><br><span class="line">        loss_l /= N</span><br><span class="line">        loss_c /= N</span><br><span class="line">		return loss_l, loss_c</span><br></pre></td></tr></table></figure><h4 id="先验框匹配策略"><a href="#先验框匹配策略" class="headerlink" title="先验框匹配策略"></a>先验框匹配策略</h4><p>上面的代码中还有一个地方没讲到，就是match函数。这是SSD算法的先验框匹配函数。在训练时首先需要确定训练图片中的ground truth是由哪一个先验框来匹配，与之匹配的先验框所对应的边界框将负责预测它。SSD的先验框和ground truth匹配原则主要有2点。第一点是对于图片中的每个ground truth，找到和它IOU最大的先验框，该先验框与其匹配，这样可以保证每个ground truth一定与某个prior匹配。第二点是对于剩余的未匹配的先验框，若某个ground truth和它的IOU大于某个阈值(一般设为0.5)，那么该prior和这个ground truth匹配，剩下没有匹配上的先验框都是负样本（如果多个ground truth和某一个先验框的IOU均大于阈值，那么prior只与IOU最大的那个进行匹配）。代码实现如下，来自<code>layers/box_utils.py</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):</span><br><span class="line">    &quot;&quot;&quot;把和每个prior box 有最大的IOU的ground truth box进行匹配，</span><br><span class="line">    同时，编码包围框，返回匹配的索引，对应的置信度和位置</span><br><span class="line">    Args:</span><br><span class="line">        threshold: IOU阈值，小于阈值设为背景</span><br><span class="line">        truths: ground truth boxes, shape[N,4]</span><br><span class="line">        priors: 先验框， shape[M,4]</span><br><span class="line">        variances: prior的方差, list(float)</span><br><span class="line">        labels: 图片的所有类别，shape[num_obj]</span><br><span class="line">        loc_t: 用于填充encoded loc 目标张量</span><br><span class="line">        conf_t: 用于填充encoded conf 目标张量</span><br><span class="line">        idx: 现在的batch index</span><br><span class="line">        The matched indices corresponding to 1)location and 2)confidence preds.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # jaccard index</span><br><span class="line">    # 计算IOU</span><br><span class="line">    overlaps = jaccard(</span><br><span class="line">        truths,</span><br><span class="line">        point_form(priors)</span><br><span class="line">    )</span><br><span class="line">    # (Bipartite Matching)</span><br><span class="line">    # [1,num_objects] 和每个ground truth box 交集最大的 prior box</span><br><span class="line">    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)</span><br><span class="line">    # [1,num_priors] 和每个prior box 交集最大的 ground truth box</span><br><span class="line">    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)</span><br><span class="line">    best_truth_idx.squeeze_(0) #M</span><br><span class="line">    best_truth_overlap.squeeze_(0) #M</span><br><span class="line">    best_prior_idx.squeeze_(1) #N</span><br><span class="line">    best_prior_overlap.squeeze_(1) #N</span><br><span class="line">    # 保证每个ground truth box 与某一个prior box 匹配，固定值为 2 &gt; threshold</span><br><span class="line">    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior</span><br><span class="line">    # TODO refactor: index  best_prior_idx with long tensor</span><br><span class="line">    # ensure every gt matches with its prior of max overlap</span><br><span class="line">    # 保证每一个ground truth 匹配它的都是具有最大IOU的prior</span><br><span class="line">    # 根据 best_prior_dix 锁定 best_truth_idx里面的最大IOU prior</span><br><span class="line">    for j in range(best_prior_idx.size(0)):</span><br><span class="line">        best_truth_idx[best_prior_idx[j]] = j</span><br><span class="line">    matches = truths[best_truth_idx]          # 提取出所有匹配的ground truth box, Shape: [M,4]</span><br><span class="line">    conf = labels[best_truth_idx] + 1         # 提取出所有GT框的类别， Shape:[M]</span><br><span class="line">    # 把 iou &lt; threshold 的框类别设置为 bg,即为0</span><br><span class="line">    conf[best_truth_overlap &lt; threshold] = 0  # label as background</span><br><span class="line">    # 编码包围框</span><br><span class="line">    loc = encode(matches, priors, variances)</span><br><span class="line">    # 保存匹配好的loc和conf到loc_t和conf_t中</span><br><span class="line">    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn</span><br><span class="line">    conf_t[idx] = conf  # [num_priors] top class label for each prior</span><br></pre></td></tr></table></figure><h3 id="位置坐标转换"><a href="#位置坐标转换" class="headerlink" title="位置坐标转换"></a>位置坐标转换</h3><p>我们看到上面出现了一个point_form函数，这是什么意思呢？这是因为目标框有2种表示方式：</p><ul><li>$\left(x_{\min }, y_{\min }, x_{\max }, y_{\max }\right)$</li><li>$(x, y, w, h)$这部分的代码在<code>layers/box_utils.py</code>下：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def point_form(boxes):</span><br><span class="line">    &quot;&quot;&quot; Convert prior_boxes to (xmin, ymin, xmax, ymax)</span><br><span class="line">   把 prior_box (cx, cy, w, h)转化为(xmin, ymin, xmax, ymax)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin</span><br><span class="line">                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def center_size(boxes):</span><br><span class="line">    &quot;&quot;&quot; Convert prior_boxes to (cx, cy, w, h)</span><br><span class="line">    把 prior_box (xmin, ymin, xmax, ymax) 转化为 (cx, cy, w, h)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy</span><br><span class="line">                            boxes[:, 2:] - boxes[:, :2], 1) # w, h</span><br></pre></td></tr></table></figure><h3 id="IOU计算"><a href="#IOU计算" class="headerlink" title="IOU计算"></a>IOU计算</h3><p>这部分比较简单，对于两个Box来讲，首先计算两个box左上角点坐标的最大值和右下角坐标的最小值，然后计算交集面积，最后把交集面积除以对应的并集面积。代码仍在<code>layers/box_utils.py</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">def intersect(box_a, box_b):</span><br><span class="line">    &quot;&quot;&quot; We resize both tensors to [A,B,2] without new malloc:</span><br><span class="line">    [A,2] -&gt; [A,1,2] -&gt; [A,B,2]</span><br><span class="line">    [B,2] -&gt; [1,B,2] -&gt; [A,B,2]</span><br><span class="line">    Then we compute the area of intersect between box_a and box_b.</span><br><span class="line">    Args:</span><br><span class="line">      box_a: (tensor) bounding boxes, Shape: [A,4].</span><br><span class="line">      box_b: (tensor) bounding boxes, Shape: [B,4].</span><br><span class="line">    Return:</span><br><span class="line">      (tensor) intersection area, Shape: [A,B].</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A = box_a.size(0)</span><br><span class="line">    B = box_b.size(0)</span><br><span class="line">     # 右下角，选出最小值</span><br><span class="line">    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),</span><br><span class="line">                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))</span><br><span class="line">    # 左上角，选出最大值</span><br><span class="line">    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),</span><br><span class="line">                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))</span><br><span class="line">    # 负数用0截断，为0代表交集为0</span><br><span class="line">    inter = torch.clamp((max_xy - min_xy), min=0)</span><br><span class="line">    return inter[:, :, 0] * inter[:, :, 1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def jaccard(box_a, box_b):</span><br><span class="line">    &quot;&quot;&quot;Compute the jaccard overlap of two sets of boxes.  The jaccard overlap</span><br><span class="line">    is simply the intersection over union of two boxes.  Here we operate on</span><br><span class="line">    ground truth boxes and default boxes.</span><br><span class="line">    E.g.:</span><br><span class="line">        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)</span><br><span class="line">    Args:</span><br><span class="line">        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]</span><br><span class="line">        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]</span><br><span class="line">    Return:</span><br><span class="line">        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    inter = intersect(box_a, box_b)# A∩B</span><br><span class="line">     # box_a和box_b的面积</span><br><span class="line">    area_a = ((box_a[:, 2]-box_a[:, 0]) *</span><br><span class="line">              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]#(N,)</span><br><span class="line">    area_b = ((box_b[:, 2]-box_b[:, 0]) *</span><br><span class="line">              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]#(M,)</span><br><span class="line">    union = area_a + area_b - inter</span><br><span class="line">    return inter / union  # [A,B]</span><br></pre></td></tr></table></figure><h3 id="L2标准化"><a href="#L2标准化" class="headerlink" title="L2标准化"></a>L2标准化</h3><p>VGG16的<code>conv4_3</code>特征图的大小为38 x 38，网络层靠前，方差比较大，需要加一个L2标准化，以保证和后面的检测层差异不是很大。L2标准化的公式如下：$\hat{x}=\frac{x}{|x|^{2}}$ ，其中$x=\left(x_{1} \ldots x_{d}\right)|x|_{2}=\left(\sum_{i=1}^{d}\left|x_{i}\right|^{2}\right)^{1 / 2}$。同时，这里还要注意的是如果简单的对一个layer的输入进行L2标准化就会改变该层的规模，并且会减慢学习速度，因此这里引入了一个缩放系数$\gamma_{i}$ ，对于每一个通道l2标准化后的结果为：$y_{i}=\gamma_{i} \hat{x}_{i}$ ，通常的值设10或者20，效果比较好。代码来自layers/modules/l2norm.py。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class L2Norm(nn.Module):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    conv4_3特征图大小38x38，网络层靠前，norm较大，需要加一个L2 Normalization,以保证和后面的检测层差异不是很大，具体可以参考：ParseNet。这个前面的推文里面有讲。</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    def __init__(self, n_channels, scale):</span><br><span class="line">        super(L2Norm, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.gamma = scale or None</span><br><span class="line">        self.eps = 1e-10</span><br><span class="line">        # 将一个不可训练的类型Tensor转换成可以训练的类型 parameter</span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(self.n_channels))</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    # 初始化参数</span><br><span class="line">    def reset_parameters(self):</span><br><span class="line">        nn.init.constant_(self.weight, self.gamma)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 计算x的2范数</span><br><span class="line">        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps # shape[b,1,38,38]</span><br><span class="line">        x = torch.div(x,norm)   # shape[b,512,38,38]</span><br><span class="line"></span><br><span class="line">        # 扩展self.weight的维度为shape[1,512,1,1]，然后参考公式计算</span><br><span class="line">        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure><h3 id="位置信息编解码"><a href="#位置信息编解码" class="headerlink" title="位置信息编解码"></a>位置信息编解码</h3><p>上面提到了计算坐标损失的时候，坐标是<code>encoding</code>之后的，这是怎么回事呢？根据论文的描述，预测框和ground truth边界框存在一个转换关系，先定义一些变量：</p><ul><li><p>先验框位置：$d=\left(d^{c x}, d^{c y}, d^{w}, d^{h}\right)$</p></li><li><p>ground truth框位置：$g=\left(g^{c x}, g^{c y}, g^{w}, g^{h}\right)$</p></li><li><p>variance是先验框的坐标方差。然后<strong>编码</strong>的过程可以表示为：$g_{j}^{\hat{c} x}=\left(g_{j}^{c x}-d_{i}^{c x}\right) / d_{i}^{w} /$varicance[0]，$g_{j}^{\hat{c} y}=\left(g_{j}^{c y}-d_{i}^{c y}\right) / d_{i}^{h} /$varicance[1]，$g_{j}^{\hat{w}}=\log \left(\frac{g_{j}^{w}}{d_{i}^{w}}\right) /$variance[2]，$g_{j}^{\hat{h}}=\log \left(\frac{g_{j}^{h}}{d_{i}^{h}}\right) /$variance[3]</p></li></ul><p><strong>解码</strong>的过程可以表示为：$g_{\text {predict}}^{c x}=d^{w} <em>\left(\text {variance}[0]</em> l^{c x}\right)+d^{c x}$，$g_{\text {predict}}^{c y}=d^{h} <em>\left(\text {variance}[1]</em> l^{c y}\right)+d^{c y}$，$g_{\text {predict}}^{w}=d^{w} \exp \left(\text {vairance}[2] <em>l^{w}\right)$，$g_{\text {predict}}^{h}=d^{h} \exp \left(\text {vairance}[3]</em> l^{h}\right)$</p><p>这部分对应的代码在<code>layers/box_utils.py</code>里面：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">def encode(matched, priors, variances):</span><br><span class="line">    &quot;&quot;&quot;Encode the variances from the priorbox layers into the ground truth boxes</span><br><span class="line">    we have matched (based on jaccard overlap) with the prior boxes.</span><br><span class="line">    Args:</span><br><span class="line">        matched: (tensor) Coords of ground truth for each prior in point-form</span><br><span class="line">            Shape: [num_priors, 4].</span><br><span class="line">        priors: (tensor) Prior boxes in center-offset form</span><br><span class="line">            Shape: [num_priors,4].</span><br><span class="line">        variances: (list[float]) Variances of priorboxes</span><br><span class="line">    Return:</span><br><span class="line">        encoded boxes (tensor), Shape: [num_priors, 4]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # dist b/t match center and prior&#x27;s center</span><br><span class="line">    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]</span><br><span class="line">    # encode variance</span><br><span class="line">    g_cxcy /= (variances[0] * priors[:, 2:])</span><br><span class="line">    # match wh / prior wh</span><br><span class="line">    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]</span><br><span class="line">    g_wh = torch.log(g_wh) / variances[1]</span><br><span class="line">    # return target for smooth_l1_loss</span><br><span class="line">    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Adapted from https://github.com/Hakuyume/chainer-ssd</span><br><span class="line">def decode(loc, priors, variances):</span><br><span class="line">    &quot;&quot;&quot;Decode locations from predictions using priors to undo</span><br><span class="line">    the encoding we did for offset regression at train time.</span><br><span class="line">    Args:</span><br><span class="line">        loc (tensor): location predictions for loc layers,</span><br><span class="line">            Shape: [num_priors,4]</span><br><span class="line">        priors (tensor): Prior boxes in center-offset form.</span><br><span class="line">            Shape: [num_priors,4].</span><br><span class="line">        variances: (list[float]) Variances of priorboxes</span><br><span class="line">    Return:</span><br><span class="line">        decoded bounding box predictions</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    boxes = torch.cat((</span><br><span class="line">        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],</span><br><span class="line">        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)</span><br><span class="line">    boxes[:, :2] -= boxes[:, 2:] / 2</span><br><span class="line">    boxes[:, 2:] += boxes[:, :2]</span><br><span class="line">return boxes</span><br></pre></td></tr></table></figure><h3 id="后处理NMS"><a href="#后处理NMS" class="headerlink" title="后处理NMS"></a>后处理NMS</h3><p>这里不再赘述了。这里IOU阈值取了0.5。这部分的代码也在<code>layers/box_utils.py</code>里面。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">def nms(boxes, scores, overlap=0.5, top_k=200):</span><br><span class="line">    &quot;&quot;&quot;Apply non-maximum suppression at test time to avoid detecting too many</span><br><span class="line">    overlapping bounding boxes for a given object.</span><br><span class="line">    Args:</span><br><span class="line">        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].</span><br><span class="line">        scores: (tensor) The class predscores for the img, Shape:[num_priors].</span><br><span class="line">        overlap: (float) The overlap thresh for suppressing unnecessary boxes.</span><br><span class="line">        top_k: (int) The Maximum number of box preds to consider.</span><br><span class="line">    Return:</span><br><span class="line">        The indices of the kept boxes with respect to num_priors.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    keep = scores.new(scores.size(0)).zero_().long()</span><br><span class="line">    if boxes.numel() == 0:</span><br><span class="line">        return keep</span><br><span class="line">    x1 = boxes[:, 0]</span><br><span class="line">    y1 = boxes[:, 1]</span><br><span class="line">    x2 = boxes[:, 2]</span><br><span class="line">    y2 = boxes[:, 3]</span><br><span class="line">    area = torch.mul(x2 - x1, y2 - y1)</span><br><span class="line">    v, idx = scores.sort(0)  # sort in ascending order</span><br><span class="line">    # I = I[v &gt;= 0.01]</span><br><span class="line">    idx = idx[-top_k:]  # indices of the top-k largest vals</span><br><span class="line">    xx1 = boxes.new()</span><br><span class="line">    yy1 = boxes.new()</span><br><span class="line">    xx2 = boxes.new()</span><br><span class="line">    yy2 = boxes.new()</span><br><span class="line">    w = boxes.new()</span><br><span class="line">    h = boxes.new()</span><br><span class="line"></span><br><span class="line">    # keep = torch.Tensor()</span><br><span class="line">    count = 0</span><br><span class="line">    while idx.numel() &gt; 0:</span><br><span class="line">        i = idx[-1]  # index of current largest val</span><br><span class="line">        # keep.append(i)</span><br><span class="line">        keep[count] = i</span><br><span class="line">        count += 1</span><br><span class="line">        if idx.size(0) == 1:</span><br><span class="line">            break</span><br><span class="line">        idx = idx[:-1]  # remove kept element from view</span><br><span class="line">        # load bboxes of next highest vals</span><br><span class="line">        torch.index_select(x1, 0, idx, out=xx1)</span><br><span class="line">        torch.index_select(y1, 0, idx, out=yy1)</span><br><span class="line">        torch.index_select(x2, 0, idx, out=xx2)</span><br><span class="line">        torch.index_select(y2, 0, idx, out=yy2)</span><br><span class="line">        # store element-wise max with next highest score</span><br><span class="line">        xx1 = torch.clamp(xx1, min=x1[i])</span><br><span class="line">        yy1 = torch.clamp(yy1, min=y1[i])</span><br><span class="line">        xx2 = torch.clamp(xx2, max=x2[i])</span><br><span class="line">        yy2 = torch.clamp(yy2, max=y2[i])</span><br><span class="line">        w.resize_as_(xx2)</span><br><span class="line">        h.resize_as_(yy2)</span><br><span class="line">        w = xx2 - xx1</span><br><span class="line">        h = yy2 - yy1</span><br><span class="line">        # check sizes of xx1 and xx2.. after each iteration</span><br><span class="line">        w = torch.clamp(w, min=0.0)</span><br><span class="line">        h = torch.clamp(h, min=0.0)</span><br><span class="line">        inter = w*h</span><br><span class="line">        # IoU = i / (area(a) + area(b) - i)</span><br><span class="line">        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)</span><br><span class="line">        union = (rem_areas - inter) + area[i]</span><br><span class="line">        IoU = inter/union  # store result in iou</span><br><span class="line">        # keep only elements with an IoU &lt;= overlap</span><br><span class="line">        idx = idx[IoU.le(overlap)]</span><br><span class="line">    return keep, count</span><br></pre></td></tr></table></figure><h3 id="检测函数"><a href="#检测函数" class="headerlink" title="检测函数"></a>检测函数</h3><p>模型在测试的时候，需要把loc和conf输入到detect函数进行nms，然后给出结果。这部分的代码在<code>layers/functions/detection.py</code>里面，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">class Detect(Function):</span><br><span class="line">    &quot;&quot;&quot;At test time, Detect is the final layer of SSD.  Decode location preds,</span><br><span class="line">    apply non-maximum suppression to location predictions based on conf</span><br><span class="line">    scores and threshold to a top_k number of output predictions for both</span><br><span class="line">    confidence score and locations.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.background_label = bkg_label</span><br><span class="line">        self.top_k = top_k</span><br><span class="line">        # Parameters used in nms.</span><br><span class="line">        self.nms_thresh = nms_thresh</span><br><span class="line">        if nms_thresh &lt;= 0:</span><br><span class="line">            raise ValueError(&#x27;nms_threshold must be non negative.&#x27;)</span><br><span class="line">        self.conf_thresh = conf_thresh</span><br><span class="line">        self.variance = cfg[&#x27;variance&#x27;]</span><br><span class="line"></span><br><span class="line">    def forward(self, loc_data, conf_data, prior_data):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            loc_data: 预测出的loc张量，shape[b,M,4], eg:[b, 8732, 4]</span><br><span class="line">            conf_data:预测出的置信度，shape[b,M,num_classes], eg:[b, 8732, 21]</span><br><span class="line">            prior_data:先验框，shape[M,4], eg:[8732, 4]</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        num = loc_data.size(0)  # batch size</span><br><span class="line">        num_priors = prior_data.size(0)</span><br><span class="line">        output = torch.zeros(num, self.num_classes, self.top_k, 5)# 初始化输出</span><br><span class="line">        conf_preds = conf_data.view(num, num_priors,</span><br><span class="line">                                    self.num_classes).transpose(2, 1)</span><br><span class="line"></span><br><span class="line">        # 解码loc的信息，变为正常的bboxes</span><br><span class="line">        for i in range(num):</span><br><span class="line">            # 解码loc</span><br><span class="line">            decoded_boxes = decode(loc_data[i], prior_data, self.variance)</span><br><span class="line">            # 拷贝每个batch内的conf，用于nms</span><br><span class="line">            conf_scores = conf_preds[i].clone()</span><br><span class="line">            # 遍历每一个类别</span><br><span class="line">            for cl in range(1, self.num_classes):</span><br><span class="line">                # 筛选掉 conf &lt; conf_thresh 的conf</span><br><span class="line">                c_mask = conf_scores[cl].gt(self.conf_thresh)</span><br><span class="line">                scores = conf_scores[cl][c_mask]</span><br><span class="line">                # 如果都被筛掉了，则跳入下一类</span><br><span class="line">                if scores.size(0) == 0:</span><br><span class="line">                    continue</span><br><span class="line">                # 筛选掉 conf &lt; conf_thresh 的框</span><br><span class="line">                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)</span><br><span class="line">                boxes = decoded_boxes[l_mask].view(-1, 4)</span><br><span class="line">                # idx of highest scoring and non-overlapping boxes per class</span><br><span class="line">                # nms</span><br><span class="line">                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)</span><br><span class="line">                # nms 后得到的输出拼接</span><br><span class="line">                output[i, cl, :count] = \</span><br><span class="line">                    torch.cat((scores[ids[:count]].unsqueeze(1),</span><br><span class="line">                               boxes[ids[:count]]), 1)</span><br><span class="line">        flt = output.contiguous().view(num, -1, 5)</span><br><span class="line">        _, idx = flt[:, :, 0].sort(1, descending=True)</span><br><span class="line">        _, rank = idx.sort(1)</span><br><span class="line">        flt[(rank &lt; self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)</span><br><span class="line">	return output</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">Qiyuan-Z</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://qiyuan-z.github.io/2020/04/23/SSD%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/">https://qiyuan-z.github.io/2020/04/23/SSD%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://Qiyuan-Z.github.io" target="_blank">Yuan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></div><div class="post_share"><div class="social-share" data-image="/img/1097380.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/04/30/%E4%B8%A4%E9%98%B6%E6%AE%B5%E5%AE%9E%E6%97%B6%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9CThunderNet/"><img class="prev-cover" src="/img/34125236_p0.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">两阶段实时检测网络ThunderNet</div></div></a></div><div class="next-post pull-right"><a href="/2020/04/20/Feature-Pyramid-Network/"><img class="next-cover" src="/img/34125236_p0.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Feature Pyramid Network</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/05/09/Perceptual-Generative-Adversarial-Networks-for-Small-Object-Detection/" title="Perceptual Generative Adversarial Networks for Small Object Detection"><img class="cover" src="/img/34125236_p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-09</div><div class="title">Perceptual Generative Adversarial Networks for Small Object Detection</div></div></a></div><div><a href="/2020/05/08/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/" title="An Analysis of Scale Invariance in Object Detection – SNIP"><img class="cover" src="/img/34125236_p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-08</div><div class="title">An Analysis of Scale Invariance in Object Detection – SNIP</div></div></a></div><div><a href="/2020/05/08/回归损失函数：L1-loss,-L2-loss以及Smooth-L1-Loss的对比/" title="回归损失函数：L1 loss, L2 loss以及Smooth L1 Loss的对比"><img class="cover" src="/img/34125236_p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-08</div><div class="title">回归损失函数：L1 loss, L2 loss以及Smooth L1 Loss的对比</div></div></a></div><div><a href="/2020/04/30/两阶段实时检测网络ThunderNet/" title="两阶段实时检测网络ThunderNet"><img class="cover" src="/img/34125236_p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-30</div><div class="title">两阶段实时检测网络ThunderNet</div></div></a></div><div><a href="/2020/04/20/自适应空间特征融合-(ASFF)/" title="自适应空间特征融合 (ASFF)"><img class="cover" src="/img/1097380.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-20</div><div class="title">自适应空间特征融合 (ASFF)</div></div></a></div><div><a href="/2020/04/20/Feature-Pyramid-Network/" title="Feature Pyramid Network"><img class="cover" src="/img/34125236_p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-20</div><div class="title">Feature Pyramid Network</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">源码解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA"><span class="toc-number">3.1.</span> <span class="toc-text">网络搭建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Anchor%E7%94%9F%E6%88%90-Prior-Box%E5%B1%82"><span class="toc-number">3.2.</span> <span class="toc-text">Anchor生成(Prior_Box层)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-1"><span class="toc-number">3.3.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss%E8%A7%A3%E6%9E%90"><span class="toc-number">3.4.</span> <span class="toc-text">Loss解析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.4.1.</span> <span class="toc-text">实现步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%88%E9%AA%8C%E6%A1%86%E5%8C%B9%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">3.4.2.</span> <span class="toc-text">先验框匹配策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E5%9D%90%E6%A0%87%E8%BD%AC%E6%8D%A2"><span class="toc-number">3.5.</span> <span class="toc-text">位置坐标转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#IOU%E8%AE%A1%E7%AE%97"><span class="toc-number">3.6.</span> <span class="toc-text">IOU计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">3.7.</span> <span class="toc-text">L2标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E7%BC%96%E8%A7%A3%E7%A0%81"><span class="toc-number">3.8.</span> <span class="toc-text">位置信息编解码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E5%A4%84%E7%90%86NMS"><span class="toc-number">3.9.</span> <span class="toc-text">后处理NMS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A3%80%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-number">3.10.</span> <span class="toc-text">检测函数</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021<i id="heartbeat" class="fa fas fa-heartbeat"></i> Qiyuan-Z</div><div class="footer_custom_text"><p><a style="margin-inline:5px" target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"></a><a style="margin-inline:5px" target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"></a><a style="margin-inline:5px" target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用 Jsdelivr 为静态资源提供CDN加速" alt="Jsdelivr"></a><a style="margin-inline:5px" target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"></a><a style="margin-inline:5px" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a><br>昨日までの私は、もうどこにもいない<br></p></div></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd"}},tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""],addClass:[200,()=>{document.querySelectorAll("mjx-container:not([display='true']").forEach(t=>{const e=t.parentNode;e.classList.contains("has-jax")||e.classList.add("mathjax-overflow")})},"",!1]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>function addGitalkSource(){const t=document.createElement("link");t.rel="stylesheet",t.href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css",document.getElementsByTagName("head")[0].appendChild(t)}function loadGitalk(){function t(){new Gitalk({clientID:"2d10cfb27783db577e70",clientSecret:"154292876bb14966f6ae57304b67859617b08c94",repo:"gitalk",owner:"Qiyuan-Z",admin:["Qiyuan-Z"],id:"cd7972669e1f8421a2454aa6139d158f",language:"zh-CN",perPage:10,distractionFreeMode:!1,pagerDirection:"last",createIssueManually:!1,updateCountCallback:commentCount}).render("gitalk-container")}"function"==typeof Gitalk?t():(addGitalkSource(),getScript("https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js").then(t))}function commentCount(t){let e=document.querySelector("#post-meta .gitalk-comment-count");e&&(e.innerHTML=t)}{function loadOtherComment(){loadGitalk()}loadGitalk()}</script></div><script defer src="https://cdn.jsdelivr.net/gh/Qiyuan-Z/live2d-widget/autoload.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax>function history_calendar_injector_config(){var i=document.getElementsByClassName("sticky_layout")[0];console.log("已挂载history_calendar"),i.insertAdjacentHTML("afterbegin",'<div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height: 100px;overflow: hidden"><div class="history_swiper-container" id="history-container" style="width: 100%;height: 100%"><div class="swiper-wrapper" id="history_container_wrapper" style="height:20px"></div></div></div></div>')}document.getElementsByClassName("sticky_layout")[0]&&(location.pathname,1)&&history_calendar_injector_config()</script><script data-pjax src="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.js"></script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script></body></html>