<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>目标检测调研 | Yuan</title><meta name="keywords" content="目标检测"><meta name="author" content="Qiyuan-Z"><meta name="copyright" content="Qiyuan-Z"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="article"><meta property="og:title" content="目标检测调研"><meta property="og:url" content="https://qiyuan-z.github.io/2020/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/index.html"><meta property="og:site_name" content="Yuan"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://qiyuan-z.github.io/img/No_Cover.jpg"><meta property="article:published_time" content="2020-01-14T12:47:23.096Z"><meta property="article:modified_time" content="2020-02-21T02:51:38.716Z"><meta property="article:author" content="Qiyuan-Z"><meta property="article:tag" content="目标检测"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://qiyuan-z.github.io/img/No_Cover.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qiyuan-z.github.io/2020/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:200,languages:{author:"作者: Qiyuan-Z",link:"链接: ",source:"来源: Yuan",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"bottom-right"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2020-02-21 10:51:38"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));const n=saveToLocal.get("global-font-size");void 0!==n&&document.documentElement.style.setProperty("--global-font-size",n+"px")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">115</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yuan</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-video"></i> <span>番剧</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">目标检测调研</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-01-14T12:47:23.096Z" title="发表于 2020-01-14 20:47:23">2020-01-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-02-21T02:51:38.716Z" title="更新于 2020-02-21 10:51:38">2020-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>47分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h3 id="目标检测的任务"><a href="#目标检测的任务" class="headerlink" title=" 目标检测的任务"></a><span id="more"></span> 目标检测的任务</h3><ul><li><strong>分类-Classification</strong>：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。</li><li><strong>定位-Location</strong>：解决“在哪里？”的问题，即定位出这个目标的的位置。</li><li><strong>检测-Detection</strong>：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。</li><li><strong>分割-Segmentation</strong>：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。</li></ul><h3 id="所面临的挑战"><a href="#所面临的挑战" class="headerlink" title="所面临的挑战"></a>所面临的挑战</h3><ul><li>目标可能出现在图像的任何位置。</li><li>目标有各种不同的大小。</li><li>目标可能有各种不同的形状。</li></ul><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/deep_learning_object_detection_dataset.PNG" alt></p><ul><li><p>Pascal Visual Object Classes(05-12) 是计算机视觉领域最重要的赛事之一。包含多任务，图像分类、目标检测、语义分割和行为检测。主要有两个版本的 Pascal-VOC 用于检测：VOC07 和 VOC12，前者包含 5K 张训练图像，共 12K 个标注目标。后者包含 11K 张训练图像，共 27K 个标注目标。两个数据集中包含了 20 个生活中常见的目标类（Person：person; Animal：bird, cat, cow, dog, horse, sheep; Vehicle：aeroplane, bicycle, boat, bus, car, motor-bike, train; Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor）</p></li><li><p>ImageNet Large Scale Visual Recognition Challenge(10-17) 包含检测挑战赛。Imagenet数据集有1400多万幅图片，涵盖2万多个类别，ILSVRC比赛会每年从ImageNet数据集中抽出部分样本, 检测数据集包括 200 类视觉目标，图像/目标比 VOC 大两个数量级。例如，ILSVRC-14 包含517K 图像以及 534K 注释目标。</p></li><li><p>MS-COCO 是当前最有挑战性的目标检测数据集。从 15 年开始举办比赛。它的目标类别比 ILSVRC 少，但是目标实例更多。例如，MS-COCO-17 包含 164K 张图像，来自 80 类的 897K 个标注目标。与 VOC 和 ILSVRC 相比，MS-COCO 除了边界框注释以外，每个目标通过实例分割进一步标注来帮助精确定位。此外，它包含更多小目标（面积小于图像的 1%），以及密集定位目标。这些特点使 MS−COCO中的目标分布更接近于真实世界。</p></li><li><p>Open Images</p><p>2018 年推出了 Open ImagesOpen~ImagesOpen Images 检测挑战赛。包含两个任务：</p><p>标准目标检测<br>视觉关系检测，用于检测特定关系中的匹配目标</p><p>目标检测，数据集包含 1910K 张图像，600 个目标类别的 15440K 个标注目标。</p></li></ul><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><div class="table-container"><table><thead><tr><th>预测\实际</th><th>正</th><th>负</th></tr></thead><tbody><tr><td>正</td><td>TP</td><td>FP</td><td></td></tr><tr><td>负</td><td>FN</td><td>TN</td><td></td></tr></tbody></table></div><p>Recall=TP/(TP+FN)，召回率，<strong>可理解为正确的被判断为正确的</strong></p><p>Precision=TP/(TP+FP)，准确度，<strong>预测为正类中，实际为正类的比例</strong></p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2018042521300715.png" alt></p><p><strong>准确率-召回率曲线（P-R曲线）</strong>：以召回率为横坐标，精确率为纵坐标</p><script type="math/tex;mode=display">\mathrm{AP}=\int_{0}^{1} P(R) d(R)</script><script type="math/tex;mode=display">\mathrm{mAP}=\frac{1}{\text { classes }} \sum_{i=1}^{\text {classes }} \int_{0}^{1} P(R) d(R)</script><p><strong>IoU</strong>：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180423003716617.png" alt></p><p>VOC07 以后使用 AP 评价检测性能。AP 定义为不同召回率下的平均检测精度，通常在某一特定类别下进行评估。为比较所有目标类的检测性能，通常使用 mAP 作为最终性能指标。</p><p>为度量目标定位精度，使用交并比（IoU）来检查预测框和 GT 框之间的 IoU是否大于预定义的阈值，如，0.5。如果大于阈值则表示成功检测，否则表示漏检。基于 0.5IoU 的 mAP 已经成为多年来用于目标检测问题的事实上的度量标准。</p><p>MS−COCO 的 AP 是在 0.5−0.95多个 IoU 阈值上的平均值。</p><h3 id="深度学习目标检测方法"><a href="#深度学习目标检测方法" class="headerlink" title="深度学习目标检测方法"></a>深度学习目标检测方法</h3><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/deep_learning_object_detection_history.PNG" alt></p><p><strong>Two-stage Detectors（两阶段目标检测器）</strong></p><p>首先由算法（algorithm）生成一系列作为样本的候选框，再通过卷积神经网络进行样本（Sample）分类。</p><p>诸如R-CNN，Fast R-CNN，Faster R-CNN到最新的Mask Scoring R-CNN等网络结构，都属于Two-stage检测方法。</p><p><strong>One-stage Detectors（单阶段目标检测器）</strong></p><p>不需要产生候选框，直接将目标框定位的问题转化为回归（Regression）问题处理(Process)。</p><p>从最早的OverFeat到现在的YOLO，SSD，RetinaNet，YOLOv2，CornerNet等都属于one stage目标检测方法。</p><p>即：</p><p><strong>基于候选区域（Region Proposal）</strong>的，如R-CNN、SPP-net、Fast R-CNN、Faster R-CNN、R-FCN；</p><p><strong>基于端到端（End-to-End）</strong>，无需候选区域（Region Proposal）的，如YOLO、SSD。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180509095302426.png" alt></p><p>对于上述两种方式，基于候选区域（Region Proposal）的方法在检测准确率和定位精度上占优，基于端到端（End-to-End）的算法速度占优。相对于R-CNN系列的“看两眼”（候选框提取和分类），YOLO只需要“看一眼”。总之，目前来说，基于候选区域（Region Proposal）的方法依然占据上风，但端到端的方法速度上优势明显。</p><h3 id="各方法在各数据集上的精度"><a href="#各方法在各数据集上的精度" class="headerlink" title="各方法在各数据集上的精度"></a>各方法在各数据集上的精度</h3><div class="table-container"><table><thead><tr><th style="text-align:center">Detector</th><th style="text-align:center">VOC07 (mAP@IoU=0.5)</th><th style="text-align:center">VOC12 (mAP@IoU=0.5)</th><th style="text-align:center">COCO (mAP@IoU=0.5:0.95)</th><th style="text-align:center">Published In</th></tr></thead><tbody><tr><td style="text-align:center">R-CNN</td><td style="text-align:center">58.5</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">CVPR’14</td></tr><tr><td style="text-align:center">SPP-Net</td><td style="text-align:center">59.2</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">ECCV’14</td></tr><tr><td style="text-align:center">MR-CNN</td><td style="text-align:center">78.2 (07+12)</td><td style="text-align:center">73.9 (07+12)</td><td style="text-align:center">-</td><td style="text-align:center">ICCV’15</td></tr><tr><td style="text-align:center">Fast R-CNN</td><td style="text-align:center">70.0 (07+12)</td><td style="text-align:center">68.4 (07++12)</td><td style="text-align:center">19.7</td><td style="text-align:center">ICCV’15</td></tr><tr><td style="text-align:center">Faster R-CNN</td><td style="text-align:center">73.2 (07+12)</td><td style="text-align:center">70.4 (07++12)</td><td style="text-align:center">21.9</td><td style="text-align:center">NIPS’15</td></tr><tr><td style="text-align:center">YOLO v1</td><td style="text-align:center">66.4 (07+12)</td><td style="text-align:center">57.9 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">CVPR’16</td></tr><tr><td style="text-align:center">G-CNN</td><td style="text-align:center">66.8</td><td style="text-align:center">66.4 (07+12)</td><td style="text-align:center">-</td><td style="text-align:center">CVPR’16</td></tr><tr><td style="text-align:center">AZNet</td><td style="text-align:center">70.4</td><td style="text-align:center">-</td><td style="text-align:center">22.3</td><td style="text-align:center">CVPR’16</td></tr><tr><td style="text-align:center">ION</td><td style="text-align:center">80.1</td><td style="text-align:center">77.9</td><td style="text-align:center">33.1</td><td style="text-align:center">CVPR’16</td></tr><tr><td style="text-align:center">HyperNet</td><td style="text-align:center">76.3 (07+12)</td><td style="text-align:center">71.4 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">CVPR’16</td></tr><tr><td style="text-align:center">OHEM</td><td style="text-align:center">78.9 (07+12)</td><td style="text-align:center">76.3 (07++12)</td><td style="text-align:center">22.4</td><td style="text-align:center">CVPR’16</td></tr><tr><td style="text-align:center">MPN</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">33.2</td><td style="text-align:center">BMVC’16</td></tr><tr><td style="text-align:center">SSD</td><td style="text-align:center">76.8 (07+12)</td><td style="text-align:center">74.9 (07++12)</td><td style="text-align:center">31.2</td><td style="text-align:center">ECCV’16</td></tr><tr><td style="text-align:center">GBDNet</td><td style="text-align:center">77.2 (07+12)</td><td style="text-align:center">-</td><td style="text-align:center">27.0</td><td style="text-align:center">ECCV’16</td></tr><tr><td style="text-align:center">CPF</td><td style="text-align:center">76.4 (07+12)</td><td style="text-align:center">72.6 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">ECCV’16</td></tr><tr><td style="text-align:center">R-FCN</td><td style="text-align:center">79.5 (07+12)</td><td style="text-align:center">77.6 (07++12)</td><td style="text-align:center">29.9</td><td style="text-align:center">NIPS’16</td></tr><tr><td style="text-align:center">DeepID-Net</td><td style="text-align:center">69.0</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">PAMI’16</td></tr><tr><td style="text-align:center">NoC</td><td style="text-align:center">71.6 (07+12)</td><td style="text-align:center">68.8 (07+12)</td><td style="text-align:center">27.2</td><td style="text-align:center">TPAMI’16</td></tr><tr><td style="text-align:center">DSSD</td><td style="text-align:center">81.5 (07+12)</td><td style="text-align:center">80.0 (07++12)</td><td style="text-align:center">33.2</td><td style="text-align:center">arXiv’17</td></tr><tr><td style="text-align:center">TDM</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">37.3</td><td style="text-align:center">CVPR’17</td></tr><tr><td style="text-align:center">FPN</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">36.2</td><td style="text-align:center">CVPR’17</td></tr><tr><td style="text-align:center">YOLO v2</td><td style="text-align:center">78.6 (07+12)</td><td style="text-align:center">73.4 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">CVPR’17</td></tr><tr><td style="text-align:center">RON</td><td style="text-align:center">77.6 (07+12)</td><td style="text-align:center">75.4 (07++12)</td><td style="text-align:center">27.4</td><td style="text-align:center">CVPR’17</td></tr><tr><td style="text-align:center">DeNet</td><td style="text-align:center">77.1 (07+12)</td><td style="text-align:center">73.9 (07++12)</td><td style="text-align:center">33.8</td><td style="text-align:center">ICCV’17</td></tr><tr><td style="text-align:center">CoupleNet</td><td style="text-align:center">82.7 (07+12)</td><td style="text-align:center">80.4 (07++12)</td><td style="text-align:center">34.4</td><td style="text-align:center">ICCV’17</td></tr><tr><td style="text-align:center">RetinaNet</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">39.1</td><td style="text-align:center">ICCV’17</td></tr><tr><td style="text-align:center">DSOD</td><td style="text-align:center">77.7 (07+12)</td><td style="text-align:center">76.3 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">ICCV’17</td></tr><tr><td style="text-align:center">SMN</td><td style="text-align:center">70.0</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">ICCV’17</td></tr><tr><td style="text-align:center">Light-Head R-CNN</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">41.5</td><td style="text-align:center">arXiv’17</td></tr><tr><td style="text-align:center">YOLO v3</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">33.0</td><td style="text-align:center">arXiv’18</td></tr><tr><td style="text-align:center">SIN</td><td style="text-align:center">76.0 (07+12)</td><td style="text-align:center">73.1 (07++12)</td><td style="text-align:center">23.2</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">STDN</td><td style="text-align:center">80.9 (07+12)</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">RefineDet</td><td style="text-align:center">83.8 (07+12)</td><td style="text-align:center">83.5 (07++12)</td><td style="text-align:center">41.8</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">SNIP</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">45.7</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">Relation-Network</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">32.5</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">Cascade R-CNN</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">42.8</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">MLKP</td><td style="text-align:center">80.6 (07+12)</td><td style="text-align:center">77.2 (07++12)</td><td style="text-align:center">28.6</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">Fitness-NMS</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">41.8</td><td style="text-align:center">CVPR’18</td></tr><tr><td style="text-align:center">RFBNet</td><td style="text-align:center">82.2 (07+12)</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">ECCV’18</td></tr><tr><td style="text-align:center">CornerNet</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">42.1</td><td style="text-align:center">ECCV’18</td></tr><tr><td style="text-align:center">PFPNet</td><td style="text-align:center">84.1 (07+12)</td><td style="text-align:center">83.7 (07++12)</td><td style="text-align:center">39.4</td><td style="text-align:center">ECCV’18</td></tr><tr><td style="text-align:center">Pelee</td><td style="text-align:center">70.9 (07+12)</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">NIPS’18</td></tr><tr><td style="text-align:center">HKRM</td><td style="text-align:center">78.8 (07+12)</td><td style="text-align:center">-</td><td style="text-align:center">37.8</td><td style="text-align:center">NIPS’18</td></tr><tr><td style="text-align:center">M2Det</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">44.2</td><td style="text-align:center">AAAI’19</td></tr><tr><td style="text-align:center">R-DAD</td><td style="text-align:center">81.2 (07++12)</td><td style="text-align:center">82.0 (07++12)</td><td style="text-align:center">43.1</td><td style="text-align:center">AAAI’19</td></tr><tr><td style="text-align:center">ScratchDet</td><td style="text-align:center">84.1 (07++12)</td><td style="text-align:center">83.6 (07++12)</td><td style="text-align:center">39.1</td><td style="text-align:center">CVPR’19</td></tr><tr><td style="text-align:center">Libra R-CNN</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">43.0</td><td style="text-align:center">CVPR’19</td></tr><tr><td style="text-align:center">Reasoning-RCNN</td><td style="text-align:center">82.5 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">43.2</td><td style="text-align:center">CVPR’19</td></tr><tr><td style="text-align:center">FSAF</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">44.6</td><td style="text-align:center">CVPR’19</td></tr><tr><td style="text-align:center">AmoebaNet + NAS-FPN</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">47.0</td><td style="text-align:center">CVPR’19</td></tr><tr><td style="text-align:center">Cascade-RetinaNet</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">41.1</td><td style="text-align:center">CVPR’19</td></tr><tr><td style="text-align:center">TridentNet</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">48.4</td><td style="text-align:center">ICCV’19</td></tr><tr><td style="text-align:center">DAFS</td><td style="text-align:center"><strong>85.3 (07+12)</strong></td><td style="text-align:center">83.1 (07++12)</td><td style="text-align:center">40.5</td><td style="text-align:center">ICCV’19</td></tr><tr><td style="text-align:center">Auto-FPN</td><td style="text-align:center">81.8 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">40.5</td><td style="text-align:center">ICCV’19</td></tr><tr><td style="text-align:center">FCOS</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">44.7</td><td style="text-align:center">ICCV’19</td></tr><tr><td style="text-align:center">FreeAnchor</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">44.8</td><td style="text-align:center">NeurIPS’19</td></tr><tr><td style="text-align:center">DetNAS</td><td style="text-align:center">81.5 (07++12)</td><td style="text-align:center">-</td><td style="text-align:center">42.0</td><td style="text-align:center">NeurIPS’19</td></tr><tr><td style="text-align:center">NATS</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">42.0</td><td style="text-align:center">NeurIPS’19</td></tr><tr><td style="text-align:center">AmoebaNet + NAS-FPN + AA</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">50.7</td><td style="text-align:center">arXiv’19</td></tr><tr><td style="text-align:center">EfficientDet</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center"><strong>51.0</strong></td><td style="text-align:center">arXiv’19</td></tr></tbody></table></div><h3 id="目标检测的候选框-Proposal"><a href="#目标检测的候选框-Proposal" class="headerlink" title="目标检测的候选框(Proposal)"></a>目标检测的候选框(Proposal)</h3><p>物体候选框获取，当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)。</p><p>根据目标候选区域的提取方式不同，传统目标检测算法可以分为基于滑动窗口的目标检测算法和基于选择性搜索的目标检测算法。</p><p><strong>滑动窗口（Sliding Window）</strong></p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/1423648-20190316210029111-840970217.png" alt></p><p>基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类</p><p>缺点：不知道要检测的目标大小是什么规模，所以要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量。</p><p>具体步骤：首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选。最终，经过NMS筛选后获得检测到的物体。</p><p><strong>非极大值抑制(Non-Maximum Suppression,NMS)</strong></p><p>根据分类器类别分类概率做排序</p><p>　　(1)从最大概率矩形框开始，分别得分后面的矩形框与其的重叠度IOU是否大于某个设定的阈值;</p><p>　　(2)假设重叠度超过阈值，那么就扔掉；并标记第一个矩形框，是我们保留下来的。</p><p>　　(3)从剩下的矩形框中，选择概率最大的，然后判断得分后面的矩形框与其的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记是我们保留下来的第二个矩形框。</p><p>　　如此循环往复知道没有剩余的矩形框，然后找到所有被保留下来的矩形框，就是我们认为最可能包含目标的矩形框。</p><p><strong>R-CNN算法中NMS的具体做法</strong>：</p><p>　　假设有20类，2000个建议框，最后输出向量维数2000*20，则每列对应一类，一行是各个建议框的得分，NMS算法步骤如下：<br>　　① 对2000×20维矩阵中每列按从大到小进行排序；<br>　　② 从每列最大的得分建议框开始，分别与该列后面的得分建议框进行IoU计算，若IoU&gt;阈值，则剔除得分较小的建议框，否则认为图像中存在多个同一类物体；<br>　　③ 从每列次大的得分建议框开始，重复步骤②；<br>　　④ 重复步骤③直到遍历完该列所有建议框；<br>　　⑤ 遍历完2000×20维矩阵所有列，即所有物体种类都做一遍非极大值抑制；<br>　　⑥ 最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框。</p><p><strong>选择性搜索(Selective Search)</strong></p><p>滑窗法类似穷举进行图像子区域搜索，但是一般情况下图像中大部分子区域是没有物体的。学者们自然而然想到只对图像中最有可能包含物体的区域进行搜索以此来提高计算效率。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/1423648-20190317100459585-1587811888.png" alt></p><p>选择搜索算法的主要观点：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。因此，选择搜索基于上面这一想法采用子区域合并的方法进行提取候选边界框（bounding boxes）。首先，对输入图像进行分割算法产生许多小的子区域(大约2000个子区域)。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做外切矩形（bounding boxes），这些子区域外切矩形就是通常所说的候选框。</p><p>优点：<br>　　　　（a）计算效率优于滑窗法。<br>　　　　（b）由于采用子区域合并策略，所以可以包含各种大小的疑似物体框。<br>　　　　（c）合并区域相似的指标多样性，提高了检测物体的概率。</p><h3 id="边界框回归-Bounding-Box-regression"><a href="#边界框回归-Bounding-Box-regression" class="headerlink" title="边界框回归(Bounding-Box regression)"></a>边界框回归(Bounding-Box regression)</h3><p>窗口一般使用四维向量$(x,y,w,h)$来表示， 分别表示窗口的中心点坐标和宽高。 对于下图, 红色的框 $P$ 代表原始的Proposal, 绿色的框 $G$ 代表目标的 Ground Truth， 目标是寻找一种关系使得输入原始的窗口 $P$ 经过映射得到一个跟真实窗口 $G$ 更接近的回归窗口$\hat{G}$。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/TIM%E5%9B%BE%E7%89%8720200106182123.png" alt></p><p>边框回归的目的既是：给定$\left(P_{x}, P_{y}, P_{w}, P_{h}\right)$寻找一种映射$f$， 使得$f\left(P_{x}, P_{y}, P_{w}, P_{h}\right)=\left(\hat{G}_{x}, \hat{G}_{y}, \hat{G}_{w}, \hat{G}_{h}\right)$并且$\left(\hat{G}_{x}, \hat{G}_{y}, \hat{G}_{w}, \hat{G}_{h}\right) \approx\left(G_{x}, G_{y}, G_{w}, G_{h}\right)$</p><p>思路: 平移 + 尺度放缩</p><ol><li><p>先做平移$(\Delta x, \Delta y), \quad \Delta x=P_{w} d_{x}(P), \Delta y=P_{h} d_{y}(P)$</p><p>$\hat{G}_{x}=P_{w} d_{x}(P)+P_{x}$<br>$\hat{G}_{y}=P_{h} d_{y}(P)+P_{y}$</p></li><li><p>然后再做尺度缩放$\left(S_{w}, S_{h}\right), S_{w}=\exp \left(d_{w}(P)\right), S_{h}=\exp \left(d_{h}(P)\right)$</p><p>$\hat{G}_{w}=P_{w} \exp \left(d_{w}(P)\right)$<br>$\hat{G}_{h}=P_{h} \exp \left(d_{h}(P)\right)$</p></li></ol><p>边框回归就是学习$d_{x}(P), d_{y}(P), d_{w}(P), d_{h}(P)$这四个变换。</p><p>线性回归就是给定输入的特征向量$X$, 学习一组参数 $W$, 使得经过线性回归后的值跟真实值 $Y$(Ground Truth)非常接近. 即$Y ≈ WX$。</p><p>输入为：$P=\left(P_{x}, P_{y}, P_{w}, P_{h}\right)$这个窗口对应的 CNN 特征(注：训练阶段输入还包括 Ground Truth， 也就是下边提到的$t_{*}=\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$ )</p><p>输出为：四个变换，因为有了这四个变换我们就可以直接得到 Ground Truth，真实值所对应的真正的变化为</p><script type="math/tex;mode=display">\begin{aligned}
&t_{x}=\left(G_{x}-P_{x}\right) / P_{w}\\
&t_{y}=\left(G_{y}-P_{y}\right) / P_{h}\\
&t_{w}=\log \left(G_{w} / P_{w}\right)\\
&t_{h}=\log \left(G_{h} / P_{h}\right)
\end{aligned}</script><p>目标函数可以表示为$d_{<em>}(P)=w_{</em>}^{T} \Phi_{5}(P)$， $\Phi_{5}(P)$是输入 Proposal 的特征向量， $w_{<em>}$是要学习的参数（</em> 表示 $x,y,w,h$， 也就是每一个变换对应一个目标函数）, $d_{*}(P)$ 是得到的预测值。</p><p>损失函数：要让预测值跟真实值$t_{*}=\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$差距最小</p><script type="math/tex;mode=display">Loss=\sum_{i}^{N}\left(t_{*}^{i}-\hat{w}_{*}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}</script><p>函数优化目标为：</p><script type="math/tex;mode=display">W_{*}=\operatorname{argmin}_{w_{*}} \sum_{i}^{N}\left(t_{*}^{i}-\hat{w}_{*}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{w}_{*}\right\|^{2}</script><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>Rich feature hierarchies for accurate object detection and semantic segmentation(CVPR 2014)</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_09-57-14.jpg" alt></p><p><strong>算法流程</strong>：</p><ul><li>使用Selective Search提取大约2000个候选区域（proposal）;</li><li>对每个候选区域的图像进行拉伸形变，使之成为固定大小的正方形图像，并将该图像输入到CNN中提取特征;</li><li>使用线性的SVM对提取的特征进行分类。</li></ul><p><strong>创新点</strong>：</p><ul><li>将大型卷积神经网络(CNNs)应用于自下而上的候选区域以定位和分割物体。</li><li>当带标签的训练数据不足时，先针对辅助任务进行有监督预训练，再进行特定任务的调优，就可以产生明显的性能提升。</li></ul><p><strong>R-CNN图片缩放</strong></p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180425214236957.png" alt></p><p><strong>缺点</strong>：</p><ul><li><p><strong>重复计算</strong> R-CNN虽然不再是穷举，但通过Proposal（Selective Search）的方案依然有两千个左右的候选框，这些候选框都需要单独经过backbone网络提取特征，计算量依然很大，候选框之间会有重叠，因此有不少其实是重复计算。</p></li><li><p><strong>训练测试不简洁</strong> 候选区域提取、特征提取、分类、回归都是分开操作，中间数据还需要单独保存。</p></li><li><p><strong>速度慢</strong> 前面的缺点最终导致R-CNN出奇的慢，GPU上处理一张图片需要十几秒，CPU上则需要更长时间。</p></li><li><p><strong>输入的图片Patch必须强制缩放成固定大小</strong> （原文采用227×227），会造成物体形变，导致检测性能下降。</p></li></ul><h3 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h3><p>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(ECCV 2014)</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-28-09.jpg" alt></p><p>卷积层的参数和输入图像的尺寸无关，它仅仅是一个卷积核在图像上滑动，不管输入图像是多少都没关系，只是对不同大小的图片卷积出不同大小的特征图，池化层对输入图像的尺寸也没有任何限制，只是获得不同的特征图而已，但是全连接层的参数就和输入图像大小有关，因为它要把输入的所有像素点连接起来,需要指定输入层神经元个数和输出层神经元个数，所以需要规定输入的feature的大小。</p><p>通过在卷积层和全连接层之间加入空间金字塔池化结构（Spatial Pyramid Pooling）代替R-CNN算法在输入卷积神经网络前对各个候选区域进行剪裁、缩放操作使其图像子块尺寸一致的做法。</p><p><strong>算法流程</strong>：</p><ul><li>首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。</li><li>特征提取阶段。这一步就是和R-CNN最大的区别了，同样是用卷积神经网络进行特征提取，但是SPP-Net用的是金字塔池化。这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度更快啊。</li><li>最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。</li></ul><p><strong>一次特征提取</strong><br>RCNN是多个regions+多次CNN+单个pooling，而SPP则是单个图像+单次CNN+多个region+多个pooling<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180425215129861.png" alt></p><p><strong>金字塔池化结构</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-41-12.jpg" alt><br>利用不同大小的刻度，对一张图片进行了划分。图中，利用了三种不同大小的刻度，对一张输入的图片进行了划分，最后总共可以得到16+4+1=21个块，从这21个图片块中，分别计算每个块的最大值，从而得到一个输出特征向量。最后把一张任意大小的图片转换成了一个固定大小的21维特征（当然可以设计其它维数的输出，增加金字塔的层数，或者改变划分网格的大小）。上面的三种不同刻度的划分，每一种刻度称之为金字塔的一层，使用多个不同刻度的层，可以提高所提取特征的鲁棒性。每一个图片块大小称之为：Sliding Windows Size。如果希望金字塔的某一层输出nxn个特征，那么就要用Windows Size大小为：(w/n,h/n)进行池化。（这里有一个问题，就是如果(5x5）也要得到3x3的话，计算得到的size=2，stride=1，利用公式算出来得到的池化为（4x4）与预期的3x3不符，这里暂时还有问题，不清楚具体原因是什么）。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_10-46-02.jpg" alt></p><p><strong>创新点：</strong></p><ul><li>利用空间金字塔池化结构；</li><li>对整张图片只进行了一次特征提取，加快运算速度。</li></ul><p><strong>缺点</strong><br>　 和R-CNN一样，它的训练要经过多个阶段，特征也要存在磁盘中，另外，SPP中的微调只更新SPP层后面的全连接层，对很深的网络这样肯定是不行的。</p><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>Girshick, R., Fast R-CNN, in 2015 IEEE International Conference on Computer Vision (ICCV). 2015. p. 1440-1448.</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-7569280b566d0e58.png" alt></p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-5519b489e581557a.png" alt></p><p>Fast R-CNN在特征提取上可以说很大程度借鉴了SPPnet，首先将图片用选择搜索算法（selective search）得到2000个候选区域（region proposals）的坐标信息。另一方面，直接将图片归一化到CNN需要的格式，整张图片送入CNN（本文选择的网络是VGG），将第五层的普通池化层替换为RoI池化层，图片然后经过5层卷积操作后，得到一张特征图（feature maps），开始得到的坐标信息通过一定的映射关系转换为对应特征图的坐标，截取对应的候选区域，经过RoI层后提取到固定长度的特征向量，送入全连接层。</p><p><strong>算法流程</strong>:</p><ul><li>输入的是一张完整图片图像归一化为224×224和一组（约2000个）物体建议框（也叫RoIs）送入网络。</li><li>对Conv feature map进行特征提取。每一个区域经过RoI pooling layer和FC layers得到一个固定长度的feature vector，这里需要注意的是，输入到后面RoI pooling layer的feature map是在Conv feature map上提取的。虽然在最开始也提取出了大量的RoI，但他们还是作为整体输入进卷积网络的，最开始提取出的RoI区域只是为了最后的Bounding box 回归时使用，用来输出原图中的位置。</li><li>这些特征向量在经过全接连层之后进入两个并列的输出层。第一个是分类，使用softmax，第二个是每一类的bounding box回归。利用SoftMax Loss和Smooth L1 Loss对分类概率和边框回归（Bounding Box Regression）联合训练。<br>整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。</li></ul><p><strong>什么是ROI呢？</strong></p><p>RoI是Region of Interest的简写，指的是在“<strong>特征图上的框</strong>”；在Fast RCNN中， RoI是指Selective Search完成后得到的“候选框”在特征图上的映射;</p><p><strong>ROI Pooling的输入</strong><br>输入有两部分组成：</p><ul><li>特征图：指的是特征图，在Fast RCNN中，它位于RoI Pooling之前，通常我们常常称之为“share_conv”；</li><li>ROIS ：在Fast RCNN中，指的是Selective Search的输出，一堆矩形候选框框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）。</li></ul><p><strong>ROI Pooling的输出</strong><br>　　输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel x w x h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w x h）的矩形框；</p><p><strong>ROI Pooling的过程</strong><br>　　先把RoI中的坐标映射到feature map上，映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标后，使用Pooling得到输出；由于输入的图片大小不一，所以这里使用的类似Spp Pooling，在Pooling的过程中需要计算Pooling后的结果对应到feature map上所占的范围，然后在那个范围中进行取max或者取average。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/3940902-a6127730f6d21abd.png" alt></p><ul><li>cls_score层用于分类，输出K+1维数组p，表示属于K类和背景的概率。</li><li>bbox_prdict层用于调整候选区域位置，输出4*K维数组t，表示分别属于K类时，应该平移缩放的参数。</li></ul><p><strong>最后一个阶段的特征输入到两个并行的全连层中。</strong><br>一个是对区域的分类Softmax（包括背景），另一个是对bounding box回归的微调。在SVM和Softmax的对比实验中说明，SVM的优势并不明显，故直接用Softmax将整个网络整合训练更好。总代价为两者加权和，Fast-RCNN把两个回归的loss进行联合训练。</p><p><strong>创新点</strong></p><ul><li>提出RoI Pooling 层，它将不同大小候选框的卷积特征图统一采样成固定大小的特征。</li><li>独立的SVM分类器和回归器需要大量特征作为训练样本，需要大量的硬盘空间，Fast-RCNN把类别判断和位置回归统一用深度神经网络实现，不再需要额外存储。</li></ul><p><strong>缺点</strong>：<br>检测速度仍然受限于 Selective Search</p><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>Ren, S., et al., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Trans Pattern Anal Mach Intell, 2017. 39(6): p. 1137-1149.</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-e64a99b38f411c337f538eb5f093bdf3_r.jpg" alt></p><p><strong>算法流程</strong>:</p><ul><li>把整张图片送入CNN，进行特征提取；</li><li>在最后一层卷积feature map上生成region proposal（通过RPN），每张图片大约300个建议窗口；</li><li>通过RoI pooling层（其实是单层的SPP layer）使得每个建议窗口生成固定大小的feature map；</li><li>继续经过两个全连接层（FC）得到特征向量。特征向量经由各自的FC层，得到两个输出向量。第一个是分类，使用softmax，第二个是每一类的bounding box回归。利用SoftMax Loss和Smooth L1 Loss对分类概率和边框回归（Bounding Box Regression）联合训练。</li></ul><p><strong>Region Proposal Networks</strong>。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-1908feeaba591d28bee3c4a754cca282_r.jpg" alt></p><p>可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。</p><p>所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：<br></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure><br>其中每行的4个值$\left(x_{1}, y_{1}, x_{2}, y_{2}\right)$表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为width: height $\in\{1: 1,1: 2,2: 1\}$三种，如图。实际上通过anchors就引入了检测中常用到的多尺度方法。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-7abead97efcc46a3ee5b030a2151643f_hd.jpg" alt><p></p><p>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p><p><strong>那么这9个anchors是做什么的呢？</strong>遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。</p><p><strong>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</strong></p><p>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-1ab4b6c3dd607a5035b5203c76b078f3_r.jpg" alt></p><p>可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9<em>2)大小的矩阵。<em>*为何这样做？</em></em>后面接softmax分类获得positive anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在positive anchors中）。</p><p><strong>那么为何要在softmax前后都接一个reshape layer？</strong>其实只是为了便于softmax分类,存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行positive/negative二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。</p><p>RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，原理类似）。</p><p><strong>对proposals进行bounding box regression</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/v2-8241c8076d60156248916fe2f1a5674a_r.jpg" alt></p><p>可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$\left[d_{x}(A), d_{y}(A), d_{w}(A), d_{h}(A)\right]$变化量</p><p><strong>创新点</strong></p><ul><li>采用RPN(Region Proposal Network)代替选择性搜索(Selective Search)，利用GPU进行计算大幅度缩减提取region proposal的速度。</li><li>产生建议窗口的CNN和目标检测的CNN共享。</li></ul><p>在主干网络中增加了RPN （Region Proposal Network）网络，通过一定规则设置不同尺度的锚点（Anchor）在RPN的卷积特征层提取候选框来代替Selective Search等传统的候选框生成方法，实现了网络的端到端训练。候选区域生成、候选区域特征提取、框回归和分类全过程一气呵成，在训练过程中模型各部分不仅学习如何完成自己的任务，还自主学习如何相互配合。这也是第一个真正意义上的深度学习目标检测算法。</p><h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>Redmon, J., et al., You Only Look Once: Unified, Real-Time Object Detection, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. p. 779-788.</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-d54136f1eb0cd733.png" alt></p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-0d1e8d42480e1c1a.png" alt></p><p><strong>算法流程</strong></p><ul><li>将图像resize到448 * 448作为神经网络的输入 ；</li><li>运行神经网络，得到一些bounding box坐标、box中包含物体的置信度和class probabilities ；</li><li>进行非极大值抑制，筛选Boxes。</li></ul><p><strong>创新点</strong></p><ul><li>YOLO将目标检测问题转化为回归问题，不需要复杂的流程。在实时检测系统中， YOLO的效果是最好的。</li><li>YOLO 在做出预测时是推理整个图像的。与滑动窗口和候选区域算法不同， YOLO 在训练和测试时，从整个图像综合考虑，不仅分析物体的 appearance 还分析其 contextual 信息。Fast R-CNN 比较容易将背景误检测为物体，因为它不考虑 contextual 信息。YOLO 把背景误检测为物体的概率不到 Fast R-CNN 的一半。</li><li>YOLO 对物体的泛化能力比较好。当在自然图像上训练，在艺术图像上检测时，YOLO的效果要比 DPM 和 R-CNN 好很多。</li></ul><p>YOLO没有选择滑动窗口（silding window）或提取proposal的方式训练网络，而是直接选用整图训练模型，将 Object Detection 的问题转化成一个 Regression 问题。</p><p>将图片分为$S\times S$个单元格(原文中S=7)，之后的输出是以单元格为单位进行的：</p><ul><li>如果一个object的中心落在某个单元格上，那么这个单元格负责预测这个物体。</li><li>每个单元格需要预测B个bbox值(bbox值包括坐标和宽高，原文中B=2)，同时为每个bbox值预测一个置信度(confidence scores)。也就是每个单元格需要预测B×(4+1)个值。</li><li>每个单元格需要预测C(物体种类个数，原文C=20，这个与使用的数据库有关)个条件概率值.</li></ul><p>所以，最后网络的输出维度为$S \times S \times (B\times 5 + C)$，这里输出为$7\times7\times(2\times5+20)$, 虽然每个单元格负责预测一种物体(这也是这篇文章的问题，当有小物体时可能会有问题)，但是每个单元格可以预测多个bbox值(这里可以认为有多个不同形状的bbox，为了更准确的定位出物体，如下图所示)。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/6983308-29a33d2e79ed722b.png" alt></p><p><strong>(x,y)是bbox的中心相对于单元格的offset</strong><br>单元格坐标为$(x_{col},y_{row})$，假设它预测的输出的bbox中心坐标为$(x_c,y_c)$,那么最终预测出来的(x,y)是经过归一化处理的，表示的是中心相对于单元格的offset</p><p><strong>(w,h)是bbox相对于整个图片的比例</strong><br>预测的bbox的宽高为$w_b,h_b$，(w,h)表示的是bbox的是相对于整张图片的占比</p><p><strong>confidence</strong><br>这个置信度是由两部分组成，一是格子内是否有目标，二是bbox的准确度。定义置信度为$P_r(Object)∗IOU^{truth}_{pred}$<br>这里，如果格子内有物体，则$P_r(Object)=1$，此时置信度等于IoU。如果格子内没有物体，则$P_r(Object)=0$，此时置信度为0</p><p><strong>C类的条件概率</strong><br>条件概率定义为$P_r(Class_i |Object)$，表示该单元格存在物体且属于第i类的概率。</p><p>在测试的时候每个单元格预测最终输出的概率定义为，如下两图所示（两幅图不一样，代表一个框会输出B列概率值）<br>$P_r(Class_i|Object) ∗ P_r(Object) ∗ IOU^{truth}_{pred} = P_r(Class_i) ∗ IOU^{truth}_{pred}$</p><p>最后将$(S\times S)\times B\times20$ 列的结果送入NMS，最后即可得到最终的输出框结果</p><p><strong>损失函数</strong></p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20180511204751564.png" alt></p><p>这里强调两点：<br>1.每个图片的每个单元格不一定都包含object，如果没有object，那么confidence就会变成0，这样在优化模型的时候可能会让梯度跨越太大，模型不稳定跑飞了。为了平衡这一点，在损失函数中，设置两个参数$λ_{corrd}$和$λ_{noobj}$，其中$λ_{corrd}$控制bbox预测位置的损失，$λ_{noobj}$控制单个格内没有目标的损失。<br>2.对于大的物体，小的偏差对于小的物体影响较大，为了减少这个影响，所以对bbox的宽高都开根号。</p><p><strong>缺点</strong></p><ul><li>YOLO的物体检测精度低于其他state-of-the-art的物体检测系统。</li><li>YOLO容易产生物体的定位错误。</li><li>YOLO对小物体的检测效果不好（尤其是密集的小物体，因为一个栅格只能预测2个物体）。</li></ul><h3 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h3><p>Redmon, J. and A. Farhadi, YOLO9000: Better, Faster, Stronger. (CVPR 2017)</p><p><strong>改进</strong>：</p><ul><li><p><strong>Batch Normalization</strong></p><p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p></li><li><p><strong>High Resolution Classifier</strong></p><p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为$224 \times 224$的图片作为输入，分辨率相对较低，不利于检测模型。所以YOLOv1在采用$224 \times 224$分类模型预训练后，将分辨率增加至$448 \times 448$，并使用这个高分辨率在检测数据集上fine tune。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。所以YOLOv2增加了在ImageNet数据集上使用$448 \times 448$来fine tune分类网络这一中间过程（10 epochs），这可以使得模型在检测数据集上fine tune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p></li><li><p><strong>Convolutionlal With Anchor Boxes</strong></p><p>anchor是RNP网络中的一个关键步骤，说的是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的建议框。</p><p><strong>YOLO v1： S*S* (B*5 + C) =&gt; 7*7（2*5+20）</strong><br>　　 其中B对应Box数量，5对应 Rect 定位+置信度。每个Grid只能预测对应两个Box，这两个Box共用一个分类结果（20 classes），这是很不合理的临时方案。</p><p><strong>YOLO v2： S*S*K* (5 + C) =&gt; 13*13*9（5+20）</strong><br>　　分辨率改成了13*13，更细的格子划分对小目标适应更好，再加上与Faster一样的K=9，计算量增加了不少。通过Anchor Box改进，mAP由69.5下降到69.2，Recall由81%提升到了88%。<br>　　<br>　　为了引入anchor boxes来预测bounding boxes，作者在网络中果断去掉了全连接层。首先，作者去掉了后面的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416x416，这一步的目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。作者观察到，大物体通常占据了图像的中间位置， 就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（factor为32），使得输入卷积网络的416x416图片最终得到13x13的卷积特征图（416/32=13）。<br>　　加入了anchor boxes后，可以预料到的结果是召回率上升，准确率下降。我们来计算一下，假设每个cell预测9个建议框，那么总共会预测13x13x9 = 1521个boxes，而之前的网络仅仅预测7x7x2 = 98个boxes。具体数据为：没有anchor boxes，模型recall为81%，mAP为69.5%；加入anchor boxes，模型recall为88%，mAP为69.2%。这样看来，准确率只有小幅度的下降，而召回率则提升了7%，说明可以通过进一步的工作来加强准确率，的确有改进空间。</p></li><li><p><strong>New Network——Darknet-19</strong></p><p>YOLOv2使用了一个新的分类网络作为特征提取部分，参考了前人的先进经验，比如类似于VGG，作者使用了较多的3x3卷积核，在每一次池化操作后把通道数翻倍。借鉴了network in network的思想，网络使用了全局平均池化（global average pooling），把1x1的卷积核置于3x3的卷积核之间，用来压缩特征。也用了batch normalization稳定模型训练。模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_18-40-15.jpg" alt></p></li></ul><ul><li><p><strong>Dimension Clusters</strong></p><p>在Faster R-CNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： $d(\text { box }, \text { centroid })=1-\mathrm{IOU}(\text { box }, \text { centroid })$</p></li><li><p><strong>Direct location prediction</strong></p><script type="math/tex;mode=display">x=\left(t_{x} * w_{a}\right)-x_{a}</script><script type="math/tex;mode=display">y=\left(t_{y} * h_{a}\right)-y_{a}</script><p>这个公式的理解为：当预测tx=1，就会把box向右边移动一定距离（具体为anchor box的宽度），预测tx=-1，就会把box向左边移动相同的距离，该公式没有任何约束，中心点可能会出现在图像任何位置，这就有可能导致回归过程震荡，甚至无法收敛。</p><p><strong>强约束方法</strong>：</p><ul><li>对应 Cell 距离左上角的边距为$\left(C_{x}, C_{y}\right)$，σ定义为sigmoid激活函数，将函数值约束到［0，1］，用来预测相对于该Cell中心的偏移（不会偏离cell）；</li><li>预定Anchor（文中描述为bounding box prior）对应的宽高为$\left(P_{w}, P_{h}\right)$，预测 Location 是相对于Anchor的宽高乘以系数得到；</li></ul></li></ul><p>如果这个cell距离图像左上角的边距为$\left(C_{x}, C_{y}\right)$以及该cell对应的box维度（bounding box prior）的长和宽分别为$\left(P_{w}, P_{h}\right)$，那么预测值可以表示为：</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_18-53-58.jpg" alt></p><ul><li><p><strong>Fine-Grained Features</strong><br>YOLO最终在13x13的特征图上进行预测，虽然这足以胜任大尺度物体的检测，但对于小物体还需要更精细的特征图（Fine-Grained Features），前面更精细的特征图可以用来预测小物体。<br>YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26x26大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为26x26x512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2x2的局部区域，然后将其转化为channel维度，对于26x26x512的特征图，经passthrough层处理之后就变成了13x13x2048的新特征图（特征图大小降低4倍，而channles增加4倍），这样就可以与后面的13x13x1024特征图连接在一起形成13x13x3072的特征图，然后在此特征图基础上卷积做预测。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-07_19-14-51.jpg" alt></p></li></ul><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/20161229150249883.jpg" alt></p><ul><li><p><strong>Multi-Scale Training</strong><br>　　为了让 YOLOv2 适应不同Scale下的检测任务，作者尝试通过不同分辨率图片的训练来提高网络的适应性。(网络只用到了卷积层和池化层，可以进行动态调整（检测任意大小图片）)<br>具体做法是：<br>　　 每经过10次训练（10 epoch），就会随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度池化{320,352，…，608}。最终最小的尺寸为320 <em>320，最大的尺寸为608</em> 608。接着按照输入尺寸调整网络进行训练。</p><p>​ 这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。</p></li><li><p><strong>交叉数据训练</strong></p><p>YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p><p>作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_09-22-20.jpg" alt><br>分类时的概率计算借用了决策树思想，某个节点的概率值等于 该节点到根节点的所有条件概率之积。<br>如果想求得特定节点的绝对概率，只需要沿着路径做连续乘积。例如 如果想知道一张图片是不是“Norfolk terrier ”需要计算：</p><p>$\operatorname{Pr}(\text { Norfolk terrier })=\operatorname{Pr}(\text { Norfolk terrier } | \text { terrier })$<br>*$\operatorname{Pr}(\text { terrier } | \text { hunting dog })$</p><p><em>. . .</em></p><p><em>$\operatorname{Pr}(\text { mammal } |\text { animal })$</em> $\operatorname{Pr}(\text { animal } | \text { physical object })$<br>softmax操作也同时应该采用分组操作，基于所有“同义词集”计算softmax，其中“同义词集”是同一概念的下位词。</p></li></ul><h3 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h3><p>Redmon, J. and A. Farhadi, YOLOv3: An Incremental Improvement. 2018.</p><ul><li><p><strong>新的网络结构Darknet-53</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-e65c08c61bfaa7c7.png" alt></p><p>借鉴了残差网络residual network的做法，在一些层之间设置了快捷链路（shortcut connections）。每个残差组 件有两个卷积层和一个快捷链路。</p></li><li><p><strong>利用多尺度特征进行对象检测</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-bc5def91d05e4d3a.png" alt><br>YOLO2曾采用passthrough结构来检测细粒度特征，在YOLO3更进一步采用了3个不同尺度的特征图来进行对象检测。</p><p>结合上图看，卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416x416的话，这里的特征图就是13x13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。</p><p>为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。</p><p>最后，第91层特征图再次上采样，并与第36层特征图融合（Concatenation），最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-9b28d0f1c682b80a.png" alt></p><p>对于一个416x416的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13x13x3 + 26x26x3 + 52x52x3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。</p><p>对比一下，YOLO2采用13x13x5 = 845个预测，YOLO3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。</p></li><li><p><strong>9种尺度的先验框</strong><br>随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。</p><p>分配上，在最小的13x13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26x26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52x52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2709767-5cc00a60004e0473.png" alt></p></li><li><p><strong>对象分类softmax改成logistic</strong><br>YOLOv3 不使用 Softmax 对每个框进行分类，主要考虑因素有两个：</p><ul><li>Softmax 使得每个框分配一个类别（得分最高的一个），而对于 Open Images这种数据集，目标可能有重叠的类别标签，因此 Softmax不适用于多标签分类。</li><li>Softmax 可被独立的多个 logistic 分类器替代，且准确率不会下降。<br>所以预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象。</li></ul></li></ul><h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p>Liu, W., et al., SSD: Single Shot MultiBox Detector. (ECCV 2016)</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_14-51-11.jpg" alt></p><p><strong>创新点</strong></p><ul><li><p><strong>在多层多尺度特征图上进行检测</strong></p><p>低层特征图具有细节信息，看得比较细，而高层特征图中具有高级语义信息，看得比较广，SSD提出同时利用低层特征图和高层特征图进行检测。SSD为了避免利用太低层的特征，从VGG后面开始，又往后添加了4层卷积层，如此就得到了多层次的特征图。这6层特征图的大小分别为：38x38, 19x19, 10x10, 5x5, 3x3, 1x1</p></li><li><p><strong>default boxes</strong></p><p>two-stage 方法太慢，计算代价大。SSD中避免使用region proposals,而采用default boxes。SSD借鉴RPN网络中的anchor box概念。首先将feature map划分为小格子叫做feature map cell，再在每个cell中设置一系列不同长宽比的default box。将其应用于不同分辨率的特征图中。在多个特征图中使用不同的默认框形状，可以有效地离散可能的输出框形状空间。</p></li><li><p><strong>用卷积层来预</strong><br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2148039-cec47c5d71ce8f22.png" alt>3x3的卷积在特征图上进行操作，在特征图的每个点提取特征，然后对每个default box生成四个偏移量来表示生成的bounding box，这四个偏移量分别表示生成框中心x坐标、y坐标、宽度、高度对应于当前default box的偏移量，这里的计算方法采用了RPN中的方法。同时，针对每个bounding box, 卷积会进行分类操作，输出C个分类数值，注意这里的C包括背景类。因此对于每个default box有C+4个输出值，而K个default box就有K(C+4)个输出值，因此，总共需要K*(C+4)个3x3卷积。</p><p>总的来说就是采用default box的方式，在多层多尺度的特征图上使用卷积进行检测（分类+回归）。</p></li><li><p><strong>数据增广</strong><br>为了使得模型对于不同大小、不同形状的物体具有鲁棒性，因此采用了data augmentation。这篇论文在训练时，首先对一张图片进行random crop，然后将其resize的原图尺度，然后以0.5的概率进行水平翻转、同时添加一些色彩变换。</p></li><li><p><strong>加入atrous</strong><br>将 VGG 中的 FC6 layer、FC7 layer 转成为 卷积层，并从模型的 FC6、FC7 上的参数，进行采样得到这两个卷积层的 parameters。还将 Pool5 layer 的参数，从2×2−s2转变成 3×3−s4，外加一个 pad（1）（猜想是不想reduce特征图大小），为了配合这种变化，采用了一种Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小。<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_15-32-00.jpg" alt><br>带孔卷积并不是卷积核里带孔，而是在卷积的时候，跳着的去卷积map（比如dilated＝2的孔卷积，就是隔一个像素点，“卷”一下，这就相当于把卷积核给放大了（3x3的核变成7x7的核，多出位置的weights给0就是。）这样就使得3x3的卷积核也能达到7x7卷积核的感受野</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/2018051716490729.png" alt></p></li></ul><p><strong>缺点</strong><br>SSD存在的缺点：</p><ul><li>需要手动设置参数prior box，无法通过训练得到，依赖经验。</li><li>存在着对小目标检测效果不好的现象。</li></ul><h3 id="DSSD"><a href="#DSSD" class="headerlink" title="DSSD"></a>DSSD</h3><p>Fu, C., et al., DSSD : Deconvolutional Single Shot Detector. (CVPR 2017)</p><p>DSSD针对小目标鲁棒性太差，提出了以下两个贡献：</p><ul><li>把SSD的基准网络从VGG换成了Resnet-101，增强了特征提取能力；</li><li>使用反卷积层（deconvolution layer ）增加了大量上下文信息。</li></ul><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-00-32.jpg" alt></p><p>换完base network之后，作者通过实验发现，准确率从77.5%降至了76.4%，说明只换网络并不能够提高准确率。如果能够提高每一个子网络的准确率，那整个网络的准确率也会得到提升。所以作者在每次预测之前加入了一个“预测模块”（prediction module）：<br><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-11-49.jpg" alt><br>如上图所示：<br>(a)为原SSD对于特征信息进行分类与定位的模型，其实并没有预测模块，直接进行预测；<br>(b)为作者在预测之前加入了residual block模块；<br>(c)将residual block模块中直接映射identity mapping换成了1x1卷积<br>(d)堆积residual block模块</p><p>经过实验证明，（c）能够获得更好的准确率。</p><p>第二个贡献就是添加了反卷积模块，引入了空间上下文信息，从而大大提高了检测准确率。添加了反卷积之后，整个网络形成不对称的沙漏结构。</p><p>蓝色块为卷积层，红色块为反卷积层。红色块做反卷积操作，然后与同大小的卷积层融合，之后再进行物体检测。其中的“反卷积模块”（Deconvolution module）</p><p><img src="https://blog-1300912400.cos.ap-shanghai.myqcloud.com/object%20detection/Snipaste_2020-01-08_16-17-17.jpg" alt></p><p>其中Eltw Product是元素点积操作。作者也尝试了元素求和，但是点积操作的准确率稍高一些。</p><h3 id="目标检测进展"><a href="#目标检测进展" class="headerlink" title="目标检测进展"></a>目标检测进展</h3><ul><li><p>用更好的引擎检测</p><p>AlexNet<br>VGG： 16—19层，使用 3×3 卷积核取代 5×5 和 7×7。<br>GoogLeNet： 即，Inception 网络家族，增加卷积神经网络深度和宽度。<br>ResNet<br>DenseNet： 受 short cut 连接的影响，作者将每一层以前馈的方式和其他所有层相连。<br>SENet： 主要贡献是将全局池化与 shuffling 结合，学习特征图通道的重要性。</p></li><li><p>用更好的特征检测<br>研究者在最新检测引擎的基础上，努力提高图像特征质量，最重要的两组方法是：</p><ul><li>特征融合<br>-自底向上融合：通过跳跃连接将浅层特征传递到深层<br>-自顶向下融合：将深层特征反馈给浅层<br>特征融合可以被看做是不同特征图间的元素运算。主要有几组方法：<br>元素求和<br>元素乘积<br>串联<br>元素乘积的优点是可以抑制或者强调某一区域内特征，可能有利于小目标检测。特征串联的优点是可以整合不同区域的上下文特征，缺点是增加内存消耗。</li><li>学习具有大感受野的高分辨率特征<br>小感受野更关注局部细节。特征图分辨率越小，越难检测小目标。增加特征分辨率最直接的方法是去除池化层，或者减小卷积下采样率。但是这样同时会减小感受野。<br>同时增加感受野和分辨率的方法是引入空洞卷积。</li></ul></li><li>定位提升<br>主要有两种方法：<br>边界框精炼<br>设计新的损失函数</li><li>对旋转和尺度变化鲁棒的检测<br>数据增强<br>为不同方位训练独立检测器</li></ul><h3 id="未来的研究"><a href="#未来的研究" class="headerlink" title="未来的研究"></a>未来的研究</h3><ul><li>轻量级目标检测：加速检测算法，使其能够在移动设备上平稳运行。</li><li>检测与AutoML：未来的一个方向是使用神经结构搜索，减少设计检测模型时的人为干预(例如，如何设计引擎，如何设置Anchor)</li><li>Proposal的Anchor生成方式</li><li>弱监督检测</li><li>小目标检测</li><li>视频中的检测（实时）</li><li>信息融合检测</li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">Qiyuan-Z</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://qiyuan-z.github.io/2020/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/">https://qiyuan-z.github.io/2020/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://Qiyuan-Z.github.io" target="_blank">Yuan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></div><div class="post_share"><div class="social-share" data-image="/img/No_Cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/01/16/%E4%BD%BF-Onedrive-%E5%90%8C%E6%AD%A5%E4%BB%BB%E6%84%8F%E6%96%87%E4%BB%B6%E5%A4%B9/"><img class="prev-cover" src="/img/No_Cover.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">使 Onedrive 同步任意文件夹</div></div></a></div><div class="next-post pull-right"><a href="/2019/12/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90%EF%BC%88%E5%88%98%E4%B8%81%E9%85%89%EF%BC%89%E8%AF%BE%E5%90%8E%E7%AD%94%E6%A1%88%E5%8F%8A%E5%8E%86%E5%B9%B4%E8%AF%95%E5%8D%B7/"><img class="next-cover" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/liudingjiu.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">矩阵分析（刘丁酉）课后答案及历年试卷</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/02/18/YOLOv3模型构建中的YOLOLayer/" title="YOLOv3模型构建中的YOLOLayer"><img class="cover" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-18</div><div class="title">YOLOv3模型构建中的YOLOLayer</div></div></a></div><div><a href="/2020/02/18/YOLOv3网络模型的构建/" title="YOLOv3网络模型的构建"><img class="cover" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-18</div><div class="title">YOLOv3网络模型的构建</div></div></a></div><div><a href="/2020/02/18/在YOLOv3模型中添加Attention机制/" title="在YOLOv3模型中添加Attention机制"><img class="cover" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-18</div><div class="title">在YOLOv3模型中添加Attention机制</div></div></a></div><div><a href="/2020/02/15/Pytorch版YOLOv3中的代码配置和数据集构建/" title="Pytorch版YOLOv3中的代码配置和数据集构建"><img class="cover" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-15</div><div class="title">Pytorch版YOLOv3中的代码配置和数据集构建</div></div></a></div><div><a href="/2020/02/15/YOLOv3中的参数进化/" title="YOLOv3中的参数进化"><img class="cover" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-15</div><div class="title">YOLOv3中的参数进化</div></div></a></div><div><a href="/2020/02/15/YOLOv3的数据加载机制和增强方法/" title="YOLOv3的数据加载机制和增强方法"><img class="cover" src="https://gitee.com/qiyuan-z/yuan-blog-image/raw/master/img/paper.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-15</div><div class="title">YOLOv3的数据加载机制和增强方法</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.</span> <span class="toc-text">目标检测的任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%80%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">2.</span> <span class="toc-text">所面临的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">4.</span> <span class="toc-text">评价指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">深度学习目标检测方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%84%E6%96%B9%E6%B3%95%E5%9C%A8%E5%90%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E7%B2%BE%E5%BA%A6"><span class="toc-number">6.</span> <span class="toc-text">各方法在各数据集上的精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E5%80%99%E9%80%89%E6%A1%86-Proposal"><span class="toc-number">7.</span> <span class="toc-text">目标检测的候选框(Proposal)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%B9%E7%95%8C%E6%A1%86%E5%9B%9E%E5%BD%92-Bounding-Box-regression"><span class="toc-number">8.</span> <span class="toc-text">边界框回归(Bounding-Box regression)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#R-CNN"><span class="toc-number">9.</span> <span class="toc-text">R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SPP-Net"><span class="toc-number">10.</span> <span class="toc-text">SPP-Net</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fast-R-CNN"><span class="toc-number">11.</span> <span class="toc-text">Fast R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Faster-R-CNN"><span class="toc-number">12.</span> <span class="toc-text">Faster R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLO"><span class="toc-number">13.</span> <span class="toc-text">YOLO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLO-v2"><span class="toc-number">14.</span> <span class="toc-text">YOLO v2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLO-v3"><span class="toc-number">15.</span> <span class="toc-text">YOLO v3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SSD"><span class="toc-number">16.</span> <span class="toc-text">SSD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DSSD"><span class="toc-number">17.</span> <span class="toc-text">DSSD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%BF%9B%E5%B1%95"><span class="toc-number">18.</span> <span class="toc-text">目标检测进展</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%E7%9A%84%E7%A0%94%E7%A9%B6"><span class="toc-number">19.</span> <span class="toc-text">未来的研究</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022<i id="heartbeat" class="fa fas fa-heartbeat"></i> Qiyuan-Z</div><div class="footer_custom_text"><p><a style="margin-inline:5px" target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"></a><a style="margin-inline:5px" target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"></a><a style="margin-inline:5px" target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用 Jsdelivr 为静态资源提供CDN加速" alt="Jsdelivr"></a><a style="margin-inline:5px" target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"></a><a style="margin-inline:5px" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a><br>昨日までの私は、もうどこにもいない<br></p></div></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd"}},tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""],addClass:[200,()=>{document.querySelectorAll("mjx-container:not([display='true']").forEach(t=>{const e=t.parentNode;e.classList.contains("has-jax")||e.classList.add("mathjax-overflow")})},"",!1]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>function addGitalkSource(){const e=document.createElement("link");e.rel="stylesheet",e.href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css",document.getElementsByTagName("head")[0].appendChild(e)}function loadGitalk(){function e(){new Gitalk({clientID:"2d10cfb27783db577e70",clientSecret:"154292876bb14966f6ae57304b67859617b08c94",repo:"gitalk",owner:"Qiyuan-Z",admin:["Qiyuan-Z"],id:"bbddefb879fec32a5eebce9ae82ffe8c",language:"zh-CN",perPage:10,distractionFreeMode:!1,pagerDirection:"last",createIssueManually:!1,updateCountCallback:commentCount}).render("gitalk-container")}"function"==typeof Gitalk?e():(addGitalkSource(),getScript("https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js").then(e))}function commentCount(e){let t=document.querySelector("#post-meta .gitalk-comment-count");t&&(t.innerHTML=e)}{function loadOtherComment(){loadGitalk()}loadGitalk()}</script></div><script defer src="https://cdn.jsdelivr.net/gh/Qiyuan-Z/live2d-widget/autoload.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax>function history_calendar_injector_config(){var i=document.getElementsByClassName("sticky_layout")[0];console.log("已挂载history_calendar"),i.insertAdjacentHTML("afterbegin",'<div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height: 100px;overflow: hidden"><div class="history_swiper-container" id="history-container" style="width: 100%;height: 100%"><div class="swiper-wrapper" id="history_container_wrapper" style="height:20px"></div></div></div></div>')}document.getElementsByClassName("sticky_layout")[0]&&(location.pathname,1)&&history_calendar_injector_config()</script><script data-pjax src="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.js"></script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script></body></html>